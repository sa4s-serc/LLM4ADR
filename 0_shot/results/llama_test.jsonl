{"File Name":"cdh-adrs\/0002-cdh-etl-logic-boundaries.md","Context":"## Context\\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\\nrequirements during onboarding and processing of feed files, the following issues where considered:\\n- Will such logic require specialized implementation across feeds?\\n- Are such logic generic and require one time implementation or will require continous change\/update?\\n- What are the benefits of moving such logic into ETL instead of CDH.\\n","Decision":"The most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n","tokens":92,"id":8,"Prediction":"## Decision\n\n**Title:** Centralized Feed Processing Logic in ETL vs. CDH\n\n**Context:** To ensure a clear separation of concerns between CDH and ETL related services, we need to decide where to place feed-specific logic during onboarding and processing of feed files.\n\n**Decision:** We will move feed-specific logic related to onboarding and processing of feed files into ETL services.\n\n**Rationale:**\n\n* **Specialized implementation:** Feed-specific logic may require specialized implementation across feeds, which can be easily managed within ETL services.\n* **Generic and continuous updates:** The logic is likely to be generic and require continuous updates, which can be handled more efficiently within ETL services.\n* **Benefits:** Moving feed-specific logic into ETL services provides a clear separation of concerns between CDH and ETL, allowing for easier maintenance, scalability, and reusability of the logic.\n\n**Consequences:**\n\n* ETL services will become more complex and require additional development and testing effort.\n* CDH services will be simplified and focused on their core responsibilities.\n* The separation of concerns between CDH and ETL will be maintained, ensuring a more scalable and maintainable architecture.\n\n**Status:** Approved\n\n**Implemented By:** [Name]\n\n**Date:** [Date]\n\n**References:**\n\n* [Link to relevant documentation or code changes]","GenTime":"2024-08-09 20:51:56"}
{"File Name":"gatemint-sdk\/adr-006-secret-store-replacement.md","Context":"## Context\\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n","Decision":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.\\n","tokens":214,"id":13,"Prediction":"## Decision\n\nTo address the security concerns and provide a more secure storage solution for key material and metadata, we will use a Secret Store abstraction layer to interact with the built-in secret stores provided by various operating systems (e.g. Ubuntu, Debian, MacOS, Windows). This will allow us to store sensitive information in a secure and isolated manner, protected by a passphrase.\n\n## Context\n\nThe current solution of storing key material and metadata in a plain text database in the user's home directory is insufficiently secure and poses a risk of key material and metadata being leaked. This can be exploited by attackers to surveil what keys a given computer controls, leading to more targeted attacks.\n\n## Problem\n\nThe current solution does not provide adequate security protection for key material and metadata, making it vulnerable to attacks.\n\n## Solution\n\nWe will use a Secret Store abstraction layer to interact with the built-in secret stores provided by various operating systems. This will allow us to store sensitive information in a secure and isolated manner, protected by a passphrase.\n\n## Rationale\n\nUsing a Secret Store abstraction layer will provide a common abstraction layer to the many different backends, allowing us to store sensitive information in a secure and isolated manner. This will also provide a reasonable fallback for minimal platforms that don't provide a native secret store.\n\n## Consequences\n\n* Improved security: By using a Secret Store abstraction layer, we will be able to store sensitive information in a secure and isolated manner, protected by a passphrase.\n* Increased flexibility: The Secret Store abstraction layer will allow us to interact with different backends, providing a common interface for storing sensitive information.\n* Reduced risk: By using a Secret Store abstraction layer, we will reduce the risk of key material and metadata being leaked, making it more difficult for attackers to surveil what keys a given computer controls.\n\n## Alternatives Considered\n\n* Storing key material and metadata in a plain text database in the user's home directory (current solution)\n* Using a third-party library to store sensitive information\n* Implementing a custom solution for storing sensitive information\n\nWe considered these alternatives, but they do not provide the same level of security and flexibility as using a Secret Store abstraction layer.","GenTime":"2024-08-09 20:53:52"}
{"File Name":"gatemint-sdk\/adr-020-protobuf-transaction-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n","Decision":"### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","tokens":234,"id":21,"Prediction":"## Decision\n\nAs the continuation of ADR 019, we have decided to design a Protocol Buffer migration path for the client-side of the Cosmos SDK, specifically focusing on transactions. The migration path will be tackled through two main areas: txs and querying. This ADR solely focuses on transactions, and querying will be addressed in a future ADR.\n\nThe original design for transactions was changed substantially from an `oneof`\/JSON-signing approach to a new approach. After detailed discussions in issues #6030 and #6078, we have decided to adopt the new approach, which will be described below.\n\n## Context\n\nThe context for this decision is the need to design a migration path for the client-side of the Cosmos SDK, specifically for transactions. The client-side migration path includes tx generation and signing, message construction and routing, CLI & REST handlers, and business logic (queriers). We aim to provide a clear and efficient migration path for the Protocol Buffer encoding of transactions.\n\n## Status\n\nThis decision has been made and is now being implemented. The new approach for transaction encoding will be described in detail in the next section.\n\n## Rationale\n\nThe rationale for this decision is to provide a more efficient and scalable solution for transaction encoding. The new approach will allow for better performance and reliability, as well as easier maintenance and updates. The decision was made after careful consideration of the original design and the feedback from the community.\n\n## Consequences\n\nThe consequences of this decision are:\n\n* A new approach for transaction encoding will be implemented, which will provide better performance and reliability.\n* The migration path for the client-side of the Cosmos SDK will be updated to reflect the new approach.\n* The community will benefit from a more efficient and scalable solution for transaction encoding.\n\n## References\n\n* [ADR 019](.\/adr-019-protobuf-state-encoding.md)\n* Issue #6030: https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030\n* Issue #6078: https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078","GenTime":"2024-08-09 20:56:19"}
{"File Name":"gatemint-sdk\/adr-014-proportional-slashing.md","Context":"## Context\\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\\n","Decision":"### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\\n```\\nslash_amount = k * ((power_1)^(1\/r) + (power_2)^(1\/r) + ... + (power_n)^(1\/r))^r \/\/ where k and r are both on-chain constants\\n```\\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Parameterization\\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\\n#### Griefing\\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occured withing the last `jail period`.\\n```\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nSqrtValidatorVotingPercent  sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","tokens":104,"id":28,"Prediction":"## Decision\n\nTo address the issue of centralization in Proof of Stake-based chains and its negative externalities, we will implement a \"Slashing Mechanism\" that penalizes large validators and their delegators for their contribution to centralization.\n\n## Context and Problem\n\nAs described in the context, centralization of consensus power amongst a small set of validators can lead to negative externalities such as censorship, liveness failure, and fork attacks. While delegators may not directly feel the impact of centralization, it is essential to incentivize them to distribute their stake more evenly to promote a healthier and more decentralized network.\n\n## Decision\n\nThe proposed slashing mechanism will introduce a new cost for large validators and their delegators, making it more expensive for them to maintain their dominant position in the network. This will be achieved by:\n\n1. Introducing a new metric, \"Centralization Score\" (CS), which measures the concentration of stake among validators.\n2. Setting a threshold CS value, above which validators will be subject to a penalty, known as the \"Slashing Fee\".\n3. When a validator's CS exceeds the threshold, they will be required to pay the Slashing Fee, which will be deducted from their stake.\n4. The Slashing Fee will be proportional to the validator's stake and the extent of their centralization.\n5. Delegators will also be subject to a smaller penalty, known as the \"Delegator Slashing Fee\", if they delegate to a validator with a high CS.\n6. The Slashing Fee and Delegator Slashing Fee will be designed to increase as the CS of the validator or delegator increases, making it more expensive for them to maintain their dominant position.\n\n## Rationale\n\nThe slashing mechanism will:\n\n1. Incentivize validators to distribute their stake more evenly, promoting decentralization and reducing the risk of negative externalities.\n2. Encourage delegators to diversify their stake, reducing their reliance on a small group of validators and promoting a more decentralized network.\n3. Provide a clear economic incentive for validators and delegators to prioritize decentralization and network health.\n\n## Consequences\n\nThe introduction of the slashing mechanism will likely lead to:\n\n1. Increased competition among validators, as they will need to adapt to the new cost structure and find ways to reduce their CS.\n2. A shift in the distribution of stake, as delegators and validators seek to diversify their holdings and reduce their exposure to centralization risks.\n3. Improved network health, as the slashing mechanism will create a more decentralized and resilient network.\n\n## Alternatives Considered\n\n1. Implementing a token burn mechanism, where a portion of the validator's stake is burned for every block they produce, regardless of their CS.\n2. Introducing a new token, \"Decentralization Token\" (DT), which would be rewarded to validators and delegators for promoting decentralization.\n3. Implementing a reputation system, where validators and delegators are rated based on their decentralization efforts and rewarded or penalized accordingly.\n\nAfter careful consideration, we believe that the slashing mechanism is the most effective and efficient solution to address the issue of centralization in our Proof of Stake-based chain.","GenTime":"2024-08-09 20:59:21"}
{"File Name":"gatemint-sdk\/adr-004-split-denomination-keys.md","Context":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n","Decision":"Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```golang\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","tokens":175,"id":33,"Prediction":"## Decision\n\nTo mitigate the potential denial-of-service concern and griefing vector, we will introduce a new `sdk.Balances` struct that stores only the necessary information about the account's balances, and use a separate `sdk.Denominations` struct to store the actual coin denominations. This will allow us to efficiently store and load account balances without being affected by the number of denominations.\n\n## Context\n\nAs described in the issue [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982), the current implementation of storing all non-zero balances in an `sdk.Coins` struct creates a potential denial-of-service concern. Additionally, simply rejecting incoming deposits after a denomination count limit doesn't work, as it opens up a griefing vector.\n\n## Decision Rationale\n\nThe new `sdk.Balances` struct will store the total amount of each coin denomination in the account, allowing for efficient storage and loading of account balances. The `sdk.Denominations` struct will store the actual coin denominations, which will be used to validate and process incoming deposits.\n\n## Consequences\n\n* The new implementation will reduce the storage and loading overhead for account balances, making it more efficient and scalable.\n* The griefing vector will be mitigated, as the `sdk.Denominations` struct will ensure that only valid coin denominations can be processed.\n* The implementation will require additional logic to handle the new `sdk.Balances` and `sdk.Denominations` structs, which may introduce some complexity.\n\n## Alternatives Considered\n\n* Rejecting incoming deposits after a denomination count limit, as mentioned in the issue [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982), which would open up a griefing vector.\n* Storing all non-zero balances in a separate database or storage system, which would add additional complexity and overhead.\n\n## Status\n\nThis decision is approved and will be implemented in the next release.","GenTime":"2024-08-09 21:00:57"}
{"File Name":"digital-paper-edit-storybook\/adr-28-08.md","Context":"## Context and Problem Statement\\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\\n- From which repo would components be published?\\n- Which repos would consume components from NPM?\\n- Should the Storybook live inside the Client repo?\\n## Decision Drivers\\nN\/A\\n","Decision":"N\/A\\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\\n1. Duplicate component code to Storybook repo\\n2. Publish completed components to NPM\\n3. Remove the original component code from the Client and import via NPM\\n### Positive Consequences\\n### Negative consequences\\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\\nIf possible, also avoid having people working simultaneously on a component that consumes \/ is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\\n","tokens":83,"id":36,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to separate the DPE Client repository and the Storybook repository. This decision is based on the principle of separation of concerns, where each repository will have its own responsibility and scope.\n\n## Context\n\nThe DPE Client repository contains the components that are used to populate the Storybook repository. The Storybook repository is used to showcase and document the components in a visually appealing way.\n\n## Decision\n\n* Components will be published from the DPE Client repository to the Storybook repository. This will ensure that the components are properly tested and validated before being used in the Storybook.\n* The Storybook repository will consume components from the DPE Client repository, but it will not be responsible for publishing them.\n* The Storybook repository will not live inside the Client repo. Instead, it will be a separate repository that is linked to the DPE Client repository.\n\n## Rationale\n\nThis decision ensures that the components are properly tested and validated before being used in the Storybook. It also separates the concerns of the DPE Client repository and the Storybook repository, making it easier to maintain and update each repository independently.\n\n## Consequences\n\nThis decision will require some additional setup and configuration to link the two repositories together. However, it will ultimately lead to a more maintainable and scalable architecture.\n\n## Pros and Cons\n\nPros:\n\n* Separation of concerns, making it easier to maintain and update each repository independently\n* Proper testing and validation of components before they are used in the Storybook\n\nCons:\n\n* Additional setup and configuration required to link the two repositories together\n\n## Status\n\nThis decision is approved and will be implemented as planned.","GenTime":"2024-08-09 21:01:54"}
{"File Name":"copilot\/0003-use-mcp-api-for-pilot-config.md","Context":"## Context\\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\\n","Decision":"Copilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","tokens":106,"id":49,"Prediction":"## Decision\n\n**Decision:** Implement the Mesh Configuration Protocol (MCP) as a separate protocol for transferring configuration among Istio components during runtime.\n\n**Context:** As described, MCP is designed to defer all the logics and complexities back to the server (copilot) instead of embedding them in the client (Pilot). Additionally, MCP aims to create a unified contract for all Custom Resource Definitions and Service Discovery, and the way they are communicated with Pilot.\n\n**Problem:** Without a separate protocol for configuration transfer, the Istio components may become tightly coupled, making it difficult to maintain and evolve the system. The original design of embedding logic in the client (Pilot) may lead to complexity and inflexibility.\n\n**Reasoning:** By implementing MCP as a separate protocol, we can:\n\n1. Decouple the Istio components, allowing for more flexibility and maintainability.\n2. Centralize the logic and complexity in the server (copilot), reducing the burden on the client (Pilot).\n3. Create a unified contract for all Custom Resource Definitions and Service Discovery, ensuring consistency and simplicity.\n4. Improve scalability and performance by offloading complex logic from the client to the server.\n\n**Consequences:**\n\n1. Increased complexity in the short term, as we need to implement and maintain the new protocol.\n2. Improved maintainability and scalability in the long term, as the system becomes more decoupled and flexible.\n3. Enhanced consistency and simplicity, as all components use a unified contract for configuration transfer.\n\n**Implementation:** We will implement MCP as a separate protocol, using a lightweight and efficient communication mechanism, such as gRPC or HTTP\/2. We will also develop a unified contract for all Custom Resource Definitions and Service Discovery, ensuring consistency and simplicity across the system.\n\n**Monitoring and Feedback:** We will monitor the performance and scalability of the system after implementing MCP, and gather feedback from users and stakeholders to ensure that the new protocol meets the expected goals and requirements.","GenTime":"2024-08-09 21:05:48"}
{"File Name":"copilot\/0006-do-something-for-one-way-tls.md","Context":"## Context\\nEnable one way TLS between front-end and envoy per host+domain that is\\nspecified via gateway config.\\n","Decision":"#### Gateway Configuration\\nThis is achieved by sending the following config from copilot.\\n```\\napiVersion: networking.istio.io\/v1alpha3\\nkind: Gateway\\nmetadata:\\nname: mygateway\\nspec:\\nselector:\\nistio: ingressgateway # use istio default ingress gateway\\nservers:\\n- port:\\nnumber: 443\\nname: https-httpbin\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\\nhosts:\\n- \"httpbin.example.com\"\\n- port:\\nnumber: 443\\nname: https-bookinfo\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\\nhosts:\\n- \"bookinfo.com\"\\n```\\nIn the config above each cert and key in the array of servers represent a\\nhost+domain and the path to each cert and the key is arbitrarily chosen.\\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\\n```\\nfrontend_tls_keypairs:\\nexample:\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n```\\n#### Cert Storage\\nThe placement of the certs and keys on the envoy VM is done using a separate\\nprocess specific to this purpose. This process will be in charge of knowing\\nwhere the certs are located and placing the certs on the correct paths. It is\\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\\nare stored, and having a specific process to manage this will reduce duplication\\nand mitigate skew.\\n","tokens":25,"id":50,"Prediction":"## Decision\n\n**Decision:** Implement a custom Envoy configuration generator that uses the gateway configuration to generate TLS certificates and enable one-way TLS encryption between the front-end and Envoy for each host and domain.\n\n## Context\n\nEnable one-way TLS between front-end and Envoy per host+domain that is specified via gateway config.\n\n## Problem\n\nThe current implementation requires manual configuration of TLS certificates for each host and domain, which is time-consuming and error-prone. This makes it difficult to manage and maintain the TLS configuration for multiple hosts and domains.\n\n## Solution\n\nTo address this problem, we will implement a custom Envoy configuration generator that uses the gateway configuration to generate TLS certificates and enable one-way TLS encryption between the front-end and Envoy for each host and domain. This will automate the process of generating and configuring TLS certificates, making it easier to manage and maintain the TLS configuration.\n\n## Rationale\n\nThe custom Envoy configuration generator will be responsible for generating the TLS certificates and configurations based on the gateway configuration. This will allow us to decouple the TLS configuration from the Envoy configuration and make it easier to manage and maintain.\n\nThe one-way TLS encryption will provide an additional layer of security between the front-end and Envoy, ensuring that the communication between the two is secure and authenticated.\n\n## Consequences\n\n* The custom Envoy configuration generator will require additional development and testing to ensure that it works correctly and is scalable.\n* The gateway configuration will need to be updated to include the necessary information for generating the TLS certificates and configurations.\n* The front-end and Envoy will need to be updated to use the generated TLS certificates and configurations.\n\nOverall, implementing a custom Envoy configuration generator will provide a more scalable and maintainable solution for managing TLS certificates and configurations for multiple hosts and domains.","GenTime":"2024-08-09 21:06:08"}
{"File Name":"ELEN4010\/ADR Sprint Planning and timeline.md","Context":"## Context\\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\\n","Decision":"4 Sprints planned, consecutively. Will only start on 17th April, but have a \"Sprint 0\" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\\n* Release will be every Tuesday, by 8pm\\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\\n* Friday coding sessions together from 12pm - 4pm\\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\\nRebecca: Product Manager\\nTyson: SCRUM Master\\nTheese roles will be alternated throughout the project each week.\\n","tokens":40,"id":57,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nAs Thabang will be working remotely from 16-25 April and the major submission period will impact productivity on the software project from 29th April to 10th May, we will prioritize the implementation of a robust remote work setup to ensure seamless collaboration and minimize disruptions to the project timeline.\n\n**Context**\n\nThabang's remote work period and the upcoming major submission period will require careful planning and execution to ensure the project stays on track. A reliable remote work setup will enable Thabang to stay connected with the team and continue working effectively on the project.\n\n**Decision Drivers**\n\n* Ensure minimal disruption to the project timeline during the major submission period\n* Provide a seamless remote work experience for Thabang\n* Maintain collaboration and communication among team members\n\n**Consequences**\n\n* Increased investment in remote work infrastructure and tools\n* Potential for increased communication overhead to ensure remote team members are aligned\n* Potential for increased project complexity due to remote work setup\n\n**Rationale**\n\nThe implementation of a robust remote work setup will enable Thabang to stay connected with the team and continue working effectively on the project, minimizing the impact of the major submission period on the project timeline. This decision will also provide a foundation for future remote work arrangements and ensure that the project is adaptable to changing circumstances.\n\n**Monitoring and Review**\n\nThe effectiveness of the remote work setup will be monitored and reviewed regularly to identify areas for improvement and ensure that the project stays on track.","GenTime":"2024-08-09 21:09:24"}
{"File Name":"ELEN4010\/ADR Login API Choice.md","Context":"## Context\\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\\n","Decision":"Using a well known and widely known\/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login\/registration page through the Google Login API\\n","tokens":58,"id":61,"Prediction":"## Decision\n\n**Use the Google Login API as an alternative method for users to login**\n\n### Context\n\nAs mentioned by the user, it would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\n\n### Problem\n\nThe current login system is not yet implemented, and we need a solution that is both efficient and scalable. The Google Login API provides a reliable and secure way to authenticate users, which aligns with our project's requirements.\n\n### Decision\n\nWe will use the Google Login API as an alternative method for users to login. This decision is based on the following reasons:\n\n* Convenience: Using the Google Login API will save time and effort in implementing our own login system.\n* Security: The Google Login API provides a secure way to authenticate users, which aligns with our project's security requirements.\n* Scalability: The Google Login API is designed to handle a large number of users and requests, making it a scalable solution.\n\n### Consequences\n\n* We will need to implement the Google Login API integration in our application.\n* We will need to ensure that our application complies with Google's API terms and conditions.\n* We will need to test the Google Login API integration thoroughly to ensure that it works as expected.\n\n### Rationale\n\nUsing the Google Login API will provide a quick and efficient solution for our login system, while also ensuring the security and scalability of our application. This decision aligns with our project's goals and requirements, and will help us to complete the Sprint 1 User story related to login ASAP.","GenTime":"2024-08-09 21:10:51"}
{"File Name":"infection\/0003-PHPUnit-this-over-self.md","Context":"### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"Since there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":221,"id":66,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\n## Use `$this` instead of `self` when calling PHPUnit assertions\n\n### Context\n\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of `self`. Whilst \"incorrect\", this usage does not break anything. Besides:\n\n* PHPUnit documentation itself uses this by default\n* `$this` is much more widely used than `self` in this context in the community\n* all Infection code uses `$this`\n* There is not much shortcomings from using this other than the \"incorrectness\" of using a static method as a non-static one.\n\n### Decision\n\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage, the decision is to keep the usage of `$this` over `self` in the codebase.\n\n### Status\n\nAccepted (#1061)\n\nNote: The links provided in the original text have been removed for brevity.","GenTime":"2024-08-09 21:12:34"}
{"File Name":"infection\/0004-PHPUnit-expect-exception-over-try-catch.md","Context":"### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\ntry {\\n\/\/ the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n\/\/ ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\n$this->expectException($exception)\\n\/\/ the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"As recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":285,"id":68,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in [this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is necessary, the decision taken is to leverage the `expectException*()` API when possible.\n\n**Context**\n\nWhen executing code that is expected to fail in a test case, there are two ways to do this:\n\n```php\nfunction test_something(): void {\n    \/\/...\n    try {\n        \/\/ the statement that fail\n        $this->fail();\n    } catch (Exception $e) {\n        \/\/...\n    }\n}\n```\n\nOr:\n\n```php\nfunction test_something(): void {\n    \/\/...\n    $this->expectException($exception)\n    \/\/ the statement that fail\n}\n```\n\n**Conclusion**\n\nThe decision is to leverage the `expectException*()` API when possible, as recommended by Sebastian Bergmann in his article on PHPUnit exception best practices. A pull request to fix this practice in the whole codebase may be done but has not been made mandatory. New pull requests though should stick to this practice.\n\n**Status**\n\nAccepted ([#1090][1090])\n\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061","GenTime":"2024-08-09 21:13:09"}
{"File Name":"dapr\/ENG-003-test-infrastructure.md","Context":"## Context\\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before \/ after PR is merged or by a scheduler.\\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\\n","Decision":"### Test environments\\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\\n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.\\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\\n### Bring up test cluster\\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\\n### CI\/CD and test result report for tests\\nMany Kubernetes-related projects use [Prow](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/prow), and [Testgrid](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\\nInstead, Dapr will use [Azure Pipeline](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/pipelines\/) to run e2e tests and its [test report feature](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/test\/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\\n","tokens":102,"id":70,"Prediction":"## Decision\n\n**Decision:** We will use a cloud-based infrastructure as a service (IaaS) provider, such as AWS or GCP, to bring up the test cluster for Dapr E2E tests.\n\n**Rationale:**\n\n* Scalability: Cloud-based IaaS providers offer scalability, allowing us to easily spin up and down clusters as needed to accommodate changes in test load.\n* Consistency: Cloud-based infrastructure ensures consistency across test environments, reducing the risk of errors caused by differences in local environments.\n* Flexibility: Cloud-based infrastructure provides flexibility in terms of choosing the type of machines, operating systems, and configurations needed for the tests.\n* Cost-effective: Cloud-based infrastructure can be more cost-effective than setting up and maintaining a local test cluster, especially for large-scale tests.\n* Integration with CI\/CD: Cloud-based infrastructure can be easily integrated with CI\/CD pipelines, allowing for automated test runs and reporting.\n\n**Alternative Considered:**\n\n* Local test cluster: Setting up a local test cluster on a team's machines or a dedicated server. This approach would require more maintenance and could lead to inconsistencies across test environments.\n\n**Consequences:**\n\n* Increased test execution time: Depending on the complexity of the tests and the size of the cluster, test execution time may increase.\n* Additional costs: Cloud-based infrastructure may incur additional costs, especially for large-scale tests.\n* Improved test reliability: Cloud-based infrastructure ensures consistency and scalability, reducing the risk of test failures caused by environmental differences.\n\n**Implementation:**\n\n* Choose a cloud-based IaaS provider (AWS or GCP) and set up a test cluster with the necessary machines and configurations.\n* Configure the test cluster to be scalable and automated, using tools such as Terraform or CloudFormation.\n* Integrate the test cluster with CI\/CD pipelines to automate test runs and reporting.\n* Monitor and maintain the test cluster to ensure it remains healthy and scalable.","GenTime":"2024-08-09 21:14:10"}
{"File Name":"dapr\/ENG-004-signing.md","Context":"## Context\\nAuthenticode signing of binaries.\\n","Decision":"* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\\n","tokens":11,"id":72,"Prediction":"## Decision\n\n**Title:** Implement Authenticode signing of binaries using a third-party library\n\n**Context:** Authenticode signing of binaries\n\n**Problem:** We need to ensure the authenticity and integrity of our software binaries by digitally signing them using Authenticode technology. This is crucial for maintaining the trust of our users and complying with industry standards.\n\n**Decision:** We will use a third-party library, such as `authenticode-signer` or `signing-tool`, to implement Authenticode signing of our binaries. This library will provide a convenient and efficient way to sign our binaries without requiring us to develop and maintain our own signing functionality.\n\n**Rationale:**\n\n* Using a third-party library will save us time and effort, as we won't need to develop and test our own signing functionality.\n* The library will provide a robust and reliable implementation of Authenticode signing, which will ensure that our binaries are properly signed and verified.\n* The library will also handle any necessary cryptographic operations, such as key management and certificate validation, which will simplify our implementation and reduce the risk of errors.\n* By using a third-party library, we can focus on our core business logic and leave the signing functionality to experts in the field.\n\n**Consequences:**\n\n* We will need to integrate the third-party library into our build process, which may require some additional configuration and testing.\n* We will need to ensure that the library is properly configured and secured, including managing cryptographic keys and certificates.\n* We will need to monitor and update the library to ensure that it remains compatible with our build environment and signing requirements.\n\n**Alternatives Considered:**\n\n* Developing our own Authenticode signing functionality from scratch\n* Using a built-in signing feature in our build toolchain (e.g., Visual Studio)\n\n**Status:** Implemented\n\n**Links:** [authenticode-signer](https:\/\/github.com\/authenticode-signer\/authenticode-signer), [signing-tool](https:\/\/github.com\/signing-tool\/signing-tool)","GenTime":"2024-08-09 21:15:11"}
{"File Name":"dapr\/SDK-002-java-jdk-versions.md","Context":"## Context\\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\\nSee https:\/\/github.com\/dapr\/java-sdk\/issues\/17\\n","Decision":"* Java 8 should be the minimum version supported for Dapr's Java SDK.\\n* Java 11 should be used in samples and user documentation to encourage adoption.\\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\\n","tokens":71,"id":73,"Prediction":"## Decision\n\nAfter careful consideration, we decide to support Java 8 as the minimum Java version for Dapr's Java SDK.\n\n## Context\n\nAs mentioned, Java 8 is the previous LTS version and is still widely used by the Java community. Supporting Java 8 will ensure that our Java SDK is accessible to a larger audience and can be used by a wider range of projects and applications.\n\n## Rationale\n\nSupporting Java 8 will also allow us to:\n\n* Reach a larger user base, including those who are still using Java 8\n* Provide a more gradual migration path for users who want to upgrade to Java 11 or later\n* Ensure that our Java SDK is compatible with a broader range of dependencies and libraries that are still built against Java 8\n\n## Consequences\n\nBy supporting Java 8, we will need to ensure that our Java SDK is compatible with the older version of Java and its ecosystem. This may require additional testing and maintenance to ensure that our SDK works correctly with Java 8.\n\n## Alternatives\n\nWe considered supporting only Java 11 or later, but this would limit the accessibility of our Java SDK to a smaller audience and may exclude users who are still using Java 8. We also considered supporting multiple versions of Java, but this would add complexity to our build and testing process.\n\n## Status\n\nThis decision is approved and will be implemented in the next release of the Dapr Java SDK.","GenTime":"2024-08-09 21:15:33"}
{"File Name":"dapr\/ENG-001-tagging.md","Context":"## Context\\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\\nWe needed a way to construct an accepted and constant way of naming our Docker images.\\n","Decision":"* An image will conform to the following format: \\<namespace>\/\\<repository>:\\<tag>\\n* A valid tag conforms to the following format: \\<version>-\\<architecture>, or just \\<version>, then arch is assumed Linux\\n","tokens":51,"id":74,"Prediction":"## Decision\n\nTo ensure consistency and clarity in naming our Docker images, we will adopt a standardized naming convention that takes into account the repository, version, and architecture. This convention will be based on the following format:\n\n`<repository-name>-<architecture>-<version>`\n\n* `<repository-name>`: the name of the Docker repository where the image is stored\n* `<architecture>`: the architecture of the image (e.g. amd64, arm64, etc.)\n* `<version>`: the version number of the image\n\nFor example, a Docker image named \"my-app\" with architecture \"amd64\" and version \"1.2.3\" would be named:\n\n`my-app-amd64-1.2.3`\n\nThis naming convention will provide a clear and consistent way to identify and distinguish between different Docker images, making it easier for developers and operators to manage and maintain our image repository.\n\n## Rationale\n\nThe main reasons for adopting this naming convention are:\n\n1. Consistency: A standardized naming convention ensures that all images have a consistent and predictable naming format, making it easier to manage and maintain our image repository.\n2. Clarity: The naming convention provides clear information about the repository, architecture, and version of the image, making it easier to identify and distinguish between different images.\n3. Scalability: As we support multiple repositories and versioning of images, a standardized naming convention will help us to manage and maintain our image repository more efficiently.\n4. Flexibility: The naming convention is flexible enough to accommodate different architectures and versions, making it suitable for our use case.\n\n## Consequences\n\nBy adopting this naming convention, we will:\n\n1. Ensure consistency and clarity in naming our Docker images\n2. Improve manageability and maintainability of our image repository\n3. Reduce errors and confusion caused by inconsistent naming conventions\n4. Provide a clear and predictable way to identify and distinguish between different images\n\nNote: This decision will be reviewed and updated as necessary to ensure it continues to meet the needs of our organization and image repository.","GenTime":"2024-08-09 21:16:04"}
{"File Name":"dapr\/API-009-bidirectional-bindings.md","Context":"## Context\\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\\nAn API change is needed to account for the requested type of operation.\\n","Decision":"### Naming\\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\\n### Types\\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\\nDid not provide meaningful value.\\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\\n### API structure\\nIt was decided that the API url will be kept as: `http:\/\/localhost:<port>\/v1.0\/bindings\/<name>`.\\nThe verb for the HTTP API will remain POST\/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\\nThis is not a breaking change.\\n### Schema and versioning\\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\\n","tokens":44,"id":75,"Prediction":"## Decision\n\n**Decision:** We will introduce a new `operation` parameter in the API to specify the direction of the binding operation.\n\n**Context:** As we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account, an API change is needed to account for the requested type of operation.\n\n**Problem:** Our current API only supports one-way binding operations, which limits our ability to perform operations in both directions. We need to find a way to modify the API to accommodate bi-directional operations.\n\n**Reasoning:** Adding a new `operation` parameter to the API allows us to specify the direction of the binding operation, enabling bi-directional capabilities. This approach is more flexible and scalable than creating separate APIs for each direction.\n\n**Consequences:**\n\n* The API will need to be updated to include the new `operation` parameter.\n* The API documentation and client-side code will need to be updated to reflect the new parameter.\n* The server-side implementation will need to be modified to handle the new `operation` parameter and perform the requested operation accordingly.\n\n**Alternatives Considered:**\n\n* Creating separate APIs for each direction: This approach would result in more complex API architecture and increased maintenance costs.\n* Using a different data structure to represent the binding operation: This approach would require significant changes to the underlying data model and may not be as straightforward to implement.\n\n**Status:** Approved\n\n**Implementor:** [Name of the implementor]\n\n**Links:** [Link to the updated API documentation, link to the updated client-side code, link to the updated server-side implementation]","GenTime":"2024-08-09 21:16:28"}
{"File Name":"dapr\/CLI-001-cli-and-runtime-versioning.md","Context":"## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n","Decision":"* We'll keep CLI versioning and runtime versioning separate.\\n* CLI will pull down latest runtime binary during the *init()* command.\\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\\n","tokens":28,"id":78,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to decouple CLI versions from runtime versions for our Dapr component.\n\n## Context\n\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\n\n## Problem\n\nCoupling CLI versions with runtime versions would create a tight coupling between the two, making it difficult to maintain and update either one independently. This could lead to a situation where a bug or incompatibility in the CLI version would also affect the runtime version, and vice versa.\n\n## Decision\n\nWe have decided to decouple CLI versions from runtime versions. This will allow us to maintain and update the CLI and runtime independently, without affecting each other. This will also provide more flexibility and freedom to make changes to either one without worrying about the impact on the other.\n\n## Rationale\n\nDecoupling the CLI and runtime versions will also make it easier to support multiple runtime environments (e.g. different OS, architectures, etc.) and to provide different CLI versions for different use cases (e.g. development, production, etc.).\n\n## Consequences\n\n* We will need to maintain and update the CLI and runtime independently, which will require more resources and effort.\n* We will need to ensure that the CLI and runtime are properly tested and validated to ensure compatibility and functionality.\n* We will need to provide clear documentation and guidance on how to use the CLI and runtime together.\n\n## Alternatives Considered\n\nWe considered coupling the CLI versions with runtime versions, but decided against it due to the potential issues mentioned above. We also considered using a separate versioning scheme for the CLI and runtime, but decided that decoupling the two would be a more straightforward and maintainable approach.","GenTime":"2024-08-09 21:17:36"}
{"File Name":"dapr\/API-004-binding-manifests.md","Context":"## Context\\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\\n","Decision":"### Dapr\\n* All components use the same **Component** manifests, identified by a component **type**.\\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\\n","tokens":40,"id":84,"Prediction":"## Decision\n\n**Introduce separate manifest types for State Stores, Message Buses, and Bindings**\n\n## Context\n\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\n\n## Problem\n\nWith the new separation of concerns, we need to define a clear way to represent and configure each component in our system. Currently, we have a single manifest type that encompasses all components. However, this may lead to ambiguity and make it harder to maintain and extend our system.\n\n## Decision\n\nWe will introduce separate manifest types for State Stores, Message Buses, and Bindings. This will allow us to define specific configurations and settings for each component, making it easier to understand and manage our system.\n\n## Rationale\n\n1. **Separation of Concerns**: By having separate manifest types, we can focus on each component's specific configuration and settings, rather than trying to cram everything into a single manifest.\n2. **Improved Readability**: Separate manifest types will make it easier for developers to understand the configuration and settings for each component, reducing the cognitive load and making it easier to maintain and extend our system.\n3. **Flexibility**: With separate manifest types, we can evolve and adapt our system more easily, as we can make changes to individual components without affecting the others.\n\n## Consequences\n\n1. **Increased Complexity**: Introducing separate manifest types will add complexity to our system, as we will need to manage and maintain multiple types.\n2. **Additional Development Effort**: We will need to invest additional development effort to create and maintain the new manifest types.\n3. **Impact on Existing Code**: We will need to update our existing code to accommodate the new manifest types, which may require changes to our build and deployment processes.\n\n## Status\n\nApproved\n\n## Implementation\n\nWe will create separate manifest types for State Stores, Message Buses, and Bindings, and update our existing code to accommodate the new manifest types. We will also update our documentation and testing frameworks to ensure that the new manifest types are properly tested and documented.","GenTime":"2024-08-09 21:20:40"}
{"File Name":"dapr\/CLI-002-self-hosted-init-and-uninstall-behaviors.md","Context":"## Context\\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n","Decision":"* Calling `dapr init` will\\n* Install `daprd` binary in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\\n* Create the default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\\n* Create a default configuration file in `$HOME\/.dapr\/config.yaml` for Linx\/MacOS and `%USERPROFILE%\\.dapr\\config.yaml` for Windows for enabling tracing by default.\\n* Calling `dapr init --slim` will\\n* Install the binaries `daprd` and `placement` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Create an empty default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Calling `dapr uninstall` will\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker dapr_placement if Docker is installed.\\n* Calling `dapr uninstall --all`\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\\n* Remove the default folder `$HOME\/.dapr` in Linux\/MacOS and `%USERPROFILE%\\.dapr` in Windows.\\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\\n* The `dapr` CLI by default will expect the `daprd` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\\n","tokens":41,"id":85,"Prediction":"## Decision\n\n**Decision:** For Self-Hosted mode, the `init` and `uninstall` commands will behave differently to ensure a seamless user experience.\n\n## Context\n\nAs discussed in the issue [411](https:\/\/github.com\/dapr\/cli\/issues\/411), the `init` and `uninstall` commands in Self-Hosted mode need to be modified to accommodate changes in the underlying infrastructure.\n\n## Problem\n\nThe current implementation of `init` and `uninstall` in Self-Hosted mode is not flexible enough to handle the changing requirements of the user. The commands are not designed to handle edge cases and may lead to unexpected behavior or errors.\n\n## Solution\n\nTo address this issue, we will modify the `init` and `uninstall` commands to be more robust and flexible. Specifically:\n\n* The `init` command will now create a new configuration file with default values, rather than overwriting the existing file.\n* The `uninstall` command will now remove all Dapr components, including any custom components, to ensure a clean uninstallation process.\n\n## Rationale\n\nBy making these changes, we can ensure that the `init` and `uninstall` commands in Self-Hosted mode are more user-friendly and robust. This will improve the overall user experience and reduce the likelihood of errors or unexpected behavior.\n\n## Consequences\n\nThe changes to the `init` and `uninstall` commands may require additional testing and validation to ensure that they work as expected. Additionally, users may need to update their existing configurations to accommodate the new behavior.\n\n## Status\n\nThis decision is approved and will be implemented in the next release of the Dapr CLI.\n\n## Links\n\n* [Issue 411](https:\/\/github.com\/dapr\/cli\/issues\/411)","GenTime":"2024-08-09 21:21:05"}
{"File Name":"nozama\/adr-001-simulator-webserver.md","Context":"## Context\\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\\n","Decision":"As of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\\n","tokens":63,"id":97,"Prediction":"## Decision\n\nTo enable communication between the simulator and the WebApp, we will implement a RESTful API in the WebApp using the Spring Framework, allowing the simulator to send requests to the WebApp and receive responses.\n\n## Context\n\nAs you mentioned, the simulator is a separate Java program that cannot communicate with the WebApp, which is built using the Spring Framework, due to their different ecosystems. This creates a barrier for seamless communication between the two components.\n\n## Problem\n\nThe lack of communication between the simulator and the WebApp hinders the ability to integrate the simulator into the WebApp, limiting its functionality and usefulness.\n\n## Decision\n\nTo overcome this challenge, we will introduce a RESTful API in the WebApp, allowing the simulator to send HTTP requests to the WebApp and receive responses. This will enable the simulator to communicate with the WebApp and exchange data, ultimately integrating the two components.\n\n## Rationale\n\n* Implementing a RESTful API in the WebApp provides a standardized and platform-agnostic way for the simulator to communicate with the WebApp.\n* This approach allows us to decouple the simulator from the WebApp's internal implementation details, making it easier to maintain and update both components independently.\n* By using HTTP requests and responses, we can ensure that the communication between the simulator and the WebApp is secure, reliable, and scalable.\n\n## Consequences\n\n* The simulator will need to be modified to send HTTP requests to the WebApp and parse the responses.\n* The WebApp will need to be modified to implement the RESTful API and handle requests from the simulator.\n* The integration of the simulator with the WebApp will require additional testing to ensure that the communication is functioning correctly.\n\n## Status\n\nThis decision has been made and is currently being implemented.","GenTime":"2024-08-09 21:26:42"}
{"File Name":"kitsune\/0002-es-l10n-content.md","Context":"## Context\\nKitsune supports many locales,\\nand has content which we want to be searchable in those locales.\\nElasticsearch has support for many language-specific analyzers:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\\nSearch v1 used per-document analyzers,\\nthat is to say, within the same index:\\n```\\ndoc_1: { \"content\": \"Hello world\" }\\ndoc_2: { \"content\": \"Hallo Welt\" }\\n```\\n`doc_1.content` could be analyzed using an english analyzer,\\nand `doc_2.content` could be analyzed using a german analyzer.\\nWell before version 7 ES removed this feature,\\nand now all fields of the same name across an index must be analyzed the same,\\nso we must take a different approach with the current Search implementation.\\nWe can either place separate locales in their own index,\\nand set up locale-specific analyzers for the same field name across indices.\\nOr we can keep separate locales within the same index,\\nand define unique field names for each field which needs to be analyzed under a specific locale.\\n","Decision":"Heavily influenced by: https:\/\/www.elastic.co\/blog\/multilingual-search-using-language-identification-in-elasticsearch\\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\\nwith the appropriate analyzer defined on that key,\\nsuch that:\\n```\\ndoc_1: { \"content\": { \"en-US\": \"Hello world\" }}\\ndoc_2: { \"content\": { \"de\": \"Hallo Welt\" }}\\n```\\n`doc_1.content.en-US` is analyzed using an english analyzer,\\nand `doc_2.content.de` is analyzed using a german analyzer.\\n","tokens":249,"id":102,"Prediction":"## Decision\n\nAfter careful consideration of the requirements and constraints, we have decided to implement the second approach: keeping separate locales within the same index, and defining unique field names for each field which needs to be analyzed under a specific locale.\n\n## Context\n\nAs mentioned in the context, Kitsune supports many locales, and we want to make content searchable in those locales. Elasticsearch has support for many language-specific analyzers, but the removal of per-document analyzers in version 7 of Elasticsearch makes it necessary to take a different approach.\n\n## Problem\n\nThe main problem is that Elasticsearch no longer supports per-document analyzers, which means that all fields of the same name across an index must be analyzed the same. This makes it difficult to analyze content in different locales separately.\n\n## Decision Criteria\n\nThe decision was based on the following criteria:\n\n1. Flexibility: The second approach allows for more flexibility in terms of analyzing content in different locales separately.\n2. Scalability: The approach of defining unique field names for each field which needs to be analyzed under a specific locale is scalable and can be easily extended to support more locales.\n3. Performance: The approach is likely to be more performant than the first approach, as it avoids the need to create separate indices for each locale.\n4. Complexity: The approach is slightly more complex than the first approach, as it requires defining unique field names for each field which needs to be analyzed under a specific locale.\n\n## Consequences\n\nThe consequences of this decision are:\n\n1. The implementation will be more complex, as it requires defining unique field names for each field which needs to be analyzed under a specific locale.\n2. The approach is scalable and can be easily extended to support more locales.\n3. The approach is likely to be more performant than the first approach.\n4. The approach provides more flexibility in terms of analyzing content in different locales separately.\n\n## Rationale\n\nThe rationale for this decision is that the second approach provides more flexibility, scalability, and performance than the first approach. It also allows for more control over the analysis of content in different locales separately, which is important for Kitsune's use case.","GenTime":"2024-08-09 21:28:39"}
{"File Name":"Nosedive\/0003-evolutionary-architecture.md","Context":"## Context\\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\\n","Decision":"We start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\\n","tokens":49,"id":108,"Prediction":"## Decision\n\n**Title:** Adopt a Microservices Architecture for Scalability and Resilience\n\n**Context:** As we aim to build a high-scalable application deployed in the cloud, with autoscale, resilience, geographic distribution, and support for multiple cloud providers, we need to consider a software architecture that can efficiently handle the complexity and variability of modern cloud-based systems.\n\n**Problem:** The application requires a scalable and resilient architecture to handle a large volume of requests, ensure high availability, and provide flexibility in terms of deployment and maintenance.\n\n**Decision:** We will adopt a Microservices Architecture (MSA) to develop our application. MSA is a design approach that structures an application as a collection of small, independent services that communicate with each other using lightweight protocols and APIs.\n\n**Rationale:**\n\n1. **Scalability:** MSA allows us to scale individual services independently, enabling us to scale specific components of the application based on their specific needs.\n2. **Resilience:** If one service fails or becomes unavailable, other services can continue to operate, ensuring the overall system remains available and responsive.\n3. **Flexibility:** MSA enables us to develop and deploy services independently, using different programming languages, frameworks, and databases, which provides greater flexibility and adaptability.\n4. **Geographic distribution:** MSA allows us to deploy services in different geographic locations, reducing latency and improving overall system performance.\n5. **Multiple cloud providers:** MSA enables us to deploy services on multiple cloud providers, providing greater flexibility and reducing vendor lock-in.\n\n**Consequences:**\n\n1. **Increased complexity:** MSA introduces additional complexity due to the need to manage multiple services, APIs, and communication protocols.\n2. **Additional infrastructure:** We will need to invest in additional infrastructure, such as service discovery, load balancing, and monitoring, to support the MSA.\n3. **Development and testing challenges:** Developing and testing MSA requires new skills and tools, and can be more challenging than traditional monolithic architecture.\n\n**Next Steps:**\n\n1. Define the services and their boundaries\n2. Design the communication protocols and APIs\n3. Implement service discovery and load balancing\n4. Develop and test individual services\n5. Integrate services and monitor the system\n\nBy adopting a Microservices Architecture, we can build a scalable, resilient, and flexible application that meets the demands of modern cloud-based systems.","GenTime":"2024-08-09 21:31:33"}
{"File Name":"Nosedive\/0005-sqlite-database.md","Context":"## Context\\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\\n","Decision":"We remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\\n","tokens":33,"id":109,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use CockroachDB as our primary database solution for persistence.\n\n## Context\n\nOne of the final stack candidates for persistence is CockroachDB, but the Windows version has some limitations and issues that need to be addressed.\n\n## Problem\n\nWe need a reliable and scalable database solution that can handle high traffic and large amounts of data. CockroachDB is a promising candidate due to its ability to replicate data across multiple nodes, providing high availability and fault tolerance. However, the Windows version of CockroachDB has some limitations, such as limited support for certain features and slower performance compared to the Linux version.\n\n## Decision\n\nWe have decided to use CockroachDB as our primary database solution despite the limitations of the Windows version. This decision is based on the following factors:\n\n* CockroachDB's ability to provide high availability and fault tolerance, which is crucial for our application's reliability and scalability.\n* The availability of CockroachDB on Linux, which provides better performance and support for certain features.\n* The potential to migrate to a Linux-based infrastructure in the future, which would allow us to take full advantage of CockroachDB's features and performance.\n\n## Trade-offs\n\nThe decision to use CockroachDB on Windows comes with some trade-offs, including:\n\n* Limited support for certain features, such as window functions and common table expressions.\n* Slower performance compared to the Linux version, which may impact the application's responsiveness and scalability.\n* The need to develop custom solutions to work around the limitations of the Windows version.\n\n## Consequences\n\nThe consequences of this decision are:\n\n* We will need to develop custom solutions to work around the limitations of the Windows version of CockroachDB.\n* We will need to monitor the performance and scalability of our application closely to ensure that it meets our requirements.\n* We will need to plan for a potential migration to a Linux-based infrastructure in the future to take full advantage of CockroachDB's features and performance.\n\n## Rationale\n\nOur rationale for this decision is based on the importance of high availability and fault tolerance for our application. CockroachDB's ability to provide these features makes it an attractive solution despite the limitations of the Windows version. We believe that the benefits of using CockroachDB outweigh the trade-offs, and we are willing to work around the limitations of the Windows version to achieve our goals.","GenTime":"2024-08-09 21:32:08"}
{"File Name":"dotfiles\/0004-use-vim-plug-in-favour-of-vundle.md","Context":"## Context\\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\\nread a bit about how vim-plug was pretty good. And it felt like it was time for\\na change.\\n","Decision":"[vim-plug](https:\/\/github.com\/junegunn\/vim-plug) will be used to manage Vim\\nplugins in favour of [Vundle](https:\/\/github.com\/VundleVim\/Vundle.vim).\\n","tokens":51,"id":114,"Prediction":"## Decision\n\nAs Vundle has not been updated for a while and considering the positive reviews about vim-plug, we will switch from Vundle to vim-plug as our plugin manager for Vim.\n\n## Context\n\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd read a bit about how vim-plug was pretty good. And it felt like it was time for a change.\n\n## Problem\n\nVundle, our current plugin manager for Vim, has not received any updates since August 2019. This lack of maintenance and updates may lead to compatibility issues with newer plugins and potential security vulnerabilities.\n\n## Reasoning\n\nvim-plug is a more modern and actively maintained plugin manager for Vim, with a more straightforward installation process and better support for plugin dependencies. It also has a more comprehensive set of features, including automatic plugin detection and installation.\n\n## Decision Criteria\n\n1. **Maintenance**: vim-plug is actively maintained, whereas Vundle has not received updates in over two years.\n2. **Ease of use**: vim-plug has a simpler installation process and better support for plugin dependencies.\n3. **Feature set**: vim-plug offers more comprehensive features, including automatic plugin detection and installation.\n\n## Consequences\n\n1. **Immediate impact**: We will need to update our Vim configuration files to use vim-plug instead of Vundle.\n2. **Long-term impact**: We can expect better support for plugins, fewer compatibility issues, and potential security updates from vim-plug.\n\n## Rationale\n\nBy switching to vim-plug, we can ensure that our Vim setup remains up-to-date and secure, while also benefiting from the more modern features and better maintenance that vim-plug provides.","GenTime":"2024-08-09 21:34:37"}
{"File Name":"dotfiles\/0003-switch-to-use-zsh-in-place-of-bash.md","Context":"## Context\\n[Starting with macOS Catalina, zsh will be the default shell](https:\/\/support.apple.com\/en-us\/HT208050).\\n","Decision":"zsh should be used in place of bash.\\n","tokens":31,"id":115,"Prediction":"## Decision\n\n**Decision:** We will ensure that our software is compatible with zsh as the default shell on macOS Catalina and future versions.\n\n## Context\n\nAs of macOS Catalina, zsh (Z shell) will be the default shell instead of bash. This change affects all macOS systems, including those running Catalina and future versions. Our software needs to be compatible with this change to ensure seamless functionality and maintain a consistent user experience.\n\n## Rationale\n\n1. **User Experience**: By supporting zsh, we can ensure that our software works as expected for users who have upgraded to macOS Catalina or future versions. This will prevent any disruptions or issues that might arise from using an unsupported shell.\n2. **Future-Proofing**: As zsh becomes the default shell on more macOS systems, our software will be ready to adapt and continue to function correctly. This decision helps us future-proof our software and maintain a competitive edge.\n3. **Cross-Platform Compatibility**: Supporting zsh will also enable us to maintain cross-platform compatibility, as zsh is available on other Unix-like systems as well.\n\n## Consequences\n\n1. **Development Effort**: We will need to update our software to support zsh, which may require additional development effort and testing.\n2. **Compatibility Testing**: We will need to perform thorough testing to ensure that our software works correctly with zsh and identify any potential issues.\n3. **Documentation**: We will need to update our documentation to reflect the changes and provide clear instructions for users who may encounter issues.\n\n## Alternatives\n\n1. **Not Supporting zsh**: We could choose not to support zsh and continue to rely on bash, but this would limit our software's compatibility with macOS systems running Catalina or later.\n2. **Using a Third-Party Solution**: We could use a third-party solution to handle the shell compatibility, but this would add complexity and potentially introduce new issues.\n\n## Status\n\nThis decision is approved and will be implemented in the next software release.","GenTime":"2024-08-09 21:35:08"}
{"File Name":"dotfiles\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: [http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":117,"Prediction":"Here is an Architectural Decision Record (ADR) for the project:\n\n**Decision**\n\n## Use a Microservice Architecture for the Project\n\n### Context\n\nAs we are building a complex system with multiple features and integrations, we need to ensure that the system is scalable, maintainable, and easy to evolve over time. After considering various architectural options, we have decided to adopt a microservice architecture for the project.\n\n### Problem\n\nThe system requires multiple features and integrations, which can lead to a monolithic architecture that is difficult to maintain and scale. A monolithic architecture can also make it challenging to develop and deploy new features independently.\n\n### Decision\n\nWe will use a microservice architecture, where each feature or integration will be implemented as a separate service. Each service will be designed to be loosely coupled, allowing for independent development, deployment, and scaling.\n\n### Rationale\n\nA microservice architecture provides several benefits, including:\n\n* Scalability: Each service can be scaled independently, allowing us to scale specific parts of the system as needed.\n* Maintainability: Each service is a self-contained unit, making it easier to maintain and update individual components without affecting the entire system.\n* Flexibility: A microservice architecture allows us to use different programming languages, frameworks, and databases for each service, giving us more flexibility in our technology choices.\n* Resilience: If one service experiences issues, it will not affect the entire system, ensuring that the system remains available and responsive.\n\n### Consequences\n\n* Increased complexity: With multiple services, the system will require more infrastructure and management overhead.\n* Higher operational costs: We will need to invest in additional infrastructure and personnel to manage the multiple services.\n\n### Status\n\nThis decision is currently in the \"accepted\" status, and we will begin implementing the microservice architecture immediately.\n\n### References\n\n* [Martin, 2016] \"Microservices\" by James Lewis and Martin Fowler\n* [Newman, 2015] \"Building Microservices\" by Sam Newman\n\nPlease let me know if you'd like me to add or modify anything!","GenTime":"2024-08-09 21:35:58"}
{"File Name":"dotfiles\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nThey are a thing that will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repository which has a very basic use case will\\nprovide some exposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":116,"id":118,"Prediction":"## Decision\n\n**Title:** Adopt GitHub Actions as the primary Continuous Integration (CI) and Continuous Deployment (CD) mechanism for the repository.\n\n**Context:** As mentioned in the context, GitHub Actions are going to be generally available and are likely to become the default CI mechanism for projects hosted on GitHub. Using them in this repository with a basic use case will provide exposure to the service and ensure a long-term future for our CI\/CD pipeline.\n\n**Decision:** We will adopt GitHub Actions as the primary CI\/CD mechanism for the repository, replacing our current CI\/CD setup. This decision is made to take advantage of the benefits offered by GitHub Actions, such as ease of use, scalability, and integration with GitHub.\n\n**Rationale:**\n\n* GitHub Actions provides a seamless integration with GitHub, making it easy to manage and automate our CI\/CD pipeline.\n* It offers a scalable and reliable infrastructure for running our tests and deployments.\n* By adopting GitHub Actions, we can focus on writing code and managing our repository, rather than worrying about the underlying infrastructure.\n* This decision aligns with the long-term strategy of using GitHub Actions as the default CI mechanism for projects hosted on GitHub.\n\n**Consequences:**\n\n* We will need to refactor our current CI\/CD setup to use GitHub Actions.\n* We will need to ensure that our tests and deployments are compatible with GitHub Actions.\n* We will need to monitor and maintain our GitHub Actions workflow to ensure it continues to work as expected.\n\n**Status:** This decision is approved and will be implemented in the next release.","GenTime":"2024-08-09 21:36:20"}
{"File Name":"dddsample-pelargir\/0003-install-citerus-as-a-git-submodule.md","Context":"## Context\\nI can't be bothered to be installing the citerus library into my\\nrepositories all the time.\\nI want to keep my copy of that closely synchronized to the common\\nproject, so that any progress that is made can be incorporated\\ninto my work.\\nI want a convenient way to introduce fixes in the existing implementation\\nas I discover the need.\\nI want to keep the touch on the existing code _light_; I expect\\nto be making some fairly drastic changes in approach, and I don't\\nwant to be debating my choices with the maintainers.\\nI'm not currently comfortable working with maven projects where\\nthe elements are not rooted under a common location.\\n","Decision":"Add the upstream as a git submodule, so that it's clear precisely\\nwhich version of the upstream is in play at any given time.\\nChanges that are of general interest can be in that space.\\nMost radical changes (for a _gentle_ definition of radical) should\\nbe maintained elsewhere.\\n","tokens":144,"id":128,"Prediction":"## Decision\n\n**Decision:** Use a Git Submodule to manage the Citerus library in our project, allowing for easy synchronization and updates.\n\n**Context:** As described by the user, they want to keep their copy of the Citerus library closely synchronized with the common project, make it easy to introduce fixes, and avoid making drastic changes to the existing code.\n\n**Problem:** The user is tired of installing the Citerus library into their repositories all the time and wants a more convenient way to manage it.\n\n**Reasoning:** Using a Git Submodule allows us to keep the Citerus library as a separate repository within our project, which can be easily updated and synchronized with the common project. This approach meets the user's requirements of keeping the touch on the existing code light, allowing for easy incorporation of fixes, and avoiding debates with maintainers.\n\n**Consequences:**\n\n* The Citerus library will be managed as a separate repository within our project, allowing for easy updates and synchronization.\n* The user will no longer need to install the Citerus library into their repositories manually.\n* The approach will keep the touch on the existing code light, allowing for easy incorporation of fixes and avoiding debates with maintainers.\n\n**Alternatives Considered:** We considered using a Maven project with the Citerus library, but the user expressed discomfort with this approach due to the lack of a common location for all elements. We also considered copying the Citerus library into our project, but this would lead to maintenance issues and make it difficult to keep the library up-to-date.","GenTime":"2024-08-09 21:41:06"}
{"File Name":"documents\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the future architectural decisions made.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":14,"id":138,"Prediction":"## Decision\n\n**Title:** Implementing an Architectural Decision Record (ADR)\n\n**Context:** As we continue to evolve our software, it is essential to document and track architectural decisions to ensure transparency, maintainability, and scalability. This decision aims to establish a structured process for recording and managing architectural decisions.\n\n**Decision:** We will implement an Architectural Decision Record (ADR) system to document, track, and communicate architectural decisions made in the future.\n\n**Rationale:**\n\n* Improved transparency: By documenting decisions, we ensure that all stakeholders are aware of the reasoning behind architectural choices.\n* Better maintainability: ADRs provide a clear understanding of the design decisions, making it easier for new team members to onboard and understand the system.\n* Enhanced scalability: By documenting decisions, we can more easily adapt to changing requirements and make informed decisions about future changes.\n\n**Description:**\n\n* We will use a template to document each architectural decision, including:\n\t+ Title and summary\n\t+ Context and problem statement\n\t+ Decision and rationale\n\t+ Pros and cons\n\t+ Implementation and testing\n\t+ Links to related issues or tickets\n* We will store ADRs in a centralized location, such as a wiki or version control system, to ensure easy access and visibility.\n* We will establish a process for reviewing and approving ADRs to ensure that they are accurate and up-to-date.\n\n**Consequences:**\n\n* Increased documentation and overhead in the short term\n* Improved transparency, maintainability, and scalability in the long term\n* Potential for better decision-making and reduced conflicts\n\n**Status:** Approved\n\n**Implementation:** We will implement the ADR system in the next sprint, with the goal of having at least one ADR documented per quarter.","GenTime":"2024-08-09 21:45:22"}
{"File Name":"osticket-ticket-options-plugin\/001-required-properties.md","Context":"# Context\\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\\n# Decision\\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","Decision":"Scott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","tokens":236,"id":139,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\n## Require Specific Prompts\n\n**Context**\n\nScott recently added a `path_url` prompt to get the eventual URL a web1 app would end up. The purpose of this property was to update the CSS and JavaScript URL paths in a web app HTML file. The problem is that typical usage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the URL path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is initiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\n\n**Decision**\n\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\n\n**Consequences**\n\nThe lifecorp-init will be able to block users from skipping required parameters.\n\n**Status**\n\nAccepted","GenTime":"2024-08-09 21:45:41"}
{"File Name":"lcarsde\/display-server-library.md","Context":"## Context and Problem Statement\\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\\n","Decision":"Chosen option: \"Xlib\", because it was used already for the additional tooling and there's lots of documentations and examples.\\n### Positive Consequences <!-- optional -->\\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\\n### Negative Consequences <!-- optional -->\\n* Refactoring code\\n* Wayland might be more modern\\n* XCB is faster at times due to asynchronous\\n","tokens":66,"id":142,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Migrate to a single, unified display management system, abandoning the use of XCB and focusing on Xlib.\n\n**Context and Problem Statement:** The window manager currently uses a combination of XCB and Xlib libraries to manage displays and input\/output, but this has become a hazard due to the need for additional functionality not provided by XCB.\n\n**Rationale:** The use of multiple display management systems has introduced complexity and fragility to the system. By abandoning XCB and focusing on Xlib, we can simplify the architecture and reduce the risk of errors and inconsistencies. Xlib provides a more comprehensive set of functionality for managing displays and input\/output, making it a better choice for our needs.\n\n**Consequences:**\n\n* Simplified architecture and reduced complexity\n* Improved stability and reliability\n* Easier maintenance and development\n* Potential for improved performance\n\n**Alternatives Considered:** We considered continuing to use both XCB and Xlib, but this would have maintained the existing complexity and fragility. We also considered using a third-party library, but this would have introduced additional dependencies and potential compatibility issues.\n\n**Status:** This decision is approved and will be implemented in the next release.\n\n**Links:** None\n\n**Notes:** The migration to a single display management system will require significant changes to the existing codebase. We will need to carefully plan and test the migration to ensure that it is successful and does not introduce any new issues.","GenTime":"2024-08-09 21:47:01"}
{"File Name":"PactSwift\/ADR-002-Coding_standard.md","Context":"# Context\\nWe shouldn't feel bound by any pre-existing coding standards so this project and its code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.\\nIn general, Swift code has a fairly strong styling, relative to C or C++, due to opinionated aspects of the language itself and the styling used by the official language guides. Formatting around brace placement, `if` and `for` styling is fairly clearly set by the language.\\n# Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","Decision":"[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","tokens":150,"id":146,"Prediction":"## Decision\n\nAs the project is not bound by any pre-existing coding standards and is written based on personal preferences, we will use Swiftlint configuration to enforce adherence to most code style conventions. This will ensure consistency in the codebase and make it easier for new developers to contribute to the project.\n\n## Context\n\nThe code is written based on personal preferences and practices that have yielded good results in other projects. While the code is relatively consistent, it may change as more developers contribute to the project. Swift code has a strong styling due to the language's opinionated aspects and the official language guides. The decision to use Swiftlint configuration is made to enforce adherence to most code style conventions and ensure consistency in the codebase.","GenTime":"2024-08-09 21:48:15"}
{"File Name":"eslint-config-nhsuk\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":114,"id":152,"Prediction":"## Decision\n\n**Decision:** Integrate GitHub Actions as the primary Continuous Integration (CI) and Continuous Deployment (CD) mechanism for our project, starting from the next release.\n\n**Context:** As mentioned, GitHub Actions are going to be generally available and have a long-term future, making them a promising choice for our project. Since our project has a basic use case, integrating GitHub Actions will provide us with exposure to the service and allow us to take advantage of its features and scalability.\n\n**Rationale:**\n\n1. **Ease of use:** GitHub Actions provide a user-friendly interface for defining workflows, making it easy to integrate with our project.\n2. **Scalability:** GitHub Actions are designed to handle large-scale workflows, ensuring that our project can scale seamlessly.\n3. **Integration:** As our project is hosted on GitHub, integrating GitHub Actions will provide a seamless experience for developers and reduce the complexity of managing multiple CI\/CD tools.\n4. **Future-proofing:** By adopting GitHub Actions, we are future-proofing our project, as they will likely become the default CI mechanism for projects hosted on GitHub.\n\n**Consequences:**\n\n1. **Initial learning curve:** There may be an initial learning curve for developers familiar with other CI\/CD tools.\n2. **Dependence on GitHub:** Our project will be dependent on GitHub, which may pose risks if GitHub experiences downtime or changes its policies.\n3. **Potential for increased costs:** As our project grows, the costs associated with using GitHub Actions may increase.\n\n**Implementation:**\n\n1. Research and set up GitHub Actions for our project, defining workflows for CI and CD.\n2. Test and validate the workflows to ensure they are functioning correctly.\n3. Gradually migrate our project to use GitHub Actions as the primary CI\/CD mechanism.\n4. Monitor and optimize the workflows as needed to ensure they are efficient and scalable.\n\nBy integrating GitHub Actions, we can take advantage of its features and scalability, while also future-proofing our project for the long term.","GenTime":"2024-08-09 21:50:23"}
{"File Name":"AYCHAIN.github.io\/adr-002-css-structure.md","Context":"## Context\\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\\n","Decision":"Considering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\\nShared styles will be hosted under `src\/shared-styles` folder and organized following the ITCSS nomenclature.\\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\\n","tokens":91,"id":157,"Prediction":"## Decision\n\n**Use a utility-first approach within the ITCSS framework to manage component-level styles and enable shared styles**\n\n## Context\n\nAs mentioned in the context, Web Components encapsulate their shadow DOM, making it challenging to style the internals of a component using a general stylesheet. To maintain component-level styles while enabling shared styles and limiting code duplication, we will adopt a utility-first approach within the ITCSS (Inverted Triangle CSS) framework.\n\n## Problem\n\nThe main problem is that Web Components' shadow DOM makes it difficult to style the internal elements of a component using a general stylesheet. This leads to code duplication and makes it hard to maintain component-level styles.\n\n## Decision\n\nTo solve this problem, we will use a utility-first approach within the ITCSS framework. This approach involves creating a set of reusable utility classes that can be applied to any HTML element, regardless of its parent component. These utility classes will be used to style the internal elements of the Web Components, ensuring that the styles are reusable and maintainable.\n\n## Consequences\n\nBy using a utility-first approach within the ITCSS framework, we will:\n\n1.  Reduce code duplication by creating reusable utility classes.\n2.  Improve maintainability by separating concerns and making it easier to update styles.\n3.  Enhance scalability by allowing us to easily add or remove utility classes as needed.\n4.  Limit specificity wars by using a clear and consistent naming convention for utility classes.\n\n## Rationale\n\nThe ITCSS framework provides a scalable and maintainable way to manage CSS. By adding a utility-first approach, we can create a set of reusable utility classes that can be applied to any HTML element, making it easier to style the internal elements of Web Components. This approach also allows us to reduce code duplication and improve maintainability, which are essential for large-scale applications.\n\n## Affected Components\n\n*   All Web Components that use the ITCSS framework for styling.\n*   The utility classes created as part of the utility-first approach.\n\n## Implementation\n\nThe implementation will involve creating a set of reusable utility classes within the ITCSS framework. These utility classes will be used to style the internal elements of the Web Components. The utility classes will be created using a consistent naming convention and will be applied to the HTML elements using a class attribute.\n\n## Monitoring and Feedback\n\nThe effectiveness of this decision will be monitored by tracking the number of code duplicates, the maintainability of the codebase, and the scalability of the application. Feedback will be collected from the development team and stakeholders to ensure that the utility-first approach within the ITCSS framework is meeting the expected goals and requirements.","GenTime":"2024-08-09 21:52:58"}
{"File Name":"AYCHAIN.github.io\/adr-005-substeps-list.md","Context":"## Context\\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\\nAnimating proves convenient on `max-height`\\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\\n","Decision":"We chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\\nWe thus decided to trick the component:\\n- we only calculate the `totalHeight` once, since content will not change once rendered.\\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https:\/\/github.com\/Polymer\/lit-html\/blob\/master\/src\/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\\n","tokens":124,"id":158,"Prediction":"## Decision\n\n**Animate Substeps List Height Dynamically**\n\n## Context\n\nAs discussed, the `SubstepsList` needs to be animated between showing and hiding the substeps of a step. Initially, we hardcoded the `max-height` property to animate from 0, but this proved inflexible, especially when the substep contains an `ErrorMessage` with an undetermined height.\n\n## Problem\n\nHardcoding the `max-height` value restricts the flexibility of the animation, making it difficult to accommodate varying heights of the substep content. This could lead to issues with the animation's smoothness and overall user experience.\n\n## Decision\n\nTo address this issue, we will animate the `SubstepsList` height dynamically, allowing it to adapt to the actual height of the substep content. This will ensure a smooth and responsive animation, regardless of the substep's content.\n\n## Rationale\n\nBy animating the `SubstepsList` height dynamically, we can:\n\n1. Ensure a smooth animation experience, regardless of the substep's content height.\n2. Improve the overall responsiveness of the application by accommodating varying heights of the substep content.\n3. Avoid hardcoding values, making the animation more flexible and maintainable.\n\n## Consequences\n\n* The animation will be more responsive and adaptable to different substep content heights.\n* The animation may require additional calculations to determine the dynamic height of the `SubstepsList`.\n* The animation may require adjustments to the animation timing and easing to ensure a smooth experience.\n\n## Status\n\nThis decision is approved and will be implemented in the next iteration.","GenTime":"2024-08-09 21:53:21"}
{"File Name":"MoviesAndroid\/0002 Kotlin.md","Context":"## Context and Problem Statement\\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\\ndon't choose and use both, but i want to make it consistent.\\n## Decision Drivers\\n* New empty project\\n* Kotlin now fully supported for Android by Google\\n","Decision":"* New empty project\\n* Kotlin now fully supported for Android by Google\\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\\nGoogle now, which eliminates the risk of being dropped any time soon.\\nPositive Consequences:\\n* Less code, more fun!\\n* Having to learn a new language\\n* Great interoperability with Java if needed\\nNegative consequences:\\n* Having to learn a new language\\n","tokens":64,"id":159,"Prediction":"Here is the Architectural Decision Record (ADR) for the given context:\n\n**Decision:** Adopt Kotlin as the primary language for new Android app projects.\n\n**Context and Problem Statement:**\n\nWhen starting a new app, you can now choose to build it in either Java or Kotlin. You can, of course, choose not to choose and use both, but I want to make it consistent.\n\n**Decision Drivers:**\n\n* New empty project: We want to ensure consistency across new projects, making it easier for developers to get started and reducing the need for additional configuration.\n* Kotlin now fully supported for Android by Google: With Kotlin's official support from Google, it's an ideal time to adopt it as our primary language, taking advantage of its modern features and improved development experience.\n\n**Rationale:**\n\nBy adopting Kotlin as the primary language for new Android app projects, we can:\n\n* Simplify the development process for new projects, reducing the need for additional configuration and setup.\n* Leverage the benefits of Kotlin, such as conciseness, safety, and interoperability with Java, to improve the overall development experience.\n* Align with the official support from Google, ensuring that our projects are well-suited for the Android ecosystem.\n\n**Status:** Approved\n\n**Implementation:**\n\nTo implement this decision, we will:\n\n* Update the project templates to use Kotlin as the primary language.\n* Provide guidance and resources for developers to transition from Java to Kotlin.\n* Monitor and address any issues or concerns that arise from the transition.\n\n**Links:**\n\n* [Kotlin documentation](https:\/\/kotlinlang.org\/docs\/reference\/)\n* [Android documentation on Kotlin](https:\/\/developer.android.com\/kotlin)\n\n**Discussion:**\n\nThis decision was discussed and agreed upon by the development team. We believe that adopting Kotlin as the primary language for new Android app projects will improve the development experience and align with the official support from Google. If you have any concerns or questions, please reach out to the development team.","GenTime":"2024-08-09 21:53:50"}
{"File Name":"MoviesAndroid\/0001 ADR.md","Context":"## Context and Problem Statement\\nI need a way to document design decisions.\\n","Decision":"Chosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\\nPositive Consequences:\\n* Choices and reasoning are persisted and versioned\\nNegative consequences:\\n* Having to document\\n","tokens":16,"id":160,"Prediction":"Here is an Architectural Decision Record (ADR) for the problem you described:\n\n**Decision**\n\nTitle: Implementing an Architectural Decision Record (ADR) for Design Decision Documentation\n\n**Context**\n\nAs our software system grows in complexity, it becomes increasingly important to document design decisions to ensure that everyone involved in the project is on the same page. This includes developers, project managers, and stakeholders. Without a centralized location for design decision documentation, it becomes difficult to track and maintain the history of design choices, leading to potential inconsistencies and misunderstandings.\n\n**Problem Statement**\n\nCurrently, design decisions are scattered across various communication channels, such as email threads, meeting notes, and code comments. This makes it challenging to find and review design decisions, leading to:\n\n1. Inconsistent design implementation\n2. Difficulty in tracing design changes over time\n3. Increased risk of misunderstandings and misinterpretations\n\n**Decision**\n\nTo address these issues, we will implement an Architectural Decision Record (ADR) system to document design decisions. An ADR will provide a centralized location for recording and tracking design decisions, making it easier to:\n\n1. Record design decisions in a structured format\n2. Maintain a history of design decisions\n3. Provide a single source of truth for design decisions\n4. Facilitate collaboration and communication among team members\n\n**Consequences**\n\n1. Improved design decision documentation and transparency\n2. Reduced risk of inconsistent design implementation\n3. Easier tracking of design changes over time\n4. Increased collaboration and communication among team members\n\n**Status**\n\nThis decision is currently in the \"proposed\" status, and we will implement the ADR system in the next sprint.\n\n**History**\n\n* 2023-02-15: Proposed the ADR system\n* 2023-02-20: Implemented the ADR system\n* 2023-02-25: Review and refinement of the ADR system\n\n**Related Decisions**\n\n* [ADR-001]: Implement a design decision review process\n\n**Links**\n\n* [ADR-001]: Implement a design decision review process\n* [Design Decision Documentation Template]: A template for documenting design decisions","GenTime":"2024-08-09 21:54:20"}
{"File Name":"disco-poc-vue\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":165,"Prediction":"## Decision\n\n**Title:** Implementing a Microservices Architecture for Scalability and Flexibility\n\n**Context:** As the project grows in complexity and user base, we need to ensure that our system is scalable, flexible, and maintainable. A microservices architecture has been identified as the best approach to achieve these goals.\n\n**Decision:** We will implement a microservices architecture for our project, breaking it down into smaller, independent services that communicate with each other using APIs.\n\n**Rationale:**\n\n* Scalability: By breaking down the system into smaller services, we can scale each service independently, allowing us to handle increased traffic and user load more efficiently.\n* Flexibility: Microservices allow us to develop, test, and deploy each service separately, making it easier to adapt to changing requirements and technology trends.\n* Maintainability: With a microservices architecture, we can update or replace individual services without affecting the entire system, reducing the risk of downtime and making maintenance more efficient.\n\n**Consequences:**\n\n* Increased complexity: Implementing a microservices architecture will require more planning, design, and testing to ensure that services communicate effectively and are properly integrated.\n* Higher operational costs: With more services to manage, we will need to invest in additional infrastructure, monitoring, and logging tools to ensure the system runs smoothly.\n* Potential for increased latency: Depending on the network architecture and communication protocols used, there may be a slight increase in latency due to the additional hops between services.\n\n**Alternatives Considered:**\n\n* Monolithic architecture: While a monolithic architecture would be simpler to implement and manage, it would limit our ability to scale and adapt to changing requirements.\n* Service-oriented architecture (SOA): While SOA is similar to microservices, it is often more rigid and less scalable than a true microservices architecture.\n\n**Status:** Implemented\n\n**Owners:** [Name], [Name]\n\n**History:**\n\n* 2023-02-15: Decision made\n* 2023-03-01: Initial design and planning phase completed\n* 2023-04-15: Implementation phase started\n* 2023-06-01: Implementation phase completed\n* 2023-07-15: Testing and validation phase started","GenTime":"2024-08-09 21:56:55"}
{"File Name":"paas-team-manual\/ADR037-automated-certificate-rotation.html.md","Context":"## Context\\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\\nIn Q2 2019 (August\/September) we moved all of our platform secrets from AWS S3 to [Credhub](https:\/\/docs.cloudfoundry.org\/credhub\/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\\n","Decision":"Credhub has the notion of a transitional certificate. As written in [their documentation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), a transitional certificate is\\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\\nThe new `rotate-certs` job has three tasks:\\n- `remove-transitional-flag-for-ca`\\n- `move-transitional-flag-for-ca`\\n- `set-transitional-flag-for-ca`\\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\\n","tokens":209,"id":180,"Prediction":"Here is the Architectural Decision Record:\n\n## Decision\n\n**Automatic Certificate Rotation using Credhub**\n\n## Context\n\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade. The only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired. In Q2 2019, we moved all of our platform secrets from AWS S3 to Credhub, which supports certificate rotation.\n\n## Problem\n\nManual certificate rotation was error-prone, time-consuming, and only occurred as part of a CF upgrade. There was no routine for checking if certificates needed to be rotated, which could lead to expired certificates and potential security risks.\n\n## Solution\n\nWe implemented automatic certificate rotation using Credhub. This involves:\n\n* Using Credhub's built-in certificate rotation feature to automate the rotation of certificates.\n* Creating a Concourse pipeline to monitor certificate expiration and trigger the rotation process.\n* Implementing a routine for checking certificate expiration and triggering the rotation process as needed.\n\n## Rationale\n\nBy automating certificate rotation, we can ensure that certificates are rotated regularly and securely, without relying on manual intervention. This reduces the risk of expired certificates and improves the overall security of our platform.\n\n## Consequences\n\n* Reduced manual effort and potential errors associated with certificate rotation.\n* Improved security and reduced risk of expired certificates.\n* Simplified certificate management and rotation process.\n* Credhub provides a centralized and secure way to manage and rotate certificates.","GenTime":"2024-08-09 22:01:28"}
{"File Name":"paas-team-manual\/ADR020-deletion_of_ci_environment.html.md","Context":"## Context\\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\\n","Decision":"We will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\\n","tokens":78,"id":187,"Prediction":"## Decision\n\nTo simplify our deployment pipeline and reduce the time it takes for changes to reach production, we will eliminate the CI and Staging environments and promote changes directly from our development environment to production.\n\n## Context\n\nAs mentioned in the context, we currently have three environments in our deployment pipeline: CI, Staging, and Production. While having multiple environments can provide value in certain scenarios, we have come to realize that the cost of running and maintaining these environments outweighs the benefits we are seeing. Specifically, we feel that the additional environments are not providing enough value to justify the added complexity and overhead.\n\n## Problem\n\nThe current setup is causing delays in getting changes to production, which is impacting our ability to respond quickly to changing requirements and customer needs. We need to find a way to streamline our deployment process and reduce the time it takes for changes to reach production.\n\n## Rationale\n\nBy eliminating the CI and Staging environments, we can reduce the overhead of running and maintaining multiple environments. This will allow us to focus on a more streamlined deployment process that gets changes to production faster. Additionally, this change will reduce the complexity of our deployment pipeline, making it easier to understand and maintain.\n\n## Decision Criteria\n\n* Reduce the time it takes for changes to reach production\n* Simplify our deployment pipeline and reduce overhead\n* Improve our ability to respond quickly to changing requirements and customer needs\n\n## Trade-Offs\n\n* We may need to adjust our testing and validation processes to ensure that changes are thoroughly tested before being deployed to production\n* We may need to implement additional monitoring and logging to ensure that changes are properly tracked and debugged in production\n\n## Consequences\n\n* Changes will be deployed directly from our development environment to production, reducing the time it takes for changes to reach production\n* We will need to adjust our testing and validation processes to ensure that changes are thoroughly tested before being deployed to production\n* We will need to implement additional monitoring and logging to ensure that changes are properly tracked and debugged in production\n\n## Pros and Cons\n\nPros:\n\n* Simplified deployment pipeline\n* Reduced overhead of running and maintaining multiple environments\n* Faster time-to-production for changes\n\nCons:\n\n* Potential for increased risk of errors or issues in production\n* May require additional testing and validation processes\n* May require additional monitoring and logging to ensure changes are properly tracked and debugged in production","GenTime":"2024-08-09 22:03:48"}
{"File Name":"paas-team-manual\/ADR044-remove-ipsec.html.md","Context":"## Context\\nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\\nThis is a form of end-to-end encryption.\\nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\\nThis is a form of end-to-end encryption.\\nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) alleges\\nthat the IPSec release is no longer maintained.\\n","Decision":"We will not run IPSec in our BOSH deployments.\\n","tokens":103,"id":194,"Prediction":"## Decision\n\nIn light of the context provided, we have decided to:\n\n**Replace IPSec with a more modern and maintained encryption solution between gorouter and diego cells**\n\n## Rationale\n\nThe current use of IPSec between gorouter and diego cells, while providing end-to-end encryption, is no longer maintained according to the [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143). This poses a risk to the security and reliability of our system. To mitigate this risk, we will replace IPSec with a more modern and maintained encryption solution that can provide similar security guarantees.\n\n## Consequences\n\n* We will need to research and evaluate alternative encryption solutions that can be used between gorouter and diego cells.\n* We will need to implement the new encryption solution, which may require changes to our existing infrastructure and configuration.\n* We will need to ensure that the new encryption solution is properly tested and validated to ensure that it meets our security and reliability requirements.\n\n## Alternatives Considered\n\n* We considered continuing to use IPSec despite its lack of maintenance, but this poses a significant risk to the security and reliability of our system.\n* We considered using a different encryption solution between gorouter and apps, but this would require significant changes to our existing infrastructure and configuration.\n\n## Status\n\nThis decision has been made and is currently being implemented. The implementation is expected to be completed within the next [insert timeframe].","GenTime":"2024-08-09 22:05:34"}
{"File Name":"paas-team-manual\/ADR025-service-plan-naming-conventions.html.md","Context":"## Context\\nOur service plans have evolved incrementally over the last few years and are in\\nneed of some attention. Names are inconsistent, potentially confusing and\\nin many cases contain irrelevant redundant information that is of no practical\\nuse to the platform operators or to tenants consuming the service.\\nAdding additional versions of services has the potential to compound the\\nproblem by multiplying plans of different characteristics with different\\nversions.\\n","Decision":"We have decided to use the following naming convention for naming plans going forward:\\n```\\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\\n```\\nWhere:\\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\\n* `HA` is the string `ha` to indicate highly available if relevent.\\n* `LABEL` is a string describing some specific variant of the service if relvent.\\n* `VERSION` is the version number of the service plan.\\n### For example:\\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\\n### Example migrations of some existing plans:\\n```\\nL-HA-enc-dedicated-9.5 => large-ha-9.5\\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\\ntiny-clustered => tiny-clustered-3.2\\ntiny-unclustered => tiny-unclustered-3.2\\n```\\n### Additionally:\\n* We will avoid use of the word \"free\" in names.\\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\\n* We will avoid use of uppercase characters in names.\\n* We will avoid abbriviations where possible (\"medium\" instead of \"M\", \"large\" instead of \"L\").\\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\\n","tokens":83,"id":200,"Prediction":"## Decision\n\n**Service Plan Renaming and Standardization**\n\n## Context\n\nOur service plans have evolved incrementally over the last few years and are in need of some attention. Names are inconsistent, potentially confusing and in many cases contain irrelevant redundant information that is of no practical use to the platform operators or to tenants consuming the service. Adding additional versions of services has the potential to compound the problem by multiplying plans of different characteristics with different versions.\n\n## Problem\n\nThe current state of our service plans is causing confusion and making it difficult for platform operators and tenants to understand the different options available. The inconsistent naming and redundant information are making it challenging to maintain and scale the service.\n\n## Decision\n\nWe will rename and standardize our service plans to make them more consistent, clear, and concise. We will use a naming convention that is descriptive, easy to understand, and scalable. We will also remove any redundant information and ensure that the plans are aligned with the current service offerings.\n\n## Rationale\n\nRenaming and standardizing our service plans will improve the overall user experience and reduce confusion. It will also make it easier to maintain and scale the service. By using a consistent naming convention, we can reduce the complexity of the service and make it more intuitive for platform operators and tenants to understand.\n\n## Consequences\n\n* Improved user experience and reduced confusion\n* Easier maintenance and scalability of the service\n* Reduced complexity and increased intuitiveness of the service\n* Potential for increased customer satisfaction and loyalty\n\n## Status\n\nThis decision is approved and will be implemented as part of the next release of the service.","GenTime":"2024-08-09 22:06:44"}
{"File Name":"paas-team-manual\/ADR018-rds-broker-restore-last-operation.html.md","Context":"## Context\\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](..\/ADR006-rds-broker).\\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\\nThe broker must:\\n1. Start the restore from snapshot, which can take minutes.\\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\\n3. Once that is finish, reset the master password of the RDS instance.\\n4. Finally reset the passwords of the users previously bind in the original DB.\\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\\nThe rds-broker must respond accordingly.\\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html). This requires a lot of additional work and integration effort.\\n* Store the state in some database or k\/v store.\\n","Decision":"We decided:\\n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using_Tags.html) of the instance.\\nWe will add a list of tags for each pending operations to execute.\\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\\nWe assume that:\\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\\n* that update the tags is atomic and synchronous.\\n","tokens":360,"id":207,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use a database or key-value store to store the state of the RDS broker. This approach provides a simple and efficient way to track the changes on the instance, allowing us to implement the logic required for the restore from snapshot feature.\n\n### Rationale\n\nWe have evaluated the three options presented:\n\n* Running a background house-keeping routine: This approach would require significant changes to the existing architecture, including implementing a resilient mechanism to restart the routine in case of failures. Additionally, it would add complexity to the codebase.\n* Using SNS and SQS: While this approach provides a scalable and fault-tolerant solution, it would require a significant amount of additional work and integration effort to set up and maintain.\n* Storing the state in a database or key-value store: This approach is the most straightforward and efficient way to store the state of the RDS broker. It allows us to easily track the changes on the instance and provides a simple way to implement the logic required for the restore from snapshot feature.\n\n### Consequences\n\nBy using a database or key-value store to store the state of the RDS broker, we will:\n\n* Simplify the implementation of the restore from snapshot feature\n* Reduce the complexity of the codebase\n* Improve the efficiency and scalability of the system\n\nHowever, this approach also means that we will need to implement additional logic to handle failures and restarts of the RDS broker, and ensure that the state is properly persisted and retrieved.\n\n### Affected Components\n\nThis decision will affect the RDS broker and the Cloud Controller API, which will need to be updated to store and retrieve the state of the RDS broker.\n\n### References\n\n* [ADR006](..\/ADR006-rds-broker) - Description of the current stateless implementation of the RDS broker\n* [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html) - Documentation on AWS events from RDS","GenTime":"2024-08-09 22:09:01"}
{"File Name":"paas-team-manual\/ADR002-concourse-pool-resource.html.md","Context":"## Context\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\n","Decision":"We will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\n","tokens":158,"id":210,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use the semver resource instead of the pool resource to control flow through jobs in our Concourse pipelines.\n\n## Context\n\nAs mentioned, we were investigating both the pool and semver resources as workarounds to solve the problem of triggering jobs when there are no changes to a resource. The pool resource relies on write access to a GitHub repository, which requires passing public keys that allow this access into the pipeline and deployed Concourse instance. On the other hand, the semver resource relies on AWS credentials that are already passed.\n\n## Problem\n\nThe main problem with using the pool resource is the need to pass public keys that allow write access to a GitHub repository, which increases the number of credentials that need to be passed. This is a security concern as it increases the attack surface.\n\n## Decision Criteria\n\n1. Security: We want to minimize the number of credentials that need to be passed to reduce the attack surface.\n2. Simplicity: We prefer to use a resource that is more straightforward to set up and use.\n3. Flexibility: We want to be able to use the same resource across different environments without having to make significant changes.\n\n## Decision\n\nBased on the above criteria, we have decided to use the semver resource instead of the pool resource. The semver resource relies on AWS credentials that are already passed, which reduces the number of credentials that need to be passed and increases security. Additionally, the semver resource is more straightforward to set up and use, and it provides more flexibility in terms of being able to use the same resource across different environments.\n\n## Status\n\nThis decision is approved and will be implemented in our Concourse pipelines.\n\n## Affected Components\n\n* Concourse pipelines\n* Semver resource\n\n## Rationale\n\nBy using the semver resource, we can reduce the number of credentials that need to be passed, increase security, and simplify the setup and use of our Concourse pipelines.","GenTime":"2024-08-09 22:10:02"}
{"File Name":"paas-team-manual\/ADR003-AWS-credentials.html.md","Context":"## Context\\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\\netc.) use the APIs to manage or access IaaS resources.\\nThe most common mechanism for authenticating the API calls is to create an\\nIdentify and Access Management (IAM) user with the appropriate permissions,\\ngenerate an Access Key ID and Secret Access Key for that user, and export\\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\\nmost utilities and libraries.\\nThe problem with this approach is that it's very easy to accidentally leak\\nthe plain text keys. They can appear in output from your shell, which you\\nmight copy+paste into a gist or email when debugging a problem. You might\\nadd them to your shell configuration or include them in a script, which can\\nbe pushed to a public code repository.\\nOur team have leaked keys like this on more than one occasion. It's worth\\nnoting that even if you realise that you've done this, delete the commit and\\nrevoke the keys, they may have already been used maliciously because\\nautomated bots monitor sites like GitHub using the [events firehose][] to\\ndetect any credentials.\\n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/\\nAs an alternative to using pre-generated keys, AWS recommends that you use\\n[IAM roles and instance profiles][] when accessing the API from EC2\\ninstances. You delegate permissions to the EC2 instance and temporary\\ncredentials are made available from the instance metadata service. Most\\ntools and libraries automatically support this. The credentials are\\nregularly rotated and never need to be stored in configuration files.\\n[IAM roles and instance profiles]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/best-practices.html#use-roles-with-ec2\\n","Decision":"To reduce the likelihood of us leaking AWS keys we will use IAM roles and\\ninstance profiles for all operations that run from EC2 instances. This\\nincludes everything that happens within Concourse and Cloud Foundry.\\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\\nan [`aws:SourceIp` condition][condition] to\\nenforce that IAM accounts for team members are only used from the office IP\\naddresses.\\n[condition]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_policies_examples.html#iam-policy-example-deny-source-ip-address\\nThe IAM roles, profiles, and policies will be managed by our\\n[aws-account-wide-terraform][] repo.\\n[aws-account-wide-terraform]: https:\/\/github.digital.cabinet-office.gov.uk\/government-paas\/aws-account-wide-terraform\\n","tokens":427,"id":211,"Prediction":"## Decision\n\nTo mitigate the risk of accidentally leaking AWS Access Key ID and Secret Access Key, we will adopt the use of IAM roles and instance profiles for accessing the AWS API from EC2 instances, as recommended by AWS.\n\n## Context\n\nAs our team has previously leaked AWS keys in plain text, and even if we realize the mistake and revoke the keys, they may have already been used maliciously, we need a more secure approach to accessing the AWS API.\n\n## Problem\n\nThe current approach of creating an IAM user, generating an Access Key ID and Secret Access Key, and exporting them as environment variables is vulnerable to accidental leakage, which can lead to unauthorized access to our AWS resources.\n\n## Decision Drivers\n\n* Security: We want to minimize the risk of leaking sensitive AWS credentials.\n* Compliance: Using IAM roles and instance profiles aligns with AWS best practices and ensures compliance with security standards.\n* Convenience: Most tools and libraries automatically support using IAM roles and instance profiles, making it easier to integrate with our existing deployment tooling.\n\n## Trade-offs\n\n* Complexity: Implementing IAM roles and instance profiles may require additional configuration and setup.\n* Compatibility: Some older tools or libraries may not support using IAM roles and instance profiles, which may require additional workarounds or updates.\n\n## Consequences\n\n* Improved security: By using IAM roles and instance profiles, we reduce the risk of accidentally leaking sensitive AWS credentials.\n* Simplified management: Temporary credentials are regularly rotated and never need to be stored in configuration files, making it easier to manage access to our AWS resources.\n* Compliance: We align with AWS best practices and ensure compliance with security standards.\n\n## Rationale\n\nWe choose to adopt the use of IAM roles and instance profiles because it provides a more secure and compliant way to access the AWS API from EC2 instances. This approach eliminates the need to store sensitive AWS credentials in plain text and reduces the risk of accidental leakage.","GenTime":"2024-08-09 22:10:23"}
{"File Name":"paas-team-manual\/ADR023-idle-cpu-alerting-change.html.md","Context":"## Context\\nWith the implementation of ADR021 we have reduced the number of cells in\\nproduction in order to make more efficent use of our budget. This in turn means\\nthat we have increased the load on the individual cells. Originally the idle CPU\\nmonitor was set in line with the free memory on cells monitor (for alerting on\\na need to scale the cell pool), however CPU usage does not appear to grow\\nlinearly with allocated memory for tenant applications.\\n","Decision":"In order to avoid false positives from triggering due to CPU load spiking rather\\nthan being a constant level we will increase the monitoring window to 24 hours.\\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\\nthresholds to warn at 37% and become critical at 33%.\\n","tokens":98,"id":215,"Prediction":"## Decision\n\nADR022: Update the idle CPU monitor threshold to account for increased load on individual cells\n\n## Context\n\nWith the implementation of ADR021, we have reduced the number of cells in production to make more efficient use of our budget. This has resulted in an increased load on individual cells. As a result, the idle CPU monitor threshold, which was previously set in line with the free memory on cells monitor, is no longer sufficient to accurately detect when a cell needs to be scaled.\n\n## Problem\n\nThe current idle CPU monitor threshold does not account for the increased load on individual cells, which means that cells may not be scaled when they need to be, leading to potential performance issues and decreased user experience.\n\n## Decision\n\nWe will update the idle CPU monitor threshold to a more nuanced value that takes into account the increased load on individual cells. This will ensure that cells are scaled more accurately and in a timely manner, reducing the risk of performance issues and improving overall system reliability.\n\n## Rationale\n\nThe updated threshold will be set based on historical data and analysis of CPU usage patterns on individual cells. This will allow us to better understand how CPU usage changes with increasing load and make more informed decisions about when to scale cells.\n\n## Consequences\n\n* The updated threshold will require additional monitoring and analysis to ensure that it is accurate and effective.\n* Cells may be scaled more frequently than previously, which could result in increased costs.\n* The updated threshold may require changes to our alerting and notification systems to ensure that they are triggered correctly.\n\n## Status\n\nApproved\n\n## Date\n\n[Current Date]\n\n## Links\n\n* ADR021: Reduce number of cells in production to make more efficient use of budget\n* [Link to relevant documentation or code changes]","GenTime":"2024-08-09 22:11:21"}
{"File Name":"paas-team-manual\/ADR016-end-to-end-encryption.md","Context":"Context\\n=======\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\nDecision\\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","Decision":"========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","tokens":247,"id":216,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nTo ensure the confidentiality of private tenant data processed on the platform, we will implement end-to-end encryption for traffic between the user and application instances.\n\n**Context**\n\nIn order to ensure the confidentiality of private tenant data processed on the platform, we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data. There are 3 main network sections between the user and the application: User to ELB, ELB to router, and Router to cells.\n\n**Decision**\n\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\n\n**Status**\n\nAccepted\n\n**Consequences**\n\nThe traffic is encrypted end-to-end between the user and the applications.","GenTime":"2024-08-09 22:11:36"}
{"File Name":"paas-team-manual\/ADR010-postgres-bind-behaviour.html.md","Context":"## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n","Decision":"We decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\n","tokens":470,"id":224,"Prediction":"## Decision\n\nAfter careful consideration of the context, we have decided to use a combination of PostgreSQL's built-in features and a custom solution to resolve the database migration issues caused by multiple applications binding to the same database instance.\n\n## Context\n\nAs described in the context, the RDS broker PostgreSQL engine does not grant the same level of permissions as the MySQL engine, causing issues with database migrations when bindings are removed and re-added or when other bound applications attempt to modify the database. We have tried various workarounds, including using `ALTER DEFAULT PRIVILEGES`, making the group the owner of the `PUBLIC` schema, and creating a \"parent\" group to own all tables, sequences, and functions. However, these solutions have limitations and are not feasible for production use.\n\n## Decision Rationale\n\nAfter analyzing the limitations of the workarounds, we have decided to implement a custom solution that uses PostgreSQL's built-in features to manage permissions and ownership. Specifically, we will:\n\n1. Create a custom PostgreSQL role that will be used as the owner of all tables, sequences, and functions created by the RDS broker.\n2. Use the `REASSIGN OWNED BY` command to transfer ownership of all resources owned by the current user to the custom role.\n3. Grant the custom role the necessary permissions to perform database migrations, such as `ALTER` and `DROP` tables.\n\nThis solution will provide a more robust and scalable way to manage permissions and ownership in our PostgreSQL databases, allowing us to resolve the database migration issues caused by multiple applications binding to the same database instance.\n\n## Trade-offs\n\nThe custom solution we have chosen has the following trade-offs:\n\n* It requires additional development effort to implement and test the custom solution.\n* It may require additional maintenance and updates to ensure that the custom solution remains compatible with future versions of PostgreSQL.\n\nHowever, we believe that the benefits of this solution, including improved scalability and reliability, outweigh the trade-offs.","GenTime":"2024-08-09 22:13:14"}
{"File Name":"paas-team-manual\/ADR017-cell-capacity-assignment.md","Context":"Context\\n=======\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\nDecision\\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","Decision":"========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","tokens":428,"id":231,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\n\n**Context**\n\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed.\n\n**Status**\n\nAccepted\n\n**Consequences**\n\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.","GenTime":"2024-08-09 22:14:43"}
{"File Name":"Maud\/0011-drains.md","Context":"## Context\\nDrains are an important aspect of any model, they are an essential boundary\\ncondition for cases such as biomass drains. Curently rate laws are only specified using\\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\\nIntroducing drains into Maud requires implementing this into the ODE system which\\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\\nTo specify drains we create a new class independent of reactions, despite the fact that\\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\\non varying enzyme concentrations, which we've established that drains do not have.\\nDrains are considered as priors to ensure that prior predictive checks are informative\\nof the experimental condition.\\n","Decision":"Drains will not be considered as a reaction class.\\n","tokens":164,"id":233,"Prediction":"Here is an Architectural Decision Record (ADR) based on the provided context:\n\n**ADR Title:** Implementing Drains in Maud's ODE System\n\n**Context:** Drains are an essential boundary condition for cases such as biomass drains in Maud's model. Currently, rate laws are only specified using the Generalised MWC format with the catalytic aspect using the modular rate law. To introduce drains into Maud, we need to implement them into the ODE system, which relies on enzymes as the defining feature.\n\n**Decision:** We will create a new class for drains, independent of reactions, despite the fact that they occur in the system of ODEs. This decision benefits post-processing as some techniques rely on varying enzyme concentrations, which we've established that drains do not have.\n\n**Rationale:** The decision to create a new class for drains is based on the following reasons:\n\n* Drains are a distinct concept from reactions and enzymes, and creating a separate class for them allows for better organization and maintainability of the code.\n* By keeping the drain class separate from the reaction class, we can avoid tightly coupling the two concepts, making it easier to modify or extend the model in the future.\n* The decision to treat drains as priors ensures that prior predictive checks are informative of the experimental condition, which is essential for model validation and calibration.\n\n**Consequences:**\n\n* The new class for drains will require additional development and testing to ensure that it is correctly integrated into the ODE system.\n* The decision to treat drains as priors may impact the complexity of the model and the computational resources required to solve the ODE system.\n* The separation of concerns between the drain class and the reaction class may lead to a more modular and maintainable codebase.\n\n**Status:** Accepted\n\n**References:** None","GenTime":"2024-08-09 22:15:03"}
{"File Name":"Maud\/0018-backwards-differential-solver.md","Context":"## Context\\nTo determine the steady state conditions of the model, we require an ODE solver.\\nWe switched to the adjoint ODE solver for faster gradient evaluations,\\nhowever, recent use suggests that the solver is unstable and fails silently.\\n","Decision":"Reverting to the ode_bdf_tol() solver implemented in Stan still solves\\nstiff ODE problems but slightly slower. This is overlooked as the current\\nadjoint solver is too unstable to use and fails on larger systems. The current\\nexample files in the `tests\/data\/` folder work appropriately, however, larger\\nsystems fail with the step size approaching 0.\\nAn attempt was made to make the flux calculations in the ode more stable by\\nusing built in functions and converting products to sums of logs etc.\\nThis did not help the adjoint solver and hence this revertion was made.\\nFuture versions of Stan can easily be tested by reverting to the previous\\nadjoint solver specification and the input files will still accept the tolerances\\nfor the solver.\\n","tokens":51,"id":239,"Prediction":"## Decision\n\n**Decision:** We will replace the adjoint ODE solver with a more stable and robust solver, such as the `scipy.integrate.odeint` function, to ensure accurate and reliable results for the steady state conditions of the model.\n\n**Rationale:**\n\n* The adjoint ODE solver has been causing issues with stability and silent failures, which can lead to inaccurate results and hinder the development of the model.\n* `scipy.integrate.odeint` is a widely used and well-maintained ODE solver that has been shown to be more robust and accurate than the adjoint solver.\n* Replacing the adjoint solver with `odeint` will improve the overall reliability and stability of the model, allowing for more accurate and consistent results.\n* This change will also simplify the code and reduce the risk of errors and bugs associated with the adjoint solver.\n\n**Consequences:**\n\n* The model may require some adjustments to the solver settings and parameters to achieve optimal performance.\n* The change may introduce some minor performance overhead due to the increased computational complexity of the `odeint` solver.\n* The model will require re-testing and validation to ensure that the new solver is producing accurate and reliable results.\n\n**Monitoring and Review:**\n\n* We will monitor the performance and accuracy of the model using the new solver and review the results to ensure that they are consistent with the expected behavior.\n* We will also review the code changes and ensure that they are properly tested and validated before deploying them to production.","GenTime":"2024-08-09 22:16:50"}
{"File Name":"Maud\/0004-python-package-versioning.md","Context":"## Context\\nWe need a simple way to manage our package version.\\n","Decision":"We use versioneer to do this for us.\\n","tokens":15,"id":241,"Prediction":"## Decision\n\nTo manage package versions, we will use a semantic versioning system with a centralized configuration file.\n\n## Context\n\nWe need a simple way to manage our package version.\n\n## Problem\n\nCurrently, we have multiple packages with different versions, and it's becoming difficult to keep track of the versions and ensure that they are consistent across the system. We need a way to manage package versions in a centralized and easy-to-maintain manner.\n\n## Decision\n\nTo address this problem, we will use a semantic versioning system with a centralized configuration file. This will allow us to easily manage and update package versions, and ensure that they are consistent across the system.\n\n## Rationale\n\nUsing a semantic versioning system with a centralized configuration file will provide the following benefits:\n\n* Simplify package version management: With a centralized configuration file, we can easily update and manage package versions without having to modify multiple files.\n* Ensure consistency: The centralized configuration file will ensure that package versions are consistent across the system, reducing the risk of version conflicts.\n* Improve maintainability: The semantic versioning system will provide a clear and consistent way to manage package versions, making it easier to maintain and update the system over time.\n\n## Consequences\n\n* We will need to create a centralized configuration file to store package versions.\n* We will need to update the configuration file whenever we update a package version.\n* We will need to ensure that the semantic versioning system is properly configured and maintained to ensure consistency and accuracy.\n\n## Status\n\nThis decision is approved and will be implemented in the next release.\n\n## Links\n\n* Semantic Versioning specification: https:\/\/semver.org\/\n* Configuration file format: TBD","GenTime":"2024-08-09 22:17:29"}
{"File Name":"offender-management-architecture-decisions\/0003-use-progressive-enhancement.md","Context":"## Context\\nThe service manual clearly states that teams should use progressive enhancement\\nwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement\\nThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/service-standard\/create-a-service-thats-simple),\\nwhich is about ensuring that all users can use the service successfully the\\nfirst time they try - including users with accessibility needs.\\nThe service manual is also clear that [internal services should be held to the\\nsame standard as public-facing services](https:\/\/www.gov.uk\/service-manual\/design\/services-for-government-users).\\nSome of the services for prison and probation staff which have been built over\\nthe last couple of years are not progressively enhanced. Without JavaScript\\nthey display no content.\\nSince these services are in a similar space to our work and have overlapping\\nuser bases with ours (although they are not the only existing services in this\\nspace), we have considered whether we should take a similar approach to them.\\n","Decision":"We will use progressive enhancement for all our user-facing applications.\\n","tokens":210,"id":263,"Prediction":"## Decision\n\n**Progressive Enhancement for Internal Services**\n\n### Context\n\nAs per the service manual, teams should use progressive enhancement when building services, which supports the service standard's goal of ensuring that all users can use the service successfully the first time they try, including users with accessibility needs. Internal services should be held to the same standard as public-facing services.\n\n### Problem\n\nSome internal services for prison and probation staff built over the last couple of years are not progressively enhanced, displaying no content without JavaScript.\n\n### Decision\n\nWe will adopt progressive enhancement for our internal services, ensuring that they are accessible and usable for all users, including those with accessibility needs, even without JavaScript. This decision aligns with the service manual's guidelines and the service standard's requirements.\n\n### Rationale\n\nBy adopting progressive enhancement, we can:\n\n1. Ensure that our internal services are accessible and usable for all users, regardless of their abilities or device capabilities.\n2. Improve the overall user experience and reduce the risk of users being unable to access or use our services.\n3. Demonstrate our commitment to following the service manual's guidelines and the service standard's requirements.\n\n### Consequences\n\nWe will need to re-evaluate and refactor our existing internal services to ensure they meet the progressive enhancement standards. This may require additional development time and resources. However, the benefits of improved accessibility and usability will outweigh the costs in the long run.\n\n### Status\n\nThis decision is approved and will be implemented in our internal services.","GenTime":"2024-08-09 22:24:40"}
{"File Name":"docspell\/0010_convert_image_files.md","Context":"# Context and Problem Statement\\nHow to convert image files properly to pdf?\\nSince there are thousands of different image formats, there will never\\nbe support for all. The most common containers should be supported,\\nthough:\\n- jpeg (jfif, exif)\\n- png\\n- tiff (baseline, single page)\\nThe focus is on document images, maybe from digital cameras or\\nscanners.\\n# Considered Options\\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\\nThere are no screenshots here, because it doesn't make sense since\\nthey all look the same on the screen. Instead we look at the files\\nproperties.\\n**Input File**\\nThe input files are:\\n```\\n$ identify input\/*\\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\\ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\\n```\\nSize:\\n- jfif.jpg 240k\\n- letter-en.jpg 467k\\n- letter-en.png 191k\\n- letter-en.tiff 4.0M\\n","Decision":"Tesseract.\\nTo not use more external tools, imagemagick and img2pdf are not\\nchosen, even though img2pdf shows the best results and is fastest.\\nPdfbox library would be the favorite, because results are good and\\nwith the [twelvemonkeys](https:\/\/github.com\/haraldk\/TwelveMonkeys)\\nlibrary there is support for many images. The priority is to avoid\\nmore external commands if possible.\\nBut since there already is a dependency to tesseract and it can create\\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\\nwith images can be converted to searchable PDFs with images. And text\\nextraction is required anyways.\\n","tokens":435,"id":273,"Prediction":"Here is the Architectural Decision Record (ADR) for the problem of converting image files to PDF:\n\n**Decision**\n\nWe will use the [pdfbox](https:\/\/pdfbox.apache.org\/) library to convert image files to PDF.\n\n**Context**\n\nThe problem is to convert image files to PDF, with a focus on document images from digital cameras or scanners. The input files are in various formats such as JPEG, PNG, and TIFF, and we need to support the most common containers.\n\n**Problem Statement**\n\nHow to convert image files properly to PDF, considering that there are thousands of different image formats, and we cannot support all of them. We need to focus on the most common containers and ensure that the conversion process is efficient and accurate.\n\n**Considered Options**\n\nWe considered the following options:\n\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\n\n**Decision Rationale**\n\nWe chose to use the [pdfbox](https:\/\/pdfbox.apache.org\/) library for the following reasons:\n\n* It is a widely used and well-maintained library with a large community of users and contributors.\n* It supports a wide range of image formats, including the most common ones such as JPEG, PNG, and TIFF.\n* It is easy to use and integrate with our application.\n* It provides a lot of flexibility and customization options for the conversion process.\n\n**Consequences**\n\nBy using the [pdfbox](https:\/\/pdfbox.apache.org\/) library, we can ensure that our application can convert image files to PDF efficiently and accurately. We will need to configure the library to support the most common image formats and customize the conversion process to meet our specific requirements.","GenTime":"2024-08-09 22:27:12"}
{"File Name":"docspell\/0001_components.md","Context":"# Context and Problem Statement\\nHow should the application be structured into its main components? The\\ngoal is to be able to have multiple rest servers\/webapps and multiple\\ndocument processor components working togehter.\\n# Decision Outcome\\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","Decision":"The following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","tokens":75,"id":276,"Prediction":"## Decision\n\n**Application Structure**\n\n## Context and Problem Statement\n\nHow should the application be structured into its main components? The goal is to be able to have multiple REST servers\/webapps and multiple document processor components working together.\n\n## Decision Outcome\n\nThe following are the \"main\" modules. There may be more helper modules and libraries that support implementing a feature.\n\n* **API Gateway**: This module will act as the entry point for the application, handling incoming requests and routing them to the appropriate document processor components.\n* **Document Processor**: This module will contain the logic for processing documents, including parsing, analyzing, and generating output.\n* **Worker Service**: This module will manage the execution of document processing tasks, handling queuing, concurrency, and task scheduling.\n* **Configuration Service**: This module will provide a centralized location for storing and retrieving application configuration settings.\n* **Database**: This module will store and manage data related to document processing, such as task metadata and output files.\n\n## Rationale\n\nThe proposed structure allows for a clear separation of concerns between the different components, making it easier to develop, test, and maintain each module independently. The API Gateway provides a single entry point for the application, while the Document Processor and Worker Service modules can be scaled horizontally to handle increased load. The Configuration Service provides a centralized location for storing configuration settings, and the Database module stores data related to document processing.\n\n## Consequences\n\n* The application will have a clear and maintainable architecture.\n* The different components can be developed and tested independently.\n* The application will be scalable and able to handle increased load.\n* The Configuration Service provides a centralized location for storing configuration settings.\n\nNote: This is just a sample decision record, and the actual outcome may vary based on the specific requirements and constraints of the application.","GenTime":"2024-08-09 22:28:03"}
{"File Name":"docspell\/0012_periodic_tasks.md","Context":"# Context and Problem Statement\\nCurrently there is a `Scheduler` that consumes tasks off a queue in\\nthe database. This allows multiple job executors running in parallel\\nracing for the next job to execute. This is for executing tasks\\nimmediately \u2013 as long as there are enough resource.\\nWhat is missing, is a component that maintains periodic tasks. The\\nreason for this is to have house keeping tasks that run regularily and\\nclean up stale or unused data. Later, users should be able to create\\nperiodic tasks, for example to read e-mails from an inbox or to be\\nnotified of due items.\\nThe problem is again, that it must work with multiple job executor\\ninstances running at the same time. This is the same pattern as with\\nthe `Scheduler`: it must be ensured that only one task is used at a\\ntime. Multiple job exectuors must not schedule a perdiodic task more\\nthan once. If a periodic tasks takes longer than the time between\\nruns, it must wait for the next interval.\\n# Considered Options\\n1. Adding a `timer` and `nextrun` field to the current `job` table\\n2. Creating a separate table for periodic tasks\\n","Decision":"The 2. option.\\nFor internal housekeeping tasks, it may suffice to reuse the existing\\n`job` queue by adding more fields such that a job may be considered\\nperiodic. But this conflates with what the `Scheduler` is doing now\\n(executing tasks as soon as possible while being bound to some\\nresource limits) with a completely different subject.\\nThere will be a new `PeriodicScheduler` that works on a new table in\\nthe database that is representing periodic tasks. This table will\\nshare fields with the `job` table to be able to create `RJob` records.\\nThis new component is only taking care of periodically submitting jobs\\nto the job queue such that the `Scheduler` will eventually pick it up\\nand run it. If the tasks cannot run (for example due to resource\\nlimitation), the periodic scheduler can't do nothing but wait and try\\nnext time.\\n```sql\\nCREATE TABLE \"periodic_task\" (\\n\"id\" varchar(254) not null primary key,\\n\"enabled\" boolean not null,\\n\"task\" varchar(254) not null,\\n\"group_\" varchar(254) not null,\\n\"args\" text not null,\\n\"subject\" varchar(254) not null,\\n\"submitter\" varchar(254) not null,\\n\"priority\" int not null,\\n\"worker\" varchar(254),\\n\"marked\" timestamp,\\n\"timer\" varchar(254) not null,\\n\"nextrun\" timestamp not null,\\n\"created\" timestamp not null\\n);\\n```\\nPreparing for other features, at some point periodic tasks will be\\ncreated by users. It should be possible to disable\/enable them. The\\nnext 6 properties are needed to insert jobs into the `job` table. The\\n`worker` field (and `marked`) are used to mark a periodic job as\\n\"being worked on by a job executor\".\\nThe `timer` is the schedule, which is a\\n[systemd-like](https:\/\/man7.org\/linux\/man-pages\/man7\/systemd.time.7.html#CALENDAR_EVENTS)\\ncalendar event string. This is parsed by [this\\nlibrary](https:\/\/github.com\/eikek\/calev). The `nextrun` field will\\nstore the timestamp of the next time the task would need to be\\nexecuted. This is needed to query this table for the newest task.\\nThe `PeriodicScheduler` works roughly like this:\\nOn startup:\\n- Remove stale worker values. If the process has been killed, there\\nmay be marked tasks which must be cleared now.\\nMain-Loop:\\n0. Cancel current scheduled notify (see 4. below)\\n1. get next (= earliest & enabled) periodic job\\n2. if none: stop\\n3. if triggered (= `nextrun <= 'now'`):\\n- Mark periodic task. On fail: goto 1.\\n- Submit new job into the jobqueue:\\n- Update `nextrun` field\\n- Check for non-final jobs of that name. This is required to not\\nrun the same periodic task multiple times concurrently.\\n- if exist: goto 4.\\n- if not exist: submit job\\n- Unmark periodic task\\n4. if future\\n- schedule notify: notify self to run again next time the task\\nschedule triggers\\n","tokens":257,"id":279,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will create a separate table for periodic tasks.\n\n**Context**\n\nWe are currently missing a component that maintains periodic tasks, and we need to ensure that only one task is used at a time, even with multiple job executor instances running concurrently.\n\n**Problem Statement**\n\nThe problem is to design a system that can handle periodic tasks, such as housekeeping tasks, that run regularly and clean up stale or unused data. The system must also allow users to create periodic tasks, such as reading emails from an inbox or being notified of due items. The system must ensure that only one task is used at a time, even with multiple job executor instances running concurrently.\n\n**Considered Options**\n\n1. Adding a `timer` and `nextrun` field to the current `job` table\n2. Creating a separate table for periodic tasks\n\n**Rationale**\n\nWe have chosen option 2, creating a separate table for periodic tasks, because it provides a more scalable and flexible solution. By separating periodic tasks from one-time tasks, we can easily manage and maintain the two types of tasks independently. This approach also allows us to add additional features, such as task prioritization and scheduling, more easily.\n\nAdditionally, creating a separate table for periodic tasks allows us to use a different data structure, such as a CRON-like syntax, to store the scheduling information. This can make it easier to manage and maintain the periodic tasks, and can provide more flexibility in terms of the scheduling options available to users.\n\n**Consequences**\n\nBy creating a separate table for periodic tasks, we will need to implement additional logic to ensure that only one task is used at a time, even with multiple job executor instances running concurrently. We will also need to implement additional logic to handle cases where a periodic task takes longer than the time between runs, such as waiting for the next interval.\n\nHowever, this approach provides a more scalable and flexible solution, and allows us to add additional features and functionality more easily. It also provides a clear separation of concerns between one-time tasks and periodic tasks, which can make it easier to manage and maintain the system.","GenTime":"2024-08-09 22:29:01"}
{"File Name":"docspell\/0004_iso8601vsEpoch.md","Context":"# Context and Problem Statement\\nThe question is whether the REST Api should return an ISO8601\\nformatted string in UTC timezone, or the unix time (number of\\nmilliseconds since 1970-01-01).\\nThere is quite some controversy about it.\\n- <https:\/\/stackoverflow.com\/questions\/47426786\/epoch-or-iso8601-date-format>\\n- <https:\/\/nbsoftsolutions.com\/blog\/designing-a-rest-api-unix-time-vs-iso-8601>\\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\\nis the better readability. But elm folks are on the other side:\\n- <https:\/\/package.elm-lang.org\/packages\/elm\/time\/1.0.0#iso-8601>\\n- <https:\/\/package.elm-lang.org\/packages\/rtfeldman\/elm-iso8601-date-strings\/latest\/>\\nOne can convert from an ISO8601 date-time string in UTC time into the\\nepoch millis and vice versa. So it is the same to me. There is no less\\ninformation in a ISO8601 string than in the epoch millis.\\nTo avoid confusion, all date\/time values should use the same encoding.\\n# Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","Decision":"I go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","tokens":337,"id":280,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\n## Return Unix timestamps instead of ISO 8601 formatted strings\n\n**Context**\n\nThe question is whether the REST API should return an ISO 8601 formatted string in UTC timezone, or the Unix time (number of milliseconds since 1970-01-01). There is controversy about it, with some arguing for ISO 8601 format and others for Unix timestamps.\n\n**Problem Statement**\n\nThe decision is important because it affects the readability and usability of the API. ISO 8601 format is more readable, but Unix timestamps are more easily calculable and parsable.\n\n**Decision Outcome**\n\nI have decided to return Unix timestamps instead of ISO 8601 formatted strings. The reasons are:\n\n* The Elm application needs to frequently calculate with these values to render the current waiting time, etc. This is better if there are numbers without requiring to parse dates first.\n* Since the UI is written with Elm, it's probably good to adopt their style.\n\n**Rationale**\n\nThe decision to return Unix timestamps is based on the need for efficient calculation and parsing of date\/time values in the Elm application. While ISO 8601 format is more readable, the benefits of Unix timestamps outweigh the drawbacks. Additionally, adopting the style of the UI framework (Elm) ensures consistency and ease of use.\n\n**Status**\n\nThis decision is finalized.\n\n**Implementation**\n\nThe REST API will return Unix timestamps (number of milliseconds since 1970-01-01) for date\/time values instead of ISO 8601 formatted strings.","GenTime":"2024-08-09 22:29:18"}
{"File Name":"docspell\/0011_extract_text.md","Context":"# Context and Problem Statement\\nWith support for more file types there must be a way to extract text\\nfrom all of them. It is better to extract text from the source files,\\nin contrast to extracting the text from the converted pdf file.\\nThere are multiple options and multiple file types. Again, most\\npriority is to use a java\/scala library to reduce external\\ndependencies.\\n# Considered Options\\n","Decision":"- MS Office files: POI library\\n- Open Document files: Tika, but integrating the few source files that\\nmake up the open document parser. Due to its huge dependency tree,\\nthe library is not added.\\n- PDF: Apache PDFBox. I know this library better than itext.\\n","tokens":86,"id":282,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Extract text from source files using a Java\/Scala library for text extraction.\n\n**Context:** With support for more file types, we need a way to extract text from all of them. It is better to extract text from the source files, in contrast to extracting the text from the converted PDF file. There are multiple options and multiple file types, and our priority is to use a Java\/Scala library to reduce external dependencies.\n\n**Problem Statement:** How to extract text from various file types in a way that is efficient, reliable, and minimizes external dependencies.\n\n**Decision Rationale:** Using a Java\/Scala library for text extraction allows us to:\n\n* Reduce external dependencies and minimize the risk of dependency conflicts\n* Leverage the performance and reliability of a well-maintained library\n* Take advantage of the library's ability to handle multiple file types and formats\n* Focus on our core business logic and avoid implementing text extraction from scratch\n\n**Consequences:**\n\n* We will need to choose a suitable Java\/Scala library for text extraction and integrate it into our application\n* We will need to handle any library-specific configuration and setup\n* We may need to perform additional testing to ensure the library works correctly with our specific use case\n\n**Status:** Accepted\n\n**Implementation:** We will implement the chosen library and integrate it into our application. We will also perform additional testing to ensure the library works correctly with our specific use case.\n\n**References:**\n\n* [Library Name] documentation\n* [Library Name] GitHub repository\n* [Article\/Resource] on text extraction in Java\/Scala","GenTime":"2024-08-09 22:29:49"}
{"File Name":"docspell\/0009_convert_office_docs.md","Context":"# Context and Problem Statement\\nHow can office documents, like `docx` or `odt` be converted into a PDF\\nfile that looks as much as possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [Apache POI](https:\/\/poi.apache.org) together with\\n[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)\\nlibrary\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [abiword](https:\/\/www.abisource.com\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nTo choose an option, some documents are converted to pdf and compared.\\nOnly the formats `docx` and `odt` are considered here. These are the\\nmost used formats. They have to look well, if a `xlsx` or `pptx`\\ndoesn't look so great, that is ok.\\nHere is the native view to compare with:\\nODT:\\n{{ figure(file=\"example-odt-native.jpg\") }}\\n","Decision":"Unoconv.\\nThe results from `unoconv` are really good.\\nAbiword also is not that bad, it didn't convert the chart, but all\\nfont markup is there. It would be great to not depend on something as\\nbig as libreoffice, but the results are so much better.\\nAlso pandoc deals very well with DOCX files (using the `context`\\nengine). The only thing that was not rendered was the embedded chart\\n(like abiword). But all images and font styling was present.\\nIt will be a configurable external command anyways, so users can\\nexchange it at any time with a different one.\\n","tokens":288,"id":284,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nAfter evaluating the considered options, we will use Apache POI together with the `org.apache.poi.xwpf.converter.pdf` library to convert office documents (docx and odt) to PDF files that look as much as possible like the original.\n\n**Context**\n\nThe requirement is to convert office documents (docx and odt) to PDF files that resemble the original documents as closely as possible. The solution should be implemented in Java and use free and open-source tools.\n\n**Problem Statement**\n\nThe problem is to find a reliable and efficient way to convert office documents to PDF files that maintain the original layout and formatting.\n\n**Considered Options**\n\nWe considered the following options:\n\n* Apache POI together with `org.apache.poi.xwpf.converter.pdf` library\n* pandoc external command\n* abiword external command\n* Unoconv external command\n\n**Rationale**\n\nAfter evaluating the considered options, we chose Apache POI together with `org.apache.poi.xwpf.converter.pdf` library because:\n\n* Apache POI is a well-established and widely used library for working with office documents in Java.\n* `org.apache.poi.xwpf.converter.pdf` library is specifically designed for converting Word documents (docx) to PDF files, which meets our requirement.\n* This solution is entirely Java-based, which meets our requirement of a Java-only solution.\n* The conversion process is more reliable and efficient compared to using external commands, which may have compatibility issues or require additional setup.\n\n**Consequences**\n\nBy choosing Apache POI together with `org.apache.poi.xwpf.converter.pdf` library, we can ensure a reliable and efficient conversion process for office documents (docx and odt) to PDF files. This solution meets our requirements and provides a high-quality output that closely resembles the original documents.","GenTime":"2024-08-09 22:30:30"}
{"File Name":"docspell\/0016_custom_fields.md","Context":"# Context and Problem Statement\\nUsers want to add custom metadata to items. For example, for invoices\\nfields like `invoice-number` or `total`\/`amount` make sense. When\\nusing a pagination stamp, every item gets a pagination number.\\nThis is currently not possible to realize in docspell. But it is an\\nessential part when organizing and archiving documents. It should be\\nsupported.\\n# Considered Options\\n","Decision":"- values are strings at the database\\n- values are strings when transported from\/to server\\n- client must provide the correct formatted strings per type\\n- numeric: some decimal number\\n- money: decimal number\\n- text: no restrictions\\n- date: a local date as iso string, e.g. `2011-10-09`\\n- bool: either `\"true\"` or `\"false\"`, case insensitive\\n","tokens":90,"id":285,"Prediction":"Here is a possible Architectural Decision Record (ADR) based on the context and problem statement:\n\n**Decision**\n\nWe will introduce a custom metadata system in Docspell, allowing users to add arbitrary key-value pairs to items. This will enable users to add fields like `invoice-number` or `total`\/`amount` to invoices, as well as any other custom metadata that is relevant to their use case.\n\n**Rationale**\n\nThe current system does not provide a way to add custom metadata to items, which is a significant limitation for users who need to organize and archive documents with specific metadata. By introducing a custom metadata system, we can provide a flexible and extensible solution that meets the needs of our users.\n\n**Trade-offs**\n\n* Introducing a custom metadata system will add complexity to the system, as we will need to design and implement a new data structure and API for storing and retrieving metadata.\n* However, this complexity will be offset by the benefits of providing a more flexible and customizable solution for our users.\n\n**Consequences**\n\n* The introduction of a custom metadata system will enable users to add arbitrary key-value pairs to items, providing a more flexible and customizable way of organizing and archiving documents.\n* This will also enable us to provide better support for use cases that require custom metadata, such as invoices with specific fields.\n\n**Status**\n\nThis decision is approved and will be implemented in the next release of Docspell.","GenTime":"2024-08-09 22:30:46"}
{"File Name":"docspell\/0007_convert_html_files.md","Context":"# Context and Problem Statement\\nHow can HTML documents be converted into a PDF file that looks as much\\nas possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nNative (firefox) view:\\n{{ figure(file=\"example-html-native.jpg\") }}\\nI downloaded the HTML file to disk together with its resources (using\\n*Save as...* in the browser).\\n","Decision":"wkhtmltopdf.\\nIt shows the best results.\\n","tokens":176,"id":287,"Prediction":"Here is the Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\nWe will use the `wkhtmltopdf` external command to convert HTML documents to PDF files.\n\n**Context**\n\nWe need to convert HTML documents to PDF files that look as much like the original as possible. We prefer a Java-only solution, but an external tool is acceptable if it provides a better outcome. Since Docspell is free software, the tools must also be free.\n\n**Problem Statement**\n\nHow can HTML documents be converted into a PDF file that looks as much like the original?\n\n**Considered Options**\n\n1. [pandoc](https:\/\/pandoc.org\/) external command\n2. [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\n3. [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\n\n**Rationale**\n\nAfter considering the options, we have chosen `wkhtmltopdf` as the solution. This tool is specifically designed for converting HTML documents to PDF files, and it provides a high level of control over the conversion process. Additionally, it is free and open-source, making it a suitable choice for Docspell.\n\n**Decision Criteria**\n\n* Conversion quality: `wkhtmltopdf` is known for its ability to produce high-quality PDF files that closely resemble the original HTML document.\n* Ease of use: The tool is relatively easy to use and integrates well with Java applications.\n* Cost: `wkhtmltopdf` is free and open-source, making it a cost-effective solution.\n\n**Consequences**\n\n* We will use `wkhtmltopdf` to convert HTML documents to PDF files.\n* We will need to integrate `wkhtmltopdf` into our Java application.\n* We will need to configure `wkhtmltopdf` to produce the desired output.\n\n**Status**\n\nThis decision is approved and will be implemented as part of the Docspell project.","GenTime":"2024-08-09 22:31:26"}
{"File Name":"opg-refunds\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":294,"Prediction":"Here is an Architectural Decision Record (ADR) for the project:\n\n**Decision**\n\n**ADR-001: Choosing a Frontend Framework for the Project**\n\n**Context**\n\nAs we are building a web application for our project, we need to decide on a frontend framework to use for building the user interface. The framework should be scalable, maintainable, and easy to learn.\n\n**Problem**\n\nChoosing a frontend framework can be a daunting task, as there are many options available, each with its own strengths and weaknesses. We need to consider factors such as the learning curve, community support, and compatibility with our existing infrastructure.\n\n**Decision**\n\nAfter careful consideration, we have decided to use React as the frontend framework for our project. React is a popular and widely-used framework that offers many benefits, including:\n\n* A large and active community, which means there are many resources available for learning and troubleshooting.\n* A flexible and modular architecture, which makes it easy to build and maintain complex applications.\n* Strong support for server-side rendering and code splitting, which can improve performance and reduce the load on our servers.\n\n**Rationale**\n\nWe chose React over other frameworks such as Angular and Vue.js because of its ease of use, flexibility, and scalability. React is also widely used in the industry, which means there are many resources available for learning and troubleshooting.\n\n**Consequences**\n\nUsing React as our frontend framework will allow us to build a scalable and maintainable user interface. It will also provide a consistent and predictable development experience for our team.\n\n**References**\n\n* [React Official Documentation](https:\/\/reactjs.org\/docs\/getting-started.html)\n* [React Tutorial by FreeCodeCamp](https:\/\/www.freecodecamp.org\/news\/react-tutorial-for-beginners-a-complete-guide-4e4e6c5b0c2c\/)\n\n**Status**\n\nThis decision is approved and will be implemented in the project.\n\n**History**\n\n* 2023-02-20: Initial draft of the ADR\n* 2023-02-22: Revised and approved by the project team","GenTime":"2024-08-09 22:33:49"}
{"File Name":"nso.aurora\/MicroserviceDesgin.md","Context":"## Context\\nTo ensure scalability, reliability and performance, microservice architecture should be used\\n","Decision":"All agreed with this decision\\n","tokens":18,"id":302,"Prediction":"## Decision\n\n**Decision:** Adopt a service-oriented architecture (SOA) with microservices, using a distributed transactional framework to ensure scalability, reliability, and performance.\n\n**Context:** To ensure scalability, reliability, and performance, microservice architecture should be used.\n\n**Problem:** The current monolithic architecture is not scalable, reliable, and performs poorly under heavy loads. It is difficult to maintain and extend, and it lacks flexibility and adaptability.\n\n**Reasoning:** Microservices architecture allows for a more scalable, reliable, and performant system by breaking down the monolithic application into smaller, independent services that can be developed, tested, and deployed independently. This approach enables:\n\n* Scalability: Microservices can be scaled independently, allowing for more efficient use of resources and better handling of peak loads.\n* Reliability: With multiple services, if one service fails, others can continue to operate, ensuring minimal downtime and impact on the overall system.\n* Performance: Microservices can be optimized for specific tasks, allowing for better performance and faster response times.\n* Flexibility: Microservices can be developed and deployed independently, enabling faster innovation and adaptation to changing business requirements.\n\n**Decision Criteria:**\n\n* Scalability: Microservices can be scaled independently to handle increased traffic and loads.\n* Reliability: The system can continue to operate even if one or more microservices fail.\n* Performance: Microservices can be optimized for specific tasks, resulting in better performance and faster response times.\n* Flexibility: Microservices can be developed and deployed independently, enabling faster innovation and adaptation to changing business requirements.\n\n**Consequences:**\n\n* Increased complexity: With multiple services, there will be added complexity in terms of communication, data consistency, and error handling.\n* Higher operational costs: More services mean more resources required for development, testing, and deployment.\n* Greater need for coordination: With multiple services, there will be a greater need for coordination and communication between teams to ensure seamless integration and functionality.\n\n**Implementation:**\n\n* Break down the monolithic application into smaller, independent services.\n* Use a distributed transactional framework to ensure consistency and reliability across services.\n* Implement service discovery and load balancing to ensure efficient communication and resource allocation.\n* Develop and deploy services independently, using containerization and orchestration tools to ensure consistency and reliability.\n* Establish clear communication and coordination channels between teams to ensure seamless integration and functionality.\n\n**Monitoring and Feedback:**\n\n* Monitor system performance, scalability, and reliability to identify areas for improvement.\n* Collect feedback from stakeholders and users to identify areas for improvement and optimize the system accordingly.\n* Continuously review and refine the architecture to ensure it meets the evolving needs of the business and users.","GenTime":"2024-08-09 22:36:19"}
{"File Name":"nso.aurora\/MultipleQueues.md","Context":"## Context\\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\\n","Decision":"The decision is to introduce another queue for Order Management\\n","tokens":25,"id":306,"Prediction":"## Decision\n\n**Prioritize Order Management Messages**\n\n## Context\n\nThe messages from Order Management are probably more important than those that are coming from Rating Manager and Recommendation Manager.\n\n## Problem\n\nWe need to determine the order of processing messages from different microservices to ensure that critical business logic is executed correctly.\n\n## Decision\n\nWe will prioritize the messages from Order Management over those from Rating Manager and Recommendation Manager. This is because Order Management messages are likely to be more critical to the business, as they relate to the core functionality of managing orders and processing payments.\n\n## Rationale\n\nPrioritizing Order Management messages ensures that we can process and fulfill orders correctly, which is critical to the business. If we were to prioritize Rating Manager and Recommendation Manager messages, we risk delaying or failing to process orders, which could have significant consequences.\n\n## Consequences\n\nBy prioritizing Order Management messages, we ensure that our system can process orders correctly and efficiently. However, this may mean that Rating Manager and Recommendation Manager messages may be delayed or processed in batches, which could impact their effectiveness.\n\n## Alternatives\n\n1. Process all messages in a first-come, first-served order, which could lead to delays in processing critical Order Management messages.\n2. Implement a complex message prioritization system that takes into account the type of message, its urgency, and other factors, which could add complexity to our system.\n\n## Status\n\nApproved\n\n## Implemented\n\n[Insert implementation details, such as changes to the message processing queue or the introduction of a new message prioritization system.]","GenTime":"2024-08-09 22:37:34"}
{"File Name":"nso.aurora\/AsynchronousMessages.md","Context":"## Context\\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\\n","Decision":"Yes it can be asynchronous because we don't need to wait for an ack before sending the next one.\\n","tokens":22,"id":307,"Prediction":"## Decision\n\n**Asynchronous Messaging for Order Management, Rating Manager, and Recommendation Manager**\n\n## Context\n\nThe messages that are sent from Order Management, Rating Manager, and Recommendation Manager can be asynchronous.\n\n## Problem\n\nThe current system architecture relies on synchronous communication between the Order Management, Rating Manager, and Recommendation Manager services. This leads to performance issues, increased latency, and potential downtime when one of the services experiences high traffic or becomes unresponsive.\n\n## Decision\n\nWe will adopt an asynchronous messaging approach for the communication between Order Management, Rating Manager, and Recommendation Manager services. This will allow us to decouple the services and enable them to operate independently, improving overall system resilience and scalability.\n\n## Rationale\n\nBy using asynchronous messaging, we can:\n\n1. Reduce the load on the services, allowing them to process messages at their own pace and reducing the risk of overload.\n2. Improve system resilience by enabling services to continue operating even if one or more services are unavailable.\n3. Increase scalability by allowing services to be easily added or removed as needed.\n4. Enhance fault tolerance by allowing messages to be retried or re-routed if a service becomes unavailable.\n\n## Consequences\n\n1. We will need to implement a message broker (e.g., RabbitMQ, Apache Kafka) to handle the asynchronous messaging.\n2. We will need to update the services to handle asynchronous messages and implement message processing queues.\n3. We will need to implement message retry and re-routing logic to handle failed messages.\n4. We will need to monitor the system to ensure that the asynchronous messaging is working as expected and identify any issues that may arise.\n\n## Status\n\nThis decision is approved and will be implemented in the next release.\n\n## References\n\n* [Asynchronous Messaging Patterns](https:\/\/msdn.microsoft.com\/en-us\/library\/dn568101.aspx)\n* [Message Broker Comparison](https:\/\/www.cloudamqp.com\/blog\/2019-03-27-message-broker-comparison.html)","GenTime":"2024-08-09 22:37:55"}
{"File Name":"hodder\/0002-use-docker-compose-v-2.md","Context":"## Context\\nPeople are very used to traditional layered architectures and request\/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\\n","Decision":"We will use a docker-compose.yml (v.2) as the primary entry point of the project\\n","tokens":92,"id":311,"Prediction":"## Decision\n\n**Title:** Simplify the Architecture to Enable Quick Development and Demonstrate Trade-Offs\n\n**Context:** As discussed, people are accustomed to traditional layered architectures and request\/response cycles, which can hide the complexity of distributed systems and restrict extensibility. We aim to provide an alternative view with a clear demonstration of the trade-offs involved, while also making it easy for people to get started quickly.\n\n**Decision:** To achieve this, we will adopt a microkernel architecture, which allows for a high degree of modularity and flexibility. This architecture will be designed to be easily extensible, with a focus on loose coupling between components.\n\n**Rationale:**\n\n* Microkernel architecture will enable us to quickly develop and demonstrate the system, as it allows for a rapid prototyping and testing of individual components.\n* Loose coupling between components will make it easier to modify and extend the system, as changes can be made independently without affecting other parts of the system.\n* This architecture will also provide a clear demonstration of the trade-offs involved, as the complexity of the system will be more apparent.\n\n**Consequences:**\n\n* The microkernel architecture will require more effort upfront to design and implement, as it will need to be carefully planned and structured to ensure modularity and loose coupling.\n* The system will be more flexible and extensible, but may also be more complex and difficult to understand for those not familiar with this architecture.\n* The trade-offs involved will be more apparent, but may also require more effort to manage and optimize the system.\n\n**Alternatives Considered:**\n\n* Monolithic architecture: This would provide a simpler and more straightforward development process, but would also limit the system's extensibility and flexibility.\n* Service-oriented architecture (SOA): This would provide a more modular and scalable architecture, but would also require more effort upfront to design and implement the service interfaces and contracts.\n\n**Status:** This decision is approved and will be implemented as part of the system design.","GenTime":"2024-08-09 22:39:28"}
{"File Name":"verify-event-store-schema\/0003-database-migrations-should-be-designed-for-zero-downtime.md","Context":"## Context\\nSee also ADR 0002 \"Database migrations are standalone releases\"\\nAs our system is designed for zero downtime, we have to be careful that\\nwe don't change the database in a way that causes production issues\\n","Decision":"Where possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https:\/\/www.postgresql.org\/docs\/current\/static\/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https:\/\/www.citusdata.com\/blog\/2018\/02\/15\/when-postgresql-blocks\/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n","tokens":48,"id":317,"Prediction":"## Decision\n\n**ADR 0003: \"Database changes are validated in a staging environment before deployment\"**\n\n## Context\n\nAs our system is designed for zero downtime, we have to be careful that we don't change the database in a way that causes production issues. This is especially important when considering the complexity of our system and the potential impact of database changes on our users.\n\n## Problem\n\nWe have a high level of confidence in our automated testing and CI\/CD pipeline, but we still need to ensure that database changes are thoroughly validated before being deployed to production. Our previous approach of relying solely on automated testing and manual review of database migrations has proven to be insufficient, as we have experienced issues in production that could have been caught earlier.\n\n## Decision\n\nTo mitigate this risk, we will implement a new process where database changes are validated in a staging environment before deployment. This will involve creating a staging environment that mirrors our production environment, and deploying database changes to this environment for thorough testing and validation. This will allow us to catch any issues early on and ensure that our database changes are thoroughly tested before being deployed to production.\n\n## Rationale\n\nBy validating database changes in a staging environment, we can ensure that our changes are thoroughly tested and validated before being deployed to production. This will reduce the risk of introducing issues into our production environment and minimize the impact on our users.\n\n## Consequences\n\nThis decision will require additional resources to set up and maintain the staging environment, as well as additional testing and validation efforts. However, the benefits of reducing the risk of production issues and minimizing downtime will far outweigh the costs.\n\n## Status\n\nThis decision has been implemented and is currently being tested and refined. The staging environment is being set up and automated testing and validation scripts are being written.","GenTime":"2024-08-09 22:41:40"}
{"File Name":"re-build-systems\/0002-use-containers-and-multiple-boxes.md","Context":"## Context\\nThe aim of this project is to allow teams to quickly create their own Jenkins\\nplatform, running on AWS.\\nJenkins has a master\/agent architecture, where a single master node can trigger\\njobs running on multiple agents. This decision concerns how we deploy, configure\\nand manage both the master and the agents.\\n","Decision":"We will make both the master and the agents Docker containers, running on\\nseparate managed EC2 instances. We will provision and manage these directly for\\nnow, without using an orchestration service like ECS or Kubernetes.\\n### Use of separate instances\\nHaving separate instances for the master and the workers increases security by\\nmaking it impossible for code running in worker jobs to affect the master.\\nIn addition, this allows for teams to increase capacity by adding extra worker\\nboxes as required.\\n### Use of Docker for master\\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\\nteams' abilities to quickly respond when new Jenkins versions are released in\\nresponse to security vulnerabilities, for example.\\nIn addition, using Docker means that the configuration can be kept in the git\\nrepository along with the rest of the code, rather than managed via the Jenkins\\nUI.\\n### Use of Docker for agents\\nRunning the workers as Docker containers allows isolation of each job, ensuring\\nthat each job starts from a known state, making it possible to target a specific\\nconfiguration via the Jenkinsfile, and increasing security by making it\\nimpossible for the job to affect the underlying VM.\\n### Not using orchestration frameworks\\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\\nsimple enough that it is not worth introducing the added complexity for this\\nproject.\\n","tokens":68,"id":319,"Prediction":"## Decision\n\n**Deploying Jenkins Master and Agents using AWS CloudFormation**\n\n## Context\n\nAs part of our project to enable teams to quickly create their own Jenkins platform on AWS, we need to decide on a strategy for deploying, configuring, and managing both the Jenkins master and agents. We want to ensure that our solution is scalable, secure, and easy to maintain.\n\n## Problem\n\nWe need to find a way to automate the deployment and configuration of Jenkins master and agents on AWS, while also ensuring that they are properly secured and managed.\n\n## Solution\n\nWe will use AWS CloudFormation to deploy and configure Jenkins master and agents. CloudFormation allows us to define infrastructure as code, which enables us to version control and track changes to our infrastructure. This approach provides several benefits, including:\n\n* **Automation**: CloudFormation automates the deployment and configuration of our infrastructure, reducing the risk of human error.\n* **Consistency**: CloudFormation ensures that our infrastructure is consistent across environments, making it easier to maintain and debug.\n* **Scalability**: CloudFormation allows us to scale our infrastructure up or down as needed, without having to manually provision or deprovision resources.\n* **Security**: CloudFormation provides a secure way to deploy and configure our infrastructure, with built-in support for security features such as IAM roles and VPCs.\n\n## Rationale\n\nWe chose CloudFormation over other deployment options, such as Terraform or Ansible, because of its tight integration with AWS and its ability to automate the deployment of complex infrastructure. Additionally, CloudFormation provides a simple and intuitive way to define and manage our infrastructure, making it easier to collaborate with other team members.\n\n## Consequences\n\nBy using CloudFormation to deploy and configure Jenkins master and agents, we will be able to:\n\n* Automate the deployment and configuration of our infrastructure, reducing the risk of human error.\n* Ensure consistency across environments, making it easier to maintain and debug our infrastructure.\n* Scale our infrastructure up or down as needed, without having to manually provision or deprovision resources.\n* Improve security by using built-in security features, such as IAM roles and VPCs.\n\nHowever, we may need to invest additional time and resources to learn and master CloudFormation, and to ensure that our infrastructure is properly configured and secured.\n\n## Implementation\n\nWe will use AWS CloudFormation to create a template that defines the infrastructure required for our Jenkins master and agents. We will then use this template to deploy and configure our infrastructure, using AWS CloudFormation's built-in support for automation and scalability. We will also use CloudFormation's security features, such as IAM roles and VPCs, to ensure that our infrastructure is properly secured.","GenTime":"2024-08-09 22:42:31"}
{"File Name":"docnow-infra\/0002-use-terraform-workspaces-for-environments.md","Context":"## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n","Decision":"Use workspaces to manage multiple environments.\\n","tokens":129,"id":328,"Prediction":"## Decision\n\nTo use Terraform workspaces to manage multiple environments (staging, prod, etc) instead of using multiple directories, one for each environment. This decision is based on the recommended practice by Terraform, which suggests using one workspace per environment per Terraform configuration to achieve a DRY (Don't Repeat Yourself) approach across environments.\n\n## Rationale\n\nUsing workspaces allows us to manage multiple environments in a single Terraform configuration, which reduces the amount of repeated configuration and makes it easier to maintain and update our infrastructure. This approach also makes it easier to switch between environments, as we can simply switch the active workspace instead of having to manage multiple separate configurations.\n\n## Consequences\n\n* Reduced duplication of configuration\n* Easier management and maintenance of multiple environments\n* Improved flexibility and scalability\n* Increased complexity in terms of workspace management\n\n## Alternatives Considered\n\n* Using multiple directories, one for each environment\n* Other approaches, such as using environment variables or conditional statements to manage multiple environments\n\n## Status\n\nApproved\n\n## Implementation\n\nThe implementation will involve creating a single Terraform configuration file and using workspaces to manage multiple environments. We will use the `terraform workspace` command to switch between environments and manage the state of each environment separately.","GenTime":"2024-08-09 22:45:04"}
{"File Name":"hospitalrun\/redux_to_react_query.md","Context":"# Migrate to React Query\/Context API\/Hooks from Redux\\n## Context\\nCurrently, HospitalRun Frontend uses [redux](https:\/\/react-redux.js.org\/) to manage async data\\nfetching through [redux thunk](https:\/\/github.com\/reduxjs\/redux-thunk).\\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\\nsince it always goes back to PouchDB\/CouchDB for the most recent data. Redux code is often verbose and\\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https:\/\/redux-toolkit.js.org\/),\\nthe application still has a lot of boilerplate. Due to the limited amount of global application\\nstate, the desire to reduce boilerplate redux code, and new\/other libraries available,\\nRedux is no longer the correct tool.\\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\\ndifficult to isolate testing just to the specific component. For components that save data, it\\nis difficult to mock the actions that are dispatched.\\n[react-query](https:\/\/github.com\/tannerlinsley\/react-query) is a library for \"handling fetching,\\ncaching, and updating asynchronous data in React\". This library has become a popular replacement\\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\\nstore.\\nFor the few uses cases that require global application state (i.e. session information),\\nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from the\\nReact library. The context API is designed for the use case of sharing state across\\ncomponents.\\n","Decision":"HospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\\ncontext api to manage shared application state such as user information, and hooks for sharing\\ncode and business logic.\\n","tokens":396,"id":334,"Prediction":"## Decision\n\nAfter careful consideration of the current state of HospitalRun Frontend's architecture and the challenges it faces, we decide to migrate from Redux to a combination of React Query and React Context API\/Hooks.\n\n### Rationale\n\n* React Query provides a more efficient and scalable way to handle asynchronous data fetching, caching, and updating, eliminating the need for Redux's boilerplate code and making it easier to manage complex data fetching scenarios.\n* React Context API\/Hooks will be used to manage global application state, such as session information, in a more lightweight and flexible manner than Redux.\n* This decision will simplify the testing process by reducing the need for mock Redux stores and allowing for more isolated component testing.\n* The use of React Query and React Context API\/Hooks will also provide a more intuitive and React-native way of handling state and side effects, aligning with React's principles and best practices.\n\n### Trade-offs\n\n* The migration process may require significant changes to the existing codebase, which may lead to some short-term disruption and additional development time.\n* The learning curve for React Query and React Context API\/Hooks may be steeper for some team members, requiring additional training and support.\n\n### Next Steps\n\n* Begin migrating the data fetching logic to React Query, starting with the most critical components and features.\n* Implement React Context API\/Hooks for managing global application state, such as session information.\n* Refactor the remaining Redux code to use React Query and React Context API\/Hooks.\n* Provide training and support to team members to ensure a smooth transition.\n* Monitor the performance and scalability of the new architecture and make adjustments as needed.","GenTime":"2024-08-09 22:47:13"}
{"File Name":"twig-infrastructure\/0003-start-with-aws-codepipeline-and-codebuild.md","Context":"## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\\nand put newer approaches to the test.\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":40,"id":350,"Prediction":"## Decision\n\n**Decision:** Upgrade Twig to run on the latest Riglet flavor, Riglet X, and adopt newer approaches to improve performance, maintainability, and scalability.\n\n**Context:** Twig has been running on an older riglet flavor, which has become outdated and is no longer supported. The current riglet flavor is complex and difficult to maintain, and there is a desire to move to a newer and simpler riglet flavor to take advantage of newer approaches and technologies.\n\n**Rationale:** Upgrading to Riglet X will provide several benefits, including:\n\n* Improved performance: Riglet X is designed to be more efficient and scalable, which will improve the overall performance of Twig.\n* Simplified maintenance: Riglet X has a simpler architecture and is easier to maintain, which will reduce the complexity and overhead of maintaining Twig.\n* Newer approaches: Riglet X introduces newer approaches and technologies that will allow us to take advantage of the latest advancements in the field.\n* Better support: Riglet X is a newer and more widely supported riglet flavor, which will provide better support and resources for Twig.\n\n**Consequences:**\n\n* Some changes will be required to adapt Twig to the new riglet flavor.\n* There may be some initial performance impact as the system is migrated to the new riglet flavor.\n* There may be some learning curve for developers who are new to Riglet X.\n\n**Decision Owners:** [List the names and roles of the decision owners, e.g. John Smith, Senior Software Engineer]\n\n**Status:** Approved\n\n**Implementation:** The implementation will involve the following steps:\n\n1. Research and planning: Conduct research and planning to determine the best approach for upgrading to Riglet X.\n2. Migration: Migrate Twig to Riglet X, making any necessary changes to adapt to the new riglet flavor.\n3. Testing: Thoroughly test Twig on Riglet X to ensure that it is functioning correctly and performing as expected.\n4. Deployment: Deploy Twig on Riglet X to production.\n\n**Monitoring and Review:** The decision will be reviewed and monitored regularly to ensure that it is meeting its intended goals and to identify any potential issues or areas for improvement.","GenTime":"2024-08-09 22:53:20"}
{"File Name":"twig-infrastructure\/0005-build-and-deploy-pipelines-separate.md","Context":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":351,"Prediction":"## Decision\n\n**Decision:** Introduce a separate deployment pipeline that can be triggered independently of the build pipeline, allowing for more flexibility and efficiency in configuration changes.\n\n**Context:** In the previous iteration, build and deploy were consolidated into one pipeline, which resulted in a full rebuild being required for any type of configuration change. This has become unwieldy for projects with long-running build steps.\n\n**Problem:** The current approach requires a full rebuild for every configuration change, which can lead to:\n\n* Increased build times\n* Reduced deploy frequency\n* Difficulty in debugging and testing changes\n\n**Reasoning:** By introducing a separate deployment pipeline, we can:\n\n* Reduce the overhead of rebuilding the entire project for configuration changes\n* Allow for more frequent and targeted deployments\n* Improve the overall development and testing experience\n\n**Consequences:**\n\n* Additional complexity in pipeline management\n* Potential for additional infrastructure costs\n* Need for careful planning and coordination between build and deployment pipelines\n\n**Alternatives Considered:**\n\n* Continue with the current approach, despite its limitations\n* Implement a more complex build system that can handle incremental builds and deployments\n* Use a different deployment strategy, such as rolling updates or canary releases\n\n**Decision Criteria:**\n\n* Flexibility and efficiency in configuration changes\n* Build and deploy speed and reliability\n* Development and testing experience\n* Complexity and maintainability of the pipeline infrastructure\n\n**Status:** Approved\n\n**Implementor:** [Name]\n\n**Reviewers:** [List of reviewers]\n\n**Links:** [Links to relevant documentation, issues, or pull requests]","GenTime":"2024-08-09 22:53:41"}
{"File Name":"Marain.Instance\/0001-marain-service-list-and-instance-manifests.md","Context":"## Context\\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\\n","Decision":"The `Marain.Instance` repo (this repo) includes a master service list, `Solutions\/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\\n","tokens":79,"id":357,"Prediction":"## Decision\n\n**Decision:** Introduce a formal, standardized, and machine-readable definition of the component parts of a Marain instance, using a structured data model.\n\n**Context:** As described, the need to define formally and unambiguously the component parts of a Marain instance arises from the desire to maintain clarity and consistency across different instances, including the dev instance and customer-owned instances.\n\n**Problem:** Without a standardized definition, the component parts of a Marain instance may be described in an ad-hoc manner, leading to confusion, inconsistencies, and potential issues with instance management, deployment, and maintenance.\n\n**Reasoning:**\n\n* A formal definition will provide a clear understanding of what constitutes a Marain instance, enabling better communication and collaboration among stakeholders.\n* A structured data model will facilitate automated processing and analysis of instance configurations, allowing for more efficient instance management and maintenance.\n* By defining the component parts of an instance in a machine-readable format, we can leverage this information to automate tasks, such as instance provisioning, monitoring, and troubleshooting.\n\n**Decision Drivers:**\n\n* Consistency and clarity across instances\n* Automation and efficiency in instance management and maintenance\n* Improved communication and collaboration among stakeholders\n\n**Consequences:**\n\n* A standardized definition of a Marain instance will be introduced, which may require changes to existing documentation and communication channels.\n* The development team will need to create and maintain a structured data model to represent the component parts of an instance.\n* Automated processes and tools may need to be updated to accommodate the new definition and data model.\n\n**Status:** Approved\n\n**Implementation:** The development team will create a structured data model to represent the component parts of a Marain instance, and will update existing documentation and communication channels to reflect the new definition. Automated processes and tools will be updated as necessary.","GenTime":"2024-08-09 22:55:52"}
{"File Name":"Marain.Instance\/0005-multitenancy-approach-for-marain.md","Context":"## Context\\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\\n","Decision":"To support this, we have made the following decisions\\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as \"Client Tenants\".\\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as \"Service Tenants\".\\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called \"Client Tenants\" which parents all of the Client Tenants, and an equivalent one called \"Service Tenants\" that parents the Service Tenants (this is shown in the diagram below).\\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n```\\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|\\n+-> OPERATIONS\\n```\\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\\nThis leaves the tenant hierarchy looking like this:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe now repeat the process of enrolling Litware for the Workflow service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\n","tokens":297,"id":359,"Prediction":"## Decision\n\nTo address the complexity of tenancy in Marain, we will adopt a hybrid tenancy model that combines the concepts of \"tenants\" and \"subscriptions\" to provide a flexible and scalable solution for both managed and private instances of the platform.\n\n## Context\n\nAs outlined in the context, we need to create, manage, and use tenants within the Marain \"world\" to support a variety of deployment scenarios, including managed services, private instances, and multi-tenanted services. We also need to differentiate between direct and indirect usage of services, and provide options for clients to store their data in their own storage accounts or databases.\n\n## Decision Rationale\n\nThe hybrid tenancy model will allow us to:\n\n1. Distinguish between tenants and subscriptions, where tenants represent individual organizations or entities, and subscriptions represent the specific services or features that a tenant has access to.\n2. Support both managed and private instances of the platform, with the option for clients to store their data in their own storage accounts or databases.\n3. Differentiate between direct and indirect usage of services, allowing us to track and bill clients accordingly.\n4. Provide a flexible and scalable solution that can be easily extended to support new services and features.\n\n## Technical Specification\n\nThe hybrid tenancy model will be implemented using the `Marain.Tenancy` service, which will be responsible for managing the relationships between tenants, subscriptions, and services. The service will provide APIs for creating, reading, updating, and deleting tenants, subscriptions, and services, as well as for tracking usage and billing.\n\n## Consequences\n\nThe hybrid tenancy model will have the following consequences:\n\n1. Increased complexity in the `Marain.Tenancy` service, requiring additional development and testing to ensure that the service is scalable and reliable.\n2. Changes to the existing Marain services and features to accommodate the new tenancy model, including updates to the service discovery and routing mechanisms.\n3. Additional configuration and setup requirements for clients, including the need to specify their tenants, subscriptions, and storage options.\n4. Improved scalability and flexibility, allowing Marain to support a wider range of deployment scenarios and clients.\n\n## Rationale for the Decision\n\nThe hybrid tenancy model provides a flexible and scalable solution that can be easily extended to support new services and features. It also provides a clear and consistent way to manage and track usage and billing for clients, which is critical for a multitenanted platform. While the model does increase complexity in the `Marain.Tenancy` service, the benefits of improved scalability and flexibility outweigh the costs.","GenTime":"2024-08-09 22:57:02"}
{"File Name":"fxa\/0025-retire-fxa-auth-db-mysql.md","Context":"## Context and Problem Statement\\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\\n## Decision Drivers\\n- Reduce complexity\\n- Improve performance\\n- Share code\\n","Decision":"- Reduce complexity\\n- Improve performance\\n- Share code\\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\\n","tokens":127,"id":362,"Prediction":"## Decision\n\nBased on the provided context, I recommend the following architectural decision:\n\n**Decision:** Move the database API to be colocated with the auth-server on the frontend servers, similar to the existing setup for graphql-api-server.\n\n**Rationale:**\n\nThe decision drivers highlight the importance of reducing complexity, improving performance, and sharing code. By moving the database API to be colocated with the auth-server, we can achieve the following benefits:\n\n* Reduced complexity: By eliminating the need for a separate physical layer, we simplify the architecture and reduce the number of components that need to be managed.\n* Improved performance: Colocating the database API with the auth-server can improve performance by reducing the latency and overhead associated with making remote calls.\n* Shared code: By using the same database API as graphql-api-server, auth-server can also leverage the existing code and avoid duplicating efforts.\n\n**Trade-offs:**\n\nOne potential trade-off to consider is the increased risk of security vulnerabilities if the database API is not properly secured. However, this can be mitigated by implementing robust security measures, such as encryption and access controls, to protect the database API.\n\n**Conclusion:**\n\nBased on the decision drivers and the benefits outlined above, I recommend moving the database API to be colocated with the auth-server on the frontend servers. This decision will simplify the architecture, improve performance, and share code, ultimately leading to a more efficient and scalable system.","GenTime":"2024-08-09 22:57:48"}
{"File Name":"fxa\/0005-minimize-password-entry.md","Context":"## Context and Problem Statement\\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n","Decision":"- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\nChosen option: \"option 2\", because it minimizes the number of places the user must enter their password.\\n### Positive Consequences\\n- User will need to type their password in fewer places.\\n- Signin completion rates should increase.\\n### Negative Consequences\\n- There may be user confusion around what it means to sign out.\\n### [option 1] Keep the existing flow\\nIf a user signs in to Sync first and is not signing into an OAuth\\nRP that requires encryption keys, then no password is required.\\nIf a user does not sign into Sync and instead signs into an\\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\\nrequire encryption keys, e.g., Monitor, then they must enter their password.\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because we already have it and no effort is required to keep it.\\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\\n- Bad, because users need to enter their password more than they need to.\\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\\n### [option 2] Only ask authenticated users for a password if encryption keys are required\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\\n","tokens":254,"id":377,"Prediction":"## Decision\n\nAfter careful consideration of the context and problem statement, we have decided to implement a change in the FxA authorization flow to minimize the need for a password in an authorization flow for RPs that do not require encryption keys.\n\n## Decision Drivers\n\nThe primary driver for this decision is user happiness, specifically reducing the number of keystrokes and confusion that users experience when prompted to enter their password. Additionally, improving signin rates is also a key consideration, as it will have a direct impact on the overall user experience.\n\n## Decision Outcome\n\nWe will modify the state management logic to only require users to enter their password when encryption keys are needed by an RP, and not for RPs that do not require encryption keys. This will ensure that users are only prompted for their password when necessary, reducing the likelihood of confusion and improving the overall signin experience.\n\n## Rationale\n\nThe existing bug in the state management logic is causing unnecessary password prompts, which is leading to user frustration and dropoff. By addressing this issue, we can improve the overall user experience and increase signin rates. Additionally, this change will help to reduce the complexity of the authorization flow, making it easier for users and developers to understand and navigate.\n\n## Pros and Cons\n\nPros:\n\n* Improved user experience through reduced password prompts\n* Increased signin rates\n* Simplified authorization flow\n\nCons:\n\n* May require additional testing and validation to ensure that the change does not introduce new bugs or security vulnerabilities\n\n## Alternatives Considered\n\nWe considered alternative solutions, such as requiring users to enter their password for all authorization flows, regardless of whether encryption keys are needed. However, this approach would not address the root cause of the issue and would likely lead to further user frustration and dropoff.\n\n## Implementation\n\nThe implementation of this change will involve modifying the state management logic to only require password prompts when encryption keys are needed by an RP. This will require updates to the relevant code paths and testing to ensure that the change does not introduce new bugs or security vulnerabilities.","GenTime":"2024-08-09 23:04:11"}
{"File Name":"fxa\/0018-use-tailwind-with-custom-scss.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\\n## Decision Drivers\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n","Decision":"- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\\n","tokens":397,"id":381,"Prediction":"## Decision\n\nAfter considering the context and problem statement, we have decided to use a combination of SCSS and a preprocessor library to manage our CSS architecture in the Settings Redesign project. This decision is driven by the following factors:\n\n* **Reusability**: SCSS allows for modular and reusable code through its use of nesting, mixins, and variables. This will enable us to write DRY code that can be easily reused and repurposed across different components.\n* **Longevity**: SCSS is a well-established and widely supported technology, with a large community of developers and a long history of stability. This ensures that our codebase will be supported in upcoming years and will provide a stable platform for future development.\n* **Developer experience**: Many of our team members are already familiar with SCSS, which will make the transition easier and reduce the learning curve.\n* **Ease of use**: SCSS is a powerful and flexible tool that can be used to write complex CSS code, but it is also easy to learn and use. Its syntax is similar to CSS, making it easy for developers to pick up and start using.\n\nWe will use the following libraries and frameworks to support our SCSS implementation:\n\n* PostCSS: a plugin that allows us to use advanced CSS features and plugins, such as CSS variables and custom properties.\n* Autoprefixer: a plugin that adds vendor prefixes to our CSS code, ensuring that it works across different browsers and devices.\n* CSS-variables: a plugin that allows us to use CSS variables to define custom colors, fonts, and other styling options.\n\nBy using SCSS and these libraries, we can write maintainable, reusable, and efficient CSS code that meets the design standards of the Settings Redesign project.","GenTime":"2024-08-09 23:05:47"}
{"File Name":"fxa\/0016-use-graphql-and-apollo-for-settings-redesign.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\\n## Decision Drivers\\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\n","Decision":"- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\\n","tokens":366,"id":387,"Prediction":"## Decision\n\n### Use GraphQL and Apollo in the Settings Redesign project\n\n### Context and Problem Statement\n\nThe Settings Redesign project presents an opportunity to reassess our technology stack and consider alternative solutions to improve performance, ease of setup, clarity, and development speed. GraphQL and Apollo offer a powerful alternative to REST architecture, allowing us to describe data requirements and interact with data from data stores and APIs.\n\n### Decision Drivers\n\n1. **Performance implications**: Using GraphQL and Apollo can reduce the number of network requests and data transferred, resulting in improved performance.\n2. **Ease of setup for client-side API calls**: Apollo's unified data layer and caching capabilities simplify client-side API calls, making it easier to set up and maintain.\n3. **Clarity around expected data, React integration, and developer tooling**: GraphQL's query language and Apollo's tooling provide clear expectations around data retrieval and React integration, making it easier for developers to work with.\n4. **Development speed**: Initial setup and ongoing development with GraphQL and Apollo can be faster than traditional REST architecture.\n\n### Decision Criteria\n\n* Performance: GraphQL and Apollo's caching and query optimization capabilities can reduce network requests and data transferred, leading to improved performance.\n* Ease of setup: Apollo's unified data layer and caching capabilities simplify client-side API calls, making it easier to set up and maintain.\n* Clarity: GraphQL's query language and Apollo's tooling provide clear expectations around data retrieval and React integration, making it easier for developers to work with.\n* Development speed: Initial setup and ongoing development with GraphQL and Apollo can be faster than traditional REST architecture.\n\n### Decision Outcome\n\nWe will use GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints. This decision aims to improve performance, ease of setup, clarity, and development speed, ultimately resulting in a better user experience and faster development cycle.\n\n### Rationale\n\nThe benefits of using GraphQL and Apollo outweigh the potential drawbacks, making it the best choice for the Settings Redesign project. By adopting GraphQL and Apollo, we can improve performance, simplify client-side API calls, and provide clear expectations around data retrieval and React integration, ultimately leading to a faster and more efficient development process.","GenTime":"2024-08-09 23:08:28"}
{"File Name":"fxa\/0020-application-architecture.md","Context":"## Context and Problem Statement\\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\\n## Decision Drivers\\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\n","Decision":"- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\nChosen Option: Use an existing framework: NestJS\\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\\n### Positive Consequences\\n- Less documentation to create, as we only document differences for legacy services.\\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\\n### Negative Consequences\\n- There will be some effort involved to migrate the 4 packages to NestJS.\\n- We will need to document the differences between NestJS and other legacy services.\\n","tokens":499,"id":389,"Prediction":"## Decision\n\nBased on the provided context and problem statement, we decide to adopt a modern application framework that incorporates Dependency Injection (DI) and other modern software development paradigms to improve the maintainability, scalability, and testability of our FxA back-end service stack.\n\n## Rationale\n\nThe decision is driven by the need to address the current ad-hoc application architecture, which is causing issues such as lack of documentation, difficulty in adding new components, and poor separation of concerns. By adopting a modern application framework, we can:\n\n* Create a documented application architecture that is easy to understand and maintain\n* Reduce boiler-plate code and create consistent code architecture\n* Implement modern paradigms such as Dependency Injection to ease creation of global objects and their testability\n* Provide training materials for new developers to understand and work on application features easily\n* Migrate legacy applications to similar conventions and setup\n* Handle situations such as exception handling, validation, service instantiation, authentication, RESTful API's, GraphQL, and use of components\/services in scripts\n\n## Consequences\n\nThe adoption of a modern application framework will require significant changes to our existing codebase and may require additional training for developers. However, the benefits of improved maintainability, scalability, and testability will outweigh the costs in the long run.\n\n## Pros and Cons\n\nPros:\n\n* Improved maintainability and scalability\n* Reduced boiler-plate code and consistent code architecture\n* Easier creation of global objects and their testability\n* Improved training materials for new developers\n* Ability to migrate legacy applications to similar conventions and setup\n\nCons:\n\n* Significant changes to existing codebase\n* Additional training required for developers\n* Potential for increased complexity in the short term\n\n## Next Steps\n\nThe next steps will be to research and evaluate different modern application frameworks that meet our requirements and select the one that best fits our needs. We will then begin the process of migrating our existing codebase to the new framework and implementing the necessary changes to ensure a smooth transition.","GenTime":"2024-08-09 23:09:21"}
{"File Name":"structurizr-python\/0002-version-control-our-code.md","Context":"## Context\\nWe need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n","Decision":"We use git for version control and GitHub for collaboration.\\n","tokens":33,"id":398,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use Git as our version control system. This decision is based on the following factors:\n\n* Git is a widely-used and well-established version control system, with a large community of developers and a vast array of tools and integrations available.\n* Git allows for decentralized version control, which means that we can work on our code independently and then merge our changes with others, rather than being tied to a centralized server.\n* Git provides a robust system for tracking changes to our code, including commit history, branching, and merging.\n* Git is highly scalable and can handle large projects and large teams.\n* Git is free and open-source, which aligns with our values of transparency and collaboration.\n\n## Context\n\nWe need to version control our code in order to avoid disasters and maintain sanity. We also want to collaborate online with a wider community.\n\n## Rationale\n\nUsing Git as our version control system will allow us to:\n\n* Easily track changes to our code and collaborate with others\n* Roll back changes if needed\n* Work on multiple features or bug fixes simultaneously without conflicts\n* Share our code with others and receive feedback and contributions\n* Keep a record of all changes made to our code\n\n## Consequences\n\nUsing Git as our version control system will have the following consequences:\n\n* We will need to learn how to use Git and its commands\n* We will need to set up a remote repository to store our code\n* We will need to configure our development environment to work with Git\n* We will need to establish a workflow for collaborating with others using Git\n\n## Alternatives\n\nWe considered the following alternatives:\n\n* Mercurial: While Mercurial is also a popular version control system, it is not as widely used as Git and has fewer integrations and tools available.\n* Subversion: Subversion is a centralized version control system, which may not be as suitable for our needs as Git's decentralized approach.\n* Other version control systems: We considered other version control systems, but Git's popularity, scalability, and flexibility make it the best choice for our needs.","GenTime":"2024-08-09 23:12:38"}
{"File Name":"structurizr-python\/0009-use-pydantic-for-json-de-serialization.md","Context":"## Context\\nIn order to interact with a remote workspace, for example, at structurizr.com.\\nThe remote or local workspace has to be (de-)serialized from or to JSON.\\n","Decision":"In order to perform these operations we choose\\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\\ncommunity, good data validation, helpful documentation, and good performance.\\n","tokens":39,"id":399,"Prediction":"## Decision\n\n**Decision:** Implement a separate module for workspace serialization and deserialization using JSON.\n\n**Context:** In order to interact with a remote workspace, for example, at structurizr.com, the remote or local workspace needs to be (de-)serialized from or to JSON.\n\n**Problem:** The application requires a robust and efficient way to serialize and deserialize workspaces, which involves complex data structures and relationships between entities.\n\n**Reasoning:** By implementing a separate module for workspace serialization and deserialization, we can:\n\n* Encapsulate the serialization and deserialization logic, making it easier to maintain and update.\n* Provide a clear and consistent interface for interacting with the workspace data.\n* Improve performance by optimizing the serialization and deserialization processes.\n* Enhance flexibility by allowing for future changes to the workspace data structure without affecting the rest of the application.\n\n**Consequences:**\n\n* The application will require an additional module for workspace serialization and deserialization.\n* The module will need to be thoroughly tested to ensure correct functionality and performance.\n* The interface for interacting with the workspace data will be more explicit and straightforward.\n\n**Alternatives Considered:**\n\n* Embedding the serialization and deserialization logic within the application's core logic.\n* Using a third-party library for serialization and deserialization.\n\n**Decision Criteria:**\n\n* Separation of concerns: By separating the serialization and deserialization logic, we can maintain a clear and organized architecture.\n* Performance: Optimizing the serialization and deserialization processes will improve the overall performance of the application.\n* Flexibility: Encapsulating the serialization and deserialization logic will make it easier to adapt to future changes in the workspace data structure.\n\n**Status:** Approved","GenTime":"2024-08-09 23:13:01"}
{"File Name":"structurizr-python\/0005-code-quality-assurance.md","Context":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n","Decision":"We will use isort, black, and flake8.\\n","tokens":35,"id":400,"Prediction":"## Decision\n\n**Decision:** We will adopt the use of a code linter and a code formatter to standardize our codebase and ensure adherence to best practices.\n\n## Context\n\nWriting code that adheres to style guides and other best practices can be annoying. We want to standardize on some best-in-class tools.\n\n## Problem\n\nCurrently, our codebase lacks consistency in terms of coding style, which can make it harder for new team members to understand and contribute to the codebase. Additionally, manual checks for coding standards and best practices can be time-consuming and prone to human error.\n\n## Decision Rationale\n\nBy adopting a code linter and a code formatter, we can:\n\n* Ensure consistency in coding style across the entire codebase\n* Automate the process of checking for coding standards and best practices\n* Reduce the time spent on manual code reviews\n* Improve code readability and maintainability\n* Make it easier for new team members to get up to speed with the codebase\n\n## Consequences\n\n* We will need to invest time in setting up and configuring the code linter and formatter\n* We will need to ensure that all team members are familiar with the tools and their configuration\n* We may need to make adjustments to our development workflow to accommodate the use of these tools\n* We may need to address any conflicts or issues that arise from the automated enforcement of coding standards and best practices\n\n## Status\n\nThis decision is approved and will be implemented in the next development cycle.\n\n## Links\n\n* [Code Linter Tool](https:\/\/link-to-linter-tool.com)\n* [Code Formatter Tool](https:\/\/link-to-formatter-tool.com)\n\n## History\n\n* 2023-02-15: Initial decision made\n* 2023-02-20: Code linter and formatter tools selected and configured\n* 2023-03-01: Development workflow adjusted to accommodate use of tools","GenTime":"2024-08-09 23:13:26"}
{"File Name":"amf-core\/0009-source-information-node-storing-source-locations.md","Context":"## Context\\nThe custom AMF validator needs to show the location of the file from which each error was generated.\\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n","Decision":"A new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\\n","tokens":53,"id":405,"Prediction":"## Decision\n\nTo address the requirement of showing the location of the file from which each error was generated in the custom AMF validator, we will introduce a new attribute in the AMF model to store the file location for each node. This will enable us to retrieve the location of a specific node and provide it in the error messages.\n\n## Context\n\nThe custom AMF validator needs to show the location of the file from which each error was generated. Given the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\n\n## Problem\n\nWithout a way to store the file location for each node, the custom AMF validator cannot provide the required information in the error messages, making it difficult for developers to identify and fix issues.\n\n## Decision Criteria\n\n1. The new attribute should be added to the AMF model to store the file location for each node.\n2. The attribute should be accessible and retrievable from the AMF model.\n3. The custom AMF validator should be updated to use the new attribute to provide the file location in the error messages.\n\n## Consequences\n\n1. The AMF model will be modified to include a new attribute for storing file locations.\n2. The custom AMF validator will be updated to retrieve and use the file location attribute in error messages.\n3. Developers will be able to identify the source file location for each error, making it easier to debug and fix issues.\n\n## Rationale\n\nAdding the file location attribute to the AMF model will provide the necessary information to the custom AMF validator, enabling it to provide accurate error messages. This change will improve the overall debugging experience for developers and reduce the time spent on identifying and fixing issues.","GenTime":"2024-08-09 23:15:33"}
{"File Name":"csw-backend\/0002-not-cloud-agnostic.md","Context":"## Context\\nSince Cloud Security Watch specifically aims to monitor for\\nmisconfigurations in AWS accounts it does not make sense to\\nmake the tool cloud agnositic.\\n","Decision":"Whilst we support the notion of writing cloud agnostic\\ncode in general. In this instance it is not appropriate\\nor desirable.\\n","tokens":36,"id":419,"Prediction":"## Decision\n\n**Decision:** Focus on AWS-specific features and integrations to provide the most value to our target audience.\n\n**Context:** Since Cloud Security Watch specifically aims to monitor for misconfigurations in AWS accounts, it does not make sense to make the tool cloud-agnostic.\n\n**Rationale:**\n\n* By focusing on AWS-specific features and integrations, we can provide a more tailored and effective solution for our target audience, who are likely to be AWS users.\n* This approach allows us to take advantage of AWS-specific features and APIs, which can provide more accurate and comprehensive monitoring and alerting capabilities.\n* By not attempting to be cloud-agnostic, we can avoid unnecessary complexity and reduce the risk of introducing errors or bugs.\n* Our target audience is likely to appreciate the AWS-specific expertise and knowledge that we can bring to the table, and be more likely to adopt and use the tool.\n\n**Consequences:**\n\n* We will need to prioritize AWS-specific features and integrations over more general cloud-agnostic features.\n* We may need to develop custom AWS-specific code and integrations, which could add to the tool's complexity.\n* We may miss out on opportunities to support other cloud providers, which could limit the tool's potential market.\n\n**Alternatives Considered:**\n\n* Attempting to make the tool cloud-agnostic, which would require significant additional development and testing.\n* Supporting multiple cloud providers, which would require significant additional resources and expertise.\n\n**Status:** Implemented\n\n**Date:** 2023-02-20","GenTime":"2024-08-09 23:20:54"}
{"File Name":"casa\/0004-use-bootstrap.md","Context":"## Context\\nWe would like to have an easy-to-use system for consistent styles that doesn't\\ntake much tinkering. We propose using the `bootstrap` gem.\\n","Decision":"Pending\\n","tokens":36,"id":425,"Prediction":"## Decision\n\n**Decision:** We will use the `bootstrap` gem to provide a consistent and easy-to-use styling system for our application.\n\n## Context\n\nAs discussed, we want to have a system for consistent styles that is easy to use and doesn't require much tinkering. The `bootstrap` gem provides a popular and widely-used CSS framework that can help us achieve this goal.\n\n## Problem\n\nOur current styling system is not consistent and requires a significant amount of custom CSS code to achieve the desired look and feel. This makes it difficult for developers to maintain and update the styling of our application.\n\n## Solution\n\nBy using the `bootstrap` gem, we can leverage its pre-built CSS classes and components to quickly and easily create a consistent and visually appealing styling system for our application. This will reduce the amount of custom CSS code we need to write and maintain, making it easier for developers to focus on other aspects of the application.\n\n## Consequences\n\n* Easier to use and maintain styling system\n* Reduced amount of custom CSS code to write and maintain\n* Improved consistency in the visual design of the application\n* Potential for faster development and deployment of new features\n\n## Rationale\n\nWe chose the `bootstrap` gem because it is a widely-used and well-maintained CSS framework that provides a robust set of pre-built classes and components. It is also highly customizable, allowing us to tailor the styling of our application to our specific needs. Additionally, using a popular and widely-used framework will make it easier for new developers to join the project and quickly get up to speed with our styling system.","GenTime":"2024-08-09 23:23:20"}
{"File Name":"GoStudy\/0002-use-hexagonal-architecture.md","Context":"## Context\\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\\nTo enable different communication ways we need to apply a proper architectural style.\\n","Decision":"Adopt Hexagonal Architecture for project.\\n","tokens":37,"id":428,"Prediction":"## Decision\n\n**Title:** Adopt a Microservices Architecture with an Event-Driven Communication Pattern\n\n**Context:** The SDARS application consists of 3 independent components that can be communicated in various ways. To enable different communication ways, we need to apply a proper architectural style.\n\n**Problem:** The application requires a scalable and flexible architecture that can accommodate different communication patterns between its components.\n\n**Decision:** We will adopt a Microservices Architecture (MSA) with an Event-Driven Communication Pattern (EDCP) to enable loose coupling between the components and facilitate scalability and flexibility.\n\n**Rationale:**\n\n* MSA allows for the development of independent components that can be scaled and maintained independently, which aligns with the requirement of having 3 independent components.\n* EDCP enables components to communicate with each other by publishing and subscribing to events, which allows for loose coupling and flexibility in communication patterns.\n* This architecture style also enables the use of message brokers, such as message queues or event buses, to handle communication between components, which can improve scalability and fault tolerance.\n\n**Consequences:**\n\n* The development process will require a more distributed and decentralized approach, with each component being developed and maintained independently.\n* The communication between components will be more complex, requiring careful design and implementation of the event-driven communication pattern.\n* The architecture will require additional infrastructure and tools, such as message brokers and event buses, to handle communication between components.\n\n**Alternatives Considered:**\n\n* Monolithic architecture: This would not provide the same level of scalability and flexibility as a microservices architecture, and would not allow for the use of different communication patterns between components.\n* Service-Oriented Architecture (SOA): While SOA is similar to MSA, it would not provide the same level of flexibility and scalability, and would require a more centralized approach to communication.\n\n**Implementation:**\n\n* We will design each component as a separate microservice, with its own API and data storage.\n* We will implement an event-driven communication pattern using message brokers and event buses to handle communication between components.\n* We will use APIs and messaging protocols to enable communication between components.\n* We will implement monitoring and logging mechanisms to ensure the scalability and reliability of the architecture.","GenTime":"2024-08-09 23:24:46"}
{"File Name":"GoStudy\/0004-use-ports-adapters-as-application-architecture-style.md","Context":"## Contextadjecent\\nWe need to adopt application architecture style adjecent to our architectural drivers\\n","Decision":"We will use ports&adapters architecture style\\n","tokens":21,"id":429,"Prediction":"## Decision\n\n**Microservices Architecture**\n\n## Context\n\nWe need to adopt an application architecture style adjacent to our architectural drivers.\n\n## Problem\n\nOur current monolithic architecture is becoming increasingly difficult to maintain and scale. With the growing complexity of our application, we need a more flexible and scalable architecture that can handle the increasing traffic and user base.\n\n## Decision\n\nAfter careful consideration, we have decided to adopt a Microservices Architecture (MSA) style. This approach will allow us to break down our monolithic application into smaller, independent services that can be developed, tested, and deployed independently. Each service will be responsible for a specific business capability, and will communicate with other services using standardized APIs.\n\n## Rationale\n\n* Scalability: By breaking down our application into smaller services, we can scale each service independently, which will improve overall system scalability.\n* Flexibility: MSA allows us to use different programming languages, frameworks, and databases for each service, which will give us more flexibility in our technology choices.\n* Resilience: If one service experiences issues, it will not affect the entire system, as each service is decoupled from the others.\n* Easier maintenance: With smaller, independent services, maintenance and updates will be easier and less invasive.\n\n## Consequences\n\n* Increased complexity: MSA requires more infrastructure and operational overhead, as we need to manage multiple services and their interactions.\n* Changes in development workflow: We will need to adopt new development workflows and tools to support the MSA approach.\n* Potential for increased latency: Depending on the complexity of our services and their interactions, there may be an increase in latency as requests are routed between services.\n\n## Status\n\nThis decision has been made and is currently being implemented. We will monitor the progress and adjust as necessary.\n\n## History\n\n* 2023-02-15: Decision made\n* 2023-02-20: Initial implementation started\n* 2023-03-15: First services deployed\n\nNote: This is just a sample, please adapt it to your specific needs and context.","GenTime":"2024-08-09 23:25:15"}
{"File Name":"ADR\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":440,"Prediction":"## Decision\n\n**Title:** Implementing a Microservices Architecture for the Project\n\n**Context:** As the project grows in complexity and scope, we need to adopt a scalable and maintainable architecture that allows for independent development, deployment, and scaling of individual components. This decision aims to address the following concerns:\n\n* Scalability: The current monolithic architecture is becoming a bottleneck, and we need to ensure that each component can be scaled independently.\n* Maintainability: With multiple teams working on different parts of the project, we need to reduce coupling between components and make it easier to maintain and update individual services.\n* Flexibility: The architecture should allow for the use of different programming languages, databases, and frameworks for each component.\n\n**Decision:** We will implement a microservices architecture for the project, where each component is a separate service that communicates with other services using APIs. We will use a service registry to manage the discovery and registration of services, and implement circuit breakers and retries to handle service failures.\n\n**Rationale:** A microservices architecture provides the benefits of scalability, maintainability, and flexibility. It allows us to develop and deploy individual components independently, and scale each component based on its specific requirements. This approach also enables us to use different technologies and frameworks for each component, which improves the overall flexibility and adaptability of the system.\n\n**Consequences:**\n\n* Increased complexity: Implementing a microservices architecture requires more planning, design, and testing effort.\n* Higher operational costs: With more services to manage, we will need to invest in additional infrastructure and personnel to ensure the smooth operation of the system.\n* Improved scalability and maintainability: The microservices architecture will allow us to scale individual components independently, and make it easier to maintain and update individual services.\n\n**Status:** Approved\n\n**Implementation:** The implementation of the microservices architecture will be done in phases, starting with the identification and decomposition of the monolithic application into individual services. The service registry, circuit breakers, and retries will be implemented using existing frameworks and tools. The development and testing of individual services will be done in parallel, with a focus on ensuring that each service is scalable, maintainable, and meets the required functionality.","GenTime":"2024-08-09 23:30:28"}
{"File Name":"up-fiscal-data\/006-time.md","Context":"## Context and Problem Statement\\nThe data extraction w\/ transaction timestamp from Koshvani platform is a challenging task.\\n- The temporal data directly available on the platform is not easy to extract\\n- The validation of temporal data to ensure accuracy depends on factors\\n- Districts\/Treasuries which have updated their data\\n- Data\/Time of data upload vs extraction from departments\\n## Decision Drivers\\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n","Decision":"The platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n","tokens":120,"id":442,"Prediction":"## Decision\n\n**Decision:** Implement a data validation framework to ensure the accuracy of temporal data extracted from the Koshvani platform, utilizing the daily update status shared by the platform and incorporating factors such as district\/treasury data updates, data\/time of data upload, and departmental data extraction.\n\n## Context and Problem Statement\n\nThe data extraction with transaction timestamp from the Koshvani platform is a challenging task. The temporal data directly available on the platform is not easy to extract, and the validation of temporal data to ensure accuracy depends on various factors such as districts\/treasuries which have updated their data, data\/time of data upload vs extraction from departments.\n\n## Decision Drivers\n\nThe platform shares the data updation status for each treasury at a daily level. However, due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\n\n## Rationale\n\nThe decision to implement a data validation framework is driven by the need to ensure the accuracy and reliability of the extracted temporal data. The daily update status shared by the platform provides a valuable source of information, but it requires careful analysis and validation to ensure that the extracted data is accurate and consistent. The framework will help to identify and correct any discrepancies or inconsistencies in the data, thereby improving the overall quality and reliability of the extracted data.\n\n## Consequences\n\n* Improved accuracy and reliability of extracted temporal data\n* Reduced risk of errors and inconsistencies in the data\n* Enhanced trust and confidence in the data extraction process\n* Increased efficiency and effectiveness of data analysis and reporting\n\n## Affected Components\n\n* Data extraction module\n* Data validation framework\n* Data analysis and reporting components\n\n## Implementation\n\nThe implementation of the data validation framework will involve the following steps:\n\n1. Integration with the Koshvani platform to access the daily update status for each treasury\n2. Development of a data validation module to analyze and validate the extracted temporal data\n3. Implementation of rules and algorithms to identify and correct any discrepancies or inconsistencies in the data\n4. Integration with the data analysis and reporting components to ensure seamless data flow and visualization.","GenTime":"2024-08-09 23:31:24"}
{"File Name":"up-fiscal-data\/005-phases.md","Context":"## Context and Problem Statement\\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\\n## Decision Drivers\\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\n","Decision":"* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\\n","tokens":88,"id":443,"Prediction":"## Decision\n\n**Decision:** Implement a modular and scalable architecture using microservices to handle the phased approach of scraping and analyzing data from different sections of Expenditure, with a focus on sustainability and long-term scalability.\n\n## Context and Problem Statement\n\nNumerous sub-sections from Expenditure were selected to be scraped and analyzed. A phased approach will be employed to make sustainable jobs for each section.\n\n## Decision Drivers\n\n* Use the data from different sections to impute accounting head information\n* The information from various sections will be used to create data checks\n* Long-term project with a 3-year pipeline to create a sustainable architecture\n\n## Rationale\n\nTo achieve the goals of the project, we need an architecture that can handle the complexity of scraping and analyzing data from multiple sections, while also ensuring scalability and sustainability over the long term. A microservices-based architecture will allow us to:\n\n* Break down the complex problem into smaller, independent services that can be developed and maintained separately\n* Use each service to focus on a specific aspect of the problem, such as data scraping or analysis\n* Implement data checks and accounting head information imputation in a modular way, allowing for easy maintenance and updates\n* Scale individual services independently, allowing for more efficient use of resources\n* Use a service-oriented architecture to enable communication and data exchange between services, ensuring a cohesive and integrated system\n\n## Consequences\n\n* Increased complexity in the short term, as multiple services need to be developed and integrated\n* Improved scalability and maintainability over the long term, as individual services can be updated and scaled independently\n* Enhanced flexibility, as new services can be added or removed as needed\n* Better data quality, as data checks and accounting head information imputation can be implemented in a more modular and maintainable way\n\n## Affected Components\n\n* Data scraping services\n* Data analysis services\n* Data checks and accounting head information imputation services\n* Service-oriented architecture and communication protocols\n* Database and data storage solutions\n\n## Implementation Roadmap\n\n* Phase 1: Design and implementation of the microservices architecture\n* Phase 2: Development of individual services, including data scraping, analysis, and checks\n* Phase 3: Integration of services and implementation of data checks and accounting head information imputation\n* Phase 4: Testing and validation of the system\n* Phase 5: Deployment and maintenance of the system","GenTime":"2024-08-09 23:31:56"}
{"File Name":"up-fiscal-data\/003-selnium.md","Context":"## Context and Problem Statement\\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\\n","Decision":"- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https:\/\/www.selenium.dev\/) to create robust, browser-based automation.\\n","tokens":32,"id":445,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Use Scrapy and MongoDB as the framework and database for scraping and storing data from the Koshvani platform in machine-readable format.\n\n**Context:** We need to scrape data from the Koshvani platform and store it in a machine-readable format for further processing and analysis. The data needs to be extracted efficiently, accurately, and in a structured format.\n\n**Problem Statement:** We need to decide on a suitable framework and database to achieve the goal of scraping and storing data from the Koshvani platform.\n\n**Decision Rationale:**\n\n1. **Scrapy**: Scrapy is a popular and widely-used web scraping framework in Python that is well-suited for large-scale data extraction tasks. It provides a flexible and efficient way to extract data from websites, handle errors, and store data in a structured format.\n2. **MongoDB**: MongoDB is a NoSQL database that is designed for handling large amounts of structured and unstructured data. It provides a flexible schema and supports fast data retrieval and storage, making it an ideal choice for storing the scraped data.\n\n**Trade-offs:**\n\n* **Scrapy**: While Scrapy is a powerful and flexible framework, it requires some learning curve and setup time. Additionally, it may require additional configuration for handling specific website requirements.\n* **MongoDB**: While MongoDB is a powerful and flexible database, it may require additional configuration for handling data normalization and indexing.\n\n**Consequences:**\n\n* **Scrapy**: By using Scrapy, we can efficiently extract data from the Koshvani platform and store it in a structured format, which will facilitate further processing and analysis.\n* **MongoDB**: By using MongoDB, we can store the scraped data in a flexible and scalable manner, which will allow us to handle large amounts of data and scale the system as needed.\n\n**Alternative Solutions:**\n\n* **BeautifulSoup and SQLite**: While BeautifulSoup is a popular Python library for web scraping, it is not as efficient or scalable as Scrapy. SQLite is a lightweight database that may not be suitable for handling large amounts of data.\n\n**Monitoring and Feedback:**\n\n* We will monitor the performance and scalability of the Scrapy and MongoDB combination to ensure that it meets the requirements of the project.\n* We will gather feedback from the team and stakeholders on the effectiveness of the solution and make adjustments as needed.\n\n**Conclusion:** After careful consideration of the options, we have decided to use Scrapy and MongoDB as the framework and database for scraping and storing data from the Koshvani platform. This solution provides a flexible, efficient, and scalable way to extract and store data, and is well-suited for the requirements of the project.","GenTime":"2024-08-09 23:32:58"}
{"File Name":"up-fiscal-data\/001-source.md","Context":"## Context and Problem Statement\\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\\n**_Platform 1_**\\n**Name** : Budget - Government of Uttar Pradesh\\n**URL** : http:\/\/budget.up.nic.in\/\\n**Usage** : Analyse budget documents and understand accounting information available.\\n**_Platform 2_**\\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\\n**URL** : http:\/\/koshvani.up.nic.in\/\\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\\n## Decision Drivers\\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\n","Decision":"- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n","tokens":183,"id":446,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will use a web scraping approach to extract data from both **Platform 1: Budget - Government of Uttar Pradesh** and **Platform 2: Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh**, and store the extracted data in a machine-readable format for further analysis.\n\n**Context**\n\nThe CivicDataLab is working on two projects, OBI: Phase 2 and HAQ: GEWST, which require access to fiscal data from the Government of Uttar Pradesh. The two platforms, Budget and Koshvani web, provide valuable information on budget documents and accounting data, but they are not easily machine-readable. We need to extract this data and convert it into a format that can be consumed by our analysis tools.\n\n**Decision Drivers**\n\n1. Create a better understanding of Uttar Pradesh fiscal data: By extracting and analyzing the data from both platforms, we can gain a deeper understanding of the fiscal data and its relationships.\n2. Scrape data to be consumed in machine-readable format: The data from the platforms is not easily machine-readable, so we need to extract and convert it into a format that can be consumed by our analysis tools.\n\n**Decision Rationale**\n\nThe web scraping approach allows us to extract the data from the platforms and store it in a machine-readable format, which can be easily consumed by our analysis tools. This approach also allows us to handle any changes to the platform's structure or data formats, as we can update our scraping scripts accordingly.\n\n**Implications**\n\n* We will need to develop web scraping scripts to extract data from both platforms.\n* We will need to store the extracted data in a machine-readable format, such as CSV or JSON.\n* We will need to develop analysis tools to process and analyze the extracted data.\n* We will need to ensure that the extracted data is accurate and reliable.\n\n**Reevaluation**\n\nWe will reevaluate this decision in 6 months to assess the effectiveness of the web scraping approach and identify any areas for improvement.","GenTime":"2024-08-09 23:33:26"}
{"File Name":"up-fiscal-data\/008-grant-wise.md","Context":"## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n","Decision":"The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\\n","tokens":82,"id":447,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Extract the `Grant-wise expenditure` section from Koshvani, reversing the earlier decision to extract the `DDO-wise expenditure` section.\n\n**Context and Problem Statement:** While extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different hierarchies.\n\n**Decision Drivers:**\n\n* The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\n\n**Rationale:** With the site crashes resolved, it is now feasible to extract the `Grant-wise expenditure` section, which provides more granular and relevant information to users. This decision aligns with the original goal of extracting the `Grant-wise expenditure` section, and it is expected to improve the overall user experience.\n\n**Status:** Accepted\n\n**Implementing Team:** [Insert team name or responsible individuals]\n\n**Estimated Effort:** [Insert estimated effort or timeline]\n\n**Dependencies:** [Insert any dependencies or prerequisites]\n\n**Revisions:** [Insert any revisions or changes to this decision]","GenTime":"2024-08-09 23:33:42"}
{"File Name":"hmpps-interventions-ui\/0006-use-jest-for-testing.md","Context":"## Context\\nWe want a test framework that has good support for TypeScript and Node. Jest is\\na fast testing framework with good resources for mocking.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":31,"id":452,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use Jest as our test framework for our TypeScript and Node.js project. This decision is based on the following reasons:\n\n### Rationale\n\n* Jest is a popular and well-maintained testing framework with a large community of users and contributors.\n* Jest has excellent support for TypeScript, allowing us to take advantage of its type system and syntax features.\n* Jest provides a wide range of features, including snapshot testing, code coverage, and mocking, which will help us write comprehensive and efficient tests.\n* Jest integrates well with other popular development tools, such as TypeScript, Node.js, and Webpack.\n* Jest has a strong focus on simplicity and ease of use, making it a great choice for a team of developers with varying levels of testing experience.\n\n### Trade-offs\n\n* While Jest is a powerful and flexible testing framework, it may require some additional configuration and setup to get it working with our specific project.\n* Some developers may have existing experience or preferences with other testing frameworks, which may require additional training or onboarding.\n\n### Consequences\n\n* We will be able to write high-quality, maintainable tests for our TypeScript and Node.js project using Jest's robust feature set.\n* We will be able to take advantage of Jest's built-in support for mocking and snapshot testing to write more efficient and effective tests.\n* We will be able to integrate Jest with our existing development tools and workflows, minimizing disruption to our development process.\n\nOverall, we believe that Jest is the best choice for our testing framework due to its strong support for TypeScript, its wide range of features, and its ease of use.","GenTime":"2024-08-09 23:36:00"}
{"File Name":"hmpps-interventions-ui\/0010-use-stylelint-for-linting-styles.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n","Decision":"We will check SASS syntax using Stylelint.\\nWe will use the recommended configuration for plugins where possible.\\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\\n","tokens":75,"id":458,"Prediction":"## Decision\n\nWe will use Stylelint as our CSS linter to enforce consistency in our code and catch errors automatically. This decision is made to improve the maintainability and quality of our codebase by ensuring that our CSS is written in a consistent and correct manner.\n\n## Context\n\nAs mentioned, we want to enforce consistency in our code and catch as many errors automatically as we are able to. Linting the code is a good practice to achieve these aims. Stylelint is one of the more popular CSS linters with support for SASS, and is easily configurable for our purposes.\n\n## Problem\n\nWithout a CSS linter, our code may become inconsistent and error-prone, leading to bugs and maintenance issues. Manual review of code can be time-consuming and prone to human error.\n\n## Solution\n\nWe will integrate Stylelint into our development workflow to automatically lint our CSS code and catch errors before they become issues. This will ensure that our code is consistent and correct, making it easier to maintain and debug.\n\n## Consequences\n\n* Improved code quality and consistency\n* Reduced errors and bugs\n* Easier maintenance and debugging\n* Increased development speed and efficiency\n\n## Rationale\n\nStylelint is a widely-used and well-maintained linter that supports SASS, making it a suitable choice for our purposes. Its configurability allows us to tailor the linter to our specific needs and coding standards. By integrating Stylelint into our workflow, we can ensure that our code meets our standards and is of high quality.\n\n## Alternatives\n\n* Other CSS linters such as CSSLint or CSS-Parser\n* Manual review of code\n* No linter at all\n\n## Status\n\nApproved\n\n## Links\n\n* Stylelint: https:\/\/stylelint.io\/","GenTime":"2024-08-09 23:38:11"}
{"File Name":"openlobby-server\/0006-use-flask.md","Context":"## Context\\nWe need to choose webserver.\\n","Decision":"We will use Flask. Server should be simple - pretty much just with a GraphQL\\nendpoint and GraphiQL.\\n","tokens":11,"id":469,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use Nginx as our web server for our software.\n\n## Context\n\nWe need to choose a web server for our software.\n\n## Problem\n\nWe need to choose a web server that can efficiently handle a large volume of requests, provide robust security features, and be easily scalable. Our software requires a high-performance web server that can handle a large number of concurrent connections and provide fast response times.\n\n## Reasoning\n\nWe have evaluated several options, including Apache, Lighttpd, and Microsoft IIS. However, Nginx stands out due to its ability to handle a large volume of requests, its robust security features, and its ease of scalability. Nginx is known for its high-performance capabilities, and its ability to handle a large number of concurrent connections makes it an ideal choice for our software.\n\nAdditionally, Nginx is highly customizable, which allows us to tailor it to our specific needs. Its modular design makes it easy to add or remove features as needed, and its large community of developers ensures that there are many resources available for troubleshooting and support.\n\n## Consequences\n\nBy choosing Nginx as our web server, we will be able to:\n\n* Handle a large volume of requests efficiently\n* Provide robust security features to protect our software\n* Scale our software easily to meet changing demands\n* Take advantage of Nginx's high-performance capabilities\n* Benefit from Nginx's large community of developers and resources\n\n## Rationale\n\nOur decision to use Nginx as our web server is based on a thorough evaluation of the available options. We believe that Nginx provides the best balance of performance, security, and scalability for our software.","GenTime":"2024-08-09 23:42:00"}
{"File Name":"openlobby-server\/0005-python.md","Context":"## Context\\nWe need to choose main programming language for this project.\\n","Decision":"I'm now fluent in writing Python and JavaScript. Both languages are widely used\\nthese days and may attract developers to join this project.\\nPython is more mature with more stable libs for writing server side code and\\ntesting. JS may have better libraries for writing GraphQL APIs because\\nGraphQL originated in JS ecosystem.\\nI'm doing this in free time and for fun. I want to focus on project's features\\nand not to waste a time with poorly designed language lacking mature and\\nstable libraries. So for obvious reasons I'm choosing Python.\\n","tokens":15,"id":472,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to choose Python as the main programming language for this project.\n\n## Context\n\nWe need to choose a main programming language for this project.\n\n## Problem\n\nThe project requires a language that is easy to learn, scalable, and has a large community of developers. Additionally, the project involves data analysis and machine learning tasks, which require a language with strong support for these areas.\n\n## Rationale\n\nWe have considered the following options:\n\n* Java: Java is a popular language with a large community, but it is not as easy to learn as Python and has a steeper learning curve.\n* C++: C++ is a powerful language, but it is not as easy to learn as Python and has a smaller community.\n* R: R is a language specifically designed for data analysis and machine learning, but it is not as versatile as Python and has a smaller community.\n\nPython, on the other hand, is a language that is easy to learn, has a large community, and has strong support for data analysis and machine learning. It is also a versatile language that can be used for web development, scripting, and more.\n\n## Consequences\n\n* Pros:\n\t+ Easy to learn and use\n\t+ Large community of developers\n\t+ Strong support for data analysis and machine learning\n\t+ Versatile language that can be used for a variety of tasks\n* Cons:\n\t+ May not be as efficient as other languages for large-scale applications\n\t+ May not have the same level of support for certain tasks as other languages\n\nOverall, we believe that Python is the best choice for this project due to its ease of use, large community, and strong support for data analysis and machine learning.","GenTime":"2024-08-09 23:43:11"}
{"File Name":"openlobby-server\/0011-add-relational-database.md","Context":"## Context\\nNumber of document types which does not use Elasticsearch's fulltext\\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\\nper index which means management of many indices.\\n","Decision":"We will add relational database as primary database. Elasticsearch will be used\\nfor denormalized reports and related data intended for fulltext search.\\n","tokens":42,"id":475,"Prediction":"## Decision\n\n**Document Types Without Fulltext Capabilities Management**\n\n### Context\n\nThe number of document types that do not utilize Elasticsearch's full-text capabilities is increasing. With the recent release of Elasticsearch 6, which introduces a type per index, managing multiple indices is becoming a challenge.\n\n### Decision\n\nTo efficiently manage document types without full-text capabilities, we will implement a separate index for each document type. This approach will allow us to take advantage of Elasticsearch's scalability and flexibility, while also simplifying the management of non-full-text document types.\n\n### Rationale\n\nBy creating a separate index for each document type, we can:\n\n1. Improve query performance: By separating non-full-text document types into their own indexes, we can reduce the complexity of queries and improve response times.\n2. Simplify index management: With a separate index for each document type, we can manage and maintain each index independently, reducing the overall complexity of our Elasticsearch setup.\n3. Enhance scalability: By separating indexes, we can scale each index independently, allowing us to handle increased traffic and data volume more efficiently.\n\n### Consequences\n\n1. Increased index count: With each document type having its own index, the total number of indexes will increase, which may require additional resources and maintenance.\n2. Additional complexity: Managing multiple indexes will require more complex query logic and additional configuration.\n\n### Alternatives Considered\n\n1. Using a single index with multiple types: This approach would allow us to maintain a single index, but would likely lead to performance issues and increased complexity as the number of document types grows.\n2. Using a different search engine: We considered using a different search engine that does not have the same limitations as Elasticsearch, but ultimately decided that Elasticsearch's features and scalability outweigh the limitations.\n\n### Status\n\nThis decision is approved and will be implemented in the next release.","GenTime":"2024-08-09 23:44:25"}
{"File Name":"openlobby-server\/0003-just-api-without-frontend.md","Context":"## Context\\nOpen Lobby must have open stable API.\\n","Decision":"Open Lobby will be server just with an API based on API First design. Frontend\\nwill be written as separate application (web based, mobile, ...). This will\\nensure that there is a working API for anything that frontend application(s)\\nwill do.\\n","tokens":12,"id":476,"Prediction":"## Decision\n\n**Decision:** Open Lobby will use RESTful API architecture for its open stable API.\n\n## Context\n\nAs per the requirement, Open Lobby must have an open stable API. This means that the API should be easily accessible, scalable, and maintainable. RESTful API architecture is a widely accepted and well-established standard for building APIs, which makes it a suitable choice for Open Lobby's open stable API.\n\n## Problem\n\nOpen Lobby's API needs to be easily accessible and scalable to accommodate a large number of users and requests. Additionally, the API should be maintainable and easy to extend with new features.\n\n## Solution\n\nUsing RESTful API architecture for Open Lobby's API ensures that it is:\n\n1. **Easy to use**: Clients can easily consume the API using standard HTTP methods (GET, POST, PUT, DELETE) and JSON data format.\n2. **Scalable**: RESTful API architecture is designed to handle high traffic and large amounts of data, making it suitable for Open Lobby's needs.\n3. **Maintainable**: The API is modular and loosely coupled, making it easy to extend and modify individual components without affecting the entire system.\n4. **Stable**: The API will be designed to be stable and backward compatible, ensuring that changes to the API do not break existing clients.\n\n## Consequences\n\nBy choosing RESTful API architecture for Open Lobby's API, we will:\n\n1. Ensure that the API is easily accessible and scalable.\n2. Make it easier for clients to consume the API.\n3. Provide a stable and maintainable API that can be extended with new features.\n\n## Rationale\n\nRESTful API architecture is a widely accepted and well-established standard for building APIs. It provides a clear and simple way to design and implement APIs, making it easier for developers to consume and maintain. Additionally, RESTful API architecture is highly scalable and can handle high traffic and large amounts of data, making it suitable for Open Lobby's needs.","GenTime":"2024-08-09 23:44:52"}
{"File Name":"elasticsearch-updater\/0002-store-configurations-in-repository.md","Context":"## Context\\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\\n","Decision":"Given the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\\n","tokens":61,"id":485,"Prediction":"## Decision\n\n**Decision:** Implement a configuration file in YAML format to store the necessary data transformation and mapping logic, rather than hardcoding it in the application or passing it as environment variables.\n\n## Context\n\nAs described in the context, the application needs to create mappings and transform data to improve search rankings or provide geolocation searches. These transformations are complex and cannot be passed as environment variables. To decouple the application from the specific transformation logic and make it more maintainable, we need a way to store and configure this logic.\n\n## Problem\n\nThe current approach of hardcoding the transformation logic in the application or passing it as environment variables has several drawbacks:\n\n* It tightly couples the application code to the specific transformation logic, making it difficult to change or modify.\n* It makes it hard to manage and maintain the transformation logic, as it is scattered throughout the codebase.\n* It limits the ability to reuse the transformation logic across different applications or environments.\n\n## Solution\n\nBy implementing a configuration file in YAML format, we can store the necessary data transformation and mapping logic in a separate file. This file can be easily edited and maintained, and the application can be configured to read and apply the transformations accordingly.\n\nThe YAML file can contain key-value pairs or nested structures to define the transformation logic, making it easy to read and understand. For example:\n```yaml\nmappings:\n  - type: geo\n    fields:\n      - name: location\n        type: geo_point\n  - type: date\n    fields:\n      - name: created_at\n        type: date\n\ntransformations:\n  - name: geo_transform\n    script: |\n      def transform(doc) {\n        doc.location = {\n          lat: doc.latitude,\n          lon: doc.longitude\n        }\n      }\n    inputs: [location]\n    outputs: [location]\n```\n## Consequences\n\nBy implementing a configuration file in YAML format, we can:\n\n* Decouple the application code from the specific transformation logic, making it easier to change or modify.\n* Manage and maintain the transformation logic in a separate file, making it easier to understand and debug.\n* Reuse the transformation logic across different applications or environments, making it more flexible and scalable.\n\nHowever, this approach also introduces additional complexity, as the application needs to read and parse the YAML file, and the transformation logic needs to be written in a specific format.","GenTime":"2024-08-09 23:49:00"}
{"File Name":"front-end-monorepo\/adr-19.md","Context":"## Context\\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\\n","Decision":"1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n","tokens":147,"id":498,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will implement the multiFrame Viewer as a separate component from the rest of the lib-classifier, allowing for a more modular and reusable design.\n\n**Context**\n\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\n\n**Problem**\n\nThe current design of the lib-classifier is tightly coupled with the classification interface, making it difficult to reuse the multi-frame viewer component in other parts of the system or in other projects.\n\n**Decision**\n\nWe will implement the multiFrame Viewer as a separate component, allowing it to be reused in other parts of the system and in other projects. This will also enable us to develop and maintain the component independently of the rest of the lib-classifier.\n\n**Consequences**\n\n* The multiFrame Viewer component will be more modular and reusable.\n* The development and maintenance of the component will be decoupled from the rest of the lib-classifier.\n* This decision will enable us to add new features to the filmstrip component as needed, without affecting the rest of the system.\n\n**References**\n\n* [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview)\n* Issue #1142","GenTime":"2024-08-09 23:54:48"}
{"File Name":"front-end-monorepo\/adr-10.md","Context":"## Context\\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\\n","Decision":"We decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\\n``` js\\nconst workflow = {\\nid: '1',\\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\\n}\\n```\\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\\n**A note about the use of arrays for the key-value pairs**\\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\\n","tokens":320,"id":523,"Prediction":"## Decision\n\n## Problem\n\nIn ADR-5, we decided to use ES6 Maps to represent the `steps` attribute of a workflow resource. However, when storing this data in the Mobx-State-Tree store and converting it to a format supported by JSON API, we need to perform type conversion to ensure the ordering of key-value pairs is preserved.\n\n## Context\n\nWe initially attempted to use an existing library for type conversion, but concerns were raised about the default behavior of these libraries, which would convert Maps to Objects, losing the guaranteed ordering of steps. Additionally, using a library would obscure the method of map type conversion, making it unclear for other Zooniverse devs how to handle this case in other client libraries.\n\n## Decision\n\nAfter careful consideration, we have decided to implement our own type conversion method for converting ES6 Maps to a format supported by JSON API. This approach will allow us to preserve the ordering of key-value pairs and provide transparency on how to handle this conversion in other client libraries.\n\n## Rationale\n\nImplementing our own type conversion method will ensure that the ordering of key-value pairs is preserved, which is a critical requirement for our workflow steps. Additionally, this approach will provide clarity on how to handle this conversion in other client libraries, making it easier for other Zooniverse devs to understand and implement.\n\n## Consequences\n\nWe will need to implement a custom type conversion method to convert ES6 Maps to a format supported by JSON API. This may require additional development time and testing to ensure the conversion is accurate and efficient. However, this approach will provide the necessary transparency and flexibility for our workflow steps attribute.","GenTime":"2024-08-10 00:04:27"}
{"File Name":"front-end-monorepo\/adr-21.md","Context":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n","Decision":"Rewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","tokens":315,"id":527,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\nWe will extract the authentication logic from the existing `panoptes-javascript-client` and create a new, standalone authentication module that can be configured independently of the client-side environment.\n\n**Context**\nAs described in the provided context, the existing authentication client bundled in `panoptes-javascript-client` has limitations when used with Next.js-based apps. Specifically, environment variables are not available on the client-side, and the only way to configure the client is through environment variables or query parameters, which are not ideal solutions.\n\n**Problem**\nThe current authentication client does not provide a flexible way to configure the authentication settings, which makes it difficult to use with Next.js-based apps.\n\n**Rationale**\nExtracting the authentication logic into a standalone module will allow us to decouple the authentication configuration from the client-side environment. This will enable us to configure the authentication settings from a single source of truth, which can be environment variables, a config file, or any other mechanism chosen by the consuming app.\n\n**Decision Drivers**\n\n* The need for a flexible authentication configuration mechanism\n* The limitations of the existing authentication client in Next.js-based apps\n* The requirement for a source-agnostic configuration approach\n\n**Consequences**\n\n* A new standalone authentication module will need to be developed and maintained\n* The existing `panoptes-javascript-client` will need to be updated to use the new authentication module\n* The consuming apps will need to be updated to use the new authentication module and configure it according to their needs\n\n**Status**\nProposed\n\n**Changelog**\nThis decision will be tracked in the changelog and will be reviewed and approved by the relevant stakeholders before implementation.","GenTime":"2024-08-10 00:05:58"}
{"File Name":"front-end-monorepo\/adr-15.md","Context":"## Context\\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\\n- Annotation \/ classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\\n","Decision":"What we do not want to do:\\n- Re-render on every pointer or touch event.\\n- update annotation state while drawing is in progress.\\n- support more than one drawing task in a step.\\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\\nWhat we do want to do:\\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse\/Panoptes-Front-End#5411\\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer\/observable event stream.\\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan\/zoom function. We have two proposed options in implementation:\\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\\n- Projects have requested each frame to have the same pan\/zoom function, but we were unable to implement in PFE: zooniverse\/Panoptes-Front-End#3465\\n- Are there any cases where projects want separate pan\/zoom function for each frame?\\n- Have a schema, or set of schemas, describing annotations.\\n","tokens":187,"id":528,"Prediction":"## Decision\n\nTo address the issues with the drawing tools on Panoptes-Front-End (PFE), we will implement a new architecture for the drawing tools that separates concerns between components and stores, simplifies the API, and standardizes the annotation\/classification payloads.\n\n### Rationale\n\nThe current architecture of the drawing tools is causing issues with performance, maintainability, and scalability. The frequent updates to the classification annotation are causing unnecessary re-rendering of the DOM, making the application slow and unresponsive. The separation of concerns between components and stores is unclear, making it difficult to debug and add new features. The complex API and inconsistent annotation\/classification payloads are also hindering the development of new features and making it difficult to maintain the existing codebase.\n\n### Decision Details\n\n1. We will refactor the drawing tools to use a centralized store for managing the classification annotation, rather than having multiple components update the annotation directly. This will simplify the code and make it easier to debug and maintain.\n2. We will create a new API for the drawing tools that is more intuitive and easier to use. This will involve removing the static methods and instead using a more object-oriented approach.\n3. We will standardize the annotation\/classification payloads to ensure that all tools use the same format and conventions. This will make it easier to add new features and integrate with other components.\n4. We will create a new component for rendering the markings, rather than having the `MarkingsRenderer` update the annotation directly. This will simplify the code and make it easier to debug and maintain.\n5. We will review the code and refactor it to ensure that it is modular, scalable, and maintainable.\n\n### Consequences\n\nThis decision will require significant refactoring of the existing codebase, which will take time and effort. However, the benefits of a more maintainable, scalable, and efficient codebase will be significant. The new architecture will make it easier to add new features, debug issues, and integrate with other components. The standardized annotation\/classification payloads will make it easier to integrate with other systems and tools.","GenTime":"2024-08-10 00:06:26"}
{"File Name":"front-end-monorepo\/adr-38.md","Context":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\\n","Decision":"- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\\n","tokens":119,"id":532,"Prediction":"## Decision\n\n**Decision:** Implement a pagination system for the subject queue in the classifier, allowing volunteers to browse and navigate through previous subjects.\n\n**Context:** As described, the current subject queue only allows for random or sequential subject selection, with no option to revisit previously viewed subjects. This limits the flexibility and efficiency of volunteers in selecting subjects to work on.\n\n**Problem:** The current system does not provide a way for volunteers to browse through previous subjects, making it difficult for them to review and select subjects they may have missed or want to revisit.\n\n**Reasoning:** Implementing a pagination system will enable volunteers to browse and navigate through previous subjects, allowing them to make more informed decisions about which subjects to work on. This will improve the overall efficiency and effectiveness of the subject selection process.\n\n**Decision Drivers:**\n\n* Improve volunteer experience and efficiency in selecting subjects\n* Enhance the ability to review and revisit previously viewed subjects\n* Increase the overall quality of subject selection\n\n**Consequences:**\n\n* Additional development effort required to implement pagination system\n* Potential impact on performance and scalability of the classifier\n* Need to consider caching and indexing strategies to optimize subject retrieval\n\n**Alternatives Considered:**\n\n* Implement a search function for subjects, which may not provide the same level of flexibility and control as a pagination system\n* Keep the current system and only allow for random or sequential subject selection, which may limit volunteer experience and efficiency\n\n**Monitoring and Feedback:** The effectiveness of the pagination system will be monitored through volunteer feedback and performance metrics, such as time spent selecting subjects and accuracy of subject classification.","GenTime":"2024-08-10 00:07:35"}
{"File Name":"tamr-client\/0006-type-checking.md","Context":"## Context\\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\\n","Decision":"Type-check via [mypy](http:\/\/mypy-lang.org\/).\\n","tokens":25,"id":536,"Prediction":"## Decision\n\n**Decision:** We will utilize the static type-checking feature available for Python, leveraging the type annotations already present in the codebase, to improve code maintainability, readability, and reliability.\n\n**Context:** Static type-checking is available for Python, making it possible to utilize the type annotations already in the codebase.\n\n**Problem:** Without type-checking, the codebase may contain type-related errors that can lead to runtime issues, making it challenging to maintain and debug.\n\n**Reasoning:** By enabling static type-checking, we can catch type-related errors early in the development process, reducing the likelihood of runtime errors and improving overall code quality. This decision will also help maintain consistency in the codebase, making it easier for new developers to understand and contribute to the project.\n\n**Consequences:**\n\n* Improved code maintainability and readability\n* Reduced likelihood of runtime errors\n* Increased confidence in the codebase's stability and reliability\n* Potential for better code completion and refactoring tools\n\n**Alternatives Considered:** We considered not utilizing static type-checking, but this would leave the codebase vulnerable to type-related errors and make it more challenging to maintain.\n\n**Status:** Accepted\n\n**Implementations:** We will implement static type-checking using a Python type-checking tool, such as mypy, and integrate it into our development workflow.","GenTime":"2024-08-10 00:09:08"}
{"File Name":"tamr-client\/0007-tamr-client-package.md","Context":"## Context\\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\\nBut, we want to rearchitect this codebase as a [library of composable functions](\/contributor-guide\/adr\/0005-composable-functions).\\n","Decision":"Implement rearchitected design as a new package named `tamr_client`.\\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\\n","tokens":64,"id":539,"Prediction":"## Decision\n\n**ADR-0006: Gradually Migrate from `tamr_unify_client` to Composable Functions**\n\n## Context\n\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes. However, we want to rearchitect this codebase as a library of composable functions.\n\n## Problem\n\nThe current `tamr_unify_client` implementation is monolithic and tightly coupled, making it difficult to maintain, extend, and reason about. This also limits our ability to adapt to changing requirements and user needs.\n\n## Decision\n\nTo address this problem, we will gradually migrate the `tamr_unify_client` codebase to a library of composable functions. This will allow us to:\n\n1. Break down the monolithic code into smaller, independent functions that can be easily composed together.\n2. Improve the maintainability and extensibility of the codebase.\n3. Provide a more flexible and adaptable architecture that can better respond to changing requirements.\n4. Reduce the risk of backwards-incompatible changes by introducing new functionality and APIs incrementally.\n\n## Solution\n\nTo achieve this, we will follow a gradual migration approach:\n\n1. Identify the most critical and frequently used functions in `tamr_unify_client` and refactor them as composable functions.\n2. Introduce a new API that provides a similar interface to the original `tamr_unify_client` API, but uses the new composable functions under the hood.\n3. Gradually refactor the rest of the `tamr_unify_client` codebase to use the new composable functions and API.\n4. Once the majority of the codebase has been migrated, we will deprecate and eventually remove the original `tamr_unify_client` API.\n\n## Rationale\n\nThis approach allows us to:\n\n* Minimize the impact on our existing userbase by introducing new functionality and APIs incrementally.\n* Ensure that the new composable functions are thoroughly tested and validated before replacing the original code.\n* Gradually refactor the codebase to reduce the risk of introducing new bugs or breaking changes.\n\n## Status\n\nThis decision is currently in progress. We will provide regular updates on our progress and any changes to this decision.\n\n## Consequences\n\nThe consequences of this decision are:\n\n* Improved maintainability and extensibility of the codebase.\n* Reduced risk of backwards-incompatible changes.\n* Increased flexibility and adaptability to changing requirements.\n* Potential for increased development time and effort due to the gradual migration approach.\n\n## References\n\n* [Library of Composable Functions](\/contributor-guide\/adr\/0005-composable-functions)\n* [Gradual Migration Approach](https:\/\/martinfowler.com\/bliki\/GradualMigration.html)","GenTime":"2024-08-10 00:10:26"}
{"File Name":"molgenis-r-armadillo\/0002-manage-data-in-armadillo-suite.md","Context":"## Context\\nIn this project we manage the data in the Armadillo suite.\\n","Decision":"We will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\\n","tokens":18,"id":545,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\nWe will use a relational database management system (RDBMS) to store and manage the data in the Armadillo suite.\n\n**Context**\nIn this project, we manage the data in the Armadillo suite.\n\n**Problem**\nThe Armadillo suite requires a robust and scalable data storage solution to handle the large amounts of data generated by various applications. The data needs to be easily accessible, searchable, and maintainable.\n\n**Reasoning**\nWe have chosen a relational database management system (RDBMS) as the data storage solution for the Armadillo suite due to its ability to handle large amounts of data, provide robust data modeling and querying capabilities, and support scalability. RDBMS also provides a high level of data consistency, integrity, and security.\n\n**Consequences**\n\n* Pros:\n\t+ Scalability: The RDBMS can handle large amounts of data and scale horizontally as needed.\n\t+ Data consistency: The RDBMS ensures data consistency and integrity by enforcing relationships between data entities.\n\t+ Security: The RDBMS provides robust security features to protect sensitive data.\n\t+ Querying: The RDBMS provides powerful querying capabilities to retrieve and manipulate data.\n* Cons:\n\t+ Complexity: RDBMS requires a good understanding of database design and querying languages (e.g., SQL).\n\t+ Cost: RDBMS requires significant resources and infrastructure to set up and maintain.\n\n**Status**\nThis decision is currently in effect.\n\n**References**\n\n* [Relational Database Management System (RDBMS)](https:\/\/en.wikipedia.org\/wiki\/Relational_database_management_system)\n* [Database Design Patterns](https:\/\/www.amazon.com\/Database-Design-Patterns-Explaining-Principles\/dp\/0321127420)\n\nPlease let me know if you want me to make any changes!","GenTime":"2024-08-10 00:12:21"}
{"File Name":"play-frontend-hmrc\/0012-create-an-endpoint-in-play-frontend-hmrc-for-surfacing-session-metadata.md","Context":"## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n","Decision":"* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n","tokens":348,"id":560,"Prediction":"## Decision\n\nTo address the issue of unnecessary signouts due to the lack of knowledge about the actual time remaining on the user's active session, we will implement a new library that provides a session timeout service. This library will be responsible for exposing an endpoint that returns the actual time remaining on the user's active session, which will be consumed by the hmrcTimeoutDialog component.\n\n## Decision Drivers\n\nThe decision to implement a new library is driven by the need to:\n\n* Minimize code changes for service teams other than a library upgrade\n* Avoid requiring service teams to add any additional routing rules\n* Avoid requiring service teams to add any additional configuration\n* Limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and any other library or service\n* Maintain loose coupling of frontend services and avoid adding single points of failure between frontend microservices\n* Ensure that the endpoint used to interrogate the session does not itself affect the session\n\n## Decision Description\n\nThe new library will be designed to be a standalone service that provides the necessary functionality to retrieve the actual time remaining on the user's active session. This will be achieved by exposing a RESTful endpoint that can be consumed by the hmrcTimeoutDialog component. The library will be implemented in a way that minimizes coupling with other services and does not introduce any additional complexity or dependencies.\n\nThe library will be responsible for:\n\n* Exposing a RESTful endpoint that returns the actual time remaining on the user's active session\n* Providing a simple and easy-to-use API for consuming the endpoint\n* Ensuring that the endpoint does not itself affect the session\n\nThe hmrcTimeoutDialog component will be updated to consume the new endpoint and use the returned time remaining to display the correct timeout message to the user.\n\n## Rationale\n\nThe decision to implement a new library is the most cost-effective and efficient way to address the issue while minimizing impact for service teams and limiting duplication of knowledge. It also ensures that the solution is loosely coupled and does not introduce any single points of failure between frontend microservices.","GenTime":"2024-08-10 00:18:06"}
{"File Name":"kuona\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":573,"Prediction":"Here is an example of an Architectural Decision Record (ADR) for the provided context:\n\n**Decision**\n\n* **ADR-001: API Design for Integration with Third-Party Services**\n\n**Context**\n\nWe need to record the architectural decisions made on this project.\n\n**Problem**\n\nAs we plan to integrate our application with multiple third-party services, we need to decide on a robust and scalable API design that can handle the varying requirements of these services.\n\n**Decision**\n\nAfter careful consideration, we have decided to use a RESTful API with JSON payloads to integrate with third-party services. This design choice provides a flexible and widely adopted standard for data exchange, allowing us to easily integrate with a variety of services.\n\n**Rationale**\n\nThe decision to use a RESTful API was based on the following factors:\n\n* Wide adoption and compatibility with most third-party services\n* Flexibility to handle varying data formats and structures\n* Easy to implement and maintain\n* Scalability to handle high traffic and large data volumes\n\n**Consequences**\n\nThis decision has the following consequences:\n\n* Simplified integration with third-party services\n* Improved data exchange and processing efficiency\n* Reduced development time and costs\n* Potential for increased scalability and performance\n\n**Status**\n\nThis decision is currently implemented and in production. It will be reviewed and updated as necessary to ensure it remains effective and efficient.\n\n**History**\n\n* 2023-02-15: Decision made and implemented\n* 2023-03-15: Review and update of decision\n\nThis is just an example, but you can customize it to fit your specific needs and project requirements.","GenTime":"2024-08-10 00:22:33"}
{"File Name":"verify-onboarding-prototypes\/0006-we-will-build-a-js-client.md","Context":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":577,"Prediction":"## Decision\n\nTo provide a seamless integration for users currently using Node.js and Passport, we will implement a Passport.js-based authentication system as the primary authentication mechanism. This decision is made to minimize friction and maximize compatibility for our existing users.\n\n## Context\n\nAs mentioned, at least one user is currently using Node.js and Passport, and we want to provide an integration that is as frictionless as possible for them. Additionally, we anticipate that other users will be using different languages and frameworks, which may require additional setup and configuration.\n\n## Problem\n\nWithout a standardized authentication system, our users may face difficulties integrating with our platform, which could lead to decreased adoption and increased support requests.\n\n## Decision\n\nTo address this problem, we will implement a Passport.js-based authentication system as the primary authentication mechanism. This decision is made for the following reasons:\n\n1. **Existing user base**: By leveraging Passport.js, we can provide a seamless integration for our existing Node.js users, minimizing the learning curve and setup time.\n2. **Flexibility**: Passport.js is a widely-used and well-documented library that supports multiple authentication strategies, making it an ideal choice for our multi-language and framework ecosystem.\n3. **Easy integration**: Passport.js provides a simple and straightforward API for integrating with our platform, reducing the complexity and effort required for users to integrate their applications.\n4. **Scalability**: By using a widely-used library, we can take advantage of the existing community support, bug fixes, and feature updates, ensuring that our authentication system remains scalable and maintainable.\n\n## Consequences\n\n1. **Additional complexity**: Implementing a Passport.js-based authentication system will require additional development effort and infrastructure setup.\n2. **Dependence on Node.js**: Our primary authentication mechanism will be tied to Node.js, which may limit our ability to support users with other language and framework preferences.\n3. **Potential security risks**: As with any third-party library, there is a risk of security vulnerabilities in Passport.js. We will need to ensure that we keep our implementation up-to-date and secure.\n\n## Rationale\n\nBy implementing a Passport.js-based authentication system, we can provide a seamless integration for our existing Node.js users while also setting ourselves up for future scalability and maintainability. While there are potential risks and consequences associated with this decision, we believe that the benefits outweigh the costs and will work to mitigate any potential issues.","GenTime":"2024-08-10 00:24:28"}
{"File Name":"verify-onboarding-prototypes\/0002-how-do-we-secure-the-api.md","Context":"## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n","Decision":"If possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n","tokens":59,"id":578,"Prediction":"## Decision\n\nTo secure the interaction between the \"client\" code (e.g. Node.js) and the server-side code (which will be a Dropwizard app), we will implement a combination of security solutions that cater to different user preferences and deployment scenarios.\n\n## Context\n\nAs mentioned, the users may want to run the service provider in different environments, which may require different security solutions. We need to ensure that the interaction between the client and server is secure, regardless of the deployment scenario.\n\n## Solution\n\nTo achieve this, we will implement the following security solutions:\n\n1. **TLS\/SSL Encryption**: We will use TLS\/SSL encryption to encrypt the communication between the client and server. This will ensure that any data exchanged between the two is encrypted and cannot be intercepted or tampered with.\n2. **Authentication and Authorization**: We will implement authentication and authorization mechanisms to ensure that only authorized clients can interact with the server. This will include the use of tokens, certificates, or other authentication methods.\n3. **JSON Web Tokens (JWT)**: We will use JWT to authenticate and authorize clients. JWT will provide a secure way to verify the identity of clients and ensure that only authorized clients can access the server.\n4. **OAuth 2.0**: We will implement OAuth 2.0 to provide a standardized way for clients to authenticate and authorize with the server. This will enable clients to request access to specific resources or APIs without sharing their credentials.\n5. **Certificate-Based Authentication**: We will also provide an option for clients to use certificate-based authentication, which will provide an additional layer of security and authenticity.\n\n## Rationale\n\nBy implementing these security solutions, we can ensure that the interaction between the client and server is secure and compliant with industry standards. The use of TLS\/SSL encryption will provide end-to-end encryption, while authentication and authorization mechanisms will ensure that only authorized clients can interact with the server. JWT and OAuth 2.0 will provide a standardized way for clients to authenticate and authorize, while certificate-based authentication will provide an additional layer of security.\n\n## Consequences\n\nImplementing these security solutions will require additional development time and resources. However, the benefits of improved security and compliance with industry standards will outweigh the costs. Additionally, the flexibility of these solutions will enable us to adapt to changing security requirements and deployment scenarios.\n\n## Alternatives Considered\n\nWe considered alternative security solutions, such as:\n\n* Using a third-party authentication service\n* Implementing a custom authentication mechanism\n* Using a different encryption protocol\n\nHowever, these alternatives were deemed less suitable due to their limitations, complexity, or lack of standardization.\n\n## Status\n\nThis decision is currently in the \"proposed\" state and is subject to change based on further discussion and feedback.","GenTime":"2024-08-10 00:25:05"}
{"File Name":"verify-onboarding-prototypes\/0008-provide-an-end-to-end-stub.md","Context":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":581,"Prediction":"## Decision\n\n**Decision:** Implement a mock Service Provider to simulate a typical Verify journey for user research and testing.\n\n**Context:** We wish to run regular user research and testing against the prototype Service Provider. To support user research, we need to provide a user journey that resembles a typical Verify journey. The area we are most interested in is the interface between the Service and the Hub.\n\n**Problem:** We need to create a realistic and repeatable user journey for testing and research purposes, but the actual Service Provider is not yet fully developed and may change frequently.\n\n**Reasoning:** Implementing a mock Service Provider allows us to create a controlled and consistent environment for user research and testing. This approach enables us to focus on the specific interactions between the Service and the Hub, while decoupling the testing from the actual Service Provider implementation. This will also allow us to test and refine the user journey independently of the Service Provider's development timeline.\n\n**Consequences:**\n\n* We will need to maintain a separate implementation of the mock Service Provider, which will require additional development and testing effort.\n* The mock Service Provider will need to be updated periodically to reflect changes in the actual Service Provider.\n* We will need to ensure that the mock Service Provider accurately simulates the behavior of the actual Service Provider, to maintain the validity of the user research and testing results.\n\n**Alternatives Considered:**\n\n* Using the actual Service Provider for testing and research, which may be unreliable due to its ongoing development and potential changes.\n* Creating a simplified or abstracted representation of the Service Provider, which may not accurately reflect the real-world user experience.\n\n**Status:** Approved","GenTime":"2024-08-10 00:26:12"}
{"File Name":"old-web-frontend\/0002-use-aws-s3-as-host-solution.md","Context":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n","Decision":"* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","tokens":53,"id":583,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to host our web application on a Kubernetes cluster using Google Cloud Platform (GCP) as our cloud provider.\n\n## Context\n\nAs we need to host our web application so clients can access it, we require a solution that is easy to manage and update. Our decision drivers highlight the importance of ease of update, high availability, easy configuration of HTTPS, and observability.\n\n## Decision Drivers and Trade-Offs\n\n* Easy to update: Kubernetes provides a declarative configuration model, allowing us to define our application's desired state and the cluster will manage the actual state. This makes it easy to roll out updates to our application.\n* High availability: Kubernetes provides built-in support for high availability through its deployment and replica set concepts. We can scale our application horizontally and vertically to ensure high availability.\n* Easy to configure HTTPS: GCP provides a managed SSL certificate service, which makes it easy to configure HTTPS for our application. Additionally, Kubernetes provides support for SSL\/TLS encryption out of the box.\n* Observability: Kubernetes provides built-in support for logging and monitoring through its integration with tools like ELK (Elasticsearch, Logstash, Kibana) and Prometheus. GCP also provides a range of monitoring and logging tools that can be integrated with Kubernetes.\n\n## Trade-Offs\n\nWhile Kubernetes provides a lot of benefits, it does require some upfront investment in learning the technology and configuring our application to run on it. However, the benefits of ease of update, high availability, and easy configuration of HTTPS and observability make it a worthwhile investment.\n\n## Conclusion\n\nBy hosting our web application on a Kubernetes cluster using GCP, we can ensure that it is easy to manage and update, has high availability, and is easily configurable for HTTPS and observability. This decision aligns with our requirements and will provide a robust and scalable foundation for our application.","GenTime":"2024-08-10 00:27:02"}
{"File Name":"abracadabra\/0008-don-t-propose-quick-fix-for-react-convert-to-pure-component.md","Context":"## Context\\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the \"Convert to Pure Component\" Quick Fix took most of the time:\\n![][flame-chart]\\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https:\/\/github.com\/reactjs\/react-codemod). It was originally meant to be run through a CLI command.\\n","Decision":"We will stop proposing a Quick Fix for this refactoring.\\n","tokens":122,"id":586,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will introduce a toggleable feature to the \"Convert to Pure Component\" Quick Fix, allowing users to opt-in or opt-out of this refactoring based on specific circumstances.\n\n**Context**\n\nAfter analyzing the performance of the extension on a big JavaScript file, we noticed that the \"Convert to Pure Component\" Quick Fix took most of the time. This refactoring can only be applied under specific circumstances and does not warrant the toll for everyone, on every trigger.\n\n**Problem**\n\nThe current implementation of the \"Convert to Pure Component\" Quick Fix is not optimized for performance and can cause significant delays for users who do not require this refactoring. Additionally, the code is implemented by react-codemod, which was originally designed to be run through a CLI command, not as a Quick Fix in an IDE.\n\n**Decision Criteria**\n\n1. Performance: The refactoring should not significantly impact the overall performance of the extension.\n2. User Experience: The toggleable feature should provide users with more control over the refactoring process.\n3. Maintainability: The code should be easy to maintain and update.\n\n**Consequences**\n\n1. Users will have more control over the refactoring process, allowing them to opt-in or opt-out of the \"Convert to Pure Component\" Quick Fix based on their specific needs.\n2. The extension's performance will not be significantly impacted by the refactoring.\n3. The code will be easier to maintain and update, as the toggleable feature will provide a clear separation of concerns.\n\n**Status**\n\nApproved\n\n**References**\n\n* [react-codemod](https:\/\/github.com\/reactjs\/react-codemod)","GenTime":"2024-08-10 00:28:18"}
{"File Name":"ehoks-ui\/0002-use-finnish-as-the-domain-language.md","Context":"## Context\\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\\nin Finnish but react component props in English even though data might be exactly the same.\\n","Decision":"We will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\\nconventions quite some time.\\nHere is one expample:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisDemonstration?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\nperiodSpecifier?: string\\ndemonstrationEnviromentDescription?: string\\n}\\n```\\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\\nOsaamisenOsoittaminen components.\\nExample fixed:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisNaytto?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\najanjaksonTarkenne?: string\\nnayttoymparistoDescription?: string\\n}\\n```\\n","tokens":41,"id":598,"Prediction":"## Decision\n\n**Decision:** Introduce a centralized translation service to manage and maintain translations for domain words across the application.\n\n**Context:** There are currently mixed conventions of translating domain words, resulting in inconsistent naming conventions and confusing data.\n\n**Problem:** The current approach to translation is ad-hoc and decentralized, leading to:\n\n* Inconsistent naming conventions across the application\n* Confusion and errors when working with data\n* Difficulty in maintaining and updating translations\n\n**Reasoning:** A centralized translation service will provide a single source of truth for translations, ensuring consistency and accuracy across the application. This will improve the overall quality and maintainability of the codebase.\n\n**Consequences:**\n\n* All translations will be stored in a single location, making it easier to maintain and update them\n* Consistent naming conventions will be enforced across the application, reducing errors and confusion\n* The translation service will provide a clear and concise way to translate domain words, making it easier for developers to work with the data\n\n**Alternatives Considered:**\n\n* Continue with the current ad-hoc approach to translation\n* Introduce a separate translation service for each component or feature\n\n**Decision Drivers:**\n\n* Consistency and accuracy of translations\n* Ease of maintenance and updates\n* Improved developer experience\n\n**Status:** Accepted\n\n**Implementation:** The centralized translation service will be implemented using a library or framework that provides translation functionality, such as i18next or react-intl. The service will be configured to store translations in a centralized location, such as a JSON file or a database. The service will also provide a way to inject translations into components and features, ensuring consistency and accuracy across the application.","GenTime":"2024-08-10 00:33:38"}
{"File Name":"lockfix\/0001-documenting-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":611,"Prediction":"Here is an Architectural Decision Record (ADR) for your software project:\n\n**Decision**\n\n* **ADR-001: Choose a Microservices Architecture for the New Project**\n* **Date:** 2023-02-15\n* **Status:** Active\n\n**Context**\n\nAs we start a new project, we need to decide on the overall architecture of the system. The project involves multiple features and functionalities that need to be developed and maintained independently. We need to choose an architecture that allows for scalability, flexibility, and maintainability.\n\n**Decision**\n\nAfter considering various options, we have decided to adopt a Microservices Architecture (MSA) for the new project. This decision is based on the following reasons:\n\n* **Scalability:** MSA allows us to scale individual services independently, which is essential for our project as different features have varying traffic patterns.\n* **Flexibility:** With MSA, we can develop and deploy services in different programming languages and frameworks, which provides more flexibility in our technology stack.\n* **Maintainability:** MSA makes it easier to maintain and update individual services without affecting the entire system.\n\n**Consequences**\n\n* **Increased complexity:** MSA introduces additional complexity due to the need to manage multiple services and their interactions.\n* **Additional overhead:** We will need to implement additional infrastructure and tools to manage service discovery, communication, and monitoring.\n\n**Rationale**\n\nWe believe that the benefits of MSA outweigh the additional complexity and overhead. By adopting MSA, we can build a more scalable, flexible, and maintainable system that meets the evolving needs of our project.\n\n**References**\n\n* [1] Microservices Architecture - Martin Fowler\n* [2] Designing Distributed Systems - Brendan Burns\n\n**Future Development**\n\nWe will continue to monitor the performance and scalability of our MSA and make adjustments as needed. We will also explore additional tools and technologies to improve service discovery, communication, and monitoring.","GenTime":"2024-08-10 00:38:49"}
{"File Name":"cloud-platform\/018-Dedicated-Ingress-Controllers.md","Context":"## Context\\nThe Cloud Platform was originally set up to have a single ingress controller to\\nmanage all ingresses in the cluster. So, every new ingress added a config block\\nto one, large nginx config file, and all traffic to all services hosted on the\\nCloud Platform is routed through a single AWS load balancer.\\nAlthough this was both easy to manage, and saved us some money on load\\nbalancers (approx. $25\/month per ingress), it has become unsustainable. We\\nusually have 6 replicas of the ingress controller pod, and we have started to\\nsee instances of several of these pods crash-looping (usually because they have\\nrun out of shared memory, which cannot be increased in kubernetes. See [this\\nissue] for more information).\\nWe believe this is because the nginx config has become so large (over 100K\\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\\nmoved.\\n","Decision":"We will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n","tokens":200,"id":620,"Prediction":"## Decision\n\n**Decision:** Implement a separate ingress controller for each service, using a more scalable and fault-tolerant architecture.\n\n**Context:** The Cloud Platform's original single ingress controller setup has become unsustainable due to the large size of the nginx config file (over 100K lines), leading to pod crashes and reload issues.\n\n**Reasoning:** The current setup is causing issues with pod crashes and reload failures, which is impacting the reliability and scalability of the Cloud Platform. By implementing a separate ingress controller for each service, we can:\n\n* Reduce the size of each nginx config file, making it more manageable and less prone to errors\n* Increase the scalability of the ingress controllers, allowing for more replicas and better fault tolerance\n* Improve the reliability of the service routing, reducing the likelihood of crashes and reload issues\n\n**Consequences:**\n\n* Increased cost due to the need for more AWS load balancers (approx. $25\/month per ingress)\n* Additional complexity in managing multiple ingress controllers\n* Potential for increased latency due to the need for multiple load balancers\n\n**Alternatives Considered:**\n\n* Attempt to optimize the current nginx config file to reduce its size and improve performance\n* Implement a more efficient ingress controller solution, such as a distributed ingress controller\n* Use a different load balancing solution, such as a cloud-native load balancer\n\n**Decision Rationale:** After considering the alternatives, we believe that implementing separate ingress controllers for each service is the most effective solution to address the scalability and reliability issues. While it may increase costs and complexity, it will provide a more robust and fault-tolerant architecture for the Cloud Platform.","GenTime":"2024-08-10 00:42:46"}
{"File Name":"cloud-platform\/015-Keeping-Terraform-modules-up-to-date.md","Context":"## Context\\nWe maintain a lot of [terraform modules] which teams use to manage AWS\\nresources for their namespaces.\\nIn the past, lettings different namespaces use different versions of terraform\\nmodules has caused problems because there has not always been a clear upgrade\\npath from a module that is several versions old, to the latest version.\\nIn these cases, it would have been easier to have upgraded every namespace when\\nthe significant change was made to the module, because at that point in time\\nthe relevant knowledge and understanding are fresh. Leaving it until later\\nmakes the whole process much more difficult.\\n","Decision":"We decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n","tokens":124,"id":621,"Prediction":"## Decision\n\nTo address the issue of different namespaces using different versions of Terraform modules, we will implement a \"Module Versioning\" strategy, where we will maintain a single, up-to-date version of each Terraform module, and require all namespaces to use the latest version of the module.\n\n## Context\n\nAs mentioned in the context, maintaining different versions of Terraform modules has caused problems in the past, particularly when there is no clear upgrade path from an older version to the latest version. This has led to difficulties in upgrading namespaces, as the relevant knowledge and understanding may not be readily available at a later time.\n\n## Decision Rationale\n\nBy implementing a single, up-to-date version of each Terraform module, we can ensure that all namespaces are using the same version of the module, which will simplify the upgrade process and reduce the risk of errors. This approach will also allow us to maintain a consistent and predictable environment for our teams, as they will always be working with the latest version of the module.\n\n## Consequences\n\n* All namespaces will need to be upgraded to the latest version of the module, which may require additional effort and resources.\n* The team will need to maintain a single, up-to-date version of each Terraform module, which may require additional testing and validation.\n* The upgrade process will be simplified and more predictable, reducing the risk of errors and downtime.\n\n## Alternatives Considered\n\n* Allowing different namespaces to use different versions of the module, which would require additional testing and validation to ensure compatibility.\n* Implementing a \"module versioning\" strategy, but allowing for multiple versions of a module to be used, which would require additional complexity and overhead.\n\n## Implementation\n\nThe implementation will involve the following steps:\n\n1. Identify all Terraform modules that are currently in use by namespaces.\n2. Upgrade each module to the latest version.\n3. Update the documentation and configuration files for each namespace to reflect the new version of the module.\n4. Test and validate each namespace to ensure that it is functioning correctly with the latest version of the module.\n5. Monitor and maintain the single, up-to-date version of each Terraform module, and update it as necessary.","GenTime":"2024-08-10 00:43:16"}
{"File Name":"cloud-platform\/005-ECR-monitoring-and-alerting.md","Context":"## Context\\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\\n","Decision":"After discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\\n","tokens":155,"id":629,"Prediction":"## Decision\n\n**Decision:** Implement a notification mechanism to alert teams when they are approaching the 100-image limit in their ECR repository, allowing them to take proactive measures to clean up their images and avoid potential issues.\n\n**Context:** As described in the context, we have implemented a lifecycle policy to manage the number of images in ECR repositories, but this has raised concerns about the potential danger of silently deleting images without notice. To address these concerns, we need a solution that balances the need to manage image storage with the requirement to provide transparency and control to application users.\n\n**Rationale:** Implementing a notification mechanism will allow teams to take proactive steps to clean up their images, avoiding potential issues and data loss. This will also provide transparency and control to application users, addressing their concerns about the potential danger of silently deleting images.\n\n**Decision Drivers:**\n\n* The need to manage image storage and avoid hitting the AWS limit of 1000 images\n* The requirement to provide transparency and control to application users\n* The need to balance the need to manage image storage with the requirement to provide transparency and control to application users\n\n**Consequences:**\n\n* Teams will receive notifications when they are approaching the 100-image limit, allowing them to take proactive measures to clean up their images\n* This will help to avoid potential issues and data loss\n* The notification mechanism will provide transparency and control to application users, addressing their concerns about the potential danger of silently deleting images\n\n**Alternatives Considered:**\n\n* Implementing a stricter lifecycle policy that deletes images without notification\n* Not implementing a notification mechanism and relying on teams to manually monitor their image storage\n\n**Monitoring and Feedback:**\n\n* We will monitor the effectiveness of the notification mechanism and gather feedback from teams to ensure it is meeting their needs\n* We will also monitor the number of images in ECR repositories and adjust the notification threshold as needed to ensure it is effective in managing image storage.","GenTime":"2024-08-10 00:46:37"}
{"File Name":"cloud-platform\/020-Environments-and-Pipeline.md","Context":"## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n","Decision":"1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.\\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\\n* RBAC - teams can only administer k8s resources in their own namespaces\\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\\n4. Isolation between AWS resources is achieved using access control.\\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\\n6. The namespace definition is held in GitHub.\\nGitHub provides a mechanism for peer-review, automated checks and versioning.\\nOther options considered for configuring a namespace do not come with these advantages, for example:\\n* a console \/ web form, implemented as a custom web app (click ops)\\n* commands via a CLI or API\\nNamespace definitions are stored in the [environments repo](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments)\\n7. Namespace changes are checked by both a bot and a human from the CP team\\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\\nTo mitigate these concerns:\\n* [automated checks](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/main\/.github\/workflows) are used to validate against common problems\\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\\n8. Pipeline to deploy namespace automatically.\\nThe \"deploy pipeline\" is a CI\/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\\n9. Teams have full control within their Kubernetes namespace\\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI\/CD, for deploying their app.\\n","tokens":152,"id":631,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will implement a declarative configuration interface for service teams to define their workloads and infrastructure, using Kubernetes Custom Resource Definitions (CRDs) and Kubernetes Deployment YAML files.\n\n**Context**\n\nAs we've chosen Kubernetes for container management on our Cloud Platform, we need to ensure a good interface for service teams to define and manage their workloads and infrastructure. We need to balance the need for simplicity and ease of use with the need for control and customization.\n\n**Problem**\n\nService teams require a way to specify the workloads and infrastructure they want running, while also having control over their Kubernetes resources. Additionally, we need to ensure isolation between dev and prod environments, as well as between different service teams' workloads.\n\n**Reasoning**\n\nUsing Kubernetes CRDs and Deployment YAML files allows service teams to define their workloads and infrastructure in a declarative way, which is easy to understand and manage. This approach also provides a high level of customization and control, as teams can specify the exact configuration they need for their workloads. Furthermore, CRDs and YAML files can be version-controlled and tracked, making it easier to manage changes and rollbacks.\n\n**Decision Criteria**\n\n* Declarative configuration interface for simplicity and ease of use\n* Customization and control over Kubernetes resources\n* Isolation between dev and prod environments, and between different service teams' workloads\n* Version-controlled and trackable configuration files\n\n**Consequences**\n\n* Service teams will need to learn Kubernetes CRDs and Deployment YAML files\n* Additional complexity in managing and tracking configuration files\n* Potential for increased error rates if configuration files are not properly formatted or validated\n\n**Status**\n\nApproved\n\n**Owner**\n\n[Your Name]\n\n**Created**\n\n[Date]\n\n**Links**\n\n* [Kubernetes Custom Resource Definitions](https:\/\/kubernetes.io\/docs\/concepts\/extend-kubernetes\/api-extension\/custom-resources\/)\n* [Kubernetes Deployment YAML files](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/deployment\/)","GenTime":"2024-08-10 00:47:31"}
{"File Name":"kafka\/0002-require-php-7-2-64-bit.md","Context":"## Context\\nDeciding which minimum PHP version to require is quite important because it\\nmight restrict people from using the library, at the same time supporting many\\nversions increases the cost to maintain the library.\\n","Decision":"For the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\\nonly).\\nThe main motivation for only allowing it to be installed in 64-bit systems is\\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\\ncan't be achieve in 32-bit systems.\\nI believe that libraries' maintainers also have the role to help the evolution\\nof the PHP ecosystem, which makes me want to require a more up-to-date version\\nof the language.\\n","tokens":42,"id":641,"Prediction":"## Decision\n\n**Decision:** We will require PHP 7.2 as the minimum version for our library.\n\n**Rationale:**\n\n* PHP 7.2 was released in 2017 and is a relatively stable and widely adopted version. Requiring this version as the minimum will allow us to take advantage of its improved performance and security features.\n* PHP 7.2 is also the minimum version required by many popular frameworks and libraries, so requiring this version will ensure compatibility with these tools.\n* By setting a minimum version of PHP 7.2, we can avoid supporting older versions of PHP that are no longer receiving security updates or bug fixes. This will help us to reduce the maintenance cost and focus on developing new features.\n* We will also provide a clear and concise documentation on the compatibility of our library with different PHP versions, so that users can plan and prepare accordingly.\n\n**Consequences:**\n\n* Users who are still using older versions of PHP (e.g. PHP 5.x) will not be able to use our library, which may impact our adoption and user base.\n* We will need to test our library thoroughly on PHP 7.2 to ensure that it works as expected and to identify any potential issues.\n* We will need to update our documentation and provide clear instructions on how to upgrade to PHP 7.2 or higher.\n\n**Alternatives:**\n\n* We could require a newer version of PHP (e.g. PHP 7.4 or 8.0), but this would likely impact a larger number of users who are still using older versions of PHP.\n* We could support multiple versions of PHP, but this would increase the complexity and maintenance cost of our library.\n\n**Status:** This decision is final and will be implemented in the next release of our library.","GenTime":"2024-08-10 00:51:54"}
{"File Name":"cena\/0014-adopt-commmand-query-separation.md","Context":"## Context\\n[Command-query separation](https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n","Decision":"Command-query separation will be enforced in the [core hexagon](.\/0003-adopt-hexagonal-architecture.md), especially in\\napplication services.\\n","tokens":113,"id":647,"Prediction":"## Decision\n\n**Decision:** Implement a strict separation of commands and queries in the `menu-generation` application, adhering to the principles of Command-Query Separation (CQS).\n\n**Context:** As outlined in the provided context, adopting CQS will allow for a clear separation of methods that change state from those that don't, enabling us to use queries with more confidence and carefully orchestrate commands.\n\n**Rationale:**\n\n* By separating commands and queries, we can ensure that queries do not have side effects and are safe to use in our application.\n* This separation will also make it easier to test and debug our application, as queries can be tested independently of commands.\n* The existing terminology of commands and queries in the `menu-generation` application makes it a natural fit for implementing CQS.\n\n**Consequences:**\n\n* We will need to refactor our existing code to ensure that methods are either commands or queries, and not both.\n* This may require creating new classes or methods to encapsulate queries and commands separately.\n* The benefits of CQS will be realized in terms of improved code maintainability, testability, and overall application quality.\n\n**Status:** Accepted\n\n**Implementation:** The implementation of CQS will be done by refactoring the existing code to separate commands and queries, and by creating new classes or methods as needed. The team will work together to ensure a smooth transition and minimize disruptions to the application.","GenTime":"2024-08-10 00:54:22"}
{"File Name":"cena\/0005-use-spring-framework.md","Context":"## Context\\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\\nservices implementations dependencies into the services defined in the core hexagon.\\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\\nSpring for many years and masters many of the provided features.\\n","Decision":"Spring framework will be the backbone for `menu-generation` application.\\n","tokens":120,"id":649,"Prediction":"## Decision\n\n**Decision:** Adopt Spring Framework as the infrastructure layer for the REST API implementation, utilizing its features for Inversion of Control (IoC), Aspect-Oriented Programming (AOP), and REST services implementation.\n\n**Context:** The hexagonal architecture requires the use of infrastructure services implementations dependencies in the services defined in the core hexagon. Additionally, the REST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\n\n**Problem:** The project needs to implement a REST API using the hexagonal architecture, which requires the use of infrastructure services implementations dependencies in the services defined in the core hexagon. The project also needs a dedicated library to define incoming adapters handling the HTTP resources.\n\n**Reasoning:** Spring Framework is a well-established and non-invasive framework for Java that provides multiple features such as IoC, AOP, and REST services implementation. It will help speed up the implementation in a cohesive way, and the author has experience with Spring and masters many of its features.\n\n**Consequences:**\n\n* The project will use Spring Framework as the infrastructure layer for the REST API implementation.\n* The project will utilize Spring's features for IoC, AOP, and REST services implementation.\n* The project will benefit from the author's experience with Spring, reducing the implementation time and increasing the overall quality of the code.\n\n**Status:** Accepted\n\n**Implementation:** The project will implement the REST API using Spring Framework, configuring the infrastructure services implementations dependencies in the services defined in the core hexagon. The project will also define incoming adapters handling the HTTP resources using Spring's REST services implementation features.","GenTime":"2024-08-10 00:55:03"}
{"File Name":"cena\/0009-test-features-with-cucumber.md","Context":"## Context\\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\\nDefining new acceptance tests must be easy, through reusable step definitions.\\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development)\\napproach.\\n","Decision":"[Cucumber](https:\/\/cucumber.io\/) will be used to describe and execute acceptance tests in `menu-generation` application.\\n","tokens":111,"id":650,"Prediction":"## Decision\n\n**Decision:** Implement acceptance tests using the Cucumber BDD framework, with step definitions written in a language-agnostic format, and integrate them with the application codebase.\n\n**Rationale:**\n\n* Cucumber is a widely-used and well-established BDD framework that allows for easy definition and execution of acceptance tests.\n* Language-agnostic step definitions enable non-technical users to understand and contribute to test development, while still providing a clear and concise description of the expected behavior.\n* Integrating acceptance tests with the application codebase ensures that tests are executed during the development lifecycle, providing immediate feedback on changes and allowing for rapid iteration and refinement.\n* This approach aligns with the Behavior-Driven Development (BDD) philosophy, which emphasizes collaboration between developers, testers, and business stakeholders in defining and verifying application behavior.\n\n**Consequences:**\n\n* Increased test coverage and confidence in application behavior\n* Improved collaboration and understanding among team members\n* Faster and more efficient testing and development cycles\n* Easier maintenance and modification of acceptance tests over time\n\n**Alternatives Considered:**\n\n* Using a different BDD framework (e.g., SpecFlow, Behave)\n* Implementing acceptance tests using a different testing framework (e.g., JUnit, TestNG)\n* Not integrating acceptance tests with the application codebase, instead running them separately\n\n**Status:** Accepted","GenTime":"2024-08-10 00:55:22"}
{"File Name":"opensmarthouse-core\/adr0000.md","Context":"## Context\\nOpenHAB uses OSGi as a runtime.\\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\\nSuch approach promotes tight coupling between contract and implementation.\\nMore over, it also promotes exposure of specific implementation classes via public API.\\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\\nIt would work properly, but openHAB core is a single jar which makes things even harder.\\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\\nopenHAB utilizes Apache Karaf for provisioning of the application.\\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\\nPresent structure of modules \/ bundles is as follows:\\n```\\n[openhab thing core] <--- [openhab rest core]\\n```\\n","Decision":"Since openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n","tokens":471,"id":660,"Prediction":"## Decision\n\nTo address the issues of tight coupling between contract and implementation, exposure of specific implementation classes via public API, and unnecessary propagation of dependencies to all callers of public APIs, we will introduce a new build tool that includes an includes\/excludes mechanism for dependencies. This will allow us to separate the implementation and contract packages more clearly, making it easier to manage dependencies and promote loose coupling between different parts of the framework.\n\n## Context\n\nAs mentioned earlier, OpenHAB uses OSGi as a runtime, which promotes clear separation between implementation and contract (API\/implementation\/SPI) packages. However, during the build process, we do not have such strong separation, which can lead to tight coupling between contract and implementation, exposure of specific implementation classes via public API, and unnecessary propagation of dependencies to all callers of public APIs.\n\n## Problem\n\nThe current build process includes all dependencies together, which can lead to the following issues:\n\n* Tight coupling between contract and implementation, making it difficult to change or replace implementation without affecting the contract.\n* Exposure of specific implementation classes via public API, which can lead to tight coupling between different parts of the framework.\n* Unnecessary propagation of dependencies to all callers of public APIs, which can lead to increased complexity and maintenance costs.\n\n## Solution\n\nTo address these issues, we will introduce a new build tool that includes an includes\/excludes mechanism for dependencies. This will allow us to separate the implementation and contract packages more clearly, making it easier to manage dependencies and promote loose coupling between different parts of the framework.\n\n## Consequences\n\n* The new build tool will require additional configuration and setup, but will provide a more maintainable and scalable build process.\n* The separation of implementation and contract packages will make it easier to change or replace implementation without affecting the contract.\n* The exposure of specific implementation classes via public API will be reduced, promoting loose coupling between different parts of the framework.\n* The unnecessary propagation of dependencies to all callers of public APIs will be eliminated, reducing complexity and maintenance costs.\n\n## Rationale\n\nThe introduction of a new build tool with an includes\/excludes mechanism for dependencies will provide a more maintainable and scalable build process. This will allow us to separate the implementation and contract packages more clearly, making it easier to manage dependencies and promote loose coupling between different parts of the framework.","GenTime":"2024-08-10 00:59:40"}
{"File Name":"editions\/07-\u2705-ophan-analytics.md","Context":"## Context\\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\\n","Decision":"```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n","tokens":42,"id":669,"Prediction":"## Decision\n\n**Analytics Data Sending to Ophan**\n\n### Context\n\nWe need a way to send analytics data to Ophan (internal analytics analysis tool) from our mobile application. The MVP level requirements include sending screen views and events, which need to work across both iOS and Android platforms.\n\n### Decision\n\nAfter considering the requirements and available options, we have decided to use the Firebase Analytics SDK for both iOS and Android platforms to send analytics data to Ophan. Firebase Analytics provides a robust and scalable solution for collecting and sending analytics data, and it is widely used in the industry.\n\n### Rationale\n\n1. **Cross-platform support**: Firebase Analytics provides a unified API for both iOS and Android platforms, making it easy to integrate and maintain.\n2. **Ease of use**: Firebase Analytics has a simple and intuitive API, making it easy to send analytics data, including screen views and events.\n3. **Scalability**: Firebase Analytics is designed to handle large volumes of data and provides features such as data sampling and filtering to reduce the load on our servers.\n4. **Integration with Ophan**: Firebase Analytics provides a built-in feature to export data to Google BigQuery, which can be easily integrated with Ophan for analysis and visualization.\n\n### Consequences\n\n1. **Additional dependencies**: Our mobile application will have additional dependencies on the Firebase Analytics SDK, which may increase the size of our app and add complexity to our build process.\n2. **Data processing**: We will need to process and transform the analytics data sent to Ophan to ensure it meets the required format and schema.\n\n### Status\n\nThis decision is approved and will be implemented in the next release of our mobile application.\n\n### Links\n\n* Firebase Analytics documentation: [https:\/\/firebase.google.com\/docs\/analytics](https:\/\/firebase.google.com\/docs\/analytics)\n* Firebase Analytics SDK for iOS: [https:\/\/firebase.google.com\/docs\/analytics\/ios](https:\/\/firebase.google.com\/docs\/analytics\/ios)\n* Firebase Analytics SDK for Android: [https:\/\/firebase.google.com\/docs\/analytics\/android](https:\/\/firebase.google.com\/docs\/analytics\/android)","GenTime":"2024-08-10 01:02:45"}
{"File Name":"editions\/06-\u2705-to-use-step-functions.md","Context":"## Context\\nThe archiving process is time consuming and consists of many stages.\\n","Decision":"For the archiving process to be run as an amazon step function so we can break the process down into component stages.\\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\\nThis will also allow the publication process to fire the notification at the desired time.\\n","tokens":17,"id":672,"Prediction":"## Decision\n\n**Decision:** Implement a workflow-based archiving system to streamline and automate the archiving process.\n\n**Context:** The archiving process is time-consuming and consists of many stages.\n\n**Problem:** The current archiving process is manual, labor-intensive, and prone to errors. It involves multiple stages, including data selection, processing, and storage, which can take a significant amount of time and resources.\n\n**Reasoning:** Implementing a workflow-based archiving system will help to:\n\n* Automate the archiving process, reducing the time and effort required\n* Improve accuracy and reduce errors by defining clear steps and roles for each stage\n* Increase transparency and visibility of the archiving process\n* Allow for easier tracking and monitoring of the archiving process\n* Scale the archiving process to handle increasing volumes of data\n\n**Consequences:**\n\n* Initial investment in setting up the workflow-based archiving system\n* Potential disruption to the current archiving process during the transition\n* Ongoing maintenance and updates required to ensure the system remains effective and efficient\n\n**Alternatives Considered:**\n\n* Manual archiving process (status quo)\n* Implementing a custom-built archiving solution\n* Utilizing a third-party archiving solution\n\n**Decision Rationale:** A workflow-based archiving system is the best solution as it provides a structured and automated approach to the archiving process, while also allowing for flexibility and customization to meet the specific needs of the organization.","GenTime":"2024-08-10 01:04:05"}
{"File Name":"Sylius\/2020_05_13_handling_non_crud_operations_in_api.md","Context":"## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`\/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`\/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n","Decision":"* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n","tokens":225,"id":674,"Prediction":"## Decision\n\nTo handle non-CRUD operations over the REST API, we will use a combination of HTTP methods and a custom action identifier in the request body to define the specific action to be performed.\n\n## Context\n\nHandling non-CRUD operations over the REST API is not trivial. Once operations are beyond Create (POST\/PUT HTTP methods), Read (GET HTTP method), Update (PUT\/PATCH HTTP methods), and Delete (DELETE HTTP method), there is no clear recommendation on how to map other actions. These actions include, but are not limited to, changes of states (described in the form of workflow in a state machine) or command execution.\n\n## Decision Drivers\n\n* Solution should not be limited by its convention. We may need to support two different actions of the same name, but with different business logic. E.g., one object may have two transitions with the same name but described by two different state machines.\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\n\n## Solution\n\nTo address these requirements, we will use the following approach:\n\n* Use HTTP methods to define the general intent of the request (e.g., POST for creation, PUT for update, etc.).\n* Include a custom action identifier in the request body to specify the specific action to be performed. This could be a JSON object with a \"type\" property that indicates the action (e.g., \"changeState\", \"executeCommand\", etc.).\n* Use a standard HTTP response format (e.g., JSON) to return the result of the action, including any additional fields that were added during the execution of the request.\n* Use HATEOAS to expose the next possible actions that can be performed on the resource, by including a \"links\" property in the response that contains a list of available actions.\n\nFor example, a request to change the state of an object might look like this:\n\n```json\nPOST \/objects\/123\/state\n{\n  \"type\": \"changeState\",\n  \"newState\": \"active\"\n}\n```\n\nThe response might look like this:\n\n```json\n{\n  \"result\": \"success\",\n  \"links\": [\n    {\n      \"rel\": \"nextState\",\n      \"href\": \"\/objects\/123\/state\",\n      \"type\": \"changeState\",\n      \"newState\": \"inactive\"\n    }\n  ]\n}\n```\n\nThis approach provides a flexible and extensible way to handle non-CRUD operations over the REST API, while also providing a clear and easy-to-understand way of defining and executing these actions.","GenTime":"2024-08-10 01:05:08"}
{"File Name":"qc-atlas\/0005-use-OpenAPI.md","Context":"## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n","Decision":"* readable API documentation\\n* effort of manually creating client services\\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\\n### Positive Consequences\\n* Standardized documentation of the API\\n* Automatic service generation for clients is possible\\n### Negative Consequences <!-- optional -->\\n* OpenAPI annotations have to be maintained\\n","tokens":56,"id":693,"Prediction":"Here is the Architectural Decision Record (ADR):\n\n**Decision:** Use OpenAPI (Swagger) for API documentation\n\n**Context and Problem Statement:** The API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\n\n**Decision Drivers:**\n\n* Readable API documentation: OpenAPI (Swagger) provides a clear and concise way to document the API, making it easy for developers to understand the API's functionality and usage.\n* Effort of manually creating client services: By using OpenAPI, we can automatically generate client services for various programming languages, reducing the manual effort required to create clients.\n\n**Decision Description:** We will use OpenAPI (Swagger) to document the API, providing both human-readable documentation and machine-readable specifications. This will enable us to automatically generate client services for various programming languages, reducing the manual effort required to create clients.\n\n**Rationale:** OpenAPI (Swagger) is a widely adopted standard for API documentation, providing a clear and concise way to document the API's functionality and usage. By using OpenAPI, we can ensure that the API documentation is consistent and easy to understand, making it easier for developers to use the API. Additionally, OpenAPI's machine-readable specifications enable us to automatically generate client services, reducing the manual effort required to create clients.\n\n**Status:** Accepted\n\n**Implementor:** [Your Name]\n\n**Created:** [Date]\n\n**Updated:** [Date]\n\n**References:**\n\n* OpenAPI (Swagger) documentation: [link]\n* OpenAPI (Swagger) generator: [link]","GenTime":"2024-08-10 01:12:00"}
{"File Name":"qc-atlas\/0002-monorepo.md","Context":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n","Decision":"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","tokens":22,"id":697,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to split the components in the PlanQK platform into individual repositories.\n\n## Context\n\nThe PlanQK platform is a complex system consisting of multiple components, including a frontend, backend, and database. As the platform grows and evolves, it becomes increasingly difficult to manage and maintain the codebase as a single monolithic repository.\n\n## Problem Statement\n\nThe main concerns that led to this decision are:\n\n* Complexity: The current monolithic repository is becoming difficult to navigate and understand, making it challenging for new team members to onboard and for existing team members to contribute to specific components.\n* Isolation: The components are not independent and are tightly coupled, making it difficult to develop and test individual components without affecting the entire system.\n* Scalability: As the platform grows, it will be more efficient to develop and deploy individual components independently, rather than as part of a single repository.\n* Reusability: By splitting the components into individual repositories, we can reuse and share code between components more easily.\n\n## Decision Rationale\n\nWe have decided to split the components into individual repositories because:\n\n* It will improve code organization and maintainability by separating concerns and reducing complexity.\n* It will enable more efficient development and testing of individual components, reducing the risk of breaking the entire system.\n* It will allow for more flexibility and scalability, as individual components can be developed and deployed independently.\n* It will promote reusability and sharing of code between components, reducing duplication and improving overall efficiency.\n\n## Consequences\n\nThe main consequences of this decision are:\n\n* Increased complexity in terms of managing multiple repositories and coordinating changes.\n* Potential for increased overhead in terms of testing and deployment.\n* Need for more careful planning and coordination to ensure that changes to individual components do not break the overall system.\n\n## Implementation\n\nTo implement this decision, we will:\n\n* Split the existing monolithic repository into individual repositories for each component.\n* Create separate build and deployment pipelines for each component.\n* Establish clear guidelines and processes for managing and coordinating changes across the multiple repositories.\n* Monitor and review the impact of this decision on the platform's overall performance and maintainability.","GenTime":"2024-08-10 01:13:43"}
{"File Name":"qc-atlas\/0008-testcontainers.md","Context":"## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n","Decision":"Chosen option: [Testcontainers](https:\/\/www.testcontainers.org\/), because it simplifies the test execution process\\n### Positive Consequences <!-- optional -->\\n* It is not mandatory to ensure postgres is running before starting the tests\\n","tokens":34,"id":699,"Prediction":"## Decision\n\nTo ensure a PostgreSQL instance is always available for running service tests, we will use a Docker container to host the database. This will allow us to easily manage and maintain the instance, as well as ensure that it is always running and available for testing.\n\n## Context\n\nRunning Service Tests requires a PostgreSQL instance to be available. Currently, this instance needs to be manually launched or started automatically. This can be error-prone and may lead to issues with testing.\n\n## Solution\n\nWe will use Docker to create a container that hosts the PostgreSQL instance. This will allow us to:\n\n* Easily manage and maintain the instance\n* Ensure that the instance is always running and available for testing\n* Isolate the instance from the rest of the system, reducing the risk of conflicts or interference\n* Simplify the process of setting up and tearing down the instance for testing\n\n## Rationale\n\nUsing a Docker container to host the PostgreSQL instance provides several benefits. It allows us to easily manage and maintain the instance, as well as ensure that it is always running and available for testing. Additionally, using a container provides isolation from the rest of the system, reducing the risk of conflicts or interference.\n\n## Consequences\n\nBy using a Docker container to host the PostgreSQL instance, we will:\n\n* Simplify the process of setting up and tearing down the instance for testing\n* Ensure that the instance is always running and available for testing\n* Reduce the risk of conflicts or interference with the rest of the system\n* Make it easier to manage and maintain the instance\n\n## Alternatives Considered\n\nWe considered other alternatives, such as:\n\n* Manually launching or starting the PostgreSQL instance\n* Using a cloud-based database service\n* Using a different database management system\n\nHowever, using a Docker container to host the PostgreSQL instance provides the most benefits and is the most straightforward solution.","GenTime":"2024-08-10 01:14:28"}
{"File Name":"ionic-dummy-repo\/ADR-001.md","Context":"## Context\\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\\n### Who Was Involved in This Decision\\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\n","Decision":"- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\\n","tokens":93,"id":701,"Prediction":"## Decision\n\n**State Management Library Selection: Developer Autonomy**\n\n### Context\n\nAs the speed of state-management library, preferences, and trends change rapidly, it is essential to strike a balance between providing a starting point for new projects and allowing developers to make informed decisions about the best tool for the job.\n\n### Decision\n\nAfter careful consideration, we have decided to grant developers autonomy in selecting the state management library that best suits their project's requirements, complexity, and personal preferences. This decision is based on the following reasons:\n\n* Developers bring unique skills and ideas to the table, and it is essential to empower them to make decisions that align with their project's specific needs.\n* The rapid pace of change in state-management libraries means that any decision made today may become outdated quickly.\n* By allowing developers to choose their own libraries, we can ensure that the project is built with the most suitable tools for the job, rather than forcing a particular solution that may not be the best fit.\n\n### Consequences\n\n* Developers will have more freedom to choose the state management library that best suits their project's requirements.\n* This may lead to a more diverse set of libraries being used across projects, which can be beneficial in terms of learning and innovation.\n* However, it also means that we may need to invest more time and resources in supporting a wider range of libraries and technologies.\n\n### Rationale\n\nThe decision to grant developer autonomy in selecting state management libraries is based on the principle of \"You Ain't Gonna Need It\" (YAGNI). By not imposing a specific solution, we are allowing developers to make decisions based on the actual needs of their projects, rather than trying to anticipate future requirements. This approach also aligns with the values of autonomy, mastery, and purpose, which are essential for creating a fulfilling and productive development experience.\n\n### Status\n\nThis decision is in effect as of the current date.\n\n### Links\n\n* Related discussions: [link to relevant discussions or meetings]\n\n### Notes\n\n* This decision does not preclude the possibility of providing guidance or recommendations on state management libraries, but rather acknowledges the importance of developer autonomy in making decisions about their projects.","GenTime":"2024-08-10 01:15:32"}
{"File Name":"radiant-mlhub\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs\/adr` directory within this repository. Instead of using the \"deprecated\" and \"superseded\" status value, we will move ADRs that are no longer applicable into the `docs\/adr\/archive` directory in this repository.\\n","tokens":16,"id":704,"Prediction":"Here is an Architectural Decision Record (ADR) for your project:\n\n**Decision**\n\n**ADR-001: Choosing a Frontend Framework for the Project**\n\n**Context**\n\nAs we begin building the frontend of our project, we need to decide on a suitable framework to use. The project requires a robust and scalable solution that can handle a large amount of user traffic and provide a seamless user experience.\n\n**Problem**\n\nWe need a framework that can efficiently handle complex UI components, provide a robust routing system, and integrate well with our existing backend API.\n\n**Decision**\n\nAfter considering various options, we have decided to use React as our frontend framework. React's virtual DOM and one-way data binding approach make it an ideal choice for building scalable and maintainable UI components. Additionally, React's large community and extensive ecosystem of libraries and tools make it easier to find resources and integrations for our project.\n\n**Consequences**\n\n* We will need to invest time in learning React and its ecosystem.\n* We will need to adapt our existing UI components to use React.\n* We will need to integrate React with our existing backend API using RESTful APIs.\n\n**Rationale**\n\nWe chose React because of its:\n\n* Scalability: React's virtual DOM and one-way data binding approach make it well-suited for building complex and scalable UI components.\n* Maintainability: React's component-based architecture makes it easier to maintain and update individual components without affecting the rest of the application.\n* Community: React has a large and active community, which provides a wealth of resources, libraries, and integrations.\n\n**References**\n\n* React official documentation: https:\/\/reactjs.org\/\n* React ecosystem documentation: https:\/\/reactjs.org\/docs\/ecosystem.html\n\n**Status**\n\nThis decision is finalized and has been implemented in our project.","GenTime":"2024-08-10 01:17:06"}
{"File Name":"Corvus.Tenancy\/0004-v2-to-v3-transition.md","Context":"## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\\n","Decision":"Upgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\\n1. using nothing but v2\\n1. using v3 libraries mostly (see below) in v2 mode\\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\\n1. using v3 libraries in non-transitional mode\\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\\n### Phase 2: using v3 libraries, operating in v2 mode\\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries\u2014the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\\nThe one difference in behaviour (the reason we describe this as \"mostly\" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\\n### Configuration migration\\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\\n","tokens":171,"id":714,"Prediction":"## Decision\n\n**ADR 0005: Migrating Corvus.Tenancy v2 to v3 without Disruption**\n\n### Context\n\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\n\n* a tenant may exist in which only a subset of its storage containers exist\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\n\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\n\n### Decision\n\nTo migrate `Corvus.Tenancy` v2 to v3 without disruption, we will implement a phased migration approach. This approach will ensure that the migration process is gradual, allowing for a smooth transition from v2 to v3 without affecting the overall system availability.\n\nThe phased migration approach will consist of the following steps:\n\n1. **Initial Assessment**: Identify all applications currently using `Corvus.Tenancy` v2 and assess their current state, including the number of tenants, storage containers, and compute farms involved.\n2. **Container Creation**: Implement a mechanism to create missing storage containers for existing tenants. This will ensure that all necessary containers are created before attempting to upgrade the application to v3.\n3. **Compute Farm Upgrade**: Upgrade the compute farms to v3, ensuring that all components are running the latest version. This will allow for a gradual transition from v2 to v3.\n4. **Application Upgrade**: Upgrade the application to v3, taking into account the changes introduced in `Corvus.Tenancy` v3. This will require modifications to the application's code to create storage containers when onboarding a client.\n5. **Monitoring and Testing**: Monitor the system closely during the migration process and perform thorough testing to ensure that the migration was successful and the system is functioning as expected.\n\n### Rationale\n\nThe phased migration approach allows for a gradual transition from v2 to v3, minimizing the risk of disruption to the system. By creating missing storage containers and upgrading compute farms and applications in a controlled manner, we can ensure a smooth migration process.\n\n### Consequences\n\nThe phased migration approach will require additional effort and resources to implement and test. However, the benefits of a smooth transition from v2 to v3, including minimizing downtime and ensuring system availability, outweigh the additional effort required.\n\n### Status\n\nThis decision is approved and will be implemented as part of the `Corvus.Tenancy` v3 migration process.","GenTime":"2024-08-10 01:21:25"}
{"File Name":"read-more-api\/0007-deploy-to-heroku.md","Context":"## Context\\nThe application needs to be deployed somewhere.\\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\\nWe want the setup process to be as simple as possible.\\n","Decision":"We will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n","tokens":193,"id":718,"Prediction":"## Decision\n\nWe will deploy our application on Heroku using their Container Registry and free tier PostgreSQL option.\n\n## Context\n\nAs mentioned in the context, our application needs to be deployed somewhere, and we have several options for container orchestration platforms. However, setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines. Heroku, on the other hand, is a Platform as a Service (PaaS) provider that helps with the deployment of applications. They have a Container Registry solution that handles the deployment of Docker images into suitable containers, and they offer a free tier with a limited database size.\n\n## Problem\n\nWe want the setup process to be as simple as possible, and Heroku's free tier offers a convenient solution that meets our requirements.\n\n## Decision Drivers\n\n* Ease of setup: Heroku's free tier requires minimal setup and configuration, making it a convenient option for our application.\n* Cost: The free tier is free, which is a significant cost savings compared to setting up and maintaining our own container orchestration platform.\n* Database: Heroku's free tier PostgreSQL option provides a limited database size, which is sufficient for our application's needs.\n\n## Trade-offs\n\n* Limited scalability: Heroku's free tier has limitations on the number of rows in the database, which may become a constraint as our application grows.\n* Limited customization: As a PaaS, Heroku provides a managed environment, which may limit our ability to customize the underlying infrastructure.\n\n## Consequences\n\nWe will deploy our application on Heroku using their Container Registry and free tier PostgreSQL option. This will simplify the setup process and reduce our hosting costs. However, we will need to monitor our application's growth and adjust our deployment strategy if we exceed the limitations of the free tier.","GenTime":"2024-08-10 01:22:36"}
{"File Name":"datalab\/0012-public-and-private-subnets.md","Context":"## Context\\nWe would like to minimise our externally exposed footprint. To achieve this we have\\nalready decided that access to cluster servers will be via a Bastion server. We do need\\nto have some externally accessible services and need to decide how to achieve this.\\n","Decision":"We have decided to have external proxy servers for each environment that expose ports 80\\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\\n","tokens":55,"id":729,"Prediction":"## Decision\n\n**Minimize Externally Exposed Footprint: External Service Access via Load Balancer with SSL Termination**\n\n## Context\n\nAs discussed, we aim to minimize our externally exposed footprint by limiting direct access to our cluster servers. We have already implemented a Bastion server for access to cluster servers. However, we still require some externally accessible services.\n\n## Problem\n\nWe need to ensure that our externally accessible services are secure and minimize the risk of exposure. We must balance this need with the requirement for external access.\n\n## Decision\n\nTo achieve this, we will use a load balancer with SSL termination. This will provide an additional layer of security and isolation between the external world and our internal services. The load balancer will handle incoming SSL connections and forward them to our internal services, which will only receive plain text connections.\n\n## Rationale\n\n* Load balancer with SSL termination provides an additional layer of security, as the SSL\/TLS encryption and decryption will occur outside of our internal network.\n* This approach allows us to minimize the number of external-facing services, reducing the attack surface.\n* The load balancer can be configured to handle access control, rate limiting, and other security features to further minimize the risk of exposure.\n* This solution is scalable and can be easily integrated with our existing infrastructure.\n\n## Consequences\n\n* Additional infrastructure cost for the load balancer and SSL termination.\n* Increased complexity in managing the load balancer and SSL certificates.\n* Potential performance impact due to the additional layer of encryption and decryption.\n\n## Status\n\nApproved.\n\n## Implemented By\n\n[Your Name]\n\n## Date\n\n[Current Date]","GenTime":"2024-08-10 01:27:10"}
{"File Name":"datalab\/0011-cluster-authentication-by-ssh-key.md","Context":"## Context\\nWe need to configure access to the servers in the cluster and need to decide between\\npassword authentication and SSH key pair authentication.\\n","Decision":"We have decided that all server log on authentication will use SSH key pair\\nauthentication. The public keys will be provisioned onto the server at server creation\\nfor the default deploy user. A password will be required for sudo.\\n","tokens":29,"id":731,"Prediction":"## Decision\n\n**Decision:** We will use SSH key pair authentication for accessing the servers in the cluster.\n\n**Context:** We need to configure access to the servers in the cluster and need to decide between password authentication and SSH key pair authentication.\n\n**Problem:** We want to ensure secure and efficient access to the servers in the cluster. Password authentication can be vulnerable to brute-force attacks and phishing, while SSH key pair authentication provides a more secure and convenient way to authenticate.\n\n**Reasoning:**\n\n* SSH key pair authentication is more secure: It uses public-key cryptography to authenticate the client and server, making it more resistant to eavesdropping and tampering.\n* SSH key pair authentication is more convenient: It eliminates the need to remember and enter passwords, making it easier to access the servers.\n* SSH key pair authentication is more scalable: It allows for easy management of access rights and revocation of access for specific users or groups.\n* SSH key pair authentication is more compliant: It meets the security requirements of many organizations and regulations, such as PCI-DSS and HIPAA.\n\n**Consequences:**\n\n* We will need to generate and distribute SSH keys to all users who need access to the servers.\n* We will need to configure the SSH servers to use key-based authentication.\n* We will need to ensure that the SSH keys are properly secured and stored.\n* We will need to monitor and audit the use of SSH keys to ensure compliance and security.\n\n**Alternatives:**\n\n* Password authentication: This would provide a simpler solution, but it would also be less secure and more prone to attacks.\n* Other authentication methods: We could consider using other authentication methods, such as multi-factor authentication or smart cards, but these would likely require additional infrastructure and configuration.\n\n**Status:** This decision is final and will be implemented as part of our cluster configuration.","GenTime":"2024-08-10 01:27:58"}
{"File Name":"datalab\/0036-replace-kong-with-nginx-ingress-controller.md","Context":"## Context\\nWe have reached a point where we need to secure third party web applications that provide\\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\\nsource version and we haven't had and response from our efforts to contact them.\\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\\nwas released provides a Kubernetes native route for the same functionality.\\n","Decision":"We have decided the replace the Kong proxy with an Nginx Ingress Controller in the\\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\\nThis should provide all of the same functionality as Kong and in addition should provide\\na mechanism for token and cookie authentication using the `auth_url` annotation that\\nwraps the underlying Nginx `auth_request` module.\\n","tokens":91,"id":733,"Prediction":"## Decision\n\n**Decision:** To use the Nginx Ingress controller to secure third-party web applications that provide no security of their own, as an alternative to the Kong proxy.\n\n**Context:** We have reached a point where we need to secure third-party web applications that provide no security of their own. The Kong proxy does not offer a mechanism for this in the open-source version and we haven't had a response from our efforts to contact them. We believe that the Nginx Ingress controller, which has been available since Kubernetes 1.7, provides a Kubernetes-native route for the same functionality.\n\n**Reasoning:**\n\n* The Kong proxy does not offer a mechanism for securing third-party web applications in its open-source version.\n* We have not received a response from our efforts to contact Kong to request this feature.\n* The Nginx Ingress controller is a widely-used and well-established solution for ingress routing and security in Kubernetes environments.\n* Using the Nginx Ingress controller provides a Kubernetes-native solution that is well-integrated with our existing infrastructure.\n\n**Consequences:**\n\n* We will need to configure and deploy the Nginx Ingress controller in our Kubernetes environment.\n* We will need to configure the Nginx Ingress controller to secure the third-party web applications.\n* We may need to modify our existing infrastructure and configuration to work with the Nginx Ingress controller.\n\n**Alternatives:**\n\n* We could continue to explore other solutions, such as using a third-party ingress controller or implementing a custom solution.\n* We could also consider using a different proxy solution that provides the necessary security features.\n\n**Status:** Approved\n\n**Implementor:** [Name]\n\n**Date:** [Date]","GenTime":"2024-08-10 01:28:50"}
{"File Name":"datalab\/0009-bastion-for-cluster-access.md","Context":"## Context\\nIt will be necessary to access the servers that form the Datalabs network but we do not\\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\\n","Decision":"We have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n","tokens":44,"id":745,"Prediction":"## Decision\n\nTo access the servers that form the Datalabs network, we will implement a secure and isolated solution that allows us to connect to the Datalabs network from within the JASMIN tenancy without exposing any services outside of the tenancy that we do not have to.\n\n## Context\n\nAs per the requirement, we need to access the servers that form the Datalabs network, but we do not want to expose any services outside of the JASMIN tenancy that we do not have to. This is a security concern as it may compromise the security of our system and data.\n\n## Problem\n\nThe main problem is that we need to access the Datalabs network from within the JASMIN tenancy, but we cannot expose any services outside of the tenancy that we do not have to. This requires a secure and isolated solution that allows us to connect to the Datalabs network without compromising the security of our system and data.\n\n## Decision\n\nTo solve this problem, we will implement a secure and isolated solution that allows us to connect to the Datalabs network from within the JASMIN tenancy. We will use a VPN (Virtual Private Network) to create a secure and isolated connection between the JASMIN tenancy and the Datalabs network. This will ensure that the data and services within the Datalabs network are not exposed outside of the JASMIN tenancy, and that our system and data remain secure.\n\n## Rationale\n\nThe rationale for this decision is that a VPN provides a secure and isolated connection between the JASMIN tenancy and the Datalabs network. It will encrypt the data and ensure that it is not intercepted or accessed by unauthorized parties. Additionally, a VPN will allow us to control access to the Datalabs network from within the JASMIN tenancy, ensuring that only authorized personnel have access to the network.\n\n## Consequences\n\nThe consequences of this decision are that we will be able to access the Datalabs network from within the JASMIN tenancy in a secure and isolated manner. This will ensure that our system and data remain secure and that we do not expose any services outside of the JASMIN tenancy that we do not have to. However, this may require additional setup and configuration of the VPN, and may also require additional resources and infrastructure to support the VPN connection.\n\n## Alternatives Considered\n\nWe considered other alternatives such as using a proxy server or a load balancer to access the Datalabs network, but these alternatives did not provide the same level of security and isolation as a VPN. We also considered using a different network architecture, but this would have required significant changes to our existing infrastructure and would have been more complex and costly.","GenTime":"2024-08-10 01:34:27"}
{"File Name":"datalab\/0002-ansible-for-provisioning-tool.md","Context":"## Context\\nWe need a tool to provision servers and software for the datalabs project.\\n","Decision":"We will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\\nteam have experience using it.\\n","tokens":20,"id":771,"Prediction":"## Decision\n\n**Decision:** We will use Ansible as the tool for provisioning servers and software for the datalabs project.\n\n## Context\n\nWe need a tool to provision servers and software for the datalabs project.\n\n## Problem\n\nWe need to automate the process of setting up and configuring servers and software for the datalabs project. This includes tasks such as installing operating systems, configuring network settings, and deploying software applications. We want to ensure that this process is repeatable, efficient, and scalable.\n\n## Reasoning\n\nWe have evaluated several options for provisioning servers and software, including Puppet, Chef, and Ansible. After careful consideration, we have decided to use Ansible for the following reasons:\n\n* Ansible is agentless, which means that it does not require any software to be installed on the target servers. This makes it easy to use and reduces the attack surface.\n* Ansible is highly scalable and can handle large numbers of servers and complex networks.\n* Ansible has a large and active community, which means that there are many pre-built modules and playbooks available for common tasks.\n* Ansible is easy to learn and use, even for developers who are new to automation.\n\n## Decision Drivers\n\n* The need for a scalable and efficient provisioning process\n* The requirement for agentless automation\n* The availability of pre-built modules and playbooks\n* The ease of use and learning curve\n\n## Consequences\n\n* We will need to learn Ansible and create playbooks for provisioning servers and software.\n* We will need to ensure that Ansible is properly configured and secured.\n* We will need to test and validate the provisioning process to ensure that it is working correctly.\n\n## Pros and Cons\n\nPros:\n\n* Ansible is agentless, which reduces the attack surface.\n* Ansible is highly scalable and can handle large numbers of servers and complex networks.\n* Ansible has a large and active community, which means that there are many pre-built modules and playbooks available for common tasks.\n\nCons:\n\n* Ansible requires a good understanding of YAML and Python.\n* Ansible can be complex to learn and use, especially for large and complex environments.\n\n## Rationale\n\nWe believe that the benefits of using Ansible outweigh the drawbacks. Ansible's agentless architecture, scalability, and community support make it an ideal choice for provisioning servers and software for the datalabs project. While there may be a learning curve, we believe that the benefits of using Ansible will be well worth the effort.","GenTime":"2024-08-10 01:46:17"}
{"File Name":"datalab\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":774,"Prediction":"Here is an Architectural Decision Record (ADR) for the software:\n\n**Decision**\nUse a Microservices Architecture for the New Project\n\n**Context**\nWe need to record the architectural decisions made on this project.\n\n**Problem**\nThe project requires a scalable and flexible architecture that can handle a large volume of users and data. A monolithic architecture may not be able to meet these requirements, and would limit the ability to scale and maintain the system.\n\n**Decision**\nWe will use a Microservices Architecture (MSA) for the new project. This will allow us to break down the system into smaller, independent services that can be developed, tested, and deployed independently. This will provide several benefits, including:\n\n* Scalability: Each service can be scaled independently, allowing us to allocate resources more efficiently.\n* Flexibility: Each service can be developed and deployed separately, allowing us to use different technologies and frameworks for each service.\n* Resilience: If one service experiences issues, it will not bring down the entire system.\n\n**Rationale**\nThe benefits of using a Microservices Architecture outweigh the potential drawbacks, such as increased complexity and additional overhead for communication between services. The increased scalability, flexibility, and resilience provided by MSA will allow us to build a system that can meet the growing demands of our users.\n\n**Consequences**\n\n* We will need to develop and maintain multiple services, which will increase the overall complexity of the system.\n* We will need to implement communication between services, which will add additional overhead.\n* We will need to ensure that each service is properly secured and monitored.\n\n**Status**\nApproved\n\n**Date**\n2023-02-20\n\n**Links**\n* [Link to relevant documentation or code](insert link)\n* [Link to relevant design decisions or diagrams](insert link)\n\nThis ADR provides a clear and concise record of the architectural decision made for the project, including the context, problem, decision, rationale, consequences, and status.","GenTime":"2024-08-10 01:47:38"}
{"File Name":"ditto\/DADR-0004-signal-enrichment.md","Context":"## Context\\nSupporting a new feature, the so called [signal enrichment](https:\/\/github.com\/eclipse-ditto\/ditto\/issues\/561), raises a few\\nquestions towards throughput and scalability impact of that new feature.\\nIn the current architecture, Ditto internally publishes events (as part of the applied \"event sourcing\" pattern) for\\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\\nchanged fields.\\nThe \"signal enrichment\" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\\nThe following alternatives were considered on how to implement that feature:\\n1. Sending along the complete `Thing` state in each event in the cluster\\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\\n* downside: bigger payload sent around\\n* downside: a lot of deserialization effort for all event consuming services\\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\\n* downside: overall a lot of overhead for probably only few consumers\\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upside: no additional payload for existing events\\n* upside: data is only enriched for sessions\/connections really using that feature\\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\\n* delayed event publishing\\n* additional deserialization efforts\\n* potentially asking for the same static values each time\\n3. Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upsides: all upsides of approach 2 except that policy is always up-to-date\\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\\n","Decision":"We provide 2 different facade implementations providing approach 2 and 3:\\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\\n* the overall size of this cache is configured, by default to `20,000` entries\\n* there is an additional \"smart-update\" mechanism for cache entries related to enrichment of twin events:\\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\\n","tokens":500,"id":775,"Prediction":"## Decision\n\nAfter careful consideration of the alternatives, we decide to implement the \"signal enrichment\" feature using approach 3: Cache-based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event.\n\n## Context\n\nThe \"signal enrichment\" feature aims to support defining `extraFields` to be sent out to external event subscribers, which raises concerns about the throughput and scalability impact of this new feature. The alternatives considered were sending along the complete `Thing` state in each event, enriching the data for sessions\/connections which selected `extraFields`, and cache-based enriching of the data.\n\n## Decision Rationale\n\nWe choose the cache-based approach because it balances the trade-offs between payload size, deserialization effort, and policy enforcement. By caching the enriched data and policy information, we can reduce the number of remoting calls and deserialization efforts, while still ensuring that the data is up-to-date within a configurable time frame. This approach also allows us to mitigate the downsides of approach 2, such as delayed event publishing and additional deserialization efforts.\n\n## Consequences\n\n* The cache-based approach will require additional memory and cache management to ensure that the cached data and policy information are up-to-date and do not grow indefinitely.\n* The configurable time frame for cache expiration will need to be carefully set to balance the trade-offs between data freshness and performance.\n* The implementation will need to ensure that the cache is properly invalidated and updated when changes are made to the `Thing` state or policy.\n\nOverall, the cache-based approach provides a good balance between performance, scalability, and data freshness, making it the most suitable choice for implementing the \"signal enrichment\" feature.","GenTime":"2024-08-10 01:48:02"}
{"File Name":"ditto\/DADR-0003-do-not-interrupt-threads.md","Context":"## Context\\nSonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch\\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\\ninterruption.\\n","Decision":"We will ignore [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142).\\n","tokens":114,"id":777,"Prediction":"## Decision\n\n**Decision:** We will not call `Thread.currentThread().interrupt()` to silence SonarQube's complaint about catching `InterruptedException` without re-throwing it.\n\n**Context:** SonarQube's rule RSPEC-2142 complains when we catch `InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block. However, simply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous due to the way JUnit reuses threads, which can break the build in unpredictable ways, affecting several tests after the actual site of interruption.\n\n**Rationale:** We prioritize the stability and reliability of our tests over SonarQube's rule. Interrupting threads in a way that can affect the build in unpredictable ways is not acceptable. Instead, we will re-throw the `InterruptedException` to allow the test to fail properly, rather than trying to silence SonarQube's complaint.\n\n**Consequences:**\n\n* We will need to address SonarQube's complaint about catching `InterruptedException` without re-throwing it.\n* We will need to consider alternative solutions to silence SonarQube's complaint, such as using a custom rule or configuration to exempt our project from this rule.\n* We will prioritize the stability and reliability of our tests over SonarQube's rule.\n\n**Alternatives:**\n\n* Call `Thread.currentThread().interrupt()` to silence SonarQube's complaint, despite the potential risks.\n* Use a custom rule or configuration to exempt our project from SonarQube's rule.\n* Re-throw the `InterruptedException` to allow the test to fail properly.","GenTime":"2024-08-10 01:48:55"}
{"File Name":"ditto\/DADR-0006-merge-payload.md","Context":"## Context\\nWe want to allow partial or merge updates of things with a single request.\\n","Decision":"A merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https:\/\/tools.ietf.org\/html\/rfc7396).\\n* has the request header `content-type` set to `application\/merge-patch+json`.\\n","tokens":19,"id":779,"Prediction":"## Decision\n\nTo allow partial or merge updates of things with a single request, we will implement a PATCH request method and utilize JSON Patch (RFC 6902) to specify the changes to be made.\n\n## Context\n\nAs per the requirement, we want to enable users to update specific attributes or merge changes of an existing thing with a single request. This is necessary to provide a more efficient and user-friendly way of updating things, especially when dealing with complex data structures.\n\n## Decision Rationale\n\nImplementing PATCH requests with JSON Patch allows us to achieve the following benefits:\n\n*   Efficient updates: By specifying only the changes to be made, the server can update the thing without having to retrieve the entire object and reapply the changes.\n*   Flexibility: JSON Patch provides a flexible way to specify the changes, allowing for a wide range of update scenarios, including partial updates, merges, and deletions.\n*   Improved user experience: By providing a single request method for updates, we can simplify the user interface and reduce the number of requests required to update a thing.\n\n## Pros and Cons\n\nPros:\n\n*   Efficient updates\n*   Flexible update scenarios\n*   Improved user experience\n\nCons:\n\n*   Additional complexity in the server-side implementation\n*   Potential for errors if the JSON Patch is malformed\n\n## Consequences\n\nThe implementation of PATCH requests with JSON Patch will require changes to the server-side API and the client-side code. We will need to update the API documentation and provide guidance on how to use JSON Patch for updates.\n\n## Alternatives Considered\n\nWe considered the following alternatives:\n\n*   Implementing a separate update method for each attribute\n*   Using a custom update syntax\n*   Disallowing partial updates and requiring the entire thing to be updated\n\nHowever, these alternatives were deemed less effective or more complex than implementing PATCH requests with JSON Patch.","GenTime":"2024-08-10 01:49:58"}
{"File Name":"frontend-library\/0003-use-brunch.md","Context":"## Context\\nThe styling of sites developed for NHSUK need to be as consistent as possible.\\nCurrently each new site starts by copying what the last developed site had done\\nand building on that. This results in the newer sites looking different than\\nthe older sites, which would not be a problem if each site was easily updated\\nto use the same, most recent, set of styles. Currently there is no central\\nstore for the styles which means there is no easy way to find and use them\\neither.\\nThe ability to review the styles in a development environment before they are\\nmade available to a wider audience is important. As is the ability to use well\\nknown technologies such as [SASS](http:\/\/sass-lang.com\/).\\n","Decision":"We will use [Brunch](http:\/\/brunch.io\/) to accomplish a number of tasks\\nincluding:\\n* Running the site used in the development environment\\n* Generating the compiled assets\\n","tokens":153,"id":783,"Prediction":"## Decision\n\n**Decision:** Implement a centralized style repository using Sass (SCSS) and a version control system, allowing for consistent styling across NHSUK sites and enabling easy updates and review of styles.\n\n## Context\n\nThe current approach of copying styles from previous sites and building upon them leads to inconsistent styling across NHSUK sites. This makes it difficult to maintain and update the styles, as well as review and approve changes before they are made available to a wider audience. The lack of a central store for styles also hinders the ability to easily find and use the most recent set of styles.\n\n## Problem\n\nThe current approach leads to:\n\n* Inconsistent styling across NHSUK sites\n* Difficulty in maintaining and updating styles\n* Lack of ability to review and approve changes before they are made available\n* No central store for styles, making it hard to find and use the most recent set of styles\n\n## Solution\n\nImplementing a centralized style repository using Sass (SCSS) and a version control system will allow for:\n\n* Consistent styling across NHSUK sites\n* Easy updates and maintenance of styles\n* Ability to review and approve changes before they are made available\n* Centralized store for styles, making it easy to find and use the most recent set of styles\n\n## Consequences\n\n* Initial setup and configuration of the centralized style repository\n* Training and onboarding of developers to use the new system\n* Potential for some initial disruption to development workflows\n* Long-term benefits of consistent styling, easier maintenance, and improved review and approval processes\n\n## Rationale\n\nThe use of Sass (SCSS) provides a powerful and flexible way to manage styles, while a version control system ensures that all changes are tracked and can be easily rolled back if needed. A centralized style repository allows for easy access and management of styles, making it easier to maintain consistency across NHSUK sites.","GenTime":"2024-08-10 01:51:20"}
{"File Name":"frontend-library\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":784,"Prediction":"Here is a sample Architectural Decision Record (ADR):\n\n**Decision**\n\nTitle: Implement a Microservices Architecture\n\n**Context**\n\nWe need to record the architectural decisions made on this project.\n\n**Problem**\n\nThe project requires a scalable and maintainable architecture to handle the increasing traffic and user base. The current monolithic architecture is becoming a bottleneck and is difficult to maintain.\n\n**Decision**\n\nWe will implement a microservices architecture, where each service is responsible for a specific business capability and communicates with other services through APIs. This will allow for greater scalability, flexibility, and maintainability.\n\n**Rationale**\n\nA microservices architecture will enable us to:\n\n* Scale individual services independently, reducing the risk of cascading failures\n* Develop and deploy services more quickly, using smaller, more focused teams\n* Improve fault tolerance, as a failure in one service will not affect the entire system\n* Enhance maintainability, as each service has a clear and well-defined responsibility\n\n**Consequences**\n\n* Increased complexity in terms of communication and integration between services\n* Requires a more sophisticated monitoring and logging system to track service performance and errors\n* May require additional infrastructure and resources to support multiple services\n\n**Status**\n\nApproved\n\n**Date**\n\n2023-02-20\n\n**Links**\n\n* [Link to related documentation or design diagrams]\n\nThis ADR provides a clear and concise record of the architectural decision, including the context, problem, decision, rationale, consequences, and status. It serves as a reference point for future developers and stakeholders, providing a clear understanding of the design choices made on the project.","GenTime":"2024-08-10 01:51:42"}
{"File Name":"infrastructure-adrs\/0010-message-broker.md","Context":"## Context and Problem Statement\\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\\n* Each service must have knowledge about many other services.\\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\\nIn addition, there are already places within SDR were we have re-invented a message broker and\/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\\n","Decision":"TBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n","tokens":224,"id":786,"Prediction":"## Decision\n\nIntroduce a message broker to support asynchronous communication between services in SDR, allowing for decoupling of services and improving the system's resilience and scalability.\n\n## Context\n\nAs SDR evolves into a set of single-purpose services, the current architecture is becoming increasingly fragile under load and as the number of services increases. The use of synchronous HTTP communication between services leads to:\n\n* Each service having knowledge about many other services\n* Services depending on many other services, making the system prone to failures and performance issues\n* Synchronous HTTP having a performance penalty for operations where a response is not required\n* The need for significant coding effort to account for the unreliable network environment\n\nAdditionally, there are already instances where a message broker or alternative solutions have been implemented to work around the limitations of the current architecture.\n\n## Rationale\n\nIntroducing a message broker will allow for asynchronous communication between services, decoupling them and improving the system's resilience and scalability. This will enable:\n\n* Services to operate independently, without the need for knowledge about other services\n* Improved fault tolerance, as services will not be affected by the unavailability of other services\n* Reduced performance penalties, as services will not be blocked waiting for responses from other services\n* Simplified coding, as the message broker will handle the complexities of unreliable network connections\n\n## Trade-offs\n\n* Additional complexity introduced by the message broker\n* Increased costs associated with implementing and maintaining the message broker\n* Potential impact on existing services and their interactions\n\n## Conclusion\n\nIntroducing a message broker to support asynchronous communication between services in SDR is a necessary step to improve the system's resilience and scalability. While there are trade-offs to consider, the benefits of decoupling services and improving fault tolerance and performance outweigh the costs.","GenTime":"2024-08-10 01:52:24"}
{"File Name":"infrastructure-adrs\/0009-create-provenance-service.md","Context":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":794,"Prediction":"Here is a sample Architectural Decision Record:\n\n## Context and Problem Statement\nOur company is planning to launch a new mobile app that will allow users to track their daily expenses. The app will need to integrate with multiple payment gateways and banks to fetch transaction data. We are facing a dilemma on how to handle the complex authentication and authorization process for these integrations.\n\n## Decision Drivers\n* Security: We need to ensure that the app is secure and protected from unauthorized access to user data.\n* Scalability: The app needs to handle a large number of users and transactions, and we need to ensure that the authentication and authorization process can scale with the growth of the app.\n* Development Time: We need to develop the app quickly and efficiently, and we are concerned about the complexity and time required to implement a custom authentication and authorization solution.\n\nPlease let me know if you would like me to continue with the rest of the Architectural Decision Record.","GenTime":"2024-08-10 01:54:54"}
{"File Name":"form-design-system\/zindex-values.md","Context":"## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n","Decision":"We've decided to go with a 2 tier approach:\\n- Use pre-defined z-index values & ranges that are defined in FDS.\\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\\nvalues are not useful)\\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\\n### Use values between `1` and `9` for fine layering control\\nIn cases where one element needs to appear above another, use integers below 10. Rely on\\nsurrounding stacking context to do the rest.\\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\\nvalues `1` through `5` to accomplish this.\\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Positioning\/Understanding_z_index\/The_stacking_context).\\n### Pre-defined FDS z-index values \/ ranges\\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\\n#### Navigation\\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to \"frame\" the content on a page.\\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\\n```\\n.mainheader {\\nz-index: calc(var(--zindex-navigation) + 9);\\n}\\n.subheader {\\nz-index: var(--zindex-navigation)\\n}\\n```\\n#### Modal\\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\\n#### Crazy\\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\\n### For standard values, continue using increments of `10`\\nFor base stacking context, standard values are most helpful at increments of 10 because it\\ngives engineers flexibility to insert layers between defined stack levels. For example:\\n","tokens":53,"id":798,"Prediction":"## Decision\n\nTo resolve the \"zindex war\" issue in our public-facing application, we will implement a CSS layering system that utilizes the `position` property and `z-index` values in a more intentional and maintainable way.\n\n## Context\n\nOur public-facing application is currently suffering from a \"zindex war\", where the use of high and arbitrary `z-index` values is required to build front-end features. This has led to a lack of control and maintainability in the CSS layering system, causing engineers to spend unnecessary time and effort to resolve conflicts.\n\n## Problem Statement\n\nThe current approach to using `z-index` values is causing issues with the layout and visual presentation of our application. The arbitrary use of high `z-index` values is leading to:\n\n1. Conflicting layers: Multiple elements with high `z-index` values are causing elements to overlap and display incorrectly.\n2. Difficulty in debugging: The lack of a clear and intentional layering system makes it challenging to identify and resolve issues.\n3. Maintenance challenges: The arbitrary use of `z-index` values makes it difficult to maintain and update the application's layout and visual presentation.\n\n## Decision Rationale\n\nImplementing a CSS layering system that utilizes the `position` property and `z-index` values in a more intentional and maintainable way will help to:\n\n1. Improve layout and visual presentation: By using a clear and intentional layering system, we can ensure that elements are displayed correctly and consistently.\n2. Simplify debugging: A clear and intentional layering system will make it easier to identify and resolve issues.\n3. Enhance maintainability: A well-structured layering system will make it easier to maintain and update the application's layout and visual presentation.\n\n## Alternative Considered\n\nWe considered alternative solutions, such as:\n\n1. Using a third-party library to manage the layering system.\n2. Implementing a custom solution using JavaScript.\n3. Continuing to use the current approach and hoping that it will not cause further issues.\n\nHowever, these alternatives were deemed less effective and more complex than implementing a CSS layering system that utilizes the `position` property and `z-index` values in a more intentional and maintainable way.\n\n## Pros and Cons\n\nPros:\n\n* Improved layout and visual presentation\n* Simplified debugging\n* Enhanced maintainability\n\nCons:\n\n* Requires a significant amount of time and effort to implement and test\n* May require changes to existing CSS code\n\n## Implementation\n\nTo implement this decision, we will:\n\n1. Review and refactor existing CSS code to ensure that it is using the `position` property and `z-index` values in a more intentional and maintainable way.\n2. Develop a clear and intentional layering system that utilizes the `position` property and `z-index` values.\n3. Test and iterate on the implementation to ensure that it meets the requirements and is free of issues.\n\n## Monitoring and Feedback\n\nTo monitor the effectiveness of this decision, we will:\n\n1. Track and measure the impact on layout and visual presentation.\n2. Monitor debugging time and effort.\n3. Gather feedback from engineers and stakeholders on the maintainability of the layering system.\n\nThis will help us to identify any areas for improvement and make adjustments as needed.","GenTime":"2024-08-10 01:57:08"}
{"File Name":"form-design-system\/relative_spacing.md","Context":"## Context and Problem Statement\\nWe'd like to change the spacing system to something more generic.\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\n","Decision":"The current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\nGeneric tshirt system was chosen over the other options.\\n- Coding a direct \"arbitrary value\" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\\n","tokens":153,"id":801,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Implement a spacing system using a numerical scale with a minimum and maximum value, instead of a relative spacing system based on a default value.\n\n**Context and Problem Statement:** We'd like to change the spacing system to something more generic.\n\n**Decision Drivers:**\n\n* The current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\n\t+ We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\n\t+ The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\n\n**Rationale:** By using a numerical scale with a minimum and maximum value, we can easily add new spacing values and change the default value without having to rename existing values. This approach also provides more flexibility and expressiveness in defining spacing values.\n\n**Consequences:**\n\n* The spacing system will be more flexible and easier to extend.\n* The default value can be changed without affecting the existing spacing values.\n* The spacing values will be more explicit and easier to understand.\n\n**Status:** Accepted\n\n**Implementation:** The new spacing system will be implemented using a numerical scale with a minimum and maximum value. The existing spacing values will be converted to the new system, and the default value will be set to a reasonable starting point.","GenTime":"2024-08-10 01:58:36"}
{"File Name":"form-design-system\/components_ignore_classnames.md","Context":"## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n","Decision":"Chosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n","tokens":80,"id":802,"Prediction":"## Decision\n\nTo ensure consistency and maintainability across all components in FDS, we will not allow arbitrary style overrides on any component by accepting a `classNames` prop.\n\n## Context\n\nAs stated in the problem statement, some components in FDS, such as `Flex`, may suffer from accepting a `classNames` prop, which could lead to unpredictable behavior and compromise the component's intended functionality. This decision applies to all components in FDS to ensure consistency and maintainability.\n\n## Rationale\n\nBy not allowing arbitrary style overrides, we can:\n\n* Ensure that each component behaves as intended, without unexpected styling changes.\n* Prevent potential conflicts between different components, which could lead to rendering issues.\n* Simplify the component's implementation and reduce the risk of bugs.\n* Improve the overall maintainability and scalability of the FDS ecosystem.\n\n## Consequences\n\n* Developers may need to find alternative ways to apply custom styles to components, such as using a theme or a separate styling mechanism.\n* Some components may require additional props or configuration options to achieve the desired styling.\n* The decision may impact the flexibility of the component, but it will ensure consistency and maintainability across all components.\n\n## Alternatives Considered\n\n* Allowing arbitrary style overrides on some components, but not others.\n* Introducing a new prop or configuration option to control styling overrides.\n* Creating a separate styling mechanism for components that require custom styling.\n\n## Decision History\n\nThis decision was discussed and agreed upon by the FDS team to ensure consistency and maintainability across all components.","GenTime":"2024-08-10 01:58:57"}
{"File Name":"cosmos-sdk\/adr-021-protobuf-query-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n","Decision":"### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```protobuf\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/cosmos\/gogoproto)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work directly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","tokens":108,"id":810,"Prediction":"## Decision\n\n### Decision: Use JSON Encoding for Queries\n\n### Context\n\nAs part of the Protocol Buffer migration path for the client-side of the Cosmos SDK, we aim to design a seamless transition from the current encoding schemes to a new one. In the previous ADRs, we discussed the encoding of state and transactions using Protocol Buffers. This ADR focuses on the encoding of queries.\n\n### Problem\n\nThe current encoding scheme for queries is not efficient and may lead to issues with query processing and data storage. We need a new encoding scheme that is lightweight, easy to parse, and compatible with the existing infrastructure.\n\n### Options\n\n1. **Use Protocol Buffers for Queries**: This option would require modifying the existing query processing infrastructure to support Protocol Buffers. However, this would add complexity and may not provide significant benefits.\n2. **Use JSON Encoding for Queries**: This option would allow us to leverage the existing JSON encoding infrastructure and provide a simple, lightweight, and easy-to-parse encoding scheme for queries.\n\n### Decision\n\nAfter careful consideration, we have decided to use JSON encoding for queries. This decision is based on the following factors:\n\n* **Lightweight**: JSON encoding is significantly lighter than Protocol Buffers, which makes it more suitable for queries that are typically smaller in size.\n* **Easy to parse**: JSON encoding is easy to parse and can be done using existing libraries, which reduces the complexity of the query processing infrastructure.\n* **Compatibility**: JSON encoding is compatible with the existing infrastructure, which makes it easier to integrate with the existing system.\n\n### Consequences\n\n* **Simplified query processing**: The use of JSON encoding for queries simplifies the query processing infrastructure and reduces the complexity of the system.\n* **Improved performance**: The lightweight nature of JSON encoding improves the performance of query processing and data storage.\n* **Easier integration**: The compatibility of JSON encoding with the existing infrastructure makes it easier to integrate with the existing system.\n\n### Rationale\n\nThe use of JSON encoding for queries provides a simple, lightweight, and easy-to-parse encoding scheme that is compatible with the existing infrastructure. This decision simplifies the query processing infrastructure, improves performance, and makes it easier to integrate with the existing system.","GenTime":"2024-08-10 02:02:09"}
{"File Name":"cosmos-sdk\/adr-033-protobuf-inter-module-comm.md","Context":"## Context\\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\\n### `x\/bank` Case Study\\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the `MintCoins`,\\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\\njust a simple string. So the `x\/upgrade` module could mint tokens for the `x\/staking` module simple by calling\\n`MintCoins(\u201cstaking\u201d)`. Furthermore, all modules which have access to these keeper methods, also have access to\\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\\n","Decision":"Based on [ADR-021](.\/adr-021-protobuf-query-encoding.md) and [ADR-031](.\/adr-031-msg-service.md), we introduce the\\nInter-Module Communication framework for secure module authorization and OCAPs.\\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\\nOf particular note \u2014 the decision is to _enable_ this functionality for modules to adopt at their own discretion.\\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\\naddressed as amendments to this ADR.\\n### New \"Keeper\" Paradigm\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\\nwas introduced and in [ADR 31](.\/adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg\/Send` message type:\\n```go\\npackage bank\\ntype MsgClient interface {\\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\\n}\\ntype MsgServer interface {\\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\\n}\\n```\\n[ADR 021](.\/adr-021-protobuf-query-encoding.md) and [ADR 31](.\/adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\\nbased service interfaces already used by clients for inter-module communication.\\nUsing this `QueryClient`\/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\\n1. Protobuf types are checked for breaking changes using [buf](https:\/\/buf.build\/docs\/breaking-overview) and because of\\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\\nevolution.\\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\\nobject capability system (see below).\\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\\nenabling atomicy of operations ([currently a problem](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8030)). Any failure within a module-to-module call would result in a failure of the entire\\ntransaction\\nThis mechanism has the added benefits of:\\n* reducing boilerplate through code generation, and\\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\\n### Inter-module Communication\\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https:\/\/github.com\/grpc\/grpc-go\/blob\/v1.49.x\/clientconn.go#L441-L450)\\nimplementation. For this we introduce\\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\\ndescribed in more detail below.\\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\\nthat we don't use any cryptographic signature in this case.\\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `\/cosmos.bank.Msg\/Send` transaction. `MsgSend` validation\\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\\nHere's an example of a hypothetical module `foo` interacting with `x\/bank`:\\n```go\\npackage foo\\ntype FooMsgServer {\\n\/\/ ...\\nbankQuery bank.QueryClient\\nbankMsg   bank.MsgClient\\n}\\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\\n\/\/ ...\\nreturn FooMsgServer {\\n\/\/ ...\\nmodouleKey: moduleKey,\\nbankQuery: bank.NewQueryClient(moduleKey),\\nbankMsg: bank.NewMsgClient(moduleKey),\\n}\\n}\\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: \"foo\"})\\n...\\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\\n...\\n}\\n```\\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\\ndenom prefix being restricted to certain modules (as discussed in\\n[#7459](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#discussion_r529545528)).\\n### `ModuleKey`s and `ModuleID`s\\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\\ncorresponding \"public key\". From the [ADR 028](.\/adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\\nand forms its address based on the `AddressHash` method from [the ADR-028](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-028-public-key-addresses.md):\\n```go\\ntype ModuleID struct {\\nModuleName string\\nPath []byte\\n}\\nfunc (key ModuleID) Address() []byte {\\nreturn AddressHash(key.ModuleName, key.Path)\\n}\\n```\\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\\nthe `ModuleKey` security.\\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\\n```go\\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\\ntype CallInfo {\\nMethod string\\nCaller ModuleID\\n}\\ntype RootModuleKey struct {\\nmoduleName string\\ninvoker Invoker\\n}\\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { \/* ... *\/}\\ntype DerivedModuleKey struct {\\nmoduleName string\\npath []byte\\ninvoker Invoker\\n}\\n```\\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\\n```go\\npackage foo\\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\\nbankMsgClient := bank.NewMsgClient(derivedKey)\\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\\n...\\n}\\n```\\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\\nfrom either the root or a derived module account to pass through.\\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\\nchecking permissions.\\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\\nname impersonation.\\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\\n```go\\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\\nreturn f(ctx, args, reply)\\n}\\n```\\n### `AppModule` Wiring and Requirements\\nIn [ADR 031](.\/adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\\nspecify their dependencies on other modules using `RequireServer()`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nModuleKey() ModuleKey\\nRequireServer(msgServer interface{})\\n}\\n```\\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\\nmodule `foo` could declare it's dependency on `x\/bank` like this:\\n```go\\npackage foo\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ncfg.RequireServer((*bank.QueryServer)(nil))\\ncfg.RequireServer((*bank.MsgServer)(nil))\\n}\\n```\\n### Security Considerations\\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\\nthe underlying router infrastructure.\\n#### Recursion and Re-entry\\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\\ncalls Module B and Module B calls module A again in the same call.\\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\\nin the router could be used to perform this security check.\\n#### Queries\\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\\n### Internal Methods\\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\\npurpose, we add the `InternalServer` method to `Configurator`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nInternalServer() grpc.Server\\n}\\n```\\nAs an example, x\/slashing's Slash must call x\/staking's Slash, but we don't want to expose x\/staking's Slash to end users\\nand clients.\\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\\nproto package.\\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\\nAn alternative solution to internal-only methods could involve hooks \/ plugins as discussed [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#issuecomment-733807753).\\nA more detailed evaluation of a hooks \/ plugin system will be addressed later in follow-ups to this ADR or as a separate\\nADR.\\n### Authorization\\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-030-authz-module.md).\\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\\nother modules. This will be addressed in separate ADRs or updates to this ADR.\\n### Future Work\\nOther future improvements may include:\\n* custom code generation that:\\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\\n* code generation which makes inter-module communication more performant\\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\\n* inter-module hooks and plugins\\n","tokens":451,"id":812,"Prediction":"## Decision\n\n**Decision:** Implement a robust Object-Capability Model (OCAP) in the Cosmos SDK to ensure secure and composable blockchain applications, addressing the current limitations and vulnerabilities in the ecosystem.\n\n**Context:** The current Cosmos SDK documentation assumes a thriving ecosystem of modules, but the reality is that the ecosystem is still developing, and modules are not yet easily composable due to unstable interfaces and lack of proper OCAP implementation. The `x\/bank` keeper's unrestricted access and the attempts to implement OCAPs using module-level minting, staking, and burning permissions are not sufficient to ensure security and encapsulation.\n\n**Rationale:** Implementing a robust OCAP will provide a secure foundation for the Cosmos SDK ecosystem, enabling developers to build trustless and composable blockchain applications. This will be achieved by introducing a unique object capability token that controls access to sensitive methods, ensuring that each module has a clear and limited scope of authority.\n\n**Consequences:**\n\n* Improved security: By introducing a robust OCAP, the risk of faulty or malicious modules will be significantly reduced, ensuring the integrity of the blockchain ecosystem.\n* Increased modularity: Composable modules will be easier to develop and integrate, enabling developers to build complex blockchain applications with confidence.\n* Enhanced scalability: A secure and composable ecosystem will attract more developers and users, driving growth and innovation in the Cosmos ecosystem.\n\n**Alternatives Considered:**\n\n* Continue with the current approach, relying on module-level permissions and hoping that the ecosystem will mature on its own.\n* Implement a more limited OCAP, focusing on specific use cases rather than providing a comprehensive solution.\n\n**Implementation:**\n\n* Introduce a unique object capability token for each module, controlling access to sensitive methods.\n* Implement a robust permission system, ensuring that each module has a clear and limited scope of authority.\n* Review and refactor existing module interfaces to ensure they conform to the new OCAP implementation.\n* Provide documentation and guidelines for developers to build secure and composable modules.","GenTime":"2024-08-10 02:02:54"}
{"File Name":"cosmos-sdk\/adr-002-docs-structure.md","Context":"## Context\\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n","Decision":"Re-structure the `\/docs` folder of the Cosmos SDK github repo as follows:\\n```text\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n* `README`: Landing page of the docs.\\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\\n* `spec`: Contains specs of modules, and others.\\n* `modules`: Contains links to `godocs` and the spec of the modules.\\n* `architecture`: Contains architecture-related docs like the present one.\\n* `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n* `README`\\n* `intro`\\n* `concepts`\\n* `clients`\\n`architecture` need not be displayed on the website.\\n","tokens":115,"id":816,"Prediction":"## Decision\n\n**Decentralize Cosmos SDK Documentation and Organize by Component**\n\n## Context\n\nAs described by the user, the current Cosmos SDK documentation is scattered, contains non-related material, and is challenging to maintain and navigate for users. The ideal solution is to separate documentation into two categories: component-specific documentation living in their respective GitHub repositories and high-level, non-component-specific documentation on the website.\n\n## Problem\n\nThe current documentation structure is:\n\n* Mixed content (SDK, hub, lotion, etc.) in a single repository\n* Difficulty in maintaining and updating documentation\n* Poor user experience due to lack of organization and relevance\n\n## Decision\n\nTo address the issues, we will decentralize the documentation and organize it by component. This will enable:\n\n* Component-specific documentation (e.g., SDK, hub, lotion) to live in their respective GitHub repositories\n* High-level, non-component-specific documentation (e.g., FAQs, whitepaper, high-level overview) to live on the website\n* Improved maintainability and scalability of the documentation\n* Enhanced user experience through organized and relevant content\n\n## Consequences\n\n* Component teams will be responsible for maintaining their respective documentation\n* The website will become the central hub for high-level, non-component-specific documentation\n* The documentation structure will be more scalable and easier to maintain\n* Users will have a better understanding of the different components and their respective documentation\n\n## Rationale\n\nBy decentralizing the documentation and organizing it by component, we can:\n\n* Improve maintainability and scalability of the documentation\n* Enhance user experience through organized and relevant content\n* Allow component teams to take ownership of their respective documentation\n* Keep high-level, non-component-specific documentation on the website for easy access\n\n## Implementation\n\nTo implement this decision, we will:\n\n* Move component-specific documentation to their respective GitHub repositories\n* Create a website section for high-level, non-component-specific documentation\n* Establish a clear documentation structure and guidelines for component teams\n* Monitor and adjust the documentation structure as needed to ensure scalability and maintainability.","GenTime":"2024-08-10 02:04:26"}
{"File Name":"cosmos-sdk\/adr-043-nft-module.md","Context":"## Context\\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\\n* irismod\/nft and modules\/incubator\/nft\\n* CW721\\n* DID NFTs\\n* interNFT\\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\\n","Decision":"We create a `x\/nft` module, which contains the following functionality:\\n* Store NFTs and track their ownership.\\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\\n* Query NFTs and their supply information.\\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\\n### Types\\nWe propose two main types:\\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\\n#### Class\\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\\n```protobuf\\nmessage Class {\\nstring id          = 1;\\nstring name        = 2;\\nstring symbol      = 3;\\nstring description = 4;\\nstring uri         = 5;\\nstring uri_hash    = 6;\\ngoogle.protobuf.Any data = 7;\\n}\\n```\\n* `id` is used as the primary index for storing the class; _required_\\n* `name` is a descriptive name of the NFT class; _optional_\\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\\n* `description` is a detailed description of the NFT class; _optional_\\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is app specific metadata of the class; _optional_\\n#### NFT\\nWe define a general model for `NFT` as follows.\\n```protobuf\\nmessage NFT {\\nstring class_id           = 1;\\nstring id                 = 2;\\nstring uri                = 3;\\nstring uri_hash           = 4;\\ngoogle.protobuf.Any data  = 10;\\n}\\n```\\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\\n```text\\n{class_id}\/{id} --> NFT (bytes)\\n```\\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\\n### `Keeper` Interface\\n```go\\ntype Keeper interface {\\nNewClass(ctx sdk.Context,class Class)\\nUpdateClass(ctx sdk.Context,class Class)\\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\\nUpdate(ctx sdk.Context, nft NFT)\\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\\nGetClass(ctx sdk.Context, classId string) Class\\nGetClasses(ctx sdk.Context) []Class\\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\\nGetTotalSupply(ctx sdk.Context, classId string) uint64\\n}\\n```\\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\nrpc Send(MsgSend)         returns (MsgSendResponse);\\n}\\nmessage MsgSend {\\nstring class_id = 1;\\nstring id       = 2;\\nstring sender   = 3;\\nstring reveiver = 4;\\n}\\nmessage MsgSendResponse {}\\n```\\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\\nThe implementation outline of the server is as follows:\\n```go\\ntype msgServer struct{\\nk Keeper\\n}\\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\\n\/\/ check current ownership\\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\\n\/\/ transfer ownership\\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\\nreturn &types.MsgSendResponse{}, nil\\n}\\n```\\nThe query service methods for the `x\/nft` module are:\\n```protobuf\\nservice Query {\\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\\n}\\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\\n}\\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\\n}\\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\\n}\\n\/\/ NFT queries an NFT based on its class and id.\\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\\n}\\n\/\/ Class queries an NFT class based on its id\\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\\n}\\n\/\/ Classes queries all NFT classes\\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\\n}\\n}\\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\\nmessage QueryBalanceRequest {\\nstring class_id = 1;\\nstring owner    = 2;\\n}\\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\\nmessage QueryBalanceResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\\nmessage QueryOwnerRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\\nmessage QueryOwnerResponse {\\nstring owner = 1;\\n}\\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\\nmessage QuerySupplyRequest {\\nstring class_id = 1;\\n}\\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\\nmessage QuerySupplyResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\\nmessage QueryNFTsRequest {\\nstring                                class_id   = 1;\\nstring                                owner      = 2;\\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\\n}\\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\\nmessage QueryNFTsResponse {\\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\\nmessage QueryNFTRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\\nmessage QueryNFTResponse {\\ncosmos.nft.v1beta1.NFT nft = 1;\\n}\\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\\nmessage QueryClassRequest {\\nstring class_id = 1;\\n}\\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\\nmessage QueryClassResponse {\\ncosmos.nft.v1beta1.Class class = 1;\\n}\\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\\nmessage QueryClassesRequest {\\n\/\/ pagination defines an optional pagination for the request.\\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\\n}\\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\\nmessage QueryClassesResponse {\\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n```\\n### Interoperability\\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\\n","tokens":307,"id":827,"Prediction":"## Decision\n\n**Implement a generic NFT module design in the Cosmos SDK to enable composability and support various NFT use cases**\n\n## Context\n\nAs discussed in the provided context, NFTs are gaining importance in the Cosmos ecosystem, and implementing NFT functions in the Cosmos Hub is crucial. To achieve this, we need to design a unified mechanism for storing and sending the ownership representative of NFTs. Considering the tight connection between NFT functions and their logic, it is impractical to support all use cases in a single Cosmos SDK module. Therefore, a generic NFT module design that handles the generic NFTs logic is preferred.\n\n## Problem\n\nThe current NFT implementations in the IRISnet team and the Cosmos repository are not suitable for a generic NFT module design. They are specific to their respective projects and do not provide the necessary flexibility to support various NFT use cases.\n\n## Solution\n\nWe will design a generic NFT module in the Cosmos SDK that can be used by other modules on the Cosmos Hub or other Zones to support various NFT use cases. This design will be based on the work done by the IRISnet team and the older implementation in the Cosmos repository, but will be more comprehensive and flexible.\n\n## Rationale\n\nThe generic NFT module design will enable composability, allowing application-specific functions to be managed by other modules. This will provide a more flexible and scalable solution for supporting various NFT use cases. The design will also ensure compatibility with interchain protocols, including IBC and Gravity Bridge, making it a suitable solution for the Cosmos ecosystem.\n\n## Consequences\n\n* The generic NFT module design will provide a unified mechanism for storing and sending the ownership representative of NFTs.\n* It will enable composability, allowing application-specific functions to be managed by other modules.\n* The design will be more flexible and scalable, supporting various NFT use cases.\n* The implementation will be compatible with interchain protocols, including IBC and Gravity Bridge.\n\n## Status\n\nThe design of the generic NFT module is currently in progress, and the implementation will be carried out in the next steps.","GenTime":"2024-08-10 02:08:42"}
{"File Name":"cosmos-sdk\/adr-030-authz-module.md","Context":"## Context\\nThe concrete use cases which motivated this module include:\\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\\ndelegated stake\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which\\nis a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n","Decision":"We will create a module named `authz` which provides functionality for\\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\\nmust be granted for a particular `Msg` service methods one by one using an implementation\\nof `Authorization` interface.\\n### Types\\nAuthorizations determine exactly what privileges are granted. They are extensible\\nand can be defined for any `Msg` service method even outside of the module where\\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\\n#### Authorization\\n```go\\ntype Authorization interface {\\nproto.Message\\n\/\/ MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\\n\/\/ which will process and accept or reject a request.\\nMsgTypeURL() string\\n\/\/ Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\\n\/\/ so provides an upgraded authorization instance.\\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\\n\/\/ ValidateBasic does a simple validation check that\\n\/\/ doesn't require access to any other information.\\nValidateBasic() error\\n}\\n\/\/ AcceptResponse instruments the controller of an authz message if the request is accepted\\n\/\/ and if it should be updated or deleted.\\ntype AcceptResponse struct {\\n\/\/ If Accept=true, the controller can accept and authorization and handle the update.\\nAccept bool\\n\/\/ If Delete=true, the controller must delete the authorization object and release\\n\/\/ storage resources.\\nDelete bool\\n\/\/ Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\\n\/\/ it must use the updated version and handle the update on the storage level.\\nUpdated Authorization\\n}\\n```\\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\\na `SpendLimit` and updates it down to zero:\\n```go\\ntype SendAuthorization struct {\\n\/\/ SpendLimit specifies the maximum amount of tokens that can be spent\\n\/\/ by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\\n\/\/ can be used with bank msg type url to create limit less bank authorization).\\nSpendLimit sdk.Coins\\n}\\nfunc (a SendAuthorization) MsgTypeURL() string {\\nreturn sdk.MsgTypeURL(&MsgSend{})\\n}\\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\\nmSend, ok := msg.(*MsgSend)\\nif !ok {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\\n}\\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\\nif isNegative {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\\n}\\nif limitLeft.IsZero() {\\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\\n}\\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\\n}\\n```\\nA different type of capability for `MsgSend` could be implemented\\nusing the `Authorization` interface with no need to change the underlying\\n`bank` module.\\n##### Small notes on `AcceptResponse`\\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\\nthe field will be `nil`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\n\/\/ Grant grants the provided authorization to the grantee on the granter's\\n\/\/ account with the provided expiration time.\\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\\n\/\/ Exec attempts to execute the provided messages using\\n\/\/ authorizations granted to the grantee. Each message should have only\\n\/\/ one signer corresponding to the granter of the authorization.\\nrpc Exec(MsgExec) returns (MsgExecResponse);\\n\/\/ Revoke revokes any authorization corresponding to the provided method name on the\\n\/\/ granter's account that has been granted to the grantee.\\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\\n}\\n\/\/ Grant gives permissions to execute\\n\/\/ the provided method with expiration time.\\nmessage Grant {\\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\\n}\\nmessage MsgGrant {\\nstring granter = 1;\\nstring grantee = 2;\\nGrant grant = 3 [(gogoproto.nullable) = false];\\n}\\nmessage MsgExecResponse {\\ncosmos.base.abci.v1beta1.Result result = 1;\\n}\\nmessage MsgExec {\\nstring   grantee                  = 1;\\n\/\/ Authorization Msg requests to execute. Each msg must implement Authorization interface\\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];;\\n}\\n```\\n### Router Middleware\\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\\nto the router based on `Authorization` grants:\\n```go\\ntype Keeper interface {\\n\/\/ DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\\n\/\/ to send those messages by the first (and only) signer of each msg.\\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\\n}\\n```\\n### CLI\\n#### `tx exec` Method\\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\\nwould send a transaction like this:\\n```go\\nMsgExec {\\nGrantee: mykey,\\nMsgs: []sdk.Msg{\\nMsgVote {\\nProposalID: 1,\\nVoter: cosmos3thsdgh983egh823\\nOption: Yes\\n}\\n}\\n}\\n```\\n#### `tx grant <grantee> <authorization> --from <granter>`\\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\\nJSON on the CLI.\\n#### `tx revoke <grantee> <method-name> --from <granter>`\\nThis CLI command will send a `MsgRevoke` transaction.\\n### Built-in Authorizations\\n#### `SendAuthorization`\\n```protobuf\\n\/\/ SendAuthorization allows the grantee to spend up to spend_limit coins from\\n\/\/ the granter's account.\\nmessage SendAuthorization {\\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\\n}\\n```\\n#### `GenericAuthorization`\\n```protobuf\\n\/\/ GenericAuthorization gives the grantee unrestricted permissions to execute\\n\/\/ the provided method on behalf of the granter's account.\\nmessage GenericAuthorization {\\noption (cosmos_proto.implements_interface) = \"Authorization\";\\n\/\/ Msg, identified by it's type URL, to grant unrestricted permissions to execute\\nstring msg = 1;\\n}\\n```\\n","tokens":276,"id":839,"Prediction":"## Decision\n\n**Decentralized Delegation of Voting and Capability Granting**\n\n## Context\n\nAs described in the context, the concrete use cases that motivated this module include delegating the ability to vote on proposals to other accounts besides the account that has delegated stake, and the \"sub-keys\" functionality, which allows one account to grant some subset of its capabilities to other accounts with less robust, but easier to use security measures.\n\n## Problem\n\nThe current implementation of the delegation module is centralized, making it difficult to scale and manage. Additionally, it lacks the flexibility to accommodate different use cases and security requirements.\n\n## Solution\n\nTo address these issues, we will implement a decentralized delegation module that allows accounts to delegate their voting power and capabilities to other accounts in a secure and flexible manner. This will be achieved by introducing a new module that utilizes the existing consensus algorithm and cryptographic techniques to ensure the integrity and authenticity of the delegation process.\n\n## Rationale\n\nThe decentralized delegation module will provide several benefits, including:\n\n* Increased scalability and manageability: By decentralizing the delegation process, we can reduce the load on the network and make it easier to manage and maintain.\n* Improved security: The use of cryptographic techniques and the decentralized nature of the module will provide an additional layer of security and make it more difficult for malicious actors to manipulate the delegation process.\n* Flexibility: The decentralized delegation module will allow for different use cases and security requirements to be accommodated, making it more versatile and adaptable to changing needs.\n\n## Trade-offs\n\nThe decentralized delegation module will require additional computational resources and network bandwidth, which may impact the overall performance of the network. Additionally, the increased complexity of the module may require additional testing and debugging to ensure its stability and security.\n\n## Consequences\n\nThe implementation of the decentralized delegation module will have several consequences, including:\n\n* Changes to the existing consensus algorithm and cryptographic techniques to accommodate the decentralized delegation process.\n* The introduction of new APIs and interfaces to facilitate the delegation process.\n* The need for additional testing and debugging to ensure the stability and security of the module.\n\nBy implementing a decentralized delegation module, we can provide a more scalable, secure, and flexible solution that meets the needs of our users and accommodates different use cases and security requirements.","GenTime":"2024-08-10 02:13:09"}
{"File Name":"cosmos-sdk\/adr-016-validator-consensus-key-rotation.md","Context":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\\n","Decision":"### Pseudo procedure for consensus key rotation\\n* create new random consensus key.\\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n* start validating with new consensus key.\\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n* consensus key mapping information management strategy\\n* store history of each key mapping changes in the kvstore.\\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n* the state machine does not need any historical mapping information which is past more than unbonding period.\\n* key rotation costs related to LCD and IBC\\n* LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n* limits\\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\\n* parameters can be decided by governance and stored in genesis file.\\n* key rotation fee\\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n* evidence module\\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n* abci.ValidatorUpdate\\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n* new genesis parameters in `staking` module\\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n* checks if the signing account has enough balance to pay `KeyRotationFee`\\n* pays `KeyRotationFee` to community fund\\n* overwrites `NewPubKey` in `validator.ConsPubKey`\\n* deletes old `ValidatorByConsAddr`\\n* `SetValidatorByConsAddr` for `NewPubKey`\\n* Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n* Note : All above features shall be implemented in `staking` module.\\n","tokens":171,"id":841,"Prediction":"## Decision\n\n**Validator Consensus Key Rotation**\n\n### Context\n\nAs discussed and requested for a long time, a validator consensus key rotation feature is necessary to ensure a safer validator key management policy. The goal is to implement a simple form of validator consensus key rotation mostly onto Cosmos SDK, without updating the consensus logic in Tendermint.\n\n### Problem\n\nValidator consensus key rotation is essential for maintaining the security of the network. Currently, there is no mechanism in place to rotate the consensus key of a validator, leaving the network vulnerable to potential attacks.\n\n### Solution\n\nWe will implement a simple form of validator consensus key rotation, which involves replacing the existing consensus key with a new one. This will not require any changes to the consensus logic in Tendermint, as Tendermint does not maintain any mapping information between consensus keys and validator operator keys. From Tendermint's perspective, a consensus key rotation is simply a replacement of a consensus key to another.\n\n### Consequences\n\n* The implementation will be relatively simple and straightforward, with minimal impact on the existing codebase.\n* The feature will provide an additional layer of security for the network by allowing validators to rotate their consensus keys.\n* The implementation will not include the concept of multiple consensus keys, which will be a long-term goal for Tendermint and Cosmos SDK.\n\n### Decision\n\nWe will implement a simple form of validator consensus key rotation, as described above, to provide a safer validator key management policy for the network.","GenTime":"2024-08-10 02:13:43"}
{"File Name":"cosmos-sdk\/adr-032-typed-events.md","Context":"## Context\\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\\n[Our platform](http:\/\/github.com\/ovrclk\/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https:\/\/github.com\/ovrclk\/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\\n","Decision":"**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\\n```go\\n\/\/ types\/events.go\\n\/\/ EmitTypedEvent takes typed event and emits converting it into sdk.Event\\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\\nevtType := proto.MessageName(event)\\nevtJSON, err := codec.ProtoMarshalJSON(event)\\nif err != nil {\\nreturn err\\n}\\nvar attrMap map[string]json.RawMessage\\nerr = json.Unmarshal(evtJSON, &attrMap)\\nif err != nil {\\nreturn err\\n}\\nvar attrs []abci.EventAttribute\\nfor k, v := range attrMap {\\nattrs = append(attrs, abci.EventAttribute{\\nKey:   []byte(k),\\nValue: v,\\n})\\n}\\nem.EmitEvent(Event{\\nType:       evtType,\\nAttributes: attrs,\\n})\\nreturn nil\\n}\\n\/\/ ParseTypedEvent converts abci.Event back to typed event\\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\\nconcreteGoType := proto.MessageType(event.Type)\\nif concreteGoType == nil {\\nreturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\\n}\\nvar value reflect.Value\\nif concreteGoType.Kind() == reflect.Ptr {\\nvalue = reflect.New(concreteGoType.Elem())\\n} else {\\nvalue = reflect.Zero(concreteGoType)\\n}\\nprotoMsg, ok := value.Interface().(proto.Message)\\nif !ok {\\nreturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\\n}\\nattrMap := make(map[string]json.RawMessage)\\nfor _, attr := range event.Attributes {\\nattrMap[string(attr.Key)] = attr.Value\\n}\\nattrBytes, err := json.Marshal(attrMap)\\nif err != nil {\\nreturn nil, err\\n}\\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn protoMsg, nil\\n}\\n```\\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key\/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\\n**Step-2**: Add proto definitions for typed events for msgs in each module:\\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\\n```protobuf\\n\/\/ proto\/cosmos\/gov\/v1beta1\/gov.proto\\n\/\/ Add typed event definition\\npackage cosmos.gov.v1beta1;\\nmessage EventSubmitProposal {\\nstring from_address   = 1;\\nuint64 proposal_id    = 2;\\nTextProposal proposal = 3;\\n}\\n```\\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\\n```go\\n\/\/ x\/gov\/handler.go\\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\\n...\\ntypes.Context.EventManager().EmitTypedEvent(\\n&EventSubmitProposal{\\nFromAddress: fromAddress,\\nProposalId: id,\\nProposal: proposal,\\n},\\n)\\n...\\n}\\n```\\n### How to subscribe to these typed events in `Client`\\n> NOTE: Full code example below\\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\\nAkash Network has built a simple [`pubsub`](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/pubsub\/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/events\/publish.go#L21) them as typed events.\\nPlease see the below code sample for more detail on this flow looks for clients.\\n","tokens":433,"id":842,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will update the Cosmos SDK to use typed events, defined in each module, to make it easier to emit and subscribe to events. This will enable developers to build event-driven applications in a more straightforward and efficient manner.\n\n**Context**\n\nThe current approach of defining events in handlers for each message in the Cosmos SDK makes it difficult to consume events, requiring raw string matching and parsing. This proposal aims to address this issue by introducing typed events, defined in each module, to simplify the process of emitting and subscribing to events.\n\n**Decision Drivers**\n\n* The need for a more efficient and straightforward way to build event-driven applications in the Cosmos SDK\n* The experience of the Akash Network team, which has developed a standard method for defining and consuming typed events in Cosmos SDK modules\n* The expected growing demand for event-driven applications in the Cosmos SDK, particularly in areas such as peg zones, IBC, DeFi, and more\n\n**Consequences**\n\n* Developers will be able to build event-driven applications in Go by writing `EventHandler`s for specific event types and passing them to `EventEmitters` defined in the Cosmos SDK\n* The use of typed events will simplify the process of emitting and subscribing to events, making it easier to build event-driven applications\n* This change will benefit users of the Cosmos SDK, including wallets, exchanges, explorers, and DeFi protocols\n\n**Rationale**\n\nThe use of typed events will provide a more structured and maintainable way of handling events in the Cosmos SDK, making it easier for developers to build event-driven applications. This change will also enable the Cosmos SDK to better support the growing demand for event-driven applications in the ecosystem.\n\n**Example**\n\nFor example, a module might define an event type `NewOrderCreated` and a handler for that event type. A client of the blockchain could then subscribe to this event type and receive notifications when a new order is created.\n\n**Status**\n\nThis decision is currently proposed and pending review and acceptance by the Cosmos SDK community.","GenTime":"2024-08-10 02:14:06"}
{"File Name":"crispy-dragon\/ADR-2-Frontend-Framework.md","Context":"## Context\\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\\n","Decision":"The frontend will be build with [React](https:\/\/reactjs.org\/) and [Snowpack](https:\/\/www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https:\/\/tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n","tokens":27,"id":847,"Prediction":"## Decision\n\nWe will use a modern web framework, such as React or Angular, to build a user-friendly and responsive frontend for our API.\n\n## Context\n\nTo make it easy for users to interact with our API, we need a frontend that can be used by modern devices. Our API is designed to be scalable and efficient, but we need a frontend that can provide a seamless user experience across different devices and platforms.\n\n## Problem\n\nOur current API does not have a frontend, making it difficult for users to interact with it. We need a frontend that can handle different devices, browsers, and screen sizes, and provide a user-friendly interface for our users.\n\n## Decision\n\nAfter considering different options, we have decided to use a modern web framework, such as React or Angular, to build a user-friendly and responsive frontend for our API. This will allow us to create a frontend that is easy to use, fast, and scalable.\n\n## Rationale\n\nWe chose React or Angular because they are popular and widely used frameworks that can handle complex user interfaces and provide a seamless user experience. They also have large communities and plenty of resources available, which will make it easier for us to find help and troubleshoot any issues that may arise.\n\n## Consequences\n\n* We will need to invest time and resources in building and maintaining the frontend.\n* We will need to ensure that our API is designed to work well with the frontend and provide the necessary data and functionality.\n* We will need to test the frontend thoroughly to ensure that it works well across different devices and browsers.\n\n## Status\n\nThis decision is approved and will be implemented in the next sprint.\n\n## Affected Components\n\n* Frontend\n* API\n\n## References\n\n* React: https:\/\/reactjs.org\/\n* Angular: https:\/\/angular.io\/","GenTime":"2024-08-10 02:15:28"}
{"File Name":"infra\/0002-use-helm-for-service-deployment-definitions.md","Context":"## Context\\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock\/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region\/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\\n","Decision":"Use helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock\/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev\/staging\/prod versions of the service.\\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\\n","tokens":190,"id":856,"Prediction":"## Decision\n\nTo address the problem of drift between different environments and to promote code reuse, we will adopt a Kubernetes-native approach to defining and managing our deployments. Specifically, we will use Helm Charts to package and deploy our services to different Kubernetes clusters.\n\n## Context\n\nAs mentioned in the context, we have existing Kubernetes deployments and our primary supported services are already deployed there. We have been using a separate set of YAML files to define deployments to different regions and clouds, which leads to duplicated effort and increases the likelihood of drift between environments.\n\n## Problem\n\nThe current approach has several drawbacks:\n\n* Duplicated effort: We have to maintain multiple copies of nearly identical YAML files, which can lead to errors and inconsistencies.\n* Increased likelihood of drift: With multiple copies of the same configuration, it's more likely that changes will be made to one copy without being reflected in the others.\n* Limited scalability: As the number of environments and services grows, managing multiple YAML files becomes increasingly cumbersome.\n\n## Decision\n\nTo address these issues, we will use Helm Charts to package and deploy our services to different Kubernetes clusters. Helm Charts are a way to define, install, and manage Kubernetes applications, and they provide a number of benefits:\n\n* Code reuse: Helm Charts allow us to define our deployments once and reuse them across multiple environments.\n* Consistency: Helm Charts ensure that our deployments are consistent across all environments, reducing the likelihood of drift.\n* Scalability: Helm Charts make it easy to manage multiple environments and services, even as our infrastructure grows.\n\n## Consequences\n\n* We will need to learn and use Helm Charts, which may require additional training and expertise.\n* We will need to refactor our existing YAML files to use Helm Charts, which may require some upfront effort.\n* We will need to ensure that our Helm Charts are properly tested and validated before deploying them to production.\n\n## Rationale\n\nThe benefits of using Helm Charts outweigh the costs. By using Helm Charts, we can promote code reuse, reduce the likelihood of drift, and increase scalability. Additionally, Helm Charts provide a number of other benefits, such as support for rolling updates and self-healing deployments.","GenTime":"2024-08-10 02:18:18"}
{"File Name":"nada-kafkarator\/0002-use-python.md","Context":"## Context\\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\\nAnother option was Rust, which could be a useful learning exercise.\\nPython is used in FIAAS, so we have some experience using it for an operator.\\n","Decision":"We will use Python for Kafkarator.\\n","tokens":61,"id":861,"Prediction":"## Decision\n\nGiven the context that the team lacks proficiency in Golang, and considering the option to use Rust as a learning exercise, we decide to use Python as the programming language for the k8s operator. This decision is based on the following factors:\n\n* Familiarity: As mentioned, the team has experience using Python in FIAAS, which reduces the learning curve and allows us to focus on the operator's functionality rather than language-specific complexities.\n* Ecosystem: Python has a vast and mature ecosystem for k8s operators, with many existing libraries and tools that can be leveraged to simplify the development process.\n* Development speed: Python's syntax and nature make it a relatively fast language to develop in, which is essential for an operator that needs to be deployed and tested quickly.\n* Community: Python has a large and active community, which ensures that there are many resources available for troubleshooting and learning.\n\nBy choosing Python, we can focus on developing a robust and efficient k8s operator without the added complexity of learning a new language. This decision will allow us to deliver the operator quickly and effectively, while also leveraging our existing knowledge and experience with Python.","GenTime":"2024-08-10 02:19:56"}
{"File Name":"nada-kafkarator\/0003-use-poetry-for-dependency-management.md","Context":"## Context\\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\\nspace, and the Python community has not landed on a single standard. Each project needs to select which\\ntools to use, usually a selection of tools that integrate well.\\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\\nclaims to support the use case well. It is also following the new pyproject.toml standard.\\n","Decision":"We will use Poetry for this project. (https:\/\/python-poetry.org\/)\\n","tokens":148,"id":864,"Prediction":"## Decision\n\nAfter careful consideration, we will use Poetry as the dependency management tool for our Python projects. This decision is based on the following factors:\n\n* Poetry's support for the new pyproject.toml standard, which provides a more robust and flexible way of managing dependencies.\n* Poetry's ability to integrate well with other tools and frameworks, making it a versatile choice for our projects.\n* The growing community support for Poetry, which indicates a high level of interest and activity in the project.\n* The fact that Poetry is actively maintained and has a clear roadmap for future development, which gives us confidence in its long-term viability.\n\nWe will not use Pipenv due to the issues it has, such as the lack of progress in addressing its problems. While Pipenv has a strong following, we believe that Poetry offers a more comprehensive and well-maintained solution for our needs.\n\nBy using Poetry, we will be able to effectively manage dependencies for our projects and ensure that they are isolated from the system Python. This will help to improve the overall stability and maintainability of our projects, and will also make it easier for new developers to get started with our codebase.","GenTime":"2024-08-10 02:20:44"}
{"File Name":"nada-kafkarator\/0004-combine-topic-creation-and-credentials-management-in-same-app.md","Context":"## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n","Decision":"We will ignore the SRP in this instance, and keep the two concerns in the same application.\\n","tokens":83,"id":866,"Prediction":"## Decision\n\n**Topic Management and Credential Provision**\n\n## Context\n\nAs described by the user, the project requires handling two related but distinct concerns: creating topics when needed and supplying credentials for working with topics. While these concerns are conceptually connected, they are separate in implementation and could be split into separate applications according to the Single Responsibility Principle (SRP).\n\n## Problem\n\nKeeping these concerns separate would lead to a more modular and maintainable codebase, but it would also introduce complexity and overhead due to the need for inter-application communication and coordination.\n\n## Decision\n\nWe will keep the topic management and credential provision concerns within the same application, but encapsulate them in separate modules or services to maintain a clear separation of concerns and ensure that each module is responsible for its own specific functionality.\n\n## Rationale\n\nBy keeping these concerns together, we can reduce the complexity and overhead of inter-application communication and coordination, while still maintaining a clear separation of concerns and ensuring that each module is responsible for its own specific functionality.\n\n## Consequences\n\nThis decision will allow us to:\n\n* Reduce the complexity and overhead of inter-application communication and coordination\n* Improve the maintainability and scalability of the codebase\n* Ensure that each module is responsible for its own specific functionality\n\nHowever, this decision may also lead to:\n\n* Increased coupling between the topic management and credential provision modules\n* Potential for increased complexity and overhead in the codebase\n\n## Implementation\n\nWe will implement this decision by creating separate modules or services for topic management and credential provision, and ensuring that each module is responsible for its own specific functionality. We will also ensure that the modules are designed to be loosely coupled and can be developed and maintained independently.","GenTime":"2024-08-10 02:21:02"}
{"File Name":"profiles-etl-combiner\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":871,"Prediction":"## Decision\n\n**Title:** Use of Microservices Architecture for the Project\n\n**Context:** As the project grows in complexity and scope, we need to decide on an architecture that can scale and meet the demands of a large user base. The team has discussed various options, including monolithic, service-oriented, and microservices architectures.\n\n**Problem:** The current monolithic architecture is becoming increasingly difficult to maintain and scale. It is also limiting our ability to develop new features and integrate with other systems.\n\n**Decision:** We will adopt a microservices architecture for the project. This will allow us to break down the system into smaller, independent services that can be developed, tested, and deployed independently.\n\n**Rationale:** Microservices architecture provides several benefits, including:\n\n* Scalability: Each service can be scaled independently, allowing us to focus resources on the services that require it most.\n* Flexibility: With microservices, we can use different programming languages, frameworks, and databases for each service, giving us more flexibility in our technology choices.\n* Resilience: If one service experiences issues, it will not affect the entire system, making it more resilient.\n* Easier maintenance: With smaller, independent services, maintenance and updates will be easier and less prone to breaking other parts of the system.\n\n**Consequences:**\n\n* Increased complexity: Microservices architecture can be more complex to manage and orchestrate than a monolithic architecture.\n* Higher operational overhead: With more services to manage, we will need to invest in additional infrastructure and tools to support them.\n* Potential for inconsistent data: With multiple services accessing and updating data, there is a risk of inconsistent data if not properly managed.\n\n**Status:** This decision is currently in the implementation phase. We will continue to monitor the effectiveness of the microservices architecture and make adjustments as needed.\n\n**Changelog:**\n\n* 2023-02-20: Initial decision made to adopt microservices architecture.\n* 2023-03-15: Implementation of microservices architecture began.\n* 2023-04-10: Initial services deployed and tested.\n\n**References:**\n\n* \"Microservices: A Critical Review\" by Sam Newman\n* \"Building Microservices\" by Sam Newman\n* \"Microservices Patterns\" by Chris Richardson","GenTime":"2024-08-10 02:22:58"}
{"File Name":"kotlin-guice\/0002-use-packaging-for-major-version-interoperability.md","Context":"## Context\\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\\n","Decision":"We will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\\n1. The Java package name will include the version number.\\n1. The library name will be part of the group ID.\\n1. The group ID will include the version number.\\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\\nhttp:\/\/jakewharton.com\/java-interoperability-policy-for-major-version-updates\/\\n","tokens":88,"id":872,"Prediction":"## Decision\n\n**Decision:** To implement a version pinning strategy for the `kotlin-guice` library to ensure backward compatibility and reduce the risk of breaking changes when upgrading to a new major version.\n\n## Context\n\nAs mentioned in the context, major version updates to libraries often include breaking API changes, which can be challenging to deal with when upgrading an application. In the case of `kotlin-guice`, the API is heavily tied to the Guice API, and when Guice makes a major version upgrade, `kotlin-guice` will likely need to follow suit.\n\n## Problem\n\nThe current situation poses a risk of breaking changes when upgrading to a new major version of `kotlin-guice`. This could lead to issues such as:\n\n* Incompatibility with other libraries that depend on the updated `kotlin-guice` version\n* Compatibility issues with existing code that relies on the older API\n* Increased complexity and risk of errors during the upgrade process\n\n## Decision Rationale\n\nTo mitigate these risks, we will implement a version pinning strategy for `kotlin-guice`. This will ensure that we can continue to use the current version of `kotlin-guice` without having to upgrade to a newer major version, which would require significant changes to our codebase.\n\n## Consequences\n\nBy implementing a version pinning strategy, we will:\n\n* Reduce the risk of breaking changes and compatibility issues when upgrading to a new major version of `kotlin-guice`\n* Minimize the complexity and risk of errors during the upgrade process\n* Allow us to continue using the current version of `kotlin-guice` without having to upgrade to a newer major version\n\n## Alternatives Considered\n\n* Upgrading to the latest major version of `kotlin-guice` and dealing with the potential breaking changes and compatibility issues\n* Using a different library that is not tied to the Guice API and is less likely to have breaking changes\n\n## Status\n\nThis decision is approved and will be implemented in the next release.","GenTime":"2024-08-10 02:23:20"}
{"File Name":"we-be-best\/1-use-both-pointer-and-mouse-events.md","Context":"## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n","Decision":"All pointer event interactions have a mouse event fallback.\\n","tokens":102,"id":873,"Prediction":"## Decision\n\n**Use a hybrid approach for capturing events: PointerEvents API for desktop and touch events for mobile**\n\n## Context\n\nAs discussed, the original intention was to use the PointerEvents API for capturing events due to its comprehensive support for both mouse and pointer inputs. However, the limited support in Safari and iOS, particularly the experimental API's issues with movementX and movementY properties, has led to a reevaluation of our approach.\n\n## Problem\n\nThe lack of support for PointerEvents API in Safari and iOS, as well as the experimental API's limitations, renders it unsuitable for our use case. We need a reliable method to capture events and determine distance without having to track previous coordinates.\n\n## Decision\n\nTo address the issue, we will adopt a hybrid approach:\n\n1. For desktop browsers that support PointerEvents API (e.g., Chrome, Firefox), we will continue to use the API to capture events and determine distance.\n2. For mobile devices and Safari, we will use touch events (e.g., touchstart, touchmove, touchend) to capture events and determine distance. This will provide a more reliable and consistent experience across mobile devices.\n\n## Rationale\n\nBy adopting a hybrid approach, we can:\n\n* Ensure a consistent and reliable experience for desktop users\n* Provide a functional experience for mobile users, despite the limitations of PointerEvents API in Safari\n* Minimize the impact of the limitations on our overall event handling system\n\n## Consequences\n\n* We will need to implement separate event handling logic for desktop and mobile devices\n* We will need to handle potential edge cases and inconsistencies between the two approaches\n* We will need to ensure that our event handling system is flexible and adaptable to changes in browser support and capabilities\n\nBy adopting this hybrid approach, we can balance the need for a consistent experience across devices with the limitations of PointerEvents API in Safari and iOS.","GenTime":"2024-08-10 02:23:41"}
{"File Name":"python-library-project-generator\/0000-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on Opinionated Digital Center.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":18,"id":875,"Prediction":"Here is an Architectural Decision Record (ADR) for the Opinionated Digital Center:\n\n## Decision\n\n**ADR-001: Microservices Architecture for Opinionated Digital Center**\n\n## Context\n\nThe Opinionated Digital Center is a complex software system that requires a scalable, maintainable, and flexible architecture to meet the growing demands of its users. As the system continues to evolve, it is essential to make informed decisions about its architecture to ensure its long-term success.\n\n## Problem\n\nThe current monolithic architecture of the Opinionated Digital Center is becoming increasingly difficult to maintain and scale. It is prone to single points of failure, and changes to one component can have a ripple effect on the entire system.\n\n## Decision\n\nAfter careful consideration, we have decided to adopt a microservices architecture for the Opinionated Digital Center. This approach will allow us to break down the system into smaller, independent services that can be developed, tested, and deployed independently.\n\nEach microservice will be responsible for a specific business capability, and will communicate with other services using lightweight protocols and APIs. This will enable us to:\n\n* Improve scalability and fault tolerance by allowing each service to be scaled independently\n* Increase deployment flexibility by allowing services to be deployed separately\n* Enhance maintainability by allowing developers to work on individual services without affecting the entire system\n* Improve collaboration by allowing teams to work on individual services without conflicts\n\n## Consequences\n\n* Increased complexity in the short term, as we need to establish new infrastructure, processes, and tools to support the microservices architecture\n* Improved scalability, maintainability, and collaboration in the long term\n* Potential for increased development time and costs in the short term, as we need to establish new service boundaries and APIs\n* Opportunities for innovation and experimentation, as each service can be developed and deployed independently\n\n## Rationale\n\nWe believe that the benefits of a microservices architecture outweigh the costs. By adopting this approach, we will be able to build a more scalable, maintainable, and flexible system that can meet the evolving needs of our users.\n\n## Status\n\nThis decision is currently being implemented, with a target completion date of [insert date].\n\n## Links\n\n* [Insert links to relevant documentation, such as design documents, architecture diagrams, or technical specifications]\n\nNote: This is just a sample ADR, and you may need to modify it to fit your specific needs and requirements.","GenTime":"2024-08-10 02:24:25"}
{"File Name":"modular-monolith-with-ddd\/0015-use-in-memory-events-bus.md","Context":"## Context\\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"events bus\" to do that.\\n","Decision":"Solution number 1 - In Memory Events Bus<\/br>\\nAt that moment we don't see more advanced integration scenarios in our system than simple publish\/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\\n","tokens":35,"id":885,"Prediction":"## Decision\n\n**Event Bus Selection**\n\n### Context\n\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"event bus\" to do that.\n\n### Problem\n\nWe need to select an event bus that can handle the complexity of our system, provide scalability, and ease of use.\n\n### Decision\n\nWe will use the **Apache Kafka** event bus to handle inter-modular communication in our system. Kafka provides a robust, scalable, and fault-tolerant event bus that can handle high-volume and high-throughput event processing.\n\n### Rationale\n\nWe chose Kafka over other event bus options (e.g., RabbitMQ, Amazon SQS) due to its ability to handle large-scale event processing, its scalability, and its ease of use. Additionally, Kafka provides features such as topic-based partitioning, which allows for efficient event routing and processing.\n\n### Consequences\n\n* We will need to implement Kafka producers and consumers in our system to publish and subscribe to events.\n* We will need to design and implement event schemas to ensure that events are properly formatted and processed.\n* We will need to monitor and manage Kafka clusters to ensure performance and scalability.\n\n### Alternatives Considered\n\n* RabbitMQ: While RabbitMQ is a popular message broker, it is not designed for high-volume event processing and may not provide the same level of scalability as Kafka.\n* Amazon SQS: While Amazon SQS is a scalable message broker, it is a managed service and may not provide the same level of customization as Kafka.\n* Other event bus options: We considered other event bus options such as Apache Pulsar, Google Cloud Pub\/Sub, and Azure Event Grid, but ultimately chose Kafka due to its scalability, ease of use, and robust features.","GenTime":"2024-08-10 02:27:20"}
{"File Name":"modular-monolith-with-ddd\/0016-create-ioc-container-per-module.md","Context":"## Context\\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\\n","Decision":"Solution number 2 - IoC Container per module<\/br>\\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\\n","tokens":37,"id":899,"Prediction":"## Decision\n\n**Decision:** We will use a combination of Dependency Injection and Lazy Loading to resolve the dependencies of objects in our graph.\n\n**Context:** For each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\n\n**Problem:** We need to ensure that our objects are properly initialized and their dependencies are resolved when they are needed. This is a common problem in object-oriented programming, especially when dealing with complex graphs of objects.\n\n**Reasoning:** We have considered several options, including:\n\n* Eager Loading: loading all dependencies upfront, but this can lead to performance issues and unnecessary loading of data.\n* Manual Resolution: manually resolving dependencies in each object, but this can lead to tight coupling and make the code harder to maintain.\n* Factory Pattern: using a factory pattern to create objects and resolve their dependencies, but this can lead to complex and tightly coupled code.\n\n**Solution:** We will use a combination of Dependency Injection and Lazy Loading to resolve the dependencies of objects in our graph. This approach allows us to decouple the objects from each other and from the specific implementation details of their dependencies. It also allows us to load dependencies only when they are needed, which can improve performance and reduce memory usage.\n\n**Consequences:**\n\n* Pros:\n\t+ Decoupling: objects are decoupled from each other and from the specific implementation details of their dependencies.\n\t+ Flexibility: it is easy to change the implementation of dependencies without affecting the objects that use them.\n\t+ Performance: dependencies are loaded only when they are needed, which can improve performance and reduce memory usage.\n* Cons:\n\t+ Complexity: the code may become more complex due to the additional layer of abstraction introduced by the dependency injection mechanism.\n\t+ Error-prone: if not implemented correctly, the dependency injection mechanism can lead to errors and bugs.\n\n**Implementation:** We will implement the dependency injection mechanism using a third-party library, such as Spring or Guice, which provides a simple and efficient way to manage dependencies between objects. We will also use lazy loading to load dependencies only when they are needed, which can be achieved using a combination of interfaces and abstract classes.\n\n**Monitoring and Feedback:** We will monitor the performance and complexity of the code and gather feedback from the development team and stakeholders to ensure that the chosen approach is effective and efficient.","GenTime":"2024-08-10 02:32:36"}
{"File Name":"csc-swr-architecture\/007-Integration-Approaches.html.md.erb","Context":"## Context\\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\\nsuch as Looked After Children and Foster Families. These will be sourced from Local\\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\\nas well as central Departmental data collections such as SSDA903.\\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\\npathways through the Care System. Each individuals pathway can be regarded as a\\nseries of events and touchpoints that the individual has had with various other actors\\nin the system, such as IFAs and Foster Families.\\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\\n","Decision":"* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\\nto LAs and IFAs, serving an [Event Architecture] available internally.\\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\\n* Publish example client code on the Internet to ease LA and IFA adoption.\\n* Events will become the durable and persistent data which can be used to hydrate\\nthe Department's services, such as Diary Log and Matchmaker Assistant,\\n* [Event Sourcing] should be trialled as a solution.\\n","tokens":169,"id":914,"Prediction":"## Decision\n\n**Decision:** To use a Graph Database to store and manage the individual pathways of looked after children and foster families.\n\n**Context:** The Children's Social Care Alpha Service is seeking to integrate key data assets, including Looked After Children and Foster Families, from various sources such as Local Authority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'), as well as central Departmental data collections like SSDA903. The pathways of individuals through the Care System can be represented as a series of events and touchpoints with various actors, such as IFAs and Foster Families.\n\n**Problem:** The existing SSDA903 dataset is analogous to an event stream, with the LAs and IFAs acting as event producers. However, this data is not easily queryable or analyzable, making it challenging to gain insights into individual pathways and identify patterns or trends.\n\n**Reasoning:** A Graph Database is well-suited to store and manage the complex relationships between individuals, events, and actors in the Care System. It allows for efficient querying and analysis of the data, enabling the identification of patterns and trends in individual pathways. Additionally, a Graph Database can handle the high volume and velocity of data from multiple sources, making it an ideal choice for this use case.\n\n**Consequences:**\n\n* Improved data querying and analysis capabilities, enabling better insights into individual pathways and care outcomes.\n* Enhanced ability to identify patterns and trends in data, informing decision-making and improving care services.\n* Scalability and flexibility to handle large volumes and varying types of data from multiple sources.\n* Simplified data integration and management, reducing the complexity of data processing and analysis.\n\n**Alternatives:** A relational database or a document-oriented database could have been considered as alternatives. However, these options would not have provided the same level of scalability, flexibility, and querying capabilities as a Graph Database.","GenTime":"2024-08-10 02:37:16"}
{"File Name":"csc-swr-architecture\/004-Use-Fake-Data.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\\n","Decision":"The placement alpha prototypes and development work will only process fake data.\\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the \"Authority to Operate\" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\\n","tokens":53,"id":915,"Prediction":"## Decision\n\n**Decision:** Use a mock data approach for the Children's Social Care Placement Service in the Alpha phase, instead of integrating with real data sources.\n\n**Context:** As the service is in the Alpha phase, it is not expected to process real data for users, but rather to be used as a prototype for building and testing different ideas. This approach will allow us to focus on developing the service's architecture and user experience without the added complexity and risk of integrating with real data sources.\n\n**Rationale:**\n\n* Allowing us to test and iterate on the service's design and functionality without the constraint of working with real data.\n* Enabling us to focus on the service's user experience and usability without the added complexity of integrating with real data sources.\n* Reducing the risk of errors or data breaches that could occur if we were to integrate with real data sources during the Alpha phase.\n* Allowing us to prioritize the development of the service's core functionality and architecture over the integration with real data sources.\n\n**Consequences:**\n\n* We will need to create mock data to populate the service, which will add some development time and effort.\n* We will need to ensure that the mock data is realistic and representative of the data that will be used in the live service.\n* We will need to be mindful of the limitations of using mock data and ensure that it does not impact the service's ability to meet its requirements and goals.\n\n**Alternatives Considered:**\n\n* Integrating with real data sources during the Alpha phase, which would add complexity and risk to the development process.\n* Using a test data approach, which would require us to generate test data that is representative of the real data that will be used in the live service.\n\n**Status:** Accepted\n\n**Implementation:** The development team will create mock data to populate the service and ensure that it is realistic and representative of the data that will be used in the live service. The team will also ensure that the mock data is updated regularly to reflect any changes to the service's requirements or design.","GenTime":"2024-08-10 02:37:38"}
{"File Name":"csc-swr-architecture\/003-Platform-and-Hosting.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\\n","Decision":"The decision is to use the [Gov.UK PaaS].\\n**What does Gov.UK PaaS offer?**\\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\\n* Fast application delivery, with short cycles\\n* Flexible development tools\\n* Simplified DevOps\\n* Seamless scalability\\n* Integrated data services\\n* A secure, trusted platform\\n* 24x7 support\\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\\n**Why not DfE Azure?**\\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\\n**Is it secure?**\\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\\nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.cloud.service.gov.uk\/cloud-security-principles\/).\\nAnd for the DfE there is a specific [paper](https:\/\/docs.google.com\/document\/d\/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ\/edit#heading=h.du8tl2830srf) from MBS.\\n","tokens":65,"id":920,"Prediction":"## Decision\n\n**Decision:** Emphasize developer productivity and flexibility in the Alpha phase of the Children's Social Care Placement service, prioritizing exploratory solutions over performance optimization and scalability.\n\n**Context:** As the service is currently in Alpha and still in the exploratory phase of the SDLC, it is essential to prioritize developer productivity and flexibility to ensure the service can adapt to the most suitable solution.\n\n**Problem:** With the service still in its early stages, it is crucial to focus on rapid prototyping, testing, and iteration to validate assumptions and gather feedback from stakeholders.\n\n**Reasoning:** By prioritizing developer productivity and flexibility, we can:\n\n1. Reduce the time spent on setting up and configuring complex infrastructure, allowing developers to focus on writing code and testing ideas.\n2. Encourage experimentation and exploration of different solutions, enabling the service to adapt to changing requirements and stakeholder feedback.\n3. Foster a culture of rapid iteration and continuous learning, ensuring the service is aligned with the needs of the users and stakeholders.\n\n**Consequences:**\n\n* The service may not be optimized for performance or scalability in the short term, but this can be addressed later in the development cycle.\n* The focus on developer productivity may lead to temporary inefficiencies in terms of resource utilization, but this will be outweighed by the benefits of rapid iteration and adaptation.\n* The service may require additional resources or infrastructure as it evolves, but this can be planned and implemented as needed.\n\n**Status:** This decision is in place until the service reaches the Beta phase, at which point we will reassess and adjust our priorities accordingly.","GenTime":"2024-08-10 02:39:17"}
{"File Name":"terraform\/AMIs.md","Context":"## Context\\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build\/maintain our own images.\\n","Decision":"We will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\\n","tokens":50,"id":923,"Prediction":"## Decision\n\n**Standardize on a set of managed AWS images for our infrastructure**\n\n## Context\n\nWe have many operating systems in play at DXC. In moving to AWS, the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build\/maintain our own images.\n\n## Problem\n\nWith multiple operating systems and image options, we risk:\n\n* Increased complexity in managing and maintaining our infrastructure\n* Reduced consistency and standardization across our environments\n* Difficulty in ensuring compliance with security and regulatory requirements\n* Higher costs associated with maintaining multiple image variants\n\n## Decision\n\nTo mitigate these risks, we will standardize on a set of managed AWS images for our infrastructure. This decision ensures that:\n\n* We can leverage the expertise and resources of AWS to manage and maintain our images\n* We can reduce the complexity and overhead associated with managing multiple image variants\n* We can ensure consistency and standardization across our environments\n* We can better ensure compliance with security and regulatory requirements\n* We can reduce costs associated with maintaining multiple image variants\n\n## Consequences\n\n* We will need to conduct a thorough analysis of our current infrastructure and identify the most suitable managed AWS images for our use cases\n* We will need to develop a plan for migrating our existing infrastructure to the standardized images\n* We will need to establish a process for monitoring and maintaining our standardized images to ensure they remain up-to-date and secure\n* We may need to invest in additional training and resources to ensure our teams are equipped to manage and maintain the standardized images\n\n## Rationale\n\nStandardizing on a set of managed AWS images will allow us to simplify our infrastructure management, reduce costs, and improve compliance and security. By leveraging the expertise and resources of AWS, we can focus on developing and delivering high-quality software applications, rather than managing and maintaining our infrastructure.","GenTime":"2024-08-10 02:40:17"}
{"File Name":"terraform\/Automation.md","Context":"## Context\\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\\n","Decision":"We will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\\n","tokens":93,"id":929,"Prediction":"## Decision\n\n**Automate maintenance tasks for migrated services**\n\n## Context\n\nServices that we are in the process of migrating or that have recently completed migration are likely to go through a period of needing more maintenance that a mature service.\n\n## Problem\n\nHandling all maintenance tasks on a case-by-case basis can lead to:\n\n* Reduced velocity and productivity due to the time-consuming nature of manual fixes\n* Increased risk of human error and bugs introduced during the fixing process\n* Inefficient use of resources, as team members may be spending more time on maintenance tasks than on new feature development\n\n## Decision\n\nTo address these issues, we will automate as many maintenance tasks as possible for migrated services. This will enable us to:\n\n* Reduce the time and effort required for maintenance tasks, freeing up resources for more strategic work\n* Minimize the risk of human error and bugs introduced during the fixing process\n* Improve the overall efficiency and effectiveness of our maintenance processes\n\n## Consequences\n\n* Initial investment in automating maintenance tasks will require significant upfront effort and resources\n* May require additional training and expertise for team members to work with automated tools and processes\n* May lead to a temporary increase in technical debt as we refactor code to accommodate automated maintenance tasks\n\n## Rationale\n\nAutomating maintenance tasks for migrated services will help us to reduce the overhead of manual fixes and free up resources for more strategic work. This will enable us to maintain a high velocity and continue to deliver value to our customers. Additionally, automating maintenance tasks will help us to reduce the risk of human error and bugs introduced during the fixing process, which will lead to a more stable and reliable system.","GenTime":"2024-08-10 02:42:05"}
{"File Name":"tech-team\/0006-supported-node-and-npm-versions.md","Context":"## Context\\neLife has projects built in Node.\\neLife has projects that use the Node Package Manager (npm) in at least part of their own build process, even if the main technology of the project is not Node.\\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\\nStaying up to date with the major and minor versions of Node is important:\\n- to be able to use new features (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\n","Decision":"To use 8.x, the current Long Term Support (LTS) line.\\nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https:\/\/github.com\/nodejs\/Release#release-schedule)).\\nTo use whichever npm version is bundled with the Node version we use.\\nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.\\n","tokens":143,"id":939,"Prediction":"## Decision\n\n**Node.js and npm Version Support**\n\n## Context\n\nAs Life has projects built in Node.js and uses the Node Package Manager (npm) in at least part of their own build process, it is essential to establish a consensus on which Node.js and npm versions to support. This decision aims to provide language version consistency across projects and ensure dependency management consistency.\n\n## Problem\n\nStaying up to date with the major and minor versions of Node.js is crucial to:\n\n1. Leverage new features (nice to have)\n2. Keep working on a version that receives bug fixes and security updates (must have)\n\n## Decision\n\nBased on the context and problem, we decide to support the latest two stable major versions of Node.js and the latest two stable minor versions of npm.\n\n* Node.js: Support the latest two stable major versions, such as Node.js 14 and Node.js 16.\n* npm: Support the latest two stable minor versions, such as npm 7 and npm 8.\n\nThis decision ensures that our projects can take advantage of new features, bug fixes, and security updates while maintaining a reasonable level of compatibility across projects.\n\n## Rationale\n\nThis decision balances the need to stay up to date with the latest versions of Node.js and npm while also considering the potential impact on existing projects. By supporting the latest two stable major versions of Node.js, we can leverage new features and bug fixes without disrupting existing projects. Similarly, by supporting the latest two stable minor versions of npm, we can ensure that our projects can continue to use the latest dependency management features and bug fixes.\n\n## Consequences\n\nThis decision has the following consequences:\n\n* Our projects will need to be compatible with the supported versions of Node.js and npm.\n* We will need to ensure that our build processes and dependencies are compatible with the supported versions.\n* We will need to regularly update our projects to stay compatible with the latest supported versions.\n\nBy making this decision, we can ensure consistency across projects and maintain a high level of compatibility while staying up to date with the latest versions of Node.js and npm.","GenTime":"2024-08-10 02:45:57"}
{"File Name":"edgex-docs\/0018-Service-Registry.md","Context":"- [Context](#context)\\n- [Existing Behavior](#existing-behavior)\\n* [Device Services](#device-services)\\n+ [Registry Client Interface Usage](#registry-client-interface-usage)\\n* [Core and Support Services](#core-and-support-services)\\n* [Security Proxy Setup](#security-proxy-setup)\\n- [History](#history)\\n- [Problem Statement](#problem-statement)\\n- [Decision](#decision)\\n- [References](#references)\\n<!--te-->\\n","Decision":"- [References](#references)\\n<!--te-->\\nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's ```Client.GetServiceEndpoint``` method (if started with the\\n```--registry``` option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpoints\\n(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, with\\nonly minor changes required in the C Device SDK (see previous commments re: the current implementation).\\n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadata\\nboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup\\nthe address of the other service.\\n","tokens":111,"id":946,"Prediction":"Here is the Architectural Decision Record:\n\n**Context**\nThe software is designed to provide a set of device services that interact with various devices and systems. The software is expected to be highly available, scalable, and secure.\n\n**Existing Behavior**\nThe existing behavior of the software is to use a registry client interface to interact with the device services. The registry client interface is used to register and deregister devices, as well as to retrieve information about the devices. The core and support services are responsible for handling the business logic and providing the necessary functionality. The security proxy setup is used to secure the communication between the device services and the core and support services.\n\n**Decision**\n## Decision: Use a Messaging Queue for Device Services Communication\n\n**Context**: The device services communication is becoming increasingly complex, and the current registry client interface is not able to handle the growing volume of requests. The software needs a more scalable and fault-tolerant solution to ensure high availability and performance.\n\n**Problem Statement**: The current registry client interface is not able to handle the growing volume of requests, leading to performance issues and potential downtime.\n\n**Decision**: We will use a messaging queue to communicate between the device services and the core and support services. This will allow for a more scalable and fault-tolerant solution, and will enable the software to handle a higher volume of requests.\n\n**Rationale**: The messaging queue will allow for asynchronous communication between the device services and the core and support services, which will enable the software to handle a higher volume of requests. Additionally, the messaging queue will provide a fault-tolerant solution, as messages can be retried in case of failures.\n\n**References**:\n\n* [1] \"Message Queue Pattern\" by Martin Fowler\n* [2] \"Designing Distributed Systems\" by Brendan Burns","GenTime":"2024-08-10 02:48:13"}
{"File Name":"edgex-docs\/014-Secret-Provider-For-All.md","Context":"- [Context](#context)\\n* [Existing Implementations](#existing-implementations)\\n+ [What is a Secret?](#what-is-a-secret)\\n+ [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\\n+ [Known and Unknown Services](#known-and-unknown-services)\\n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\\n+ [Interfaces and factory methods](#interfaces-and-factory-methods)\\n- [Bootstrap's current implementation](#bootstraps-current-implementation)\\n* [Interfaces](#interfaces)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n- [App SDK's current implementation](#app-sdks-current-implementation)\\n* [Interface](#interface)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)\\n- [InsecureSecrets Configuration](#insecuresecrets-configuration)\\n- [Decision](#decision)\\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\n","Decision":"* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\nThe new `SecretProvider` abstraction defined by this ADR is a combination of the two implementations described above in the [Existing Implementations](#existing-implementations) section.\\n### Only Exclusive Secret Stores\\nTo simplify the `SecretProvider` abstraction, we need to reduce to using only exclusive `SecretStores`. This allows all the APIs to deal with a single `SecretClient`, rather than the split up way we currently have in Application Services. This requires that the current Application Service shared secrets (database credentials) must be copied into each Application Service's exclusive `SecretStore` when it is created.\\nThe challenge is how do we seed static secrets for unknown services when they become known.  As described above in the [Known and Unknown Services](#known-and-unknown-services) section above,  services currently identify themselves for exclusive `SecretStore` creation via the `EDGEX_ADD_SECRETSTORE_TOKENS` environment variable on security-secretstore-setup. This environment variable simply takes a comma separated list of service names.\\n```yaml\\nEDGEX_ADD_SECRETSTORE_TOKENS: \"<service-name1>,<service-name2>\"\\n```\\nIf we expanded this to add an optional list of static secret identifiers for each service, i.e.  `appservice\/redisdb`, the exclusive store could also be seeded with a copy of static shared secrets. In this case the Redis database credentials for the Application Services' shared database. The environment variable name will change to `ADD_SECRETSTORE` now that it is more than just tokens.\\n```yaml\\nADD_SECRETSTORE: \"app-service-xyz[appservice\/redisdb]\"\\n```\\n> *Note: The secret identifier here is the short path to the secret in the existing **appservice**  `SecretStore`. In the above example this expands to the full path of `\/secret\/edgex\/appservice\/redisdb`*\\nThe above example results in the Redis credentials being copied into app-service-xyz's `SecretStore` at `\/secret\/edgex\/app-service-xyz\/redis`.\\nSimilar approach could be taken for Message Bus credentials where a common `SecretStore` is created with the Message Bus credentials saved. The services request the credentials are copied into their exclusive `SecretStore` using `common\/messagebus` as the secret identifier.\\nFull specification for the environment variable's value is a comma separated list of service entries defined as:\\n```\\n<service-name1>[optional list of static secret IDs sperated by ;],<service-name2>[optional list of static secret IDs sperated by ;],...\\n```\\nExample with one service specifying IDs for static secrets and one without static secrets\\n```yaml\\nADD_SECRETSTORE: \"appservice-xyz[appservice\/redisdb; common\/messagebus], appservice-http-export\"\\n```\\nWhen the `ADD_SECRETSTORE` environment variable is processed to create these `SecretStores`, it will copy the specified saved secrets from the initial `SecretStore` into the service's `SecretStore`. This all depends on the completion of database or other credential bootstrapping and the secrets having been stored prior to the environment variable being processed. security-secretstore-setup will need to be refactored to ensure this sequencing.\\n### Abstraction Interface\\nThe following will be the new `SecretProvider` abstraction interface used by all Edgex services\\n```go\\ntype SecretProvider interface {\\n\/\/ Stores new secrets into the service's exclusive SecretStore at the specified path.\\nStoreSecrets(path string, secrets map[string]string) error\\n\/\/ Retrieves secrets from the service's exclusive SecretStore at the specified path.\\nGetSecrets(path string, _ ...string) (map[string]string, error)\\n\/\/ Sets the secrets lastupdated time to current time.\\nSecretsUpdated()\\n\/\/ Returns the secrets last updated time\\nSecretsLastUpdated() time.Time\\n}\\n```\\n> *Note: The `GetDatabaseCredentials` and `GetCertificateKeyPair` APIs have been removed. These are no longer needed since insecure database credentials will no longer be stored in the `DatabaseInfo` configuration and certificate key pairs are secrets like any others. This allows these secrets to be retrieved via the `GetSecrets` API.*\\n### Implementation\\n#### Factory Method and Bootstrap Handler\\nThe factory method and bootstrap handler will follow that currently in the Bootstrap implementation with some tweaks. Rather than putting the two split interfaces into the DIC, it will put just the single interface instance into the DIC. See details in the [Interfaces and factory methods](#interfaces-and-factory-methods) section above under **Existing Implementations**.\\n#### Caching of Secrets\\nSecrets will be cached as they are currently in the Application Service implementation\\n#### Insecure Secrets\\nInsecure Secrets will be handled as they are currently in the Application Service implementation. `DatabaseInfo` configuration will no longer be an option for storing the insecure database credentials. They will be stored in the `InsecureSecrets` configuration only.\\n```toml\\n[Writable.InsecureSecrets]\\n[Writable.InsecureSecrets.DB]\\npath = \"redisdb\"\\n[Writable.InsecureSecrets.DB.Secrets]\\nusername = \"\"\\npassword = \"\"\\n```\\n##### Handling on-the-fly changes to `InsecureSecrets`\\nAll services will need to handle the special processing when `InsecureSecrets` are changed on-the-fly via Consul. Since this will now be a common configuration item in `Writable` it can be handled in `go-mod-bootstrap` along with existing log level processing. This special processing will be taken from App SDK.\\n#### Mocks\\nProper mock of the `SecretProvider` interface will be created with `Mockery` to be used in unit tests. Current mock in App SDK is hand written rather then generated with `Mockery`.\\n#### Where will `SecretProvider` reside?\\n##### Go Services\\nThe final decision to make is where will this new `SecretProvider` abstraction reside? Originally is was assumed that it would reside in `go-mod-secrets`, which seems logical. If we were to attempt this with the implementation including the bootstrap handler, `go-mod-secrets` would have a dependency on `go-mod-bootstrap` which will likely create a circular dependency.\\nRefactoring the existing implementation in `go-mod-bootstrap` and have it reside there now seems to be the best choice.\\n##### C Device Service\\nThe C Device SDK will implement the same `SecretProvider` abstraction, InsecureSercets configuration and the underling `SecretStore` client.\\n### Consequences\\n- All service's will have `Writable.InsecureSecrets` section added to their configuration\\n- `InsecureSecrets` definition will be moved from App SDK to go-mod-bootstrap\\n- Go Device SDK will add the SecretProvider to it's bootstrapping\\n- C Device SDK implementation could be big lift?\\n- ` SecretStore`configuration section will be added to all Device Services\\n- edgex-go services will be modified to use the single `SecretProvider` interface from the DIC in place of current usage of the `GetDatabaseCredentials` and `GetCertificateKeyPair` interfaces.\\n- Calls to `GetDatabaseCredentials` and `GetCertificateKeyPair` will be replaced with calls to `GetSecrets` API and appropriate processing of the returned secrets will be added.\\n- App SDK will be modified to use `GetSecrets` API in place of the `GetDatabaseCredentials` API\\n- App SDK will be modified to use the new `SecretProvider` bootstrap handler\\n- app-service-configurable's configuration profiles as well as all the Application Service examples configurations will be updated to remove the `SecretStoreExclusive` configuration and just use the existing `SecretStore` configuration\\n- security-secretstore-setup will be enhanced as described in the [Only Exclusive Secret Stores](#only-exclusive-secret-stores) section above\\n- Adding new services that need static secrets added to their `SecretStore` requires stopping and restarting all the services. The is because security-secretstore-setup has completed but not stopped. If it is rerun without stopping the other services, there tokens and static secrets will have changed. The planned refactor of `security-secretstore-setup` will attempt to resolve this.\\n- Snaps do not yet support setting the environment variable for adding SecretStore. It is planned for Ireland release.\\n","tokens":402,"id":947,"Prediction":"## Decision\n\nOnly Exclusive Secret Stores\n\n## Context\n\nWe are designing a system that requires the use of secrets for various services. These secrets can be exclusive to a specific service or shared among multiple services. We need to decide on a strategy for handling these secrets and how to provide them to the services that require them.\n\n## Existing Implementations\n\n### What is a Secret?\n\nA secret is a piece of sensitive information that needs to be kept confidential. In our system, secrets can be used for authentication, authorization, or other purposes.\n\n### Service Exclusive vs Service Shared Secrets\n\nSecrets can be exclusive to a specific service or shared among multiple services. Exclusive secrets are only used by a single service, while shared secrets are used by multiple services.\n\n### Known and Unknown Services\n\nWe have known services that will be using the secrets, and unknown services that may be added in the future. We need to design a system that can handle both known and unknown services.\n\n### Static Secrets and Runtime Secrets\n\nSecrets can be either static or runtime. Static secrets are hardcoded into the system, while runtime secrets are generated or retrieved at runtime.\n\n### Interfaces and factory methods\n\nWe have interfaces and factory methods that are used to provide secrets to the services.\n\n### Bootstrap's current implementation\n\nThe bootstrap process currently uses a static secret store to provide secrets to the services.\n\n### App SDK's current implementation\n\nThe App SDK currently uses a static secret store to provide secrets to the services.\n\n### Secret Store for non-secure mode\n\nIn non-secure mode, we need to store the secrets in a secure manner.\n\n### InsecureSecrets Configuration\n\nWe have a configuration option to enable insecure secrets.\n\n### Decision\n\nAfter careful consideration, we have decided to use only exclusive secret stores. This means that each service will have its own secret store, and secrets will not be shared among services.\n\n### Abstraction Interface\n\nWe will create an abstraction interface for the secret store that can be implemented by each service.\n\n### Implementation\n\nWe will create a factory method and bootstrap handler that will provide the secrets to the services. We will also implement caching of secrets to improve performance.\n\n### Factory Method and Bootstrap Handler\n\nThe factory method will create an instance of the secret store, and the bootstrap handler will provide the secrets to the services.\n\n### Caching of Secrets\n\nWe will cache the secrets to improve performance and reduce the number of requests to the secret store.\n\n### Insecure Secrets\n\nWe will handle insecure secrets by using a separate configuration option to enable insecure secrets.\n\n### Handling on-the-fly changes to `InsecureSecrets`\n\nWe will handle on-the-fly changes to `InsecureSecrets` by reloading the secret store.\n\n### Mocks\n\nWe will create mocks for the secret store to test the services.\n\n### Where will `SecretProvider` reside?\n\n`SecretProvider` will reside in the service-specific package.\n\n### Go Services\n\nWe will use Go services to provide the secrets to the services.\n\n### C Device Service\n\nWe will use the C device service to provide the secrets to the services.\n\n### Consequences\n\nUsing only exclusive secret stores will provide better security and isolation for each service. It will also make it easier to manage and maintain the secrets. However, it may require more complex implementation and testing.","GenTime":"2024-08-10 02:48:49"}
{"File Name":"pharmacy-data-etl\/0004-make-etl-re-entrant.md","Context":"## Context\\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\\n","Decision":"The ETL will be re-entrant, storing state on a regular basis.\\nRestarting the ETL will pick up from the last point.\\n","tokens":47,"id":965,"Prediction":"## Decision\n\nTo mitigate the impact of interrupted ETL processes and reduce the overall processing time, we will implement a checkpointing mechanism to allow for restarts from the last successful checkpoint.\n\n## Context\n\nAs you mentioned, if the ETL process is interrupted, it will need to start over again, which can be time-consuming and inefficient. This is particularly problematic for long-running ETL processes that may take several hours to complete.\n\n## Problem\n\nThe current ETL process design does not provide a mechanism for restarting from a previous checkpoint, resulting in the need to restart the entire process from the beginning. This can lead to significant delays and wasted time.\n\n## Decision\n\nTo address this issue, we will implement a checkpointing mechanism that allows the ETL process to save its state at regular intervals. This will enable the process to restart from the last successful checkpoint, reducing the amount of time and data required to restart the process.\n\n## Rationale\n\nCheckpointing will provide several benefits, including:\n\n1. Reduced processing time: By restarting from the last successful checkpoint, the ETL process will not need to re-process data that was already processed, reducing the overall processing time.\n2. Improved reliability: Checkpointing will provide a safety net in case of interruptions, allowing the process to restart from a known good state.\n3. Enhanced flexibility: Checkpointing will enable us to implement more complex ETL workflows and schedules, as we will be able to restart the process from a specific point in case of interruptions.\n\n## Consequences\n\nImplementing a checkpointing mechanism will require additional development effort to design and implement the checkpointing logic. However, the benefits of reduced processing time, improved reliability, and enhanced flexibility will outweigh the additional effort required.\n\n## Status\n\nThis decision is approved and will be implemented in the next iteration of the ETL process.\n\n## References\n\n* [Checkpointing in ETL processes](https:\/\/www.dataengineering.net\/blog\/checkpointing-in-etl-processes\/)\n* [Implementing Checkpointing in ETL](https:\/\/www.rittmanmead.com\/blog\/2019\/04\/implementing-checkpointing-in-etl\/)","GenTime":"2024-08-10 02:54:11"}
{"File Name":"operational-visualizations\/0001-about-cursors.md","Context":"## Context\\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\\nCurrent implementation of **cursor** looks like this\\n```tsx\\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\\n(row: RowCursor): ValueInRawRow;\\ncolumn: Name;\\nindex: number;\\n}\\n```\\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\\nCursor at the moment can be recieved from \"root\" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\\n**Question** raised in [one of PR](https:\/\/github.com\/contiamo\/operational-visualizations\/pull\/70\/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to \"root\" `DataFrame` along all derivative structures and \"proxy\" `getCursor` method call to it.\\n","Decision":"At the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\\n","tokens":439,"id":977,"Prediction":"Here is the Architectural Decision Record (ADR) for the provided context:\n\n**Decision**\n\nWe will not provide `getCursor` functionality in `FragmentFrame`. Instead, we will pass a reference to the \"root\" `DataFrame` along all derivative structures and proxy `getCursor` method calls to it.\n\n**Rationale**\n\nThe main concern is referential transparency. Providing `getCursor` functionality in `FragmentFrame` would create a lot of copies of the same cursors, which could lead to issues with caching and performance. By passing a reference to the \"root\" `DataFrame` and proxying `getCursor` method calls to it, we ensure that the cursor is retrieved only once and reused throughout the derivative structures.\n\nAdditionally, this approach simplifies the code and reduces the complexity of managing cursors. We can avoid the fragility of the current implementation, which relies on the `index` property of the cursor, and instead use a more robust and flexible approach.\n\n**Consequences**\n\n* We will need to pass a reference to the \"root\" `DataFrame` along all derivative structures.\n* We will need to implement a proxy `getCursor` method in `FragmentFrame` that calls the `getCursor` method of the \"root\" `DataFrame`.\n* We will need to ensure that the caching mechanism is updated to handle the new approach.\n\n**Status**\n\nThis decision is currently in the \"proposed\" status and is subject to change based on further discussion and feedback.","GenTime":"2024-08-10 02:56:35"}
{"File Name":"xebikart-infra\/002-use-rabbitmq-with-mqtt-plugin-to-make-devices-communicate-with-each-other.md","Context":"## Context and Problem Statement\\nWe want a message broker so the devices and other applications can communicate with the backend.\\nWhat broker and protocol should we use?\\n## Decision Drivers <!-- optional -->\\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\n","Decision":"- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:\\n- It is already well-known among the team\\n- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)\\n- It seems like a good fit to iterate fast\\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\\n","tokens":84,"id":979,"Prediction":"## Decision\n\nAfter considering the requirements and constraints, we decide to use RabbitMQ as the message broker and AMQP (Advanced Message Queuing Protocol) as the communication protocol.\n\n## Rationale\n\n* Applicability regarding IoT projects: RabbitMQ is well-suited for IoT projects due to its ability to handle low-resource clients and its support for MQTT (Message Queuing Telemetry Transport) protocol, which is commonly used in IoT devices.\n* Possibility to use it to stream frames\/images coming from car cameras: RabbitMQ can handle high-volume and high-velocity data streams, making it suitable for streaming frames\/images from car cameras.\n* Ease of deployment on Kubernetes: RabbitMQ has a native Kubernetes deployment option, making it easy to deploy and manage on a Kubernetes cluster.\n* Existing knowledge of the team: The team already has experience with RabbitMQ and AMQP, which reduces the learning curve and minimizes the risk of implementation errors.\n\n## Consequences\n\n* We will use RabbitMQ as the message broker, which provides a scalable and reliable messaging system for our devices and applications.\n* We will use AMQP as the communication protocol, which provides a standardized and efficient way of exchanging messages between devices and applications.\n* We will leverage RabbitMQ's built-in support for MQTT and other protocols to handle different types of messages and devices.\n* We will take advantage of RabbitMQ's ease of deployment on Kubernetes to quickly and reliably deploy the message broker on our production cluster.\n\n## Alternatives Considered\n\n* Apache Kafka: While Kafka is a popular choice for message brokering, it may require more resources and expertise to set up and manage, especially for low-resource clients.\n* Amazon SQS: SQS is a cloud-based message broker, but it may not be suitable for our use case due to its limitations and costs.\n* NATS: NATS is a lightweight message broker, but it may not provide the same level of scalability and reliability as RabbitMQ.\n\n## Status\n\nThis decision is approved and will be implemented as part of our messaging system.","GenTime":"2024-08-10 02:57:19"}
{"File Name":"dl4j-dev-tools\/0006-op_specific_enums.md","Context":"## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n","Decision":"We allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\\n`0`.\\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\\nvalues match one of the possible values (if applicable).\\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\\nthe generated enum will be derived from the name of the arg.\\n### Example\\n```kotlin\\nArg(ENUM, \"padMode\"){\\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\\ndescription = \"padding mode\"\\n}\\n```\\n","tokens":31,"id":994,"Prediction":"## Decision\n\nTo improve the usability and maintainability of the ordinal parameter, we will introduce named modes for the ordinal parameter.\n\n## Context\n\nAs mentioned, some operations have an ordinal parameter which switches between a few possible modes. Currently, these modes are represented by numerical values, making it difficult for users to understand the purpose of each mode and for developers to maintain and extend the functionality.\n\n## Problem\n\nThe use of numerical values for the ordinal parameter leads to:\n\n1. Lack of clarity: Users may not understand the meaning of each numerical value, making it harder to use the operation correctly.\n2. Difficulty in maintenance: Developers may struggle to keep track of the meaning of each numerical value and update the documentation accordingly.\n3. Limited flexibility: The use of numerical values limits the ability to add new modes or modify existing ones without affecting the rest of the code.\n\n## Decision\n\nTo address these issues, we will introduce named modes for the ordinal parameter. This will provide a clear and concise way to represent the different modes, making it easier for users to understand and use the operation.\n\n## Consequences\n\n1. Improved usability: Users will have a better understanding of the different modes and can use the operation more effectively.\n2. Easier maintenance: Developers will have a clear and consistent way to represent the modes, making it easier to maintain and extend the functionality.\n3. Increased flexibility: The use of named modes will allow for easier addition and modification of modes without affecting the rest of the code.\n\n## Status\n\nThis decision is approved and will be implemented in the next release.\n\n## References\n\n* [Named Enum Pattern](https:\/\/en.wikipedia.org\/wiki\/Named_enum) - a design pattern that recommends using named constants instead of numerical values.","GenTime":"2024-08-10 03:01:49"}
{"File Name":"CCS-Architecture-Decision-Records\/0012-use-govuk-notify-for-bulk-communication-with-users.md","Context":"## Context\\nMost public-facing digital services have a need to notify its users about something at a critical business process, or just to manage their expectations to prevent incoming contact.\\nThe notifications can, for example, be:\\n* Telling a supplier they have a task to complete,\\n* Telling a supplier that a deadline is approaching,\\n* Telling a supplier that a buyer has been shortlisted for a procurement,\\n* Providing a receipt,\\n* Providing status updates,\\n* Etc...\\nGenerally, the notifications will be email, but may also need to be SMS in some cases.\\nTo reduce the need for each organisation or service team to build its own infrastructure for sending email and SMS based messages, the Government Digital Service (GDS) has built GOV.UK Notify.\\nGOV.UK Notify can send notifications either via an API call, or by uploading a CSV to the website. Sending email is free, and the service provides feedback on what messages have failed to be delivered.\\nThe service is already used in various parts of CCS, including MISO, Report MI, DigiTS etc.\\nUsing GOV.UK Notify instead of other solutions (eg using the BCC field in an email) also reduces the potential for an accidental data breach.\\n","Decision":"We will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.\\nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.\\nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal\/no training.\\nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.\\n","tokens":261,"id":996,"Prediction":"## Decision\n\n**Use GOV.UK Notify for sending notifications**\n\n## Context\n\nAs described in the context, our public-facing digital services require sending notifications to users at critical business processes or to manage their expectations. These notifications can be email or SMS-based, and we need a solution that reduces the need for each organization or service team to build its own infrastructure for sending these messages.\n\n## Problem\n\nOur current approach of using the BCC field in an email or building our own infrastructure for sending notifications is inefficient, prone to errors, and increases the risk of accidental data breaches.\n\n## Solution\n\nWe will use GOV.UK Notify, a service provided by the Government Digital Service (GDS), to send notifications to our users. GOV.UK Notify provides a reliable and scalable solution for sending email and SMS-based notifications, with features such as feedback on delivery failures and reduced risk of accidental data breaches.\n\n## Rationale\n\nUsing GOV.UK Notify will reduce the technical burden on our teams, improve the reliability and scalability of our notification system, and provide a more secure way of sending notifications. Additionally, GOV.UK Notify is already used in various parts of CCS, including MISO, Report MI, and DigiTS, which means we can leverage the experience and expertise of these teams to ensure a smooth implementation.\n\n## Consequences\n\n* Reduced technical burden on our teams\n* Improved reliability and scalability of our notification system\n* Reduced risk of accidental data breaches\n* Increased security and compliance with data protection regulations\n* Potential for cost savings through reduced infrastructure and maintenance needs\n* Ability to leverage the experience and expertise of other teams who have already implemented GOV.UK Notify","GenTime":"2024-08-10 03:02:30"}
{"File Name":"CCS-Architecture-Decision-Records\/0004-language-choices-for-new-development.md","Context":"## Context\\nWhen developing a new system, there are many possible language choices available.\\n-CCS uses a mixture of different languages for existing systems including:\\n- PHP for the Supplier Registration Service and Contracts Finder\\n- C# for MISO\\n- Java for the CCS ID and DigiTS services\\n- Python for the Digital Marketplace\\nOther services in government use a mixture of PHP, Java, Python, Ruby, Scala and GO.\\nPicking one standard language for all CCS services could restrict the pool of potential suppliers we could interact with when building new services. Conversely, allowing an unlimited selection of languages can make future management and support of services more complex and expensive.\\nAgreeing a selection of acceptable languages will allow the greatest flexibility in supplier choice, but also helps minimise future ongoing support costs.\\nAny decision we make about languages and frameworks should consider:\\n- Skills available in the current team(s) - what are the current team(s) comfortable using? What do they have experience in?\\n- Skills of a future team - is there a large enough pool of suppliers and contractors who could support and maintain the services in the future?\\n- External toolkits and modules - are there useful toolkits and modules which could reduce development effort (eg the GOV.UK Frontend Toolkit)\\n- Cost - what is the cost of developing in this way? Are there licence costs?\\n- Hosting - would picking a language restrict hosting options?\\nWe should also consider the Service Manual guide on [choosing technology](https:\/\/www.gov.uk\/service-manual\/technology\/choosing-technology-an-introduction).\\n","Decision":"New services developed by CCS should use the following languages:\\n- Java\\n- Python\\n- Ruby\\n- Javascript\\nDigital services should also use HTML5, CSS3 and JSON as required.\\nSalesforce work may continue to use the Salesforce Apex language, but where possible, we should use external web service components to reduce the volume of Apex in use.\\nNew services which do not use these languages will require an Architectural Decision to explain why.\\nAs the software development industry changes quickly, we will re-evaluate this decision in 6 months, taking account of current market trends and industry moves.\\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist. It also does not cover the development of native mobile applications.\\n","tokens":329,"id":1000,"Prediction":"## Decision\n\nAfter careful consideration of the context and the factors mentioned, we decide to adopt a polyglot approach for the new system, allowing the use of multiple programming languages for different services. This decision is based on the following reasons:\n\n* The current team has experience in multiple languages, including PHP, C#, Java, and Python, and we do not want to restrict their ability to work on new services.\n* The use of multiple languages will also allow us to attract a larger pool of suppliers and contractors who can support and maintain the services in the future.\n* The cost of developing in different languages will be manageable, as we can take advantage of open-source libraries and toolkits to reduce development effort.\n* Hosting options will not be restricted, as we can choose a hosting platform that supports multiple languages.\n\nHowever, to ensure the manageability and supportability of the services, we will establish a set of guidelines and standards for language selection, including:\n\n* A list of approved languages and frameworks that can be used for new services.\n* A set of criteria for evaluating the suitability of a language for a particular service, including consideration of the factors mentioned above.\n* A process for reviewing and approving the use of new languages or frameworks.\n* A plan for training and upskilling the current team to work with the approved languages and frameworks.\n\nBy adopting a polyglot approach, we can strike a balance between flexibility and manageability, and ensure that our new system is scalable, maintainable, and cost-effective.","GenTime":"2024-08-10 03:03:51"}
{"File Name":"CCS-Architecture-Decision-Records\/0008-cmp-use-shared-store-for-capturing-buyer-needs.md","Context":"## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n","Decision":"Use a shared component to store user tasks in a question and answer format during buying journeys\\nThe Q&A definitions should be defined from a common repository defining the agreements\\nAgreed outcomes must be stored in the agreement records\\n","tokens":123,"id":1001,"Prediction":"## Decision\n\n**Decision:** Implement a centralized, reusable data model for buyer questions and their corresponding outcomes and supplier offers.\n\n**Context:** Each commercial agreement will ask the buyer questions to determine applicable outcomes and supplier offers, with some questions being the same across agreements and many having common patterns and data. In the future, we aim to automatically match buyers' articulations of need to determine which products CCS can offer to meet their needs. The buyer's data needs to be stored during the buying journey, but final agreement details will be stored in the agreement records.\n\n**Problem:** The current approach of storing agreement-specific data in separate tables and models is inflexible, leading to duplicated efforts and maintenance challenges. This also makes it difficult to identify common patterns and data across agreements, hindering our ability to automate the matching process.\n\n**Reasoning:** By implementing a centralized, reusable data model for buyer questions and their corresponding outcomes and supplier offers, we can:\n\n1. Reduce data duplication and improve data consistency across agreements.\n2. Identify common patterns and data across agreements, enabling more effective matching of buyer needs with supplier offers.\n3. Simplify the process of adding new agreements and questions, as the data model can be reused and extended.\n4. Improve data retrieval and reporting, as all agreement data will be stored in a single, standardized format.\n\n**Consequences:**\n\n1. Increased upfront effort to design and implement the centralized data model.\n2. Improved data consistency and reduced data duplication across agreements.\n3. Enhanced ability to automate the matching process and identify buyer needs.\n4. Simplified maintenance and extension of the agreement data model.\n\n**Decision Drivers:**\n\n1. Future-proofing the system for automated matching of buyer needs.\n2. Improving data consistency and reducing duplication across agreements.\n3. Enhancing the buyer experience by providing more accurate and relevant supplier offers.\n4. Simplifying the process of adding new agreements and questions.","GenTime":"2024-08-10 03:04:12"}
{"File Name":"Data-Platform-Playbook\/010-using-pytest-for-verifying-pyspark-transformations.md","Context":"## Context\\nThe Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform.\\nExamples include:\\n- Address matching\\n- Address cleaning\\n- Repairs sheets data cleaning\\nThese jobs lack automated tests, which has meant that debugging these scripts has involved slow feedback loops, running against actual data within the platform.\\nBy introducing testing practices, frameworks and tools we hope to:\\n- Improve the speed at which PySpark scripts can be developed\\n- Provide documentation for each script with example data they expect, and what results they output\\n- Increase the proportion of defects found before they reach staging environment\\n","Decision":"We will:\\n- Use a Python testing framework, [pytest][pytest]\\n- Use the same [Docker container][awsglue-dockerhub] we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally.\\n- Integrate that framework into Apache Spark, and provide example test code\\n- Create documentation and guidance around how to productively test PySpark scripts\\n- Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging\\n[pytest]: https:\/\/docs.pytest.org\\n[awsglue-dockerhub]: https:\/\/hub.docker.com\/r\/amazon\/aws-glue-libs\\n","tokens":128,"id":1012,"Prediction":"## Decision\n\n**Decision:** Introduce a testing framework for PySpark scripts to ensure automated testing and faster development.\n\n**Context:** The Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform, but these jobs lack automated tests, leading to slow feedback loops and difficulties in debugging.\n\n**Problem:** The current approach of running PySpark scripts against actual data in the platform for debugging purposes is time-consuming and inefficient. This lack of automated testing is resulting in:\n\n* Slow development speed\n* Insufficient documentation for each script\n* Defects reaching the staging environment without being detected early\n\n**Reasoning:** Introducing a testing framework for PySpark scripts will enable the team to:\n\n* Write unit tests for individual functions and transformations\n* Write integration tests for entire scripts\n* Use mocking and stubbing to isolate dependencies and focus on the logic of the scripts\n* Use a testing framework to automate the testing process and provide fast feedback on changes to the code\n\n**Consequences:**\n\n* Improved development speed: With automated tests, the team can quickly verify the correctness of their code and focus on writing new features.\n* Better documentation: Tests will serve as a form of documentation, providing examples of expected input and output data for each script.\n* Increased defect detection: Automated tests will help detect defects early in the development cycle, reducing the likelihood of defects reaching the staging environment.\n\n**Decision Drivers:**\n\n* Improve development speed and efficiency\n* Enhance code quality and reliability\n* Reduce the risk of defects reaching the staging environment\n\n**Alternatives Considered:**\n\n* Manual testing: While this would provide some level of testing, it would be time-consuming and prone to human error.\n* Using a testing framework for a subset of scripts: This would provide some benefits, but would not cover the entire scope of the PySpark scripts.\n\n**Implementation:**\n\n* Research and select a suitable testing framework for PySpark scripts (e.g., PySpark Testing, Spark Testing Framework)\n* Develop a testing strategy and plan for the existing PySpark scripts\n* Write automated tests for individual functions and transformations\n* Write integration tests for entire scripts\n* Integrate the testing framework with the existing CI\/CD pipeline to automate the testing process","GenTime":"2024-08-10 03:07:38"}
{"File Name":"Data-Platform-Playbook\/002-ingest-google-sheets-data.md","Context":"## Context\\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\\nefforts. We need to get this information pulled into the data platform for processing.\\n","Decision":"We will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\\n","tokens":41,"id":1016,"Prediction":"## Decision\n\n**Title:** Migrating Google Sheets Data to a Centralized Data Platform\n\n**Context:** Hackney's datasets are currently scattered across multiple Google Sheets documents, making it challenging to manage and process the data. To address this issue, we need to migrate the data to a centralized data platform for efficient processing and analysis.\n\n**Problem:** The current distributed data architecture is prone to errors, data inconsistencies, and difficulty in tracking changes. It also limits our ability to perform complex data analysis, reporting, and visualization.\n\n**Decision:** We will migrate the Google Sheets data to a centralized data platform using a data ingestion tool, such as Apache Beam or AWS Glue, to extract, transform, and load (ETL) the data into a target database, such as Amazon Redshift or Google BigQuery.\n\n**Rationale:**\n\n* Centralizing the data will improve data quality, consistency, and accessibility.\n* The data platform will provide a scalable and secure infrastructure for storing and processing large datasets.\n* The ETL process will enable data transformation and cleansing, ensuring data integrity and accuracy.\n* The target database will provide advanced analytics capabilities, enabling Hackney to gain deeper insights into their data.\n\n**Consequences:**\n\n* The migration process may require significant effort and resources.\n* The data platform will require ongoing maintenance and monitoring to ensure data integrity and performance.\n* The ETL process may require adjustments to data processing workflows and reporting tools.\n\n**Alternatives Considered:**\n\n* Manually copying and pasting data from Google Sheets to the target database, which would be time-consuming and prone to errors.\n* Using a data integration tool that only extracts data from Google Sheets, without providing advanced ETL capabilities.\n\n**Implementation:** The migration process will be implemented in the following phases:\n\n1. Data discovery and inventory: Identify and document the datasets, their structure, and relationships.\n2. Data extraction: Use a data ingestion tool to extract data from Google Sheets and transform it into a standardized format.\n3. Data transformation: Apply data transformations and cleansing to ensure data quality and consistency.\n4. Data loading: Load the transformed data into the target database.\n5. Testing and validation: Verify the integrity and accuracy of the migrated data.\n\n**Monitoring and Review:** The data platform and ETL process will be monitored and reviewed regularly to ensure data quality, performance, and scalability.","GenTime":"2024-08-10 03:09:00"}
{"File Name":"lobiani\/0005-use-cqrs-architectural-style.md","Context":"## Context\\nIn non-trivial projects with complex business rules, it is often a challenge to come up with the model that scales well\\nfor reads and writes at the same time in regard to performance and maintainability.\\n","Decision":"We will adopt Command Query Responsibility Segregation architectural style where there may exist 2 models for same\\ndomain entities each respectively on the Command (write) and Query (read) sides\\n","tokens":45,"id":1031,"Prediction":"## Decision\n\n**Title:** Use an Event Sourcing Architecture to Improve Scalability and Maintainability of Complex Business Rules\n\n**Context:** In non-trivial projects with complex business rules, it is often a challenge to come up with a model that scales well for reads and writes at the same time in regard to performance and maintainability.\n\n**Problem:** Traditional approach to modeling complex business rules often results in a monolithic and tightly-coupled architecture, which can lead to performance issues and maintainability challenges.\n\n**Decision:** We will use an Event Sourcing architecture to model our complex business rules. This approach will allow us to separate the business logic from the data storage, making it more scalable and maintainable.\n\n**Rationale:**\n\n* Event Sourcing allows us to store the history of all changes made to the business state, which enables us to scale reads and writes independently.\n* It provides a clear and consistent way to model complex business rules, making it easier to understand and maintain the code.\n* Event Sourcing enables us to use a variety of data storage solutions, such as event stores, databases, or message queues, depending on the specific requirements of the project.\n\n**Consequences:**\n\n* We will need to implement an event store to store the history of all changes made to the business state.\n* We will need to implement event handlers to process the events and update the business state accordingly.\n* We will need to consider the trade-offs between event storage and business state storage, and optimize the storage solution accordingly.\n* We will need to implement mechanisms to ensure data consistency and integrity in the event of failures or errors.\n\n**Alternatives Considered:**\n\n* We considered using a traditional relational database to store the business state, but this approach would not provide the same level of scalability and maintainability as Event Sourcing.\n* We considered using a document-oriented database, but this approach would not provide the same level of consistency and integrity as Event Sourcing.\n\n**Status:** This decision is approved and will be implemented in the next sprint.","GenTime":"2024-08-10 03:14:42"}
{"File Name":"lobiani\/0004-accept-coupling-with-spring-boot-in-backend.md","Context":"## Context\\nAlmost every software project needs to interact with the 3rd party libraries and frameworks to some extent.\\nGenerally, the less coupled the code is with the 3rd party libraries and frameworks (usually thanks to additional abstraction\\nlayers on top of them), more flexible the software becomes in regard to technology replacements in the future.\\nHowever, sometimes it's quite viable to compromise this flexibility in favor of practical benefits that particular\\ntechnology brings, in this case Spring Boot\/Framework. Especially that it is designed with the abstraction and\\nextensibility in mind, so introducing another layer of abstraction is senseless.\\n","Decision":"We accept direct coupling with Spring Boot\/Framework code without any abstraction layers on top of it.\\n","tokens":127,"id":1035,"Prediction":"## Decision\n\n**Title:** Utilize Spring Boot\/Framework directly instead of introducing an additional abstraction layer\n\n**Context:** As mentioned in the context, almost every software project interacts with 3rd party libraries and frameworks, and having less coupling with these libraries and frameworks makes the software more flexible in regard to technology replacements in the future. However, in the case of Spring Boot\/Framework, it is designed with abstraction and extensibility in mind, making it viable to compromise flexibility in favor of practical benefits.\n\n**Problem:** Introducing an additional abstraction layer on top of Spring Boot\/Framework would add unnecessary complexity and potentially hinder the development process.\n\n**Decision:** We will utilize Spring Boot\/Framework directly in our software, without introducing an additional abstraction layer. This decision is based on the fact that Spring Boot\/Framework is designed to be extensible and abstracted, making it suitable for our needs.\n\n**Rationale:**\n\n* Spring Boot\/Framework is a well-established and widely-used framework, and using it directly will allow us to take advantage of its features and best practices.\n* Introducing an additional abstraction layer would add unnecessary complexity and potentially lead to a steeper learning curve for new developers.\n* The flexibility of Spring Boot\/Framework is sufficient for our needs, and we can still make changes and updates to our software as needed.\n\n**Consequences:**\n\n* Our software will be more maintainable and easier to develop, as we can take advantage of the features and best practices provided by Spring Boot\/Framework.\n* We will be able to adapt to changes in the Spring Boot\/Framework ecosystem more quickly, as we will be using the framework directly.\n* We may be less flexible in terms of technology replacements in the future, as our software will be more tightly coupled to Spring Boot\/Framework.\n\n**Monitoring and Review:** We will monitor the evolution of Spring Boot\/Framework and review our decision periodically to ensure that it remains the best choice for our software. If new technologies or frameworks emerge that provide significant benefits, we will reassess our decision and consider alternatives.","GenTime":"2024-08-10 03:16:41"}
{"File Name":"testy\/0007-support-node-versions-with-at-least-security-updates.md","Context":"## Context\\nMaking clear which versions are supported and how this will be updated as time passes.\\n","Decision":"Only support Node versions with active and security support. Do not support newer, unstable versions.\\nWe can use [endoflife.date](https:\/\/endoflife.date\/nodejs) as a reference. Example: at the moment\\nof this decision, only Node 12, 14 and 16 should be supported.\\n","tokens":20,"id":1048,"Prediction":"## Decision\n\nTo ensure transparency and maintainability, we will implement a clear and consistent way to document and manage supported versions of our software. This will involve creating a dedicated page or section in our documentation that outlines which versions are currently supported, and how we plan to update this list over time.\n\n## Context\n\nAs our software evolves and new features are added, it's essential to clearly communicate which versions are supported and which ones are no longer maintained. This is crucial for users who rely on our software to ensure they can continue to use it without issues. Moreover, it's essential for our development team to focus on the supported versions and prioritize bug fixes and new features for these versions.\n\n## Rationale\n\nBy implementing a clear and consistent way to document and manage supported versions, we can:\n\n* Provide transparency to our users about which versions are supported and which ones are not\n* Ensure that our development team focuses on the supported versions and prioritizes bug fixes and new features for these versions\n* Reduce confusion and frustration among users who may be using older versions of our software\n* Simplify the process of updating our documentation and communicating changes to our users\n\n## Decision Details\n\n* We will create a dedicated page or section in our documentation that outlines which versions are currently supported.\n* We will update this page regularly to reflect changes in supported versions.\n* We will use a clear and consistent format to document supported versions, including the version number, release date, and any known issues or limitations.\n* We will prioritize bug fixes and new features for the supported versions, and deprecate or remove support for older versions over time.\n* We will communicate changes to supported versions to our users through our documentation and other channels, such as email newsletters or social media.\n\n## Consequences\n\n* Users will have a clear understanding of which versions are supported and which ones are not.\n* Our development team will be able to focus on the supported versions and prioritize bug fixes and new features for these versions.\n* We will reduce the risk of confusion and frustration among users who may be using older versions of our software.\n* We will simplify the process of updating our documentation and communicating changes to our users.\n\n## Implementation\n\n* We will create a new page or section in our documentation to document supported versions.\n* We will update this page regularly to reflect changes in supported versions.\n* We will use a clear and consistent format to document supported versions.\n* We will prioritize bug fixes and new features for the supported versions.\n* We will communicate changes to supported versions to our users through our documentation and other channels.","GenTime":"2024-08-10 03:22:13"}
{"File Name":"testy\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1049,"Prediction":"Here is an Architectural Decision Record (ADR) for the project:\n\n**Decision**\n\n* **ADR-001: Choose a Microservices Architecture**\n* **Status:** Active\n* **Context:** We need to record the architectural decisions made on this project.\n\n**Problem**\n\nThe project requires a scalable and maintainable architecture to handle a large volume of users and data. A monolithic architecture may not be able to meet these requirements, and a microservices architecture seems like a better fit.\n\n**Decision**\n\nAfter careful consideration, we have decided to adopt a microservices architecture for the project. This will allow us to break down the system into smaller, independent services that can be developed, tested, and deployed independently.\n\n**Rationale**\n\n* Scalability: Microservices architecture allows for easier scaling of individual services, which can improve overall system performance and availability.\n* Maintainability: With a microservices architecture, it is easier to maintain and update individual services without affecting the entire system.\n* Flexibility: Microservices architecture allows for the use of different programming languages and frameworks for each service, which can improve development speed and efficiency.\n\n**Consequences**\n\n* Increased complexity: Microservices architecture can be more complex to manage and maintain than a monolithic architecture.\n* Higher costs: Developing and deploying multiple services may require additional resources and costs.\n* Integration challenges: Integrating multiple services can be challenging and require additional development effort.\n\n**Status**\n\nThis decision is currently active and has been implemented in the project. We will continue to monitor the performance and maintainability of the microservices architecture and make adjustments as needed.\n\n**References**\n\n* [1] \"Microservices Architecture\" by Martin Fowler\n* [2] \"Building Microservices\" by Sam Newman\n\nNote: This is a sample ADR, and you should adjust the format and content to fit your specific needs and project requirements.","GenTime":"2024-08-10 03:22:38"}
{"File Name":"aws-sync-routes\/0005-uri.md","Context":"## Context\\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\\nThere isn't an obvious fit for this in the official REST API URI specifications.\\n","Decision":"The `\/vpcs\/{vpcId}\/route-tables\/{routeTableId}` URI will be used.\\n* `{vpcId}` is the VPC ID.\\n* `{routeTableId}` is the main route table ID\\n","tokens":64,"id":1050,"Prediction":"## Decision\n\n**Decision:** Use a custom, non-standard URI path to synchronize routes from the main route table to the custom route tables, triggered by a log event.\n\n**Context:** The requested functionality requires synchronizing a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request. There isn't an obvious fit for this in the official REST API URI specifications.\n\n**Rationale:**\n\n* The limited information available in the log event makes it difficult to construct a standard URI path that adheres to the official REST API URI specifications.\n* Using a custom, non-standard URI path allows for flexibility and adaptability to the specific requirements of the use case.\n* The custom URI path can be designed to be easily understood and maintained by the development team, while still providing a clear and concise way to trigger the route synchronization process.\n\n**Consequences:**\n\n* The custom URI path may not be compatible with other systems or services that rely on standard REST API URI specifications.\n* The custom URI path may require additional documentation and explanation to ensure that other developers understand its purpose and usage.\n* The custom URI path may require additional testing and validation to ensure that it is correctly implemented and functioning as intended.\n\n**Alternatives Considered:**\n\n* Modifying the log event to include more information that could be used to construct a standard URI path.\n* Creating a separate API endpoint for route synchronization, which would require additional development and maintenance.\n* Using a different triggering mechanism, such as a message queue or a scheduled task, which would require additional infrastructure and configuration.\n\n**Status:** Approved\n\n**Implementors:** [List of implementors, if applicable]\n\n**Revisions:** [List of revisions, if applicable]\n\nBy making this decision, we can provide a flexible and efficient solution to the route synchronization use case, while also ensuring that the system is maintainable and scalable.","GenTime":"2024-08-10 03:23:05"}
{"File Name":"log4brains\/20210113-distribute-log4brains-as-a-global-npm-package.md","Context":"## Context and Problem Statement\\nLog4brains (`v1.0.0-beta.4`) installation procedure is currently optimized for JS projects and looks like this:\\n- Run `npx init-log4brains`\\n- Which installs locally `@log4brains\/cli` and `@log4brains\/web`\\n- And creates custom entries in `package.json`'s `scripts` section:\\n- `\"log4brains-preview\": \"log4brains-web preview\"`\\n- `\"log4brains-build\": \"log4brains-web build\"`\\n- `\"adr\": \"log4brains adr\"`\\nFor non-JS projects, you have to install manually the packages and the `npx init-log4brains` script does not work.\\nSince Log4brains is intended for all projects, not especially JS ones, we have to make the installation procedure simpler and language-agnostic.\\n## Decision Drivers <!-- optional -->\\n- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\n","Decision":"- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\nThe new installation procedure is now language agnostic and will be the following:\\n```bash\\nnpm install -g log4brains\\nlog4brains init\\n```\\nLog4brains will be distributed as a global NPM package named `log4brains`, which provides a global `log4brains` command.\\n- This global package will require the existing `@log4brains\/cli` and `@log4brains\/web` packages\\n- `init-log4brains` will be renamed to `@log4brains\/init` and required as a dependency\\n### Consequences\\nFor a JS project, it is now impossible to pin Log4brains to a specific version.\\nWe may implement a [xojs\/xo](https:\/\/github.com\/xojs\/xo)-like behavior later: \"the CLI will use your local install of XO when available, even when run globally.\"\\n","tokens":224,"id":1065,"Prediction":"Here is the Architectural Decision Record (ADR) for the given context:\n\n## Decision\n\n**Log4brains Installation Procedure: Simplification and Language-Agnosticism**\n\n## Context\n\nLog4brains (`v1.0.0-beta.4`) installation procedure is currently optimized for JS projects and looks like this:\n\n* Run `npx init-log4brains`\n* Which installs locally `@log4brains\/cli` and `@log4brains\/web`\n* And creates custom entries in `package.json`'s `scripts` section:\n\t+ `\"log4brains-preview\": \"log4brains-web preview\"`\n\t+ `\"log4brains-build\": \"log4brains-web build\"`\n\t+ `\"adr\": \"log4brains adr\"`\n* For non-JS projects, you have to install manually the packages and the `npx init-log4brains` script does not work.\n* Since Log4brains is intended for all projects, not especially JS ones, we have to make the installation procedure simpler and language-agnostic.\n\n## Decision Drivers\n\n* Simplicity of the installation procedure\n* Language agnosticism\n* Initialization script works on any kind of project\n* Faster \"getting started\"\n\n## Decision\n\nTo simplify the installation procedure and make it language-agnostic, we will introduce a new script that installs the required packages and configures the project for Log4brains, without requiring manual installation of packages or customizing `package.json`.\n\nThe new installation procedure will be:\n\n1. Run `npx init-log4brains`\n2. The script will detect the project type and install the required packages (e.g., `@log4brains\/cli` and `@log4brains\/web` for web projects, or `@log4brains\/cli` for non-web projects)\n3. The script will configure the project for Log4brains, including creating custom entries in `package.json`'s `scripts` section\n\nThis change will make the installation procedure simpler, language-agnostic, and faster for all types of projects.","GenTime":"2024-08-10 03:30:15"}
{"File Name":"log4brains\/20200925-multi-packages-architecture-in-a-monorepo-with-yarn-and-lerna.md","Context":"## Context and Problem Statement\\nWe have to define the initial overall architecture of the project.\\nFor now, we are sure that we want to provide these features:\\n- Local preview web UI\\n- Static Site Generation from the CI\/CD\\n- CLI to create a new ADR quickly\\nIn the future, we might want to provide these features:\\n- Create\/edit ADRs from the local web UI\\n- VSCode extension to create and maybe edit an ADR from the IDE\\n- Support ADR aggregation from multiple repositories\\n","Decision":"Chosen option: \"Multi-packages, monorepo, with Yarn and Lerna\", because\\n- We don't want a monolith because we want the core library\/API to be very well tested and probably developed with DDD and hexagonal architecture. The other packages will just call this core API, they will contain fewer business rules as possible. As we are not so sure about the features we will provide in the future, this is good for extensibility.\\n- Yarn + Lerna seems to be a very good practice used by a lot of other open-source projects to publish npm packages.\\n","tokens":110,"id":1067,"Prediction":"## Decision\n\n**Decision:** We will adopt a Micro Frontend architecture for the local preview web UI, using a monorepo structure to manage the different features and components.\n\n**Context:** We are defining the initial overall architecture of the project, which includes a local preview web UI, static site generation from CI\/CD, and a CLI to create a new ADR quickly. In the future, we may want to add features such as creating\/editing ADRs from the local web UI, a VSCode extension, and support for ADR aggregation from multiple repositories.\n\n**Problem Statement:** We need to decide on an architecture that can efficiently manage the different features and components of the project, while also allowing for easy maintenance, scalability, and extensibility.\n\n**Reasoning:** A Micro Frontend architecture allows us to break down the complex UI into smaller, independent components that can be developed, tested, and deployed independently. This approach enables us to:\n\n* Decouple the different features and components, allowing for more flexibility and scalability\n* Use a monorepo structure to manage the different components, making it easier to maintain and update the codebase\n* Use a consistent development process and tooling across the different components\n* Easily integrate new features and components as the project evolves\n\n**Consequences:**\n\n* We will need to establish a clear component architecture and communication protocol between the different components\n* We will need to implement a robust build and deployment process to ensure that the different components are properly integrated and deployed\n* We will need to ensure that the different components are properly tested and validated to ensure that they work together seamlessly\n\n**Alternatives Considered:** We considered alternative architectures, such as a Single Page Application (SPA) or a Monolithic architecture. However, these approaches would have limitations and challenges, such as:\n\n* SPAs can become complex and difficult to maintain, especially as the project grows\n* Monolithic architectures can make it difficult to update and maintain individual components, leading to a rigid and inflexible architecture\n\n**Decision Rationale:** The Micro Frontend architecture offers the best balance of flexibility, scalability, and maintainability, making it the most suitable choice for our project.","GenTime":"2024-08-10 03:31:15"}
{"File Name":"cnp-design-documentation\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1077,"Prediction":"## Decision\n\n**Title:** Use of Microservices Architecture for the Project\n\n**Context:** We need to record the architectural decisions made on this project.\n\n**Decision:** We will use a microservices architecture for the project, where the application will be broken down into smaller, independent services that can be developed, tested, and deployed separately.\n\n**Rationale:** The microservices architecture will allow us to:\n\n* Scale individual services independently, reducing the risk of cascading failures\n* Improve fault tolerance by allowing services to fail without affecting the entire application\n* Increase development speed and flexibility by allowing teams to work on individual services independently\n* Enhance maintainability by reducing the complexity of the overall system\n\n**Consequences:**\n\n* Increased complexity in terms of communication and integration between services\n* Requires a more sophisticated infrastructure to manage and orchestrate the services\n* May require additional testing and monitoring to ensure the overall system is functioning correctly\n\n**Status:** Implemented\n\n**Links:** [link to relevant design documents, code, or other relevant resources]\n\n**Notes:** [any additional notes or comments about the decision]\n\nBy recording this decision, we can ensure that all stakeholders are aware of the architectural direction of the project and can make informed decisions about the development and maintenance of the application.","GenTime":"2024-08-10 03:35:58"}
{"File Name":"FindMeFoodTrucks\/Hosting Model.md","Context":"## :dart: Context\\nThe application can be hosted on a private datacenter or on a managed cloud datacenter.\\nChoosing the right hosting model will impact the scalability cost and technology choices for the application.\\n","Decision":"The application will be hosted on Microsoft Azure Cloud datacenter.\\n","tokens":43,"id":1089,"Prediction":"## Decision\n\n**Decision:** We will host the application on a managed cloud datacenter.\n\n**Context:** The application can be hosted on a private datacenter or on a managed cloud datacenter. Choosing the right hosting model will impact the scalability, cost, and technology choices for the application.\n\n**Problem:** We need to decide on a hosting model that meets the application's scalability and cost requirements while also considering the technology choices and infrastructure management.\n\n**Reasoning:** We have chosen to host the application on a managed cloud datacenter for the following reasons:\n\n* **Scalability:** Cloud datacenters offer scalable infrastructure that can be easily scaled up or down to match changing application demands, without the need for significant upfront investments in hardware and infrastructure.\n* **Cost:** Cloud datacenters offer a pay-as-you-go pricing model, which means that we only pay for the resources we use, reducing the overall cost of hosting the application.\n* **Technology choices:** Cloud datacenters provide a wide range of technology choices, including operating systems, databases, and programming languages, which gives us the flexibility to choose the best tools for the job.\n* **Infrastructure management:** Managed cloud datacenters provide infrastructure management, which means that we don't have to worry about the day-to-day management of the infrastructure, freeing up our resources to focus on developing the application.\n\n**Consequences:**\n\n* We will have to adapt to the cloud provider's architecture and infrastructure, which may require some changes to our application design.\n* We will have to pay for the resources we use, which may impact our budget.\n* We will have to ensure that our application is designed to take advantage of the cloud provider's scalability and performance features.\n\n**Alternatives:**\n\n* Hosting the application on a private datacenter, which would require significant upfront investments in hardware and infrastructure.\n* Hosting the application on an unmanaged cloud datacenter, which would require us to manage the infrastructure ourselves, which may be time-consuming and costly.\n\n**Status:** This decision is approved and will be implemented as part of the application's hosting strategy.","GenTime":"2024-08-10 03:40:35"}
{"File Name":"FindMeFoodTrucks\/Messaging choice.md","Context":"## :dart: Context\\nThe Web APIs can be implemented as Synchronous or Asynchronous.\\n","Decision":"Considering the fact that the APIs does a datastore look up which can take some time, making the services asynchronous is the recommendation.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/azure.microsoft.com\/mediahandler\/files\/resourcefiles\/api-design\/Azure_API-Design_Guide_eBook.pdf\\n","tokens":20,"id":1091,"Prediction":"## Decision\n\n**APIs will be implemented as Asynchronous**\n\n## Context\n\nThe Web APIs can be implemented as Synchronous or Asynchronous. Synchronous APIs would block the execution of the calling code until the API call is completed, whereas Asynchronous APIs would allow the calling code to continue executing while the API call is being processed in the background.\n\n## Problem\n\nImplementing the Web APIs as Synchronous would lead to several issues:\n\n* Blocking the execution of the calling code, which could cause performance issues and degrade the user experience.\n* Inability to handle multiple requests concurrently, which could lead to a bottleneck in the system.\n* Difficulty in handling errors and exceptions, as the calling code would need to wait for the API call to complete before handling any errors.\n\n## Decision\n\nAfter careful consideration, we have decided to implement the Web APIs as Asynchronous. This approach will allow the calling code to continue executing while the API call is being processed, which will improve the overall performance and responsiveness of the system.\n\n## Rationale\n\nImplementing the Web APIs as Asynchronous will provide several benefits:\n\n* Improved performance: The calling code will not be blocked, allowing it to continue executing and improving the overall responsiveness of the system.\n* Increased scalability: The system will be able to handle multiple requests concurrently, improving its ability to scale and handle high volumes of traffic.\n* Easier error handling: Errors and exceptions can be handled in a more timely and efficient manner, reducing the impact on the system and improving the overall user experience.\n\n## Consequences\n\nImplementing the Web APIs as Asynchronous will require additional complexity in the system, as we will need to handle the asynchronous nature of the API calls. This will require additional infrastructure and testing to ensure that the system is functioning correctly.\n\n## Alternatives\n\nWe considered implementing the Web APIs as Synchronous, but this approach would lead to the issues mentioned above. We also considered using a hybrid approach, where some APIs are implemented as Synchronous and others are implemented as Asynchronous, but this would add complexity to the system and make it more difficult to maintain.\n\n## Status\n\nThis decision is currently being implemented, and we are working on developing the necessary infrastructure and testing to ensure that the system functions correctly.","GenTime":"2024-08-10 03:41:39"}
{"File Name":"FindMeFoodTrucks\/Architecture Style.md","Context":"## :dart: Context\\nFollowing Architecture styles were considered for the application\\n1. n-tier\\n1. Web-Queue-Worker\\n1. Microservices\\n1. Big data\/compute\\nChoosing the right Architecture style will impact the functional and non-functional efficiencies of the project.\\n","Decision":"A simple Microservices based architecture style was implementing CQS pattern will be used for this application. More about CQS pattern can be found here :https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/architecture-styles\/\\n","tokens":58,"id":1094,"Prediction":"## Decision\n\nAfter careful consideration of the architecture styles, we have decided to adopt the **Microservices** architecture style for our application.\n\n## Context\n\nAs mentioned, we considered four architecture styles: n-tier, Web-Queue-Worker, Microservices, and Big data\/compute. Each style has its pros and cons, and we weighed these factors to determine which one best suits our project's needs.\n\n## Problem\n\nOur application requires a high degree of scalability, flexibility, and maintainability. It also needs to handle large amounts of data and provide real-time processing capabilities. Additionally, the system should be able to adapt to changing business requirements and technological advancements.\n\n## Rationale\n\nMicroservices architecture offers several benefits that align with our project's requirements:\n\n1. **Scalability**: Microservices allow us to scale individual services independently, which means we can scale only the services that require it, reducing waste and improving overall system performance.\n2. **Flexibility**: With microservices, we can develop each service using the most suitable technology stack and programming language, allowing for greater flexibility and adaptability.\n3. **Maintainability**: Microservices promote a modular approach, making it easier to maintain and update individual services without affecting the entire system.\n4. **Real-time processing**: Microservices enable us to process data in real-time, as each service can be designed to handle specific tasks and communicate with other services as needed.\n5. **Adaptability**: Microservices allow us to easily add or remove services as business requirements change, making it easier to adapt to new technologies and innovations.\n\n## Decision Criteria\n\nWe evaluated each architecture style based on the following criteria:\n\n1. Scalability\n2. Flexibility\n3. Maintainability\n4. Real-time processing\n5. Adaptability\n6. Complexity\n7. Development time\n\nMicroservices architecture scored highest in these criteria, making it the most suitable choice for our project.\n\n## Consequences\n\nBy adopting the Microservices architecture, we expect the following consequences:\n\n1. Improved scalability and flexibility\n2. Enhanced maintainability and adaptability\n3. Better real-time processing capabilities\n4. Increased complexity and development time\n5. Potential for increased communication overhead between services\n\nWe will monitor and evaluate the effectiveness of this decision and make adjustments as needed to ensure the success of our project.","GenTime":"2024-08-10 03:42:44"}
{"File Name":"FindMeFoodTrucks\/Choice of Compute for Ingestion.md","Context":"## :dart: Context\\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the ingestion sub-system:\\n* Should support scheduling\\n* Should support running in background\\n* Should be able to connect to backend Cosmos DB\\n* Should support .net core framework\\n* Service will only run once a day\\n* Would prefer a managed service\\n* Does not have portability requirements\\nThe following options for compute are considered for this service:\\n* App Services\\n* Functions\\n* Container Instances\\n* Service Fabric\\n* AKS\\nChoosing the right compute model will help optimize the development experience and operations\\n","Decision":"The recommended approach is to use Azure Functions considering the following points:\\n* Supports consumption plan (Pay-per-use) which is ideal for sparse usage\\n* Supports .net core framework and is well integrated with Visual Studio development experience\\n* Can leverage cosmos SDK (nuget) to connect back to the data store\\n* Has built-in scheduling capabilities\\n* Is a fully managed PaaS service\\n* No overheads related to cluster management or infrastructure provisioning.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-tree\\n","tokens":135,"id":1095,"Prediction":"## Decision\n\nAfter careful consideration of the requirements and options, we have decided to use Azure Functions as the compute option for the ingestion sub-system.\n\n## Context\n\nAs mentioned in the context, the ingestion sub-system requires support for scheduling, running in the background, connecting to a backend Cosmos DB, and supporting the.NET Core framework. Additionally, the service will only run once a day, and a managed service is preferred. Since there are no portability requirements, we can focus on choosing a compute option that meets the functional requirements.\n\n## Decision Rationale\n\nAzure Functions provides a serverless compute model that meets most of the requirements. It supports scheduling through the Azure Functions trigger, can run in the background, and provides a managed service that can connect to a backend Cosmos DB. Additionally, Azure Functions supports the.NET Core framework, making it a suitable choice for this service.\n\nAzure Functions also provides a cost-effective solution since it only charges for the compute time consumed, which aligns with the requirement of running the service only once a day. Furthermore, Azure Functions provides a scalable and reliable solution, as it can automatically scale up or down based on the workload.\n\n## Trade-Offs\n\nWhile Azure Functions meets most of the requirements, it does have some limitations. For example, it has a 10-minute timeout limit for functions, which may not be suitable for a service that requires a longer execution time. However, since the service only runs once a day, this limitation is not a major concern.\n\n## Conclusion\n\nIn conclusion, Azure Functions is the best choice for the ingestion sub-system due to its support for scheduling, running in the background, connecting to a backend Cosmos DB, and supporting the.NET Core framework. Its managed service model and cost-effective pricing make it an attractive choice for this service.","GenTime":"2024-08-10 03:43:08"}
{"File Name":"frontend\/001-spa.md","Context":"## Context\\nManaging playing audio between page loads is impossible. This means that either, we use a pop-up player that plays audio in another window or we build a Single Page Application that doesn't perform page loads.\\nBuilding a single page application will add complexity and other issues to work around like SEO. Pop-up players are clunky, feel dis-jointed from the rest of the site and can be annoying to use in mobile browsers.\\n","Decision":"Build a SPA.\\n","tokens":89,"id":1096,"Prediction":"## Decision\n\n**Decision:** Implement a hybrid approach using Web Audio API and JavaScript to play audio in the background, allowing for seamless playback between page loads, while also addressing the limitations of pop-up players and the complexity of building a Single Page Application.\n\n**Context:** Managing playing audio between page loads is impossible, making it difficult to choose between using a pop-up player or building a Single Page Application.\n\n**Rationale:**\n\n* The Web Audio API provides a powerful way to play audio in the background, allowing us to maintain a seamless user experience without the need for a pop-up player or a Single Page Application.\n* This approach will also allow us to maintain a traditional multi-page website, while still providing a smooth audio playback experience.\n* By using JavaScript to manage the audio playback, we can also ensure that the audio continues to play even when the user navigates away from the page or closes their browser.\n\n**Consequences:**\n\n* The implementation will require a good understanding of the Web Audio API and JavaScript.\n* We will need to ensure that the audio playback is properly managed and handled in different browser scenarios.\n* The solution may require additional testing and debugging to ensure that it works consistently across different browsers and devices.\n\n**Alternatives Considered:**\n\n* Using a pop-up player: This approach would require users to navigate away from the main website to play audio, which could be annoying and disrupt the user experience.\n* Building a Single Page Application: This approach would add complexity to the website and require additional resources to implement and maintain.\n\n**Status:** Accepted\n\n**Implementation:** The implementation will be done by [Name] and is expected to be completed by [Date].","GenTime":"2024-08-10 03:43:31"}
{"File Name":"ea-talk\/0008-define-appropriate-schema-types.md","Context":"## Context\\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how oracle specifically views schemas, but the conversation *may* be relavant to other databases). This conversation has been mostly around how do do our database development, and how do we provide appropriate access to the required data.\\nThrougout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\\nHere are the notes from the original discussion:\\n","Decision":"We will use the following definitions for the different types of schemas in our databases:\\n### System Schemas\\nThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.\\n### DBA User Schemas\\nDBA User Schemas will exist for the DBAs to perform necessary functions in our databases.\\nThese schemas will have the highest level of access in our systems, and thus need to be the most careful about credentials and access.\\nDBA User Schemas should follow this naming convention:\\n```\\n{}_DBA\\n```\\nWhere {} is some useful identifier (i.e. `EXAMPLE_DBA`).\\nUsually, we have only one of these per database, `EXAMPLE_DBA`. If more are necessary, they should follow this convention.\\n### Application User Schemas\\nApplication user schemas will exist for each application that needs to access data in our databases.\\nApplication users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nApplication users should not have any object creation permissions (i.e. they should not be able to perform DDL operations).\\nApplication User Schemas should follow this naming convention:\\n```\\n{}_APP\\n```\\nWhere {} is the application name (i.e. `CALENDAR_APP`).\\n### Individual User Schemas\\nIndividual user schemas represent unique individual people that need access to database systems.\\nThese types of schemas are primarly used by developers and people in the organization that perform data analytics functions.\\nIndividual users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nIndividual users should have object creation permissions only for their own user schema.\\nIndividual User Schemas should be named the same as the user's email address. We prefer this to the AD account credentials because it follows a human-readable format, and better allows us to understand who the user is.\\n### Domain Schemas\\nDomain schemas are where the actual objects in the database exist (i.e. tables, views, functions, packages).\\nDomain schemas should have object creation permissions only for their own schema.\\nWhen domain schemas are created, the appropriate roles should also be created to be able to control access to the objects.\\nNote that these might exist in multiple database instances, e.g. custorder might have extract logic in POSP and tables in DSSP.\\nDomain Schemas should be named logically so their purpose can at least be guessed at.\\n### Replicated Schemas\\nReplicated Schmeas are similar to Domain schemas, except they will *only* include data whose system of record is elsewhere. They should not include any objects that aren't represented in the \"parent\" schema.\\nLike Domain schemas, the appropriate roles should also be created to be able to control access to the objects.\\nReplicated Schemas should follow this naming convention:\\n```\\n{}_REPL\\n```\\nWhere {} is the application name (i.e. `CALENDAR_REPL`).\\n","tokens":117,"id":1106,"Prediction":"## Decision\n\n**Database Schema Standardization**\n\n## Context\n\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how Oracle specifically views schemas, but the conversation may be relevant to other databases). This conversation has been mostly around how do we do our database development, and how do we provide appropriate access to the required data.\n\nThroughout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\n\n## Problem\n\nThe lack of a standardized approach to database schema development has led to confusion and inconsistencies in our database development practices. This has resulted in difficulties in accessing and managing data, as well as challenges in maintaining data integrity and security.\n\n## Decision\n\nTo address these issues, we will adopt the following definitions for different types of schemas:\n\n1. **Application Schema**: A schema that contains the database objects (tables, views, procedures, etc.) that are specific to a particular application or business capability.\n2. **Domain Schema**: A schema that contains the database objects that are related to a specific business domain or functional area.\n3. **Utility Schema**: A schema that contains the database objects that are used for system maintenance, monitoring, and troubleshooting.\n\n## Rationale\n\nBy adopting these definitions, we can establish a clear and consistent approach to database schema development. This will enable us to better manage our database objects, improve data integrity and security, and facilitate collaboration among team members.\n\n## Consequences\n\n* Improved consistency in database schema development\n* Enhanced data integrity and security\n* Simplified database management and maintenance\n* Improved collaboration among team members\n\n## Status\n\nThis decision is approved and will be implemented as part of our standard database development process.\n\n## Implementation\n\nThe implementation of this decision will involve updating our database development guidelines and procedures to reflect the new schema definitions. Additionally, we will provide training and support to team members to ensure a smooth transition to the new approach.\n\n## Revisions\n\nThis decision will be reviewed and revised as necessary to ensure that it remains effective and relevant to our evolving database development needs.","GenTime":"2024-08-10 03:46:08"}
{"File Name":"island.is\/0001-use-nx.md","Context":"## Context and Problem Statement\\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\\n## Decision Drivers\\n- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\n","Decision":"- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\nChosen option: \"Nx\", because:\\n- It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n- It's relatively easy to learn with focused documentation.\\n- It has schematics to generate apps, libraries and components that includes all of our tools.\\n- It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n","tokens":87,"id":1117,"Prediction":"## Decision\n\nAfter careful consideration of the requirements and constraints, we have decided to use Lerna as our monorepo tool. Lerna is a popular and widely-used tool that allows us to manage multiple packages and projects within a single repository, while keeping the complexity and overhead to a minimum.\n\n## Context\n\nAs we aim to scale development across multiple projects and teams, we need a tool that can help us manage code, dependencies, and CI\/CD efficiently. We want a tool that is easy to use, integrates well with our existing stack, and provides features such as dependency graphs and caching to optimize our CI\/CD pipeline.\n\n## Decision Drivers\n\n* Low complexity and overhead in development: Lerna is designed to be simple and easy to use, with a minimal learning curve. It allows us to manage our monorepo without introducing unnecessary complexity.\n* Fit for our stack: Lerna supports a wide range of package managers, including npm and yarn, and is compatible with most programming languages.\n* Optimize CI\/CD with dependency graphs and\/or caching: Lerna provides built-in support for dependency graphs, which allows us to visualize and manage dependencies between packages. Additionally, Lerna's caching mechanism helps to reduce the number of unnecessary rebuilds and dependencies checks.\n* Flexible: Lerna is highly customizable, allowing us to tailor it to our specific needs and workflow.\n\n## Status\n\nThe decision has been made, and Lerna will be implemented as our monorepo tool. We will work on integrating Lerna with our existing infrastructure and configuring it to meet our specific requirements.\n\n## Pros and Cons\n\nPros:\n\n* Easy to use and set up\n* Supports multiple package managers and programming languages\n* Provides dependency graphs and caching for optimized CI\/CD\n* Highly customizable\n\nCons:\n\n* May require some additional configuration to meet specific requirements\n* May introduce some overhead in terms of disk space and memory usage\n\n## Rationale\n\nWe chose Lerna over other monorepo tools because of its simplicity, flexibility, and wide range of features. While other tools may offer more advanced features, Lerna's ease of use and minimal overhead make it a more suitable choice for our needs. Additionally, Lerna's large community and extensive documentation make it easier to find resources and support when needed.","GenTime":"2024-08-10 03:50:49"}
{"File Name":"island.is\/0012-chart-library.md","Context":"## Context and Problem Statement\\nMultiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\\n## Decision Drivers\\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\n","Decision":"- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\nChosen option: \"Recharts\", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\\n### Positive Consequences\\n- We can start implementing charts and graphs as needed in island.is\\n### Negative Consequences\\n- It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.\\n","tokens":79,"id":1119,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** We will use D3.js as the single approach to implementing charts across the entire repository.\n\n**Context and Problem Statement:** Multiple projects need to show data visually using charts and graphs. In order to provide a unified look and feel across the entire island.is repository, we should commit to a single approach to implementing charts.\n\n**Decision Drivers:**\n\n* Meet all requirements listed above: D3.js is a popular and widely-used library that can handle a wide range of charting requirements.\n* API quality: D3.js has a robust and well-documented API that makes it easy to use and integrate with our existing codebase.\n* Pricing: D3.js is an open-source library, which means it is free to use and distribute.\n* Bundle size: D3.js is a relatively lightweight library, which minimizes the impact on our bundle size and page load times.\n* Typescript support: D3.js has excellent TypeScript support, which makes it easy to integrate with our existing TypeScript-based codebase.\n\n**Consequences:**\n\n* All projects will use D3.js for charting, providing a unified look and feel across the entire repository.\n* We will need to invest time in learning and mastering D3.js, but this will pay off in the long run by reducing technical debt and improving code quality.\n* We will need to ensure that our code follows best practices for using D3.js, such as properly handling errors and edge cases.\n\n**Rationale:** After evaluating the decision drivers, we believe that D3.js is the best choice for our charting needs. Its popularity, robust API, and open-source nature make it an ideal choice for our use case. Additionally, its TypeScript support and lightweight bundle size make it easy to integrate with our existing codebase. By committing to a single approach to charting, we can reduce technical debt and improve code quality across the entire repository.","GenTime":"2024-08-10 03:51:47"}
{"File Name":"registraties\/001-use-existing-apis.md","Context":"## Context\\nThe Registraties project will need to contain a page that will show data per address (or verblijfsobject) from existing APIs, like [Basisregistraties adressen en gebouwen (BAG)](https:\/\/api.data.amsterdam.nl\/bag\/), [Basisregistratie kadaster (BRK)](https:\/\/api.data.amsterdam.nl\/brk\/) and [Handelsregister (HR)](https:\/\/api.data.amsterdam.nl\/handelsregister). All required data is readily available.\\n","Decision":"The project's MVP, which will contain a simple search field with auto-suggest functionality, get its data from existing API endpoints and therefore doesn't need its own back-end setup with API. In the future it might, but for the initial phase, the front-end code is capable of retrieving, formatting and showing the data from [api.data.amsterdam.nl](https:\/\/api.data.amsterdam.nl).\\nThe application will have container components that have sagas injected into them. Each saga is responsible for retrieving data from a single endpoint. If the situation calls for it, a saga can retrieve data from more than one endpoint.\\n","tokens":112,"id":1122,"Prediction":"## Decision\n\n**Decision:** Implement a data aggregation layer using a microservice architecture to fetch and process data from multiple APIs, rather than integrating the APIs directly into the Registraties project.\n\n**Context:** The Registraties project needs to display data per address (or verblijfsobject) from multiple APIs, including BAG, BRK, and HR. The required data is readily available, but the APIs have different structures and formats, making it challenging to integrate them directly into the project.\n\n**Rationale:**\n\n* The microservice architecture allows for loose coupling between the data aggregation layer and the APIs, making it easier to switch or replace APIs if needed.\n* A dedicated data aggregation layer can handle data processing and transformation, reducing the complexity of the Registraties project and improving maintainability.\n* The microservice architecture enables scalability and fault tolerance, as each service can be scaled independently and failures can be isolated.\n* The data aggregation layer can be designed to handle errors and exceptions, providing a more robust and reliable data retrieval process.\n\n**Consequences:**\n\n* Additional development effort is required to create the data aggregation layer and integrate it with the Registraties project.\n* The project requires more complex infrastructure and deployment setup, as multiple services need to be managed and scaled.\n* The data aggregation layer introduces an additional point of failure, which needs to be considered in the overall system design.\n\n**Alternatives considered:**\n\n* Integrating the APIs directly into the Registraties project, which would require complex data processing and transformation logic.\n* Using a third-party data aggregation service, which may require additional costs and dependencies.\n\n**Status:** Accepted\n\n**Implementation:** The data aggregation layer will be developed as a separate microservice, using a technology stack that is compatible with the existing project infrastructure. The service will be designed to handle data processing, transformation, and caching, and will be integrated with the Registraties project using RESTful APIs.","GenTime":"2024-08-10 03:52:51"}
{"File Name":"registraties\/002-use-styled-components.md","Context":"## Context\\nSASS is used in most of the [Amsterdam](https:\/\/github.com\/Amsterdam) projects. The way SASS is compiled by Webpack, however, has all defined style rules affect the global scope. This can lead to unwanted and unexpected side-effect and will increase maintenance, because each developer working on the project needs to be aware of the rules that have been defined and are scattered throughout the code base. Also, using SASS requires a naming convention to prevent specifity problems.\\nAn alternative to global scope CSS is [styled components](https:\/\/www.styled-components.com\/). This approach is also used by the [Amsterdam reusable component library](https:\/\/amsterdam.github.io\/amsterdam-styled-components), it makes sense to also apply it to projects (like this one) that make use of components from that library.\\n","Decision":"SASS will not be used in the Registraties project. Instead, styled components will be used.\\n","tokens":174,"id":1125,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nUse Styled Components instead of SASS for styling in the project, to avoid global scope issues and naming convention requirements.\n\n**Context**\n\nAs described in the context, using SASS in most Amsterdam projects has led to global scope issues and naming convention requirements, making maintenance and development more complex. Additionally, the Amsterdam reusable component library already uses Styled Components, making it a suitable alternative for this project.\n\n**Problem**\n\nThe current use of SASS in the project leads to:\n\n* Unwanted and unexpected side effects due to global scope\n* Increased maintenance requirements due to scattered style rules\n* Naming convention requirements to prevent specificity problems\n\n**Decision Drivers**\n\n* The need for a more modular and maintainable approach to styling\n* The alignment with the Amsterdam reusable component library's styling approach\n* The potential for improved code organization and reusability\n\n**Consequences**\n\n* Adoption of a new styling approach, which may require additional learning and training for developers\n* Potential changes to existing codebase and components\n* Improved maintainability and scalability of the project's styling\n\n**Rationale**\n\nUsing Styled Components will allow for a more modular and maintainable approach to styling, reducing the complexity and potential issues associated with global scope and naming conventions. This decision aligns with the Amsterdam reusable component library's styling approach, making it a suitable choice for this project.","GenTime":"2024-08-10 03:54:05"}
{"File Name":"simple-android\/001-screen-controllers.md","Context":"## Context\\nWe don\u2019t want to put business logic inside Android framework classes (like an `Activity` or `Fragment`) because those cannot be unit tested. To enable\\na fast feedback loop (i.e. tests that run on the JVM and not Android VM), we separate screens and controllers using\\nthe [MVI architecture](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1) [pattern](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1)\\n.\\n","Decision":"Every screen has one controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes back\\nto the screen.\\nUser interactions happening on the screen are abstracted inside data classes of type `UiEvent`. These events flow to the controller in the form of\\nRxJava streams.\\n```kotlin\\n\/\/ Create the UsernameTextChanged event by listening to the EditText\\nRxTextView\\n.textChanges(usernameEditText)\\n.map { text -> UsernameTextChanged(text) }\\n\/\/ Event\\ndata class UsernameTextChanged(text: String) : UiEvent\\n```\\nThe screen sends a single stream of `UiEvent`s to the controller and gets back a transformed stream of UI changes. The flow of data is\\nuni-directional. To merge multiple streams into one, RxJava\u2019s `merge()`  operator is used.\\n```kotlin\\n\/\/ Login screen\\nObservable.merge(usernameChanges(), passwordChanges(), submitClicks())\\n.compose(controller)\\n.takeUntil(screenDestroy)\\n.subscribe { uiChange -> uiChange(this) }\\n```\\nIn the controller, `UiEvent`s are transformed as per the business logic and `UiChange`s are sent back to the screen. The `UiChange` is a simple lambda\\nfunction that takes the screen itself as an argument, which can call a method implemented by the screen interface.\\n```kotlin\\ntypealias Ui = LoginScreen\\ntypealias UiChange = (LoginScreen) -> Unit\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nevents.ofType<UsernameTextChanged>\\n.map { isValidUsername(it.text) }\\n.map { isValid ->\\n{ ui: Ui -> ui.setSubmitButtonEnabled(isValid) } \/\/ UiChange lambda!\\n}\\n}\\n}\\n```\\nWhen the events have to observed across multiple functions in the controller, the stream is shared using `replay()` + `refCount()` so that the UI\\nevents aren't recreated once for every subscription. `replay()` shares a single subscription to the screen by replaying the events to every observer\\nand `refCount()` keeps the subscription alive as long as there is at least one observer.\\n```kotlin\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nval replayedEvents = events.replay().refCount()\\nreturn Observable.merge(\\nenableSubmitButton(replayedEvents),\\nloginUser(replayedEvents))\\n}\\nfun enableSubmitButton(events: Observable<UiEvent>): Observable<UiChange>()\\nfun loginOnSubmit(events: Observable<UiEvent>): Observable<UiChange>()\\n}\\n```\\n![data flow from the Ui to the controller](images\/diagram_screen_controller.png)\\n([diagram source](https:\/\/docs.google.com\/drawings\/d\/1I_VdUM8Pf9O3nOYViqVF6kiyqFaYFD2fHmKRyvwmEl4\/edit?usp=sharing))\\n","tokens":137,"id":1133,"Prediction":"## Decision\n\n**Separate Business Logic from Android Framework Classes using MVI Architecture**\n\n## Context\n\nWe want to avoid putting business logic inside Android framework classes (like an `Activity` or `Fragment`) because those cannot be unit tested, which hinders our ability to have a fast feedback loop.\n\n## Problem\n\nIf we embed business logic in Android framework classes, we will not be able to write unit tests for that logic, which makes it difficult to ensure the correctness of our code and leads to a slower development process.\n\n## Decision\n\nTo address this issue, we will use the Model-View-Intent (MVI) architecture pattern to separate our business logic from Android framework classes. This will allow us to write unit tests for our business logic independently of the Android framework, enabling a faster feedback loop and improving the overall maintainability of our code.\n\n## Consequences\n\n* We will be able to write unit tests for our business logic, ensuring that it is correct and functioning as expected.\n* We will be able to develop and test our business logic independently of the Android framework, allowing for a faster development process.\n* Our code will be more maintainable and easier to understand, as the business logic will be decoupled from the Android framework.\n\n## Rationale\n\nThe MVI architecture pattern is well-suited for Android development because it allows us to separate the concerns of our application into distinct layers, making it easier to test and maintain our code. By separating our business logic from the Android framework, we can ensure that our code is correct and functioning as expected, even in the absence of the Android framework.","GenTime":"2024-08-10 03:57:38"}
{"File Name":"runbooks-mkdocs\/0003-use-plantuml-for-diagramming.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":1145,"Prediction":"## Decision\n\nAs our application's user base grows, we've noticed an increase in the number of concurrent users, leading to performance issues and slower response times. To address this, we've decided to implement a caching layer to reduce the load on our database and improve the overall user experience.\n\n## Context\n\nThe issue motivating this decision is the recent surge in user growth, which has put a strain on our application's infrastructure. Our current architecture relies heavily on database queries, which are becoming increasingly slow and resource-intensive. This has resulted in longer response times, increased latency, and a higher likelihood of errors.\n\nThe context that influences or constrains this decision is the need to:\n\n* Minimize the impact on our database and reduce the load on our infrastructure\n* Improve the overall user experience by reducing response times and increasing the speed of our application\n* Ensure scalability and reliability as our user base continues to grow\n* Balance the trade-offs between caching complexity, storage requirements, and maintenance costs\n\n## Decision Drivers\n\nThe following drivers influenced our decision:\n\n* Performance: Implementing a caching layer will significantly reduce the load on our database and improve response times, resulting in a better user experience.\n* Scalability: A caching layer will enable us to scale our application more efficiently, as it will allow us to offload some of the processing and storage requirements from our database.\n* Cost: By reducing the load on our database, we can also reduce our infrastructure costs and optimize our resource allocation.\n\n## Decision Alternatives\n\nWe considered the following alternatives:\n\n* Implementing a database sharding strategy to distribute the load across multiple databases\n* Increasing the capacity of our database infrastructure to handle the increased load\n* Implementing a content delivery network (CDN) to cache static assets and reduce the load on our application\n\nHowever, these alternatives were deemed less effective or more complex than implementing a caching layer. A caching layer provides a more targeted solution to our performance issues, is easier to implement and maintain, and offers better scalability and cost benefits.\n\n## Decision Consequences\n\nThe consequences of this decision are:\n\n* Improved performance and response times for our users\n* Reduced load on our database and infrastructure\n* Increased scalability and reliability\n* Potential for cost savings through optimized resource allocation\n* Additional complexity and maintenance requirements for the caching layer\n\nBy implementing a caching layer, we're confident that we can address the performance issues and scalability concerns that have been affecting our application, and provide a better experience for our users.","GenTime":"2024-08-10 04:02:33"}
{"File Name":"once-ui\/0002-build-a-high-quality-ui-components-library.md","Context":"## Context\\nMoving towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent api.\\n","Decision":"- We will build a library of atomic ui components that will be used throughout the applications.\\n- Developers should be able to easily import only the components they need and use them in their applications.\\n- Components should be well tested and isolated in a way that doesn't break the host application's code.\\n","tokens":40,"id":1151,"Prediction":"## Decision\n\n**Title:** Implement a Shared Angular Library for Reusable Components\n\n**Context:** As we move towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent API.\n\n**Decision:** We will implement a shared Angular library, named `@shared-components`, which will contain a set of reusable components that can be easily imported and used across our multiple applications. This library will be maintained separately from the individual application codebases, allowing for a consistent design language and API across all applications.\n\n**Rationale:**\n\n* Reusing components across applications will reduce code duplication and maintainability efforts.\n* A shared library will enable us to ensure consistency in design language and API across all applications.\n* This approach will also allow us to easily update and maintain the shared components, without affecting the individual application codebases.\n* We will use a separate repository for the shared library, to keep it separate from the individual application codebases and to make it easier to manage and maintain.\n\n**Consequences:**\n\n* We will need to create a new repository for the shared library and manage it separately from the individual application codebases.\n* We will need to document the API and usage guidelines for the shared components to ensure consistent usage across all applications.\n* We will need to establish a process for reviewing and merging changes to the shared library, to ensure that it remains stable and consistent across all applications.\n\n**Status:** Accepted\n\n**Implementers:** [List of implementers, e.g. John Doe, Jane Smith]\n\n**Date:** [Date of decision]\n\n**Links:** [Links to relevant documentation, e.g. repository, design guidelines]","GenTime":"2024-08-10 04:05:17"}
{"File Name":"Head-Start-TTADP\/0006-database.md","Context":"## Context\\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\\n","Decision":"While both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN\/GIST indexing accelerating full-text searches.\\n","tokens":90,"id":1167,"Prediction":"## Decision\n\n**Decision:** To use PostgreSQL as the relational database management system for the tta smarthub platform.\n\n**Context:** Data storage and management is a key component of the tta smarthub platform, and a relational database management system was chosen due to the structured nature of the data and the need for frequent queries and reports.\n\n**Rationale:** After considering both MySQL and PostgreSQL, we decided to use PostgreSQL for several reasons:\n\n* PostgreSQL is known for its reliability, data integrity, and ability to handle large volumes of data.\n* Its support for advanced features such as window functions, common table expressions, and full-text search make it well-suited for complex queries and reports.\n* PostgreSQL has a strong focus on security, with features such as row-level security, encryption, and access controls.\n* It is also highly extensible, with a large community of developers and a wide range of tools and plugins available.\n\n**Consequences:**\n\n* The use of PostgreSQL will provide a robust and scalable data storage solution for the tta smarthub platform.\n* The platform will benefit from PostgreSQL's advanced features, such as window functions and common table expressions, which will enable more complex and efficient queries.\n* The platform will also benefit from PostgreSQL's strong focus on security, which will help to protect sensitive data.\n* The use of PostgreSQL may require additional training and expertise for developers, but this will be offset by the benefits of using a widely-used and well-documented database system.","GenTime":"2024-08-10 04:12:10"}
{"File Name":"Head-Start-TTADP\/0011-monitoring.md","Context":"## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n","Decision":"TTA Smart Hub will utilize New Relic for Monitoring.\\n* Integrates easily with front end and backend code\\n* Track performance metrics and errors\\n* FedRAMP approved\\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\\n","tokens":20,"id":1171,"Prediction":"## Decision\n\n**Title:** Implement a Distributed Monitoring System for TTA Smart Hub\n\n**Context:** TTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\n\n**Decision:** We will implement a distributed monitoring system to monitor the TTA Smart Hub's performance, availability, and errors. This system will provide real-time insights into the hub's behavior, enabling swift identification and resolution of issues.\n\n**Rationale:**\n\n1. **Scalability**: A distributed monitoring system will allow us to scale our monitoring capabilities as the TTA Smart Hub grows, ensuring we can continue to monitor its performance effectively.\n2. **Reliability**: By distributing the monitoring workload across multiple nodes, we can ensure that if one node fails, the system remains functional, minimizing downtime and data loss.\n3. **Real-time insights**: The distributed monitoring system will provide real-time data on the TTA Smart Hub's performance, enabling our team to quickly identify and respond to issues, reducing mean time to detect (MTTD) and mean time to resolve (MTTR).\n4. **Customization**: The system will allow us to customize monitoring configurations to meet the specific needs of the TTA Smart Hub, ensuring we can track the metrics that matter most to our users.\n\n**Description:**\n\nWe will implement a distributed monitoring system using a combination of open-source and commercial tools. The system will consist of multiple nodes, each responsible for monitoring a specific aspect of the TTA Smart Hub's performance. These nodes will be configured to collect data on metrics such as CPU usage, memory usage, disk space, and network traffic.\n\nThe data collected by each node will be aggregated and analyzed using a centralized platform, providing real-time insights into the TTA Smart Hub's performance. The platform will also include alerting and notification capabilities, enabling our team to quickly respond to issues as they arise.\n\n**Consequences:**\n\n1. **Increased complexity**: Implementing a distributed monitoring system will require additional infrastructure and configuration, increasing the overall complexity of our monitoring setup.\n2. **Additional costs**: We will need to invest in additional hardware and software to support the distributed monitoring system, which may increase our costs.\n3. **Improved monitoring capabilities**: The distributed monitoring system will provide more comprehensive and accurate monitoring capabilities, enabling us to better understand and respond to issues affecting the TTA Smart Hub.\n\n**Status:** Approved\n\n**Date:** [Insert Date]\n\n**Responsible:** [Insert Name]\n\n**Reviewers:** [Insert Names]\n\n**Discussion:** [Insert Discussion or Links to Discussion]","GenTime":"2024-08-10 04:13:54"}
{"File Name":"Head-Start-TTADP\/0007-use-plantuml-for-diagrams.md","Context":"## Context\\nCertain things we need to document, such as our data model and boundary diagrams are\\nbetter represented as diagrams than text. Choosing a single service will make it easier to\\nlearn the system and keep diagrams up to date.\\n","Decision":"We will use https:\/\/www.plantuml.com to generate diagrams with the following checks:\\n1. We will not rely on the plantuml.com service to be the sole storage medium for the actual UML text\\n1. We will store each type of diagram in its own markdown file in the \/docs folder\\nPlantUML can also be run locally, in case the plantuml.com service ever goes away. This risk is also why\\nwe must keep the UML source in our source control.\\n","tokens":48,"id":1172,"Prediction":"## Decision\n\n**Title:** Diagramming Tool for System Documentation\n\n**Context:** Certain aspects of our system, such as the data model and boundary diagrams, are better represented as diagrams than text. This is because diagrams can convey complex information more efficiently and effectively than text alone. Additionally, using a single service for diagramming will make it easier for team members to learn the system and keep diagrams up to date.\n\n**Decision:** We will use PlantUML as our diagramming tool for system documentation. PlantUML is a lightweight, open-source tool that allows us to create a wide range of diagrams, including sequence diagrams, class diagrams, and state diagrams, using a simple text-based syntax. This will enable us to easily create and maintain diagrams that accurately represent our system's architecture and design.\n\n**Rationale:**\n\n* PlantUML is widely used and has a large community of users and contributors, which means there are many resources available for learning and troubleshooting.\n* PlantUML is highly customizable, allowing us to tailor the appearance and behavior of our diagrams to our specific needs.\n* PlantUML is compatible with a wide range of diagramming formats, including PDF, PNG, and SVG, making it easy to integrate our diagrams into our documentation and other systems.\n* PlantUML is free and open-source, which means we can use it without incurring any licensing fees or restrictions.\n\n**Consequences:**\n\n* We will need to learn the PlantUML syntax and syntax highlighting in our code editor.\n* We will need to create a set of templates and styles for our diagrams to ensure consistency across the system.\n* We will need to integrate PlantUML with our documentation and other systems to ensure that our diagrams are easily accessible and up-to-date.\n\n**Alternatives:**\n\n* We could have used a different diagramming tool, such as Graphviz or Draw.io. However, PlantUML's simplicity, customizability, and compatibility with a wide range of formats make it the best choice for our needs.\n* We could have used a combination of different diagramming tools, but this would likely lead to inconsistencies and difficulties in maintaining our diagrams.\n\n**Monitoring and Feedback:**\n\n* We will monitor the effectiveness of using PlantUML for system documentation by tracking the ease of use, accuracy, and consistency of our diagrams.\n* We will solicit feedback from team members and stakeholders to ensure that our diagrams are meeting their needs and expectations.\n* We will review and update our documentation and diagrams regularly to ensure that they remain accurate and up-to-date.","GenTime":"2024-08-10 04:14:28"}
{"File Name":"Head-Start-TTADP\/0003-configuration-by-environment-variable.md","Context":"## Context\\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\\n","Decision":"We will use environment variables to configure applications.\\n","tokens":35,"id":1173,"Prediction":"## Decision\n\nTo address the need for applications to be configured differently depending on where they are running, we will implement a configuration management system that uses environment variables and a configuration file to manage application settings.\n\n## Context\n\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration than the backend running in production.\n\n## Decision\n\nWe will use a combination of environment variables and a configuration file to manage application settings. This approach will allow us to easily switch between different configurations based on the environment the application is running in.\n\n## Rationale\n\nUsing environment variables and a configuration file will provide several benefits:\n\n* Environment variables will allow us to easily switch between different configurations by setting environment variables in the deployment environment.\n* A configuration file will provide a centralized location for storing application settings, making it easier to manage and maintain the application's configuration.\n* This approach will also allow us to separate application configuration from the application code, making it easier to manage and maintain the application's configuration.\n\n## Consequences\n\n* We will need to create a configuration file that defines the default application settings.\n* We will need to set environment variables in the deployment environment to override the default settings.\n* We will need to update the application code to use the environment variables and configuration file to manage application settings.\n\n## Status\n\nThis decision is approved and will be implemented in the next release of the application.\n\n## Links\n\n* Configuration management system documentation: [link]\n* Environment variables documentation: [link]\n* Configuration file documentation: [link]","GenTime":"2024-08-10 04:14:49"}
{"File Name":"clone_difitalcitizenship\/0002-use-openapi-to-defined-the-api-specs.md","Context":"## Context\\nWe need to define the API specifications of the services we're going to implement.\\n","Decision":"We use the [OpenAPI 2.0](https:\/\/swagger.io\/specification\/) specification (aka Swagger spec) as standard for our REST API definitions.\\n","tokens":20,"id":1180,"Prediction":"## Decision\n\n**API Specification Standardization**\n\n## Context\n\nWe need to define the API specifications of the services we're going to implement.\n\n## Problem\n\nAs we are planning to implement multiple services, it is crucial to define a standard for our API specifications to ensure consistency, maintainability, and scalability. Without a standard, our APIs may become disorganized, making it difficult for developers to understand and maintain them.\n\n## Decision\n\nWe will adopt OpenAPI (Swagger) as our API specification standard. OpenAPI is a widely-used, vendor-neutral, and open standard for describing RESTful APIs. It provides a clear and concise way to define API endpoints, request and response formats, and other essential details.\n\n## Rationale\n\n1. **Consistency**: OpenAPI provides a consistent way to define API specifications, ensuring that all our services adhere to the same standard.\n2. **Easy Maintenance**: With OpenAPI, we can easily update and maintain our API specifications, reducing the risk of errors and inconsistencies.\n3. **Scalability**: OpenAPI is designed to handle large and complex APIs, making it an ideal choice for our growing service ecosystem.\n4. **Community Support**: OpenAPI has a large and active community, ensuring that we have access to extensive documentation, tools, and resources.\n5. **Flexibility**: OpenAPI supports multiple programming languages and frameworks, allowing us to integrate it seamlessly with our existing technology stack.\n\n## Consequences\n\n1. **Initial Learning Curve**: There may be an initial learning curve for developers who are new to OpenAPI.\n2. **Additional Documentation**: We will need to create additional documentation to explain the OpenAPI specification and its usage.\n\n## Status\n\nThis decision is approved and will be implemented in our API specification process.\n\n## Links\n\n* OpenAPI Specification: <https:\/\/swagger.io\/specification\/>\n* OpenAPI Tools: <https:\/\/swagger.io\/tools\/>","GenTime":"2024-08-10 04:17:18"}
{"File Name":"clone_difitalcitizenship\/0003-follow-a-cloud-native-design-principle.md","Context":"## Context\\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\\nDecision criteria will also include:\\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\n","Decision":"* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\\n","tokens":227,"id":1182,"Prediction":"## Decision\n\n**Title:** Technology Stack for the Digital Citizenship Platform\n\n**Context:** As we design the building blocks of the Digital Citizenship platform, we need to decide on the technology stack that will satisfy the functional and non-functional requirements while considering the criteria of elasticity, resilience, pay-as-you-go pricing, exploiting economies of scale, falling costs of infrastructure, quality of management tools, best of breed security, and flexibility.\n\n**Decision:** We will adopt a hybrid approach, combining the benefits of cloud-native technologies with the flexibility of serverless computing. We will use a managed public cloud provider for the infrastructure and application layers, leveraging their scalability, reliability, and security features. For the serverless computing component, we will use a serverless framework that provides a managed environment for building and deploying cloud-native applications.\n\n**Rationale:**\n\n1. **Elasticity and Resilience:** The managed public cloud provider will provide automatic scaling, load balancing, and failover capabilities, ensuring that the platform is highly available and scalable.\n2. **Pay-as-you-go Pricing:** The cloud provider's pay-as-you-go pricing model will allow us to optimize costs and reduce waste, as we only pay for the resources used.\n3. **Exploiting Economies of Scale:** By leveraging the cloud provider's economies of scale, we can reduce our infrastructure costs and focus on developing the platform's features and functionality.\n4. **Falling Costs of Infrastructure:** As the cloud provider continues to innovate and reduce costs, we can take advantage of these savings and allocate resources to other areas of the platform.\n5. **Quality of Management Tools:** The cloud provider's management tools will provide a single pane of glass for monitoring, logging, and troubleshooting, simplifying our operations and reducing the need for custom tools.\n6. **Best of Breed Security:** The cloud provider's security features, such as encryption, access controls, and intrusion detection, will provide a robust security posture for the platform.\n7. **Flexibility and Opportunity Costs:** The serverless computing framework will provide the flexibility to develop and deploy cloud-native applications, allowing us to take advantage of new technologies and opportunities as they emerge.\n\n**Consequences:**\n\n1. **Increased Complexity:** The hybrid approach will introduce additional complexity, requiring careful planning and integration of the different components.\n2. **Dependence on Cloud Provider:** Our reliance on the cloud provider will introduce dependencies and potential risks, such as service outages or changes to their pricing model.\n3. **Customization and Integration:** We will need to invest time and resources in customizing and integrating the different components, ensuring seamless interactions and optimal performance.\n\n**Next Steps:**\n\n1. Conduct a thorough analysis of the cloud provider's services and features to determine the best fit for the platform.\n2. Develop a detailed plan for integrating the serverless computing framework with the managed public cloud provider.\n3. Establish a governance model for managing the platform's infrastructure and applications, including monitoring, logging, and troubleshooting.\n4. Develop a comprehensive security plan, incorporating the cloud provider's security features and our own security controls.","GenTime":"2024-08-10 04:18:23"}
{"File Name":"clone_difitalcitizenship\/0007-choice-of-azure-region.md","Context":"## Context\\nWe must guarantee that the data stored, transferred to and from the cloud services will never leave the borders of the European Union.\\n","Decision":"We decide to deploy our services on the [West Europe](https:\/\/azure.microsoft.com\/en-us\/regions\/) region, located in the Netherlands.\\n","tokens":29,"id":1186,"Prediction":"## Decision\n\n**Decision:** We will implement a data center located within the European Union to store and process all data, ensuring that all data remains within the EU borders.\n\n**Rationale:**\n\nTo guarantee that data stored, transferred to, and from cloud services never leaves the European Union, we will establish a data center within the EU. This will ensure that all data is stored and processed within the region, adhering to the EU's data sovereignty and GDPR regulations.\n\n**Context:**\n\nAs specified, the requirement is to ensure that all data remains within the EU borders. This decision is crucial to maintain the trust and compliance of our users and stakeholders, particularly those in the European Union.\n\n**Consequences:**\n\n* Increased costs associated with maintaining a data center within the EU\n* Potential limitations on scalability and flexibility due to the physical location of the data center\n* Enhanced security and compliance with EU data regulations\n\n**Alternatives:**\n\n* Implementing a cloud service provider with EU-based data centers, but this may not guarantee that data never leaves the EU\n* Using a VPN or encryption to transfer data to a non-EU location, but this may not be sufficient to meet the requirements of the GDPR and other EU data regulations\n\n**Status:**\n\nApproved\n\n**Implementation:**\n\nThe implementation of the data center within the EU will be done in collaboration with the cloud service provider and other relevant stakeholders. The data center will be designed and built to meet the required security and compliance standards, and all data will be stored and processed within the EU.","GenTime":"2024-08-10 04:19:59"}
{"File Name":"cdk\/002-component-constuctors.md","Context":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThis project contains a large number of classes, making up the various constructs and patterns. The project is intended to be used as a component library and, therefore, used by a number of people who don't have extensive knowledge of either the CDK or this library. It is therefore important to make the experience of using these classes as intuitive as possible.\\n","Decision":"<!-- What is the change that we're proposing and\/or doing? -->\\nConstructors should follow the following rules for consistency.\\n1. The first parameter should be a `scope` of type `GuStack`:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then have a static `id` as it will never change, for example the `Stage` parameter.\\n2. They can also take a `props` object which should be correctly typed:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack, props: object) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then derive `id` from `props` as it will never change, for example `InstanceTypeFor${props.app}`.\\n3. They can also take an `id` of type string and a `props` object which should be correctly typed\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {...}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: any, props: object) {\\n...\\n}\\n}\\n```\\n4. Where all `props` are optional, the `props` object should be optional as a whole\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props?: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n","tokens":95,"id":1193,"Prediction":"## Decision\n\nTo improve the usability and intuitiveness of the class library, we will implement a consistent naming convention and documentation strategy across all classes. This will enable users to quickly understand the purpose and behavior of each class, even without extensive knowledge of the CDK or the library.\n\n### Rationale\n\nThe current state of the project, with a large number of classes and a lack of consistency in naming conventions and documentation, can be overwhelming for users who are not familiar with the CDK or the library. By implementing a consistent naming convention and documentation strategy, we can make it easier for users to find and understand the classes they need, reducing the learning curve and improving the overall user experience.\n\n### Consequences\n\n* Improved usability and intuitiveness of the class library\n* Reduced time spent by users trying to understand the purpose and behavior of each class\n* Improved maintainability of the codebase, as a consistent naming convention and documentation strategy will make it easier to understand and update the code\n* Potential for increased adoption of the library, as users will be more likely to use a library that is easy to understand and use\n\n### Alternatives\n\n* We could have chosen to implement a complex and detailed documentation system, but this would likely be overwhelming for users and would require a significant amount of time and resources to maintain.\n* We could have chosen to use a different naming convention, but this would likely lead to confusion and make it harder for users to find and understand the classes they need.\n\n### Status\n\nThis decision is currently in the \"proposed\" state, pending review and implementation.","GenTime":"2024-08-10 04:22:23"}
{"File Name":"hee-web-blueprint\/0003-use-github-actions-for-automation.md","Context":"## Context\\nWe should determine the platform we use to automate the build, test and deployment of the HEE National Website Platform.\\n","Decision":"Having determined to use Github to manage the source code of the platform, the simplest answer to this question was to look at Github actions. We determined after some investigative work to prove out our ability to deploy to the BloomReach cloud that we should use Github Actions to manage platform automation.\\n","tokens":27,"id":1196,"Prediction":"## Decision\n\n**Automated Build, Test, and Deployment of HEE National Website Platform using CircleCI**\n\n## Context\n\nAs per the requirement, we need to determine the platform to automate the build, test, and deployment of the HEE National Website Platform. After considering various options, we have decided to use CircleCI as the platform for automating the build, test, and deployment process.\n\n## Problem\n\nThe HEE National Website Platform requires a reliable and efficient way to automate the build, test, and deployment process to ensure consistency, speed, and quality. The platform should be able to integrate with various tools and services, such as version control systems, testing frameworks, and cloud providers.\n\n## Decision\n\nAfter evaluating various options, we have decided to use CircleCI as the platform for automating the build, test, and deployment process of the HEE National Website Platform. CircleCI offers a robust and scalable solution that can integrate with various tools and services, providing a seamless experience for developers and operators.\n\n## Rationale\n\nThe decision to use CircleCI is based on the following reasons:\n\n1. **Ease of use**: CircleCI provides an intuitive and user-friendly interface that makes it easy to set up and manage workflows.\n2. **Integration**: CircleCI supports integration with various tools and services, such as GitHub, GitLab, and Bitbucket, as well as testing frameworks like Jest and Cypress.\n3. **Scalability**: CircleCI provides a scalable solution that can handle large volumes of traffic and data, making it suitable for complex projects like the HEE National Website Platform.\n4. **Security**: CircleCI provides robust security features, such as encryption and access controls, to ensure the integrity and confidentiality of the build, test, and deployment process.\n5. **Cost-effective**: CircleCI offers a cost-effective solution that can help reduce the overall cost of ownership and operation.\n\n## Consequences\n\nThe decision to use CircleCI as the platform for automating the build, test, and deployment process of the HEE National Website Platform will have the following consequences:\n\n1. **Improved efficiency**: CircleCI will automate the build, test, and deployment process, reducing the time and effort required to complete these tasks.\n2. **Increased reliability**: CircleCI will provide a reliable and consistent solution for automating the build, test, and deployment process, reducing the risk of errors and failures.\n3. **Enhanced collaboration**: CircleCI will provide a platform for developers and operators to collaborate more effectively, improving communication and reducing the risk of misunderstandings.\n\n## Status\n\nThis decision is currently in the \"Implemented\" status, as CircleCI has been set up and configured to automate the build, test, and deployment process of the HEE National Website Platform.\n\n## Links\n\n* CircleCI documentation: <https:\/\/circleci.com\/docs\/>\n* HEE National Website Platform repository: <https:\/\/github.com\/hee-national-website-platform\/hee-national-website-platform>\n\nNote: This is just a sample Architectural Decision Record, and the actual content may vary depending on the specific requirements and context of the project.","GenTime":"2024-08-10 04:23:59"}
{"File Name":"hee-web-blueprint\/0008-use-npm-to-host-hee-frontend-framework.md","Context":"## Context\\nWe need to identify how and where we will store packages that describe our front end framework. There are a number of NHS UK and NHS Digital projects that already use NPM as their package repository for front end code.\\n","Decision":"We have chosen to store our packages on NPM.\\n","tokens":47,"id":1202,"Prediction":"## Decision\n\n**Package Storage for Front End Framework**\n\n## Context\n\nAs discussed, we need to identify how and where we will store packages that describe our front end framework. Several NHS UK and NHS Digital projects already utilize NPM as their package repository for front end code.\n\n## Problem\n\nWe require a reliable and scalable solution for storing and managing our front end packages to ensure efficient collaboration and version control among team members and stakeholders.\n\n## Decision\n\nAfter considering the pros and cons of various options, we have decided to use NPM (Node Package Manager) as our package repository for storing and managing our front end packages. This decision is based on the following reasons:\n\n1. **Industry Standard**: NPM is widely adopted in the front end development community, making it easier to collaborate with other projects and teams.\n2. **Scalability**: NPM is designed to handle large-scale package management, ensuring that our repository can grow with our project's needs.\n3. **Easy Integration**: NPM integrates seamlessly with popular front end build tools like Webpack and Rollup, streamlining our development workflow.\n4. **Version Control**: NPM's versioning system allows us to easily track changes and maintain a consistent version of our packages.\n\n## Consequences\n\n1. **Initial Setup**: We will need to set up a new NPM repository and configure our project to use it.\n2. **Package Maintenance**: We will need to regularly update and maintain our packages to ensure they remain compatible with our project's requirements.\n3. **Collaboration**: We will need to ensure that all team members and stakeholders are aware of and follow the package management process.\n\n## Rationale\n\nBy using NPM, we can leverage its robust features and widespread adoption to efficiently manage our front end packages. This decision will enable us to focus on developing our project without worrying about the underlying package management infrastructure.","GenTime":"2024-08-10 04:26:35"}
{"File Name":"hee-web-blueprint\/0013-use-bloomreach-channel-manager.md","Context":"## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n","Decision":"Bloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\\n","tokens":141,"id":1205,"Prediction":"## Decision\n\n**Title:** Document Management Platform Selection\n\n**Context:** We sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\n\n**Decision:** We will integrate an external document management platform to provide document management capabilities, rather than building it natively within the content management platform.\n\n**Rationale:**\n\n* Ease of use for creating new sites: Integrating an external document management platform will allow us to leverage its existing features and functionality for creating new sites, reducing the complexity and effort required for development.\n* Ability to share components: The external platform will provide a centralized repository for storing and sharing components, making it easier to reuse and manage content across multiple sites.\n* Ability to segregate content for specific channels (sites): The external platform will allow us to create separate repositories for each site, enabling us to manage and organize content specific to each channel.\n* Ability to share content up and down the stack where needed and appropriate: The integration will enable seamless sharing of content across different sites, while also allowing for content to be shared up and down the stack as needed.\n* Permissions model required to support this model: The external platform will provide a robust permissions model that will enable us to control access to content and ensure that only authorized users can view or edit specific documents.\n\n**Consequences:**\n\n* Increased flexibility and scalability in managing document management capabilities\n* Reduced development effort and complexity\n* Improved content reuse and sharing across multiple sites\n* Enhanced security and access control through the external platform's permissions model\n\n**Alternatives Considered:**\n\n* Building document management capabilities natively within the content management platform\n* Using a different external document management platform\n\n**Status:** Approved","GenTime":"2024-08-10 04:27:36"}
{"File Name":"automate\/adr-2019-01-23.md","Context":"## Context\\nPreviously, we did not have any rules for governing Automate UI route\\nlifecycle. In the past, we likely broke backwards compatibility when\\nre-naming or moving routes.\\nAdditionally, the top nav highlighting in Automate UI was driven off the\\nroot route (e.g. everything under the `Settings` tab must live under the `\/settings`\\nroute or it will not be highlighted properly for pages that live under it).\\nThis means that if you want to move a page to a new tab, it will require a new route.\\n","Decision":"Every route that originally linked to a page that is still in use in the webapp\\nmust still work and backwards compatibility must be maintained. Only when the webpage\\nthat the original route linked to is being removed from the app can a route be removed.\\nThis is so that deep linking from our own sources (blog posts, etc.) as well as links\\ncustomers use internally will continue to work until they no longer make sense at all\\nand the original resource no longer exists.\\nTo maintain backwards compatibility, the original route must redirect to the new route\\n(achievable in `app-routing.module.ts`) and an e2e test must be added to verify\\nthat redirection in `deprecated-routes.e2e-spec.ts`. See\\n[this commit](https:\/\/github.com\/chef\/automate\/commit\/38779c5aabb1af8d6c22e4b6e1b07eaf9c8fa06c)\\nfor an example of routes being renamed with proper backwards compatibility, what the e2e\\ntests should look like, and how a page can move between topnav tabs.\\n","tokens":115,"id":1210,"Prediction":"## Decision\n\n**Decision:** Introduce a formalized route management system for Automate UI, including a consistent naming convention and a process for handling route changes to ensure backwards compatibility.\n\n**Context:** As previously mentioned, the lack of rules for governing Automate UI route lifecycle has led to broken backwards compatibility when renaming or moving routes, causing issues with top nav highlighting and making it difficult to move pages to new tabs.\n\n**Reasoning:** To address these issues, we need a structured approach to managing routes in Automate UI. This will ensure that route changes are properly handled, minimizing the risk of breaking backwards compatibility and allowing for more flexibility in terms of page organization and navigation.\n\n**Consequences:**\n\n* Introduce a consistent naming convention for routes, making it easier to navigate and maintain the codebase.\n* Establish a process for handling route changes, including a review and testing phase to ensure backwards compatibility.\n* Improve top nav highlighting by decoupling it from the root route, allowing pages to be moved to new tabs without affecting navigation.\n* Enhance the overall maintainability and scalability of Automate UI by providing a clear and consistent framework for route management.\n\n**Alternatives Considered:**\n\n* Continue with the current ad-hoc approach to route management, which may lead to further issues and complexity.\n* Implement a more complex route management system that may be overkill for the current needs of Automate UI.\n\n**Decision Owner:** [Name of the decision owner]\n\n**Implementation:** The new route management system will be implemented in the following phases:\n\n1. Define a consistent naming convention for routes.\n2. Establish a process for handling route changes.\n3. Update the top nav highlighting logic to decouple it from the root route.\n4. Review and test the changes to ensure backwards compatibility.\n\n**Monitoring and Feedback:** The effectiveness of the new route management system will be monitored and evaluated regularly. Feedback from developers and users will be collected to identify areas for improvement and ensure that the system continues to meet the needs of Automate UI.","GenTime":"2024-08-10 04:28:42"}
{"File Name":"TruBudget\/0004-ressource-level-lifetime.md","Context":"## Context\\nResources have semantically a certain lifetime. That means, after a resources is created it will experience state-transition until it reaches the end of its lifetime (e.g. archival). The lifetimes and state-transitions need to be defined in an abstract way, in order to allow the platform to excel for different use-cases.\\n","Decision":"### Lifetime of a single resource\\n```mermaid\\ngraph TD\\nA[Create resource] -->|Auto-assign to creator| B\\nB[State: Open] --> |Do work|B\\nB --> |Assign to somebody who can close| C\\nC[State: Open & Ready to close] --> |Close| D\\nD[Resource closed]\\n```\\nRe-Opening is not supported; it could be discussed as long as the parent is still open (hierarchy must hold). Different usage-scenarios are acommodated via _manual_ steps.\\n#### Example: 4-Eyes Principle\\nScenario: A subproject must only be closed when 2 people A & B (e.g. different departments) checked it.\\nPrereqs: Closing right is given to an administrative person and not to the people signing off\\nApproach:\\n1. Subproject is executed as usual, then for sign-off is assigned to Person A\\n2. Sign-Off Person A enters a Workflow-Item validating the state and assigns Person B\\n3. Sign-Off Person B enters a Workflow-Item validating the state and assigns to administrative personell that has the close-rights\\n4. Administrative person checks whether all required sign-offs are there and then finally closes; otherwise assigns back\\n### Impact to child resources\\nA resource hierarchically describes a node in a tree-like structure. Therefore we need to look at impact of a closed resource to other nodes in the tree.\\nIf a resource has no children, there is no impact on other resources.\\nIf a resource has children (e.g. subprojects \/ workflowitems), **the resource cannot be closed**. All children need to be in state closed before a parent can be closed.\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n","tokens":69,"id":1216,"Prediction":"## Decision\n\nTo define the abstract representation of resource lifetimes and state-transitions, we will implement a State Machine pattern using a combination of Finite State Machines (FSMs) and Domain-Driven Design (DDD) principles.\n\n## Context\n\nAs mentioned in the context, resources have a defined lifetime and undergo state-transitions until they reach the end of their lifecycle. To achieve this, we need to define these lifetimes and state-transitions in an abstract way, allowing the platform to be adaptable to different use-cases.\n\n## Problem\n\nWithout a standardized approach to modeling resource lifetimes and state-transitions, we risk creating a complex and inflexible system that is difficult to maintain and extend. This could lead to:\n\n* Inconsistent behavior across different resources and use-cases\n* Difficulty in adding new resources or modifying existing ones\n* Increased complexity and maintenance costs\n\n## Decision\n\nTo address these concerns, we will implement a State Machine pattern using a combination of Finite State Machines (FSMs) and Domain-Driven Design (DDD) principles. This will allow us to:\n\n* Define a standardized, abstract representation of resource lifetimes and state-transitions\n* Create a flexible and modular system that can be easily extended and maintained\n* Improve the overall maintainability and scalability of the platform\n\n## Consequences\n\n* The platform will require a more comprehensive understanding of the resource lifecycle and state-transitions\n* The system will need to be designed with a clear separation of concerns, using DDD principles to define the business logic and FSMs to model the state-transitions\n* The implementation will require a more rigorous approach to testing and validation, to ensure that the state machine is correctly implemented and behaves as expected\n\n## Rationale\n\nThe State Machine pattern is well-suited for modeling complex, state-based systems like resource lifetimes and state-transitions. By using a combination of FSMs and DDD principles, we can create a flexible and maintainable system that is easy to extend and adapt to different use-cases. This approach will also allow us to decouple the business logic from the implementation details, making it easier to change or replace individual components without affecting the overall system.","GenTime":"2024-08-10 04:30:57"}
{"File Name":"TruBudget\/0003-project-data-model.md","Context":"## Context\\nWe need to define how to save and query project, subproject and workflow data efficiently.\\n### Definition of terms\\n- Project: Is the root element of the hierarchy. It contains metadata and Subprojects\\n- Subproject: Are a subset of Projects and collect a number of Workflowitems\\n- Workflowitems: Are a number of steps which need to be executed in a certain order\\n- Resources: Projects, Subprojects, Workflowitems are resources. This means they contain permissions, a log and metadata.\\n### Relations\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n### Options\\n1. Each resource has its own stream. Each resource has an array which links to the underlying resources (e.g. A Projects has an array of the ids of its Subrojects). That means if we want to query Subprojects of a specific Project, we need to lookup the Subproject ID's inside the projects to afterswards fetch all the underlying Subproject streams. **Pro:** We are only fetching the data which is necessary, changes inside the resources are storage efficent. **Con:** Resources with a high number of relations result in multiple (maybe hundreds) of queries.\\n2. Each Project has its own stream. All relational resources of the projects are stored inside a JSON and resolved on the API Layer. **Pro:** Only one query necessary to resolve the whole resource tree for one project. **Con:** Large amount of storage needed, since every change inside a single resource would trigger a whole copy of the project tree. Large datastructures have to be kept in memory\\n3. Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource. **Pro:** Easy to query, low storage \/ memory footprint **Con:** None\\n","Decision":"Since we need to be able to scale out to larger projects, it is important to be able to execute fast queries and having a low storage \/ memory footprint. The goal is to exploit the Multichain API as much as possible, since we can expect it to be more performant than our API. Option 3 is the option, which covers the required capabilities in the best way.\\n","tokens":438,"id":1225,"Prediction":"## Decision\n\nWe will choose option 3: Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource.\n\n## Context\n\nWe need to define how to save and query project, subproject, and workflow data efficiently. The data consists of projects, subprojects, workflow items, and resources, which are hierarchical and have relationships with each other.\n\n## Rationale\n\nWe chose option 3 because it offers a good balance between querying efficiency, storage efficiency, and update efficiency. By organizing the relational resources with multiple key-items inside the project stream, we can query specific resources with a single query. This is more efficient than option 1, which would require multiple queries to fetch the data.\n\nAdditionally, updates to resources will only result in the copy of the affected resource, which is more efficient than option 2, which would require copying the whole project tree.\n\nOption 3 also has a low storage and memory footprint, making it a good choice for large datasets.\n\n## Consequences\n\n* We will need to implement a mechanism to organize the relational resources with multiple key-items inside the project stream.\n* We will need to implement a query mechanism that can efficiently query specific resources with a single query.\n* We will need to implement an update mechanism that only copies the affected resource, rather than the whole project tree.\n\nOverall, option 3 provides a good balance between querying efficiency, storage efficiency, and update efficiency, making it the best choice for our use case.","GenTime":"2024-08-10 04:33:59"}
{"File Name":"TruBudget\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1226,"Prediction":"## Decision\n\n**Title:** Use of a Microservices Architecture for the New Project\n\n**Context:** As we embark on a new project, we need to decide on the overall architecture of the system. The project requires scalability, flexibility, and maintainability, and we need to choose an architecture that can support these requirements.\n\n**Decision:** We will use a Microservices Architecture for the new project. This decision is based on the following factors:\n\n* Scalability: Microservices allow us to scale individual services independently, which means we can scale specific parts of the system without affecting the entire system.\n* Flexibility: With microservices, we can develop and deploy services using different programming languages, frameworks, and databases, which gives us more flexibility in terms of technology choices.\n* Maintainability: Microservices make it easier to maintain and update individual services without affecting the entire system, which reduces the risk of introducing bugs or breaking existing functionality.\n* Resilience: Microservices allow us to isolate individual services, which means that if one service fails, it won't bring down the entire system.\n\n**Consequences:**\n\n* Increased complexity: Microservices require more complexity in terms of communication between services, which can be challenging to manage.\n* Higher operational overhead: With microservices, we need to manage multiple services, which can increase operational overhead.\n* Potential for increased latency: Communication between services can introduce latency, which can impact system performance.\n\n**Rationale:** We believe that the benefits of using a Microservices Architecture outweigh the drawbacks. By using microservices, we can build a scalable, flexible, and maintainable system that meets the project's requirements.\n\n**Status:** Accepted\n\n**Implementors:** [List the names of the implementors, if applicable]\n\n**Created:** [Date and time the decision was made]\n\n**Updated:** [Date and time the decision was updated, if applicable]\n\nThis decision record provides a clear and concise summary of the decision, its context, consequences, and rationale. It also provides a record of who made the decision, when it was made, and any updates to the decision.","GenTime":"2024-08-10 04:34:22"}
{"File Name":"jupyter-nbrequirements\/0000-dependencies-management-jupyter-notebooks.md","Context":"## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component\/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n","Decision":"* user prospective\\n* reproducibility\\n* traecability\\nThe option selected is 3. because:\\n* enforce reproducibility\\n* enforce traceability between notebook\\n### Positive Consequences <!-- optional -->\\n* Satisfy reproducibility, traecability, shareability.\\n* Notebooks are coupled with dependencies in their metadata.\\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\\n","tokens":140,"id":1228,"Prediction":"## Decision\n\nTo guarantee reproducibility of Jupyter Notebooks, we will store each notebook with its direct and transitive dependencies as part of the notebook's metadata. This will ensure that each notebook can be run with the exact same dependencies, regardless of the environment it is run in.\n\n## Context\n\nAs discussed, the goal is to allow any user to re-run a Jupyter Notebook with similar behavior, which requires including direct and transitive dependencies in the notebook's requirements. This approach will not only ensure reproducibility but also support security, traceability, and the ability to build images to run the notebooks or share them with others.\n\n## Decision Drivers\n\n* User prospective: By storing dependencies with the notebook, users will be able to run the notebook with the exact same environment, without worrying about compatibility issues or missing dependencies.\n* Reproducibility: This approach ensures that the notebook can be run in any environment, with the same dependencies, to produce the same results.\n* Traecability: Storing dependencies with the notebook provides a clear record of the dependencies used, making it easier to track changes and identify issues.\n\n## Status\n\nApproved\n\n## Affected Components\n\n* Jupyter Notebooks\n* Notebook metadata storage\n* Dependency management system\n\n## Changelog\n\n* [Insert changelog entries]\n\n## Rationale\n\nStoring dependencies with the notebook is the most effective way to ensure reproducibility, as it guarantees that the notebook can be run with the exact same dependencies, regardless of the environment. This approach also supports security and traceability by providing a clear record of the dependencies used.\n\n## Pros and Cons\n\nPros:\n\n* Ensures reproducibility\n* Supports security and traceability\n* Allows for easy sharing and reuse of notebooks\n\nCons:\n\n* Increased storage requirements\n* Additional complexity in dependency management\n\n## Implementation\n\nThe implementation will involve modifying the Jupyter Notebook metadata storage to include direct and transitive dependencies. The dependency management system will be updated to manage these dependencies and ensure that they are included with the notebook.","GenTime":"2024-08-10 04:35:10"}
{"File Name":"WorkoutLog\/0009_one_time_events_livedata.md","Context":"## Context\\nLiveData events will be pushed again after certain LifeCycle events. Ie. when rotating the device or\\nafter returning to a fragment from another navigation path. Errors or special events like automatic\\nnavigation should only be executed once.\\n","Decision":"A new event will be introduced for the ViewModels which has the clear purpose of only occur once.\\n","tokens":49,"id":1231,"Prediction":"## Decision\n\nTo address the issue of LiveData events being pushed again after certain LifeCycle events, and to ensure that errors or special events are only executed once, we will implement a caching mechanism to store the last processed event.\n\n## Context\n\nAs you mentioned, LiveData events will be pushed again after certain LifeCycle events, such as device rotation or returning to a fragment from another navigation path. This can lead to duplicate event handling and errors or special events being executed multiple times. To mitigate this issue, we need a solution that ensures events are only processed once.\n\n## Problem\n\n* Duplicate event handling can lead to inconsistent behavior and errors.\n* Special events like automatic navigation should only be executed once to avoid unintended consequences.\n\n## Decision\n\nWe will implement a caching mechanism using a simple in-memory data structure (e.g., a HashMap) to store the last processed event. When a new event is received, we will check if the event is the same as the last processed event. If it is, we will skip processing the event to avoid duplicates.\n\n## Rationale\n\nThis approach ensures that events are only processed once, even in the presence of LifeCycle events that cause the fragment to be recreated. The caching mechanism is simple and easy to implement, and it does not introduce significant overhead or complexity.\n\n## Consequences\n\n* This approach assumes that the event processing logic is idempotent, meaning that processing the same event multiple times does not have unintended consequences.\n* This approach may not work if the event processing logic is not idempotent, or if the event data is not easily comparable.\n* This approach may require additional error handling to account for cases where the caching mechanism fails or is not effective.\n\n## Alternatives Considered\n\n* We considered implementing a more complex caching mechanism using a database or a cache library, but this would introduce additional complexity and overhead.\n* We considered using a flag or a boolean variable to track whether an event has been processed, but this would not work in cases where the fragment is recreated due to LifeCycle events.\n\n## Status\n\nThis decision is approved and will be implemented in the next release.","GenTime":"2024-08-10 04:36:11"}
{"File Name":"stamper\/0007-docker.md","Context":"## Context\\nWe need to choose how we will deploy our app.\\n","Decision":"We use Docker because it allows us to isolate app with a container and easily deploy to the server with Terraform.\\n","tokens":15,"id":1239,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to deploy our app using a containerization platform, specifically Docker, in conjunction with a cloud-based platform, such as AWS Elastic Container Service (ECS).\n\n## Rationale\n\nWe chose Docker for several reasons:\n\n* **Portability**: Docker allows us to package our application and its dependencies into a single container, making it easy to deploy and run on any platform that supports Docker.\n* **Isolation**: Docker provides a high level of isolation between containers, ensuring that each container runs in its own isolated environment and does not interfere with other containers.\n* **Efficient resource utilization**: Docker containers use fewer resources than traditional virtual machines, making them more efficient and cost-effective.\n\nWe chose AWS ECS for several reasons:\n\n* **Scalability**: AWS ECS provides scalable and high-availability architecture, allowing us to easily scale our application up or down as needed.\n* **Integration with other AWS services**: AWS ECS integrates well with other AWS services, such as Amazon RDS, Amazon S3, and Amazon Elastic Load Balancer, making it easy to build a comprehensive and scalable architecture.\n* **Security**: AWS ECS provides built-in security features, such as network isolation and access controls, to ensure the security and integrity of our application.\n\n## Consequences\n\nBy choosing Docker and AWS ECS, we can:\n\n* **Simplify deployment and management**: With Docker and AWS ECS, we can automate the deployment and management of our application, reducing the risk of human error and increasing efficiency.\n* **Improve scalability and reliability**: By using a scalable and high-availability architecture, we can ensure that our application is always available and can handle increased traffic and demand.\n* **Enhance security**: By using a secure containerization platform and cloud-based infrastructure, we can ensure the security and integrity of our application.\n\nHowever, we also acknowledge that:\n\n* **Additional complexity**: Using Docker and AWS ECS may require additional expertise and training for our development and operations teams.\n* **Cost**: While Docker and AWS ECS can provide cost-effective solutions, they may also require additional costs for infrastructure and support.\n\nOverall, we believe that the benefits of using Docker and AWS ECS outweigh the costs and complexities, and we are confident that this decision will enable us to build a scalable, secure, and efficient application.","GenTime":"2024-08-10 04:39:00"}
{"File Name":"stamper\/0004-bootsptrap-as-frontend-framework.md","Context":"## Context\\nWe need to choose a frontend framework.\\n","Decision":"We don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\\n","tokens":12,"id":1240,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use React as our frontend framework for the project.\n\n## Context\n\nAs we need to choose a frontend framework, we have evaluated several options including Angular, Vue.js, and React. The choice of framework will have a significant impact on the project's development speed, maintainability, and scalability.\n\n## Problem\n\nThe main problem we are trying to solve is to find a framework that can efficiently handle the complex user interface requirements of our application, while also being easy to learn and maintain for our development team.\n\n## Reasoning\n\nWe have chosen React for several reasons:\n\n* **Component-based architecture**: React's component-based architecture aligns well with our project's requirements, allowing us to break down the UI into smaller, reusable components.\n* **Large community and ecosystem**: React has a massive community and a rich ecosystem of tools and libraries, which will make it easier for us to find solutions to common problems and integrate with other services.\n* **Fast development and iteration**: React's virtual DOM and one-way data binding make it an ideal choice for rapid prototyping and iteration, which is critical for our project's development timeline.\n* **Easy to learn and maintain**: React's syntax is easy to learn, and its ecosystem is well-documented, making it easier for our development team to get up to speed quickly.\n\n## Consequences\n\nBy choosing React, we expect to:\n\n* **Improve development speed**: React's component-based architecture and fast development cycle will enable us to develop the UI quickly and efficiently.\n* **Enhance maintainability**: React's modular design and large community will make it easier for us to find and integrate with libraries and tools that meet our project's requirements.\n* **Simplify testing**: React's virtual DOM and one-way data binding make it easier to write unit tests and integrate with other testing frameworks.\n\n## Status\n\nThis decision is currently in the \"accepted\" status, and we will begin implementing React in our project immediately.","GenTime":"2024-08-10 04:39:21"}
{"File Name":"stamper\/0003-spring-boot-as-web-framework.md","Context":"## Context\\nWe need to choose a web framework for the app.\\n","Decision":"We use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\\n","tokens":15,"id":1243,"Prediction":"## Decision\n\nAfter careful consideration of the requirements and constraints, we have decided to use Flask as the web framework for our application.\n\n## Context\n\nWe need to choose a web framework for the app.\n\n## Problem\n\nChoosing the right web framework is crucial for the success of our application. We need a framework that is scalable, flexible, and easy to maintain. Our team has limited experience with web development, and we want a framework that has a large community and plenty of resources available.\n\n## Decision\n\nWe have decided to use Flask as our web framework for the following reasons:\n\n* Lightweight and flexible: Flask is a microframework, which means it is lightweight and flexible, making it easy to learn and use.\n* Large community: Flask has a large and active community, which means there are plenty of resources available, including documentation, tutorials, and third-party libraries.\n* Scalable: Flask is designed to be scalable, making it suitable for large and complex applications.\n* Easy to maintain: Flask has a simple and modular design, which makes it easy to maintain and extend.\n\n## Consequences\n\nBy choosing Flask as our web framework, we can:\n\n* Quickly develop and deploy our application\n* Easily maintain and extend our application\n* Take advantage of the large community and resources available\n* Scale our application as needed\n\n## Rationale\n\nWe considered other web frameworks, such as Django and Pyramid, but ultimately decided on Flask due to its simplicity and flexibility. Flask's microframework design allows us to build a custom application that meets our specific needs, rather than being forced into a particular architecture or design pattern.\n\n## References\n\n* Flask documentation: <https:\/\/flask.palletsprojects.com\/en\/2.0.x\/>\n* Flask community: <https:\/\/flask.palletsprojects.com\/en\/2.0.x\/community\/>","GenTime":"2024-08-10 04:40:32"}
{"File Name":"stamper\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1244,"Prediction":"Here is a sample Architectural Decision Record (ADR) for your project:\n\n**Decision**\n\n**Title**: Implement a Microservices Architecture for the Project\n\n**Status**: Accepted\n\n**Context**: As our project grows in complexity and scale, we need to consider a more scalable and maintainable architecture to ensure the system's reliability and flexibility. The current monolithic architecture is becoming a bottleneck, and we need to adopt a more distributed approach to meet the increasing demands.\n\n**Problem**: The current monolithic architecture is:\n\n* Difficult to scale and maintain\n* Prone to single points of failure\n* Limited in terms of flexibility and adaptability\n* Not easily testable and debuggable\n\n**Decision**: We will implement a microservices architecture for the project, where each service is responsible for a specific business capability and can be developed, tested, and deployed independently.\n\n**Rationale**: A microservices architecture will:\n\n* Allow for greater scalability and flexibility\n* Enable independent development and deployment of services\n* Improve fault tolerance and reduce the risk of single points of failure\n* Simplify testing and debugging\n\n**Consequences**:\n\n* Increased complexity in terms of service communication and orchestration\n* Need for a service registry and discovery mechanism\n* Potential for increased overhead due to network calls and serialization\/deserialization\n\n**Decision Date**: 2023-02-20\n\n**Responsible**: [Your Name]\n\n**Discussion**: [Any relevant discussion or debate that took place]\n\n**Links**: [Any relevant links to documentation, research, or other resources]\n\nThis is just a sample, and you can modify it to fit your specific needs and project requirements. Remember to keep your ADR concise, clear, and easy to understand.","GenTime":"2024-08-10 04:40:51"}
{"File Name":"buy-for-your-school\/0005-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":1246,"Prediction":"## Decision\n\n**Title:** Implement a query logging mechanism to detect inefficient database queries\n\n**Context:** It can be easy to miss an inefficient database query during code review. These can build up and have detrimental performance on the application and effect the user experience.\n\n**Problem:** Inefficient database queries can lead to slow application performance, increased resource usage, and poor user experience. Manual code reviews may not always catch these issues, and they can go unnoticed until they become a significant problem.\n\n**Decision:** We will implement a query logging mechanism to detect inefficient database queries. This will allow us to identify and address these issues proactively, ensuring better performance and user experience.\n\n**Rationale:**\n\n1. **Improved performance monitoring**: A query logging mechanism will provide visibility into database query performance, enabling us to identify and optimize inefficient queries.\n2. **Proactive issue detection**: By logging queries, we can detect potential issues before they become significant problems, reducing the likelihood of performance degradation and user complaints.\n3. **Code review assistance**: Query logging will aid code reviews by providing a clear record of query performance, making it easier to identify and address inefficient queries.\n\n**Consequences:**\n\n1. **Additional logging complexity**: Implementing a query logging mechanism will require additional development effort and infrastructure setup.\n2. **Potential performance impact**: Logging queries may introduce a small performance overhead, which will need to be carefully managed to ensure it does not compromise application performance.\n\n**Status:** Accepted\n\n**Implementation:** We will implement a query logging mechanism using a combination of database query logging and application-level logging. This will involve:\n\n1. Configuring database query logging to capture query performance metrics.\n2. Implementing application-level logging to capture query execution times and other relevant metrics.\n3. Integrating the query logging mechanism with our monitoring and analytics tools to provide visibility into query performance.\n\n**Monitoring and Review:** We will regularly review query logs to identify inefficient queries and optimize them as needed. This will ensure that our application remains performant and user-friendly.","GenTime":"2024-08-10 04:41:36"}
{"File Name":"buy-for-your-school\/0013-use-dfe-sign-in-as-auth-provider.md","Context":"## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https:\/\/github.com\/DFE-Digital\/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n","Decision":"We are going to use DSI as our single sign-on provider using the OIDC standard.\\n","tokens":126,"id":1252,"Prediction":"## Decision\n\nAfter careful consideration of the requirements and constraints, we have decided to use the OpenID Connect (OIDC) protocol for authentication, specifically the DfE Sign-in (DSI) service, as recommended by DfE governance.\n\n## Context\n\nAs mentioned in the context, the service requires a way to authenticate trusted school buying professionals and restrict access to the public. A simpler password-less authentication mechanism is desired, and DfE governance has specified the use of DfE Sign-in (DSI) for this service.\n\n## Problem\n\nThe main problem is to find an authentication mechanism that meets the requirements of simplicity, modern security standards, and compliance with DfE governance recommendations.\n\n## Decision\n\nWe have decided to use OIDC, a widely adopted and modern authentication protocol, to authenticate users. OIDC provides a simple and secure way to authenticate users without the need for passwords. By using OIDC, we can ensure that our service meets the security and compliance requirements.\n\n## Rationale\n\nThe main rationale for this decision is that OIDC is a widely adopted and well-established standard for authentication. It provides a simple and secure way to authenticate users without the need for passwords. Additionally, OIDC is compatible with the DfE Sign-in (DSI) service, which is recommended by DfE governance.\n\n## Consequences\n\nThe consequences of this decision are:\n\n* The service will use OIDC for authentication, providing a simple and secure way to authenticate users.\n* The service will be compliant with DfE governance recommendations.\n* The service will not require the storage of passwords, reducing the risk of password-related security breaches.\n\n## Alternatives\n\nWe considered the following alternatives:\n\n* Password-based authentication: This would require the storage of passwords, which increases the risk of security breaches.\n* Custom authentication mechanism: This would require significant development effort and may not be as secure as a widely adopted standard like OIDC.\n\n## References\n\n* OpenID Connect (OIDC) specification: https:\/\/openid.net\/connect\/\n* DfE Sign-in (DSI) technical guidance: https:\/\/github.com\/DFE-Digital\/technical-guidance","GenTime":"2024-08-10 04:43:32"}
{"File Name":"buy-for-your-school\/0006-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":1254,"Prediction":"## Decision\n\n**Title:** Centralized Environment Variable Management with a Wrapper\n\n**Context:** As discussed, accessing ENV directly without a wrapper is limited and can introduce problems. We want our tooling to help guard against missing environment variables and fail fast with an explicit message when `nil` is accidentally provided. We also want to be able to stub our environment variables in our test suite.\n\n**Decision:** We will implement a centralized environment variable wrapper using a supported gem, such as `dotenv`. This wrapper will provide a simple and robust way to manage environment variables, allowing us to:\n\n1. Fail fast with an explicit message when `nil` is accidentally provided.\n2. Stub environment variables in our test suite, ensuring we can test both scenarios without being influenced by real values being loaded.\n\n**Rationale:**\n\n* Using a supported gem like `dotenv` ensures we get support in the form of fixes and security patches.\n* A centralized wrapper provides a single point of truth for environment variable management, making it easier to maintain and debug.\n* Stubbing environment variables in our test suite allows us to test both scenarios without being influenced by real values being loaded, reducing the risk of unexpected consequences.\n\n**Consequences:**\n\n* We will need to refactor our code to use the new environment variable wrapper instead of accessing ENV directly.\n* We will need to update our test suite to use the wrapper for stubbing environment variables.\n* We will need to ensure that the wrapper is properly configured and integrated with our existing codebase.\n\n**Status:** Accepted\n\n**Implementor:** [Name of the implementor]\n\n**Date:** [Date of the decision]\n\n**Links:** [Link to the gem documentation, if applicable]","GenTime":"2024-08-10 04:44:08"}
{"File Name":"buy-for-your-school\/0020-use-accessible-autocomplete-for-autocomplete-fields.md","Context":"## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n","Decision":"We will use [accessible-autocomplete](https:\/\/github.com\/alphagov\/accessible-autocomplete) to provide the autocomplete capability in our pages.\\nThis package has been chosen because accessibility has been carefully considered when developing the package.\\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\\nand not be jarring to the user.\\n","tokens":26,"id":1255,"Prediction":"## Decision\n\n**Autocomplete Functionality: Using a Library vs. Implementing from Scratch**\n\n## Context\n\nAs per the requirement, we need to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\n\n## Problem\n\nWe need to decide whether to use an existing library to implement autocomplete functionality or to implement it from scratch.\n\n## Decision\n\nAfter considering the pros and cons of both options, we have decided to use an existing library to implement autocomplete functionality. This decision is based on the following reasons:\n\n* Time-to-market: Using an existing library will save us time and effort, allowing us to quickly implement the autocomplete functionality and get it to the users.\n* Maintenance: The library will be maintained by its authors, which means we won't have to worry about keeping up with bug fixes and updates.\n* Scalability: The library has been tested and proven to handle large datasets, which is essential for our application.\n* Reusability: We can reuse the library in other parts of the application where autocomplete functionality is required.\n\n## Consequences\n\n* We will need to choose a suitable library that meets our requirements and integrate it into our application.\n* We will need to ensure that the library is properly configured and customized to fit our application's needs.\n* We will need to test the autocomplete functionality thoroughly to ensure it works as expected.\n\n## Rationale\n\nUsing an existing library will allow us to focus on other critical aspects of the application while still providing a high-quality autocomplete experience to the users.","GenTime":"2024-08-10 04:44:24"}
{"File Name":"buy-for-your-school\/0009-use-simplecov-to-monitor-code-test-coverage.md","Context":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n","Decision":"Use Simplecov with RSpec to monitor coverage changes on every test run\\n","tokens":32,"id":1258,"Prediction":"## Decision\n\nTo achieve high test coverage without relying on manual checks, we will implement an automated testing framework that integrates with our CI\/CD pipeline. This will ensure that our tests are run automatically and continuously, providing immediate feedback on the quality of our code.\n\n## Context\n\nWe want to keep our test coverage as high as possible without having to run manual checks as these take time and are easy to forget.\n\n## Decision Drivers\n\n* High test coverage is essential for maintaining the quality and reliability of our software.\n* Manual testing is time-consuming and prone to human error, which can lead to missed bugs and defects.\n* Automated testing allows us to catch issues early and often, reducing the likelihood of downstream problems.\n* Integration with the CI\/CD pipeline ensures that testing is a seamless part of our development process.\n\n## Alternatives Considered\n\n* Manual testing: While this approach is simple and easy to implement, it is time-consuming and prone to human error.\n* Manual testing with checklists: This approach provides some structure, but it still relies on human memory and can be easily forgotten.\n* Automated testing with a custom framework: This approach would require significant development effort and may not be feasible within our current timeline.\n\n## Consequences\n\n* Implementation of an automated testing framework will require additional development effort and resources.\n* Integration with the CI\/CD pipeline will require configuration and testing to ensure seamless execution.\n* High test coverage will provide immediate feedback on the quality of our code, reducing the likelihood of downstream problems.\n\n## Rationale\n\nAutomated testing with integration into our CI\/CD pipeline is the best approach to achieve high test coverage without relying on manual checks. This approach provides immediate feedback on the quality of our code, reduces the likelihood of downstream problems, and ensures that testing is a seamless part of our development process.","GenTime":"2024-08-10 04:45:28"}
{"File Name":"libelektra\/cryptograhic_key_handling.md","Context":"## Problem\\nThe crypto plugin applies cryptographic operations to Keys and KeySets.\\nIn order to do that it needs keys and initialization vectors (IV).\\nThe problem is how to retrieve or derivate those keys in a safe way and how\\nto pass them on to the underlying crypto libraries (OpenSSL and libgcrypt\\nat the time of writing).\\n","Decision":"### General Approach\\nThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.\\nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).\\nThis way we can safely derivate cryptographic keys for symmetric value encryption.\\nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).\\nThe PBKDF2 needs an iteration number and a salt in order to work.\\nThose values will be stored per Key as MetaKey.\\n### Implementation Details\\nDuring the **mount phase** a random master password _r_ is being generated. _r_ is sent to the gpg binary for encryption. The resulting encrypted master password _m_ is stored in the plugin configuration at `config\/masterChallenge`.\\nDuring the **set phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.\\nDuring the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be decrypted. The salt _s(k)_ is read from the encrypted message. By applying the PBKDF2 with _r_ and _s(k)_ the values of _e(k)_ and _i(k)_ are restored. Then the encrypted message can be decrypted.\\n","tokens":73,"id":1270,"Prediction":"## Decision\n\n**Decision:** To use a Key Manager to securely retrieve and derive keys, and to pass them to the underlying crypto libraries.\n\n## Context\n\nThe crypto plugin needs to apply cryptographic operations to Keys and KeySets, which requires retrieving or deriving keys and initialization vectors (IV) in a safe way. The plugin currently uses OpenSSL and libgcrypt as underlying crypto libraries. The problem is how to securely retrieve and derive these keys and pass them on to these libraries.\n\n## Rationale\n\nUsing a Key Manager provides several benefits:\n\n* It ensures that keys are securely stored and retrieved, reducing the risk of unauthorized access or tampering.\n* It allows for key derivation and wrapping, which is necessary for secure key storage and transmission.\n* It provides a centralized point for key management, making it easier to manage and rotate keys.\n* It decouples the crypto plugin from the underlying crypto libraries, making it easier to switch to different libraries or implementations in the future.\n\n## Consequences\n\n* The crypto plugin will need to be modified to use the Key Manager API instead of directly accessing the underlying crypto libraries.\n* The Key Manager will need to be implemented and integrated into the system.\n* The system will need to be designed to handle key rotation and revocation, which will require additional infrastructure and processes.\n\n## Alternatives Considered\n\n* Directly accessing the underlying crypto libraries and storing keys in a simple data structure.\n* Using a separate key storage mechanism, such as a file or database, and passing the keys to the crypto libraries manually.\n* Implementing a custom key derivation and wrapping mechanism within the crypto plugin.\n\nHowever, these alternatives do not provide the same level of security and manageability as using a Key Manager, and may introduce additional complexity and vulnerabilities.","GenTime":"2024-08-10 04:49:47"}
{"File Name":"libelektra\/empty_files.md","Context":"## Problem\\nAn empty KeySet is passed to kdbSet(). What is the correct persistent\\nrepresentation?\\n","Decision":"Remove files on empty KeySet.\\n","tokens":23,"id":1271,"Prediction":"## Decision\n\nIn the context of an empty KeySet being passed to kdbSet(), the correct persistent representation should be a null or empty string.\n\n## Context\n\nWhen an empty KeySet is passed to kdbSet(), it is important to determine the correct persistent representation to ensure data integrity and consistency. The persistent representation should accurately reflect the state of the KeySet, which in this case is empty.\n\n## Rationale\n\nA null or empty string is the most straightforward and consistent way to represent an empty KeySet in a persistent format. This approach ensures that the data is easily readable and writable, and it is also easy to check for emptiness when loading the data back into the program.\n\n## Consequences\n\nUsing a null or empty string to represent an empty KeySet has several consequences:\n\n* It simplifies the data storage and retrieval process, as it eliminates the need for complex data structures or additional logic to handle empty sets.\n* It ensures data consistency, as the same empty KeySet will always be represented in the same way.\n* It makes it easy to check for emptiness when loading the data back into the program, which can help prevent errors and improve program stability.\n\n## Alternatives\n\nAn alternative approach could be to use a special value, such as a sentinel value or a magic number, to represent an empty KeySet. However, this approach can lead to complexity and errors, as it requires additional logic to handle the special value.\n\n## Status\n\nThis decision is approved and will be implemented in the software.\n\n## Links\n\n* Relevant code: kdbSet() function\n* Related issues: #123, #456","GenTime":"2024-08-10 04:50:05"}
{"File Name":"libelektra\/highlevel_help_message.md","Context":"## Problem\\nWe want to allow to print the help message no matter what errors happened in `kdbOpen` or `kdbGet`.\\n","Decision":"Ignore missing `require`d keys (in help mode), but fail for every other error.\\n","tokens":31,"id":1274,"Prediction":"## Decision\n\nTo allow printing the help message no matter what errors happened in `kdbOpen` or `kdbGet`, we will introduce a global error handling mechanism that catches and re-throws any exceptions that occur during the execution of these functions. This will ensure that the help message is printed regardless of whether an error occurs.\n\n## Context\n\nWe are currently implementing a software that uses the `kdbOpen` and `kdbGet` functions to interact with a database. These functions are critical to the functionality of our software, and it is essential to provide a robust error handling mechanism to ensure that our software remains stable and user-friendly in the event of errors.\n\n## Rationale\n\nIntroducing a global error handling mechanism will allow us to catch and re-throw any exceptions that occur during the execution of `kdbOpen` or `kdbGet`. This will enable us to print the help message regardless of whether an error occurs, providing users with a clear and concise message on how to use our software.\n\n## Consequences\n\nBy introducing a global error handling mechanism, we will be able to:\n\n* Provide a consistent user experience by printing the help message in the event of an error\n* Ensure that our software remains stable and user-friendly in the event of errors\n* Reduce the complexity of our code by centralizing error handling in a single location\n\nHowever, this approach may also introduce some overhead in terms of exception handling and re-throwing, which may impact the performance of our software.\n\n## Solution\n\nWe will introduce a global error handling mechanism using a try-except block that catches and re-throws any exceptions that occur during the execution of `kdbOpen` or `kdbGet`. We will then print the help message using the `print_help_message` function.\n\nHere is an example of how this could be implemented:\n```\ntry:\n    # Call kdbOpen or kdbGet\n    result = kdbOpen()\nexcept Exception as e:\n    print_help_message()\n    raise\n```\nBy introducing this global error handling mechanism, we will be able to ensure that the help message is printed regardless of whether an error occurs, providing users with a clear and concise message on how to use our software.","GenTime":"2024-08-10 04:51:11"}
{"File Name":"libelektra\/spec_metadata.md","Context":"## Problem\\nTo make storage-plugins suitable for `spec` they need to be able to store\\nall the metadata as specified in [METADATA.ini](\/doc\/METADATA.ini).\\nMost file formats do not have support for that.\\nIf metadata is merged from different namespaces, e.g., `spec:` and `user:`,\\nmetadata from one namespace might end up in keys of other namespaces, e.g.,\\nmetadata from `spec:` might end up in `user:`.\\n","Decision":"Do not store metadata unrelated to the configuration file structure in any namespace except in `spec:\/`.\\n- Trying to store any other metadata in any other namespace leads to an error.\\nE.g. `kdb set-meta user:\/data metadata_not_suitable_for_storage_plugins something` would fail\\n(validated by `spec` plugin).\\n- Metadata that is designed to be stored by storage plugins to preserve configuration file structure.\\nE.g. `comment` or `order`, might be stored in any namespace.\\nSometimes, the same metadata can be used in several namespaces but with different meanings\\nand ways of serialization, e.g. `type` and `array`:\\n- In `spec:\/` the metadata `array=` (empty value) means \"this is an array\".\\nIf you give it a value e.g. `array=#4` it means \"this is an array with default size X\" (e.g. `#4` = size 5).\\n- In any other namespace `array=` means \"this is an empty array\" and e.g. `array=#4` means \"this is an array with max index #4\".\\n`array=#4` is not stored literally but inferred.\\n- Either the storage plugin does not support arrays, then the metadata will be discarded\\non `kdbSet` but `spec` will keep on adding it for every `kdbGet`.\\n- Or, if the storage plugin supports arrays, the data will be serialized as array\\n(even if the metadata comes from `spec`) and as such available in the next `kdbGet`\\nfrom the storage plugin to be validated by `spec`.\\nUse different storage plugins, or plugins with different configurations,\\nfor the `spec:\/` namespace:\\n- `ni`\\n- TOML with `meta` configuration\\nThe `kdb mount` tool will add the `meta` plugin configuration when mounting a storage plugin to `spec:\/`.\\n- [Spec Expressiveness](spec_expressiveness.md)\\n- [Arrays](array.md)\\n","tokens":103,"id":1278,"Prediction":"## Decision\n\nTo address the problem of storing metadata as specified in [METADATA.ini] and handling merged metadata from different namespaces, we will implement a custom metadata storage system using a key-value store (KVS) that can handle nested keys and namespace-specific metadata.\n\n## Context\n\nThe current storage-plugins do not have built-in support for storing metadata as specified in [METADATA.ini]. This makes it difficult to store and retrieve metadata correctly. Additionally, when metadata is merged from different namespaces, it can lead to conflicts and inconsistencies in the stored metadata.\n\n## Decision Rationale\n\nWe chose to implement a custom metadata storage system using a KVS because it provides a flexible and efficient way to store and retrieve metadata. A KVS allows us to store metadata as a collection of key-value pairs, which can be easily extended to handle nested keys and namespace-specific metadata.\n\nUsing a KVS also allows us to decouple the metadata storage from the specific storage-plugin implementation, making it easier to switch or replace storage-plugins in the future.\n\n## Consequences\n\n* The custom metadata storage system will require additional development and testing to ensure it meets the requirements specified in [METADATA.ini].\n* The KVS will need to be implemented and integrated with the existing storage-plugins.\n* The custom metadata storage system will provide a flexible and efficient way to store and retrieve metadata, making it easier to handle merged metadata from different namespaces.\n\n## Pros and Cons\n\nPros:\n\n* Provides a flexible and efficient way to store and retrieve metadata\n* Decouples metadata storage from storage-plugin implementation\n* Allows for easy handling of merged metadata from different namespaces\n\nCons:\n\n* Requires additional development and testing\n* Adds complexity to the system\n* May require changes to existing storage-plugins to integrate with the custom metadata storage system","GenTime":"2024-08-10 04:52:41"}
{"File Name":"libelektra\/cmake_spec.md","Context":"## Problem\\nThe compilation variants of plugins blow up the number of plugins.\\nAdditionally there is the concept of default storage + resolver that is\\nneeded for bootstrapping plugins.\\n","Decision":"Rejected: keep default plugins as-is\\n","tokens":37,"id":1279,"Prediction":"## Decision\n\nTo address the issue of compilation variants of plugins blowing up the number of plugins, and to simplify the concept of default storage and resolver for bootstrapping plugins, we will introduce a plugin manager that provides a unified interface for managing and bootstrapping plugins.\n\n## Context\n\nThe current implementation of plugins in our software has several compilation variants, which has resulted in a large number of plugins being created. This has made it difficult to manage and maintain the plugins, and has also introduced complexity in the bootstrapping process. Additionally, the concept of default storage and resolver is not clearly defined, which has led to confusion and inconsistencies in the implementation.\n\n## Solution\n\nTo address these issues, we will introduce a plugin manager that provides a unified interface for managing and bootstrapping plugins. The plugin manager will be responsible for:\n\n* Providing a single entry point for plugins to register themselves\n* Managing the compilation variants of plugins and ensuring that only one instance of each plugin is loaded\n* Providing default storage and resolver implementations for bootstrapping plugins\n* Providing a mechanism for plugins to declare dependencies on other plugins\n* Providing a mechanism for plugins to specify their own storage and resolver implementations\n\nThe plugin manager will be designed to be extensible and flexible, allowing developers to easily add new plugins and customize the behavior of the plugin manager.\n\n## Consequences\n\nThe introduction of a plugin manager will have several consequences, including:\n\n* Simplification of the plugin bootstrapping process\n* Reduction in the number of plugins and compilation variants\n* Improved manageability and maintainability of plugins\n* Improved flexibility and extensibility of the plugin system\n* Potential for improved performance and scalability\n\nHowever, the introduction of a plugin manager will also require significant changes to the existing codebase and may introduce some complexity and overhead. Therefore, careful consideration and planning will be necessary to ensure a smooth transition to the new plugin manager.","GenTime":"2024-08-10 04:53:02"}
{"File Name":"libelektra\/internal_cache.md","Context":"## Problem\\nWhen doing kdbGet() possible more keys are returned which might be\\nconfusing. When doing a second kdbGet() with a new keyset\\nno keys might be returned, because it is up-to-date.\\nWhen doing kdbSet() a deep duplication is needed.\\nIdea: keep a duplicated keyset internally. Return (non-deep?)\\nduplications exactly of the parts as requested.\\n","Decision":"Not yet decided.\\n- [Global Validation](global_validation.md)\\n","tokens":90,"id":1289,"Prediction":"## Decision\n\nTo address the issues with `kdbGet()` returning unexpected keys, `kdbGet()` returning no keys when a new keyset is used, and the need for deep duplication when using `kdbSet()`, we will implement an internal duplicated keyset. This keyset will be updated whenever a new key is added or removed.\n\n## Context\n\nThe current implementation of `kdbGet()` and `kdbSet()` in our software is causing confusion and complexity. When `kdbGet()` is called, it returns all possible keys, which can be overwhelming. When a new keyset is used in a subsequent `kdbGet()` call, no keys are returned because the internal state is up-to-date. Additionally, when using `kdbSet()`, a deep duplication of the keyset is required, which can be inefficient and error-prone.\n\n## Decision Rationale\n\nBy maintaining an internal duplicated keyset, we can ensure that `kdbGet()` returns only the requested keys, without including any unnecessary keys. This will simplify the API and make it easier to use. Additionally, the internal keyset will be updated whenever a new key is added or removed, ensuring that `kdbGet()` always returns the correct keys.\n\n## Pros and Cons\n\nPros:\n\n* Simplifies the API and makes it easier to use\n* Reduces the complexity of the code\n* Improves the performance of `kdbGet()` by returning only the requested keys\n\nCons:\n\n* Requires additional memory to store the internal duplicated keyset\n* May introduce additional complexity in the implementation of `kdbSet()` and `kdbGet()`\n\n## Alternatives Considered\n\nAlternative 1: Do not use an internal duplicated keyset and instead return all possible keys in `kdbGet()`. This would simplify the implementation of `kdbGet()` but would still cause confusion and complexity.\n\nAlternative 2: Use a shallow duplication of the keyset in `kdbSet()`. This would reduce the complexity of `kdbSet()` but would still require deep duplication in some cases.\n\nAlternative 3: Implement a more complex algorithm to determine which keys to return in `kdbGet()`. This would be more complex and may still not provide the desired results.\n\n## Implementation\n\nThe internal duplicated keyset will be implemented as a separate data structure that is updated whenever a new key is added or removed. The `kdbGet()` function will use this internal keyset to determine which keys to return. The `kdbSet()` function will use the internal keyset to perform the deep duplication.\n\n## Verification\n\nThe implementation will be verified by testing the `kdbGet()` and `kdbSet()` functions with various input scenarios. The tests will ensure that the internal duplicated keyset is updated correctly and that `kdbGet()` returns only the requested keys.","GenTime":"2024-08-10 04:56:35"}
{"File Name":"libelektra\/store_name.md","Context":"## Problem\\nMemory Consumption in Elektra is quite high as the key names\\nare long and stored twice in memory.\\n","Decision":"Only store the unescaped key name, suitable for comparing\/searching\/iterating over name, i.e.:\\n- Remove the escaped name from `struct _Key` and use it only when necessary.\\n- Clarify and reduce [terminology](\/doc\/help\/elektra-glossary.md).\\n- API Changes:\\n- `keyNew (const char*, size_t)`\\n- `keyName` returns the unescaped name\\n- remove `keyUnescapedName`, `keyGetUnescapedNameSize`.\\n- reverse terminology: with \"key name\" we will refer to the unescaped (base) name,\\nthe escaped name will be explicitly called \"escaped key name\".\\n- escaped name will be outside the core for tooling\\n- `keyDup(.., int)` with options to filter which parts are copied\\n(to allow copy of keys where only the key name is copied)\\n","tokens":26,"id":1291,"Prediction":"## Decision\n\nTo reduce memory consumption in Elektra, we will introduce a caching mechanism to store key names only once in memory.\n\n## Context\n\nAs mentioned in the problem, Elektra's current implementation stores key names twice in memory, leading to high memory consumption. This is particularly problematic when dealing with long key names, which can result in significant memory usage. To address this issue, we need a solution that minimizes memory usage while still providing efficient access to key names.\n\n## Solution\n\nWe will implement a caching mechanism that stores key names in a hash table. This will allow us to store each key name only once in memory, reducing the overall memory consumption. When a key name is requested, we will check the cache first before retrieving it from the original storage location. This approach will provide a significant reduction in memory usage while maintaining the same level of performance.\n\n## Rationale\n\nThe caching mechanism will provide several benefits:\n\n1. Reduced memory consumption: By storing key names only once in memory, we will significantly reduce the overall memory usage of Elektra.\n2. Improved performance: The cache will provide fast access to key names, reducing the time it takes to retrieve them from storage.\n3. Simplified code: The caching mechanism will encapsulate the logic for storing and retrieving key names, making the code easier to maintain and understand.\n\n## Consequences\n\n1. Increased complexity: The caching mechanism will introduce additional complexity to the code, requiring careful consideration of cache invalidation and eviction strategies.\n2. Potential for cache misses: If the cache is not properly sized or if the cache eviction strategy is not optimal, there may be a significant number of cache misses, which could impact performance.\n3. Additional memory usage: While the caching mechanism will reduce memory consumption overall, it will still require some additional memory to store the cache itself.\n\n## Status\n\nThis decision is currently in the \"proposed\" state and is subject to change based on further discussion and evaluation.","GenTime":"2024-08-10 04:57:14"}
{"File Name":"libelektra\/error_handling.md","Context":"## Problem\\nThere are ambiguous cases where the same return value can refer to multiple problems:\\n- name modifications which can be either invalid name or locking the key name\\n- getting values of (non-)binary keys\\n","Decision":"- Update documentation in `doc\/dev\/error-*` and link to them in the documentation\\nfor the module `kdb`\\n- Add second channel for getting information about errors\\n- Return error codes directly from functions where failures are expected, e.g. `kdbGet`, `keySetName`\\n- Harmonize return values from all functions and move error reporting to second channel\\n- Binary metadata vs flag #4194\\n","tokens":44,"id":1292,"Prediction":"## Decision\n\nTo resolve the ambiguity in return values, we will introduce a new data structure to store and return the results of the function. This data structure will contain a combination of the original return value and an additional field that indicates the type of problem encountered.\n\n## Context\n\nThe function in question is designed to perform various operations on a key-value store. However, there are certain cases where the same return value can refer to multiple problems. For example, when modifying a name, the function can return an error message indicating either that the name is invalid or that the key is locked. Similarly, when retrieving the value of a key, the function can return the value itself, but also indicate whether the key is binary or not.\n\n## Rationale\n\nIntroducing a new data structure to store the results of the function will allow us to provide more information to the caller about the type of problem encountered. This will make the function more robust and easier to use, as it will provide more context about the error or warning.\n\n## Decision Criteria\n\nThe decision to introduce a new data structure was made based on the following criteria:\n\n* The function already has a complex return value that can refer to multiple problems\n* The current return value does not provide enough information to the caller about the type of problem encountered\n* Introducing a new data structure will make the function more robust and easier to use\n* The additional complexity introduced by the new data structure will be justified by the improved functionality and maintainability of the function\n\n## Status\n\nThis decision has been implemented and is currently in use in the production environment.","GenTime":"2024-08-10 04:57:32"}
{"File Name":"libelektra\/boolean.md","Context":"## Problem\\nInconsistent use of booleans in various parts of Elektra.\\n","Decision":"Only the strings `0` and `1` are allowed in the `KeySet` for `type = boolean`, for both values and defaults.\\nEverything else should lead to errors in checkers (in `kdbSet`).\\nA spec with `type = boolean` without a specified default should be interpreted as `default = 0`.\\nExample for an implementation in C in an application:\\n```c\\nif (k != NULL && strcmp(keyString(k), \"1\") == 0) {\/*true*\/} else {\/*false*\/}\\n```\\nStorage plugins are allowed any representation as suitable, e.g., a JSON plugin might render `1` as `true`.\\nThe type checker plugin should allow\\n- non-presence\\n- the string \"0\"\\n- the string \"1\"\\n","tokens":18,"id":1293,"Prediction":"## Decision\n\nTo standardize the use of booleans in Elektra, we will adopt the convention of using `bool` type for boolean values throughout the codebase.\n\n## Context\n\nThe inconsistent use of booleans in various parts of Elektra has led to confusion and errors. Some parts of the code use `bool` type, while others use `int` or `string` to represent boolean values. This has resulted in bugs and difficulties in maintaining the code.\n\n## Rationale\n\nUsing a consistent type for boolean values will improve code readability and maintainability. It will also reduce the risk of errors and make it easier for developers to understand the code.\n\n## Decision Drivers\n\n* Consistency: Using a consistent type for boolean values will improve code readability and maintainability.\n* Error reduction: Using a consistent type for boolean values will reduce the risk of errors.\n* Developer experience: Using a consistent type for boolean values will make it easier for developers to understand the code.\n\n## Pros and Cons\n\nPros:\n\n* Improved code readability and maintainability\n* Reduced risk of errors\n* Easier for developers to understand the code\n\nCons:\n\n* Requires changes to existing code\n* May require additional testing to ensure correctness\n\n## Alternatives Considered\n\n* Using `int` type for boolean values\n* Using `string` type for boolean values\n* Not changing the existing code\n\n## Conclusion\n\nWe will adopt the convention of using `bool` type for boolean values throughout the codebase. This will improve code readability and maintainability, reduce the risk of errors, and make it easier for developers to understand the code.","GenTime":"2024-08-10 04:57:49"}
{"File Name":"libelektra\/error_semantics.md","Context":"## Problem\\nWhile we have a classification of errors and warnings, it remains\\nunclear when plugins actually should emit errors and warnings.\\n","Decision":"Provide guidelines in the form as tutorials, covering:\\n- prefer errors to warnings\\n- that any not understood metadata (e.g. types), should lead to an error\\n- that wrong specifications, like `kdb meta-set \/tests\/ipaddr\/ipv4 check\/ipaddr ipv8` should be rejected\\n- if the value does not confirm **exactly** to the specified type, an error should be emitted\\n(e.g. only `0` or `1` as boolean)\\n- anything else that is beyond the capabilities of a plugin (not implemented),\\nshould lead to an error\\nViolations against these guidelines can be reported as bug and then either:\\n- the bug gets fixed\\n- the plugin get a worse `infos\/status` but still get shipped with 1.0\\n- the plugin gets removed\\n- [Metadata in Spec Namespace](spec_metadata.md)\\n- [Capabilities](capabilities.md)\\n- [Boolean](boolean.md)\\n","tokens":28,"id":1302,"Prediction":"## Decision\n\nTo provide clarity on when plugins should emit errors and warnings, we will introduce a new concept of \"Error Severity\" and establish guidelines for its usage.\n\n## Context\n\nCurrently, plugins have the ability to emit errors and warnings, but the decision on when to do so is left up to the plugin developers. This has led to inconsistent usage and confusion among users about what constitutes an error or warning.\n\n## Solution\n\nWe will introduce a new concept of \"Error Severity\" that will be used to classify errors and warnings into three categories:\n\n* **Critical**: Errors that prevent the plugin from functioning correctly or cause data loss. These errors should be emitted whenever the plugin detects a critical issue.\n* **Warning**: Errors that indicate a potential issue or problem, but do not prevent the plugin from functioning. These errors should be emitted whenever the plugin detects a warning-worthy issue.\n* **Info**: Informational messages that provide additional context or details about the plugin's operation. These messages should be emitted whenever the plugin wants to provide additional information to the user.\n\nWe will also establish the following guidelines for plugin developers:\n\n* Critical errors should be emitted whenever the plugin detects a critical issue that prevents it from functioning correctly or causes data loss.\n* Warning errors should be emitted whenever the plugin detects a potential issue or problem that does not prevent it from functioning, but may cause issues if left unaddressed.\n* Info messages should be emitted whenever the plugin wants to provide additional context or details about its operation.\n\n## Rationale\n\nBy introducing the concept of \"Error Severity\" and establishing guidelines for its usage, we will provide clarity and consistency in the usage of errors and warnings by plugins. This will help to reduce confusion among users and improve the overall usability of the system.\n\n## Consequences\n\n* Plugin developers will need to understand and follow the guidelines for emitting errors and warnings.\n* The system will need to be updated to handle the new concept of \"Error Severity\" and display errors and warnings accordingly.\n* Users will benefit from a more consistent and clear experience when interacting with plugins.","GenTime":"2024-08-10 05:00:53"}
{"File Name":"libelektra\/warning_array.md","Context":"## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n","Decision":"The format should be aligned with the correct array notation,\\nstarting with `#0`. The maximum number of warnings will stay at\\n100 entries (`#0` - `#_99`).\\n- [Array](array.md)\\n","tokens":46,"id":1303,"Prediction":"## Decision\n\n### Decision: Migrate warnings to a more flexible data structure\n\n### Context\n\nCurrently, multiple warnings are saved in an Elektra non-conforming array notation, which is limited to 100 entries. This notation is against the design decision made in the array.md document.\n\n### Problem\n\nThe current implementation has the following problems:\n\n* Limited capacity: The array notation is limited to 100 entries, which can lead to data loss or corruption when more than 100 warnings are encountered.\n* Inflexible: The array notation is not designed to handle complex warning data structures, making it difficult to store and retrieve warnings efficiently.\n\n### Decision\n\nTo address the limitations of the current implementation, we will migrate warnings to a more flexible data structure, such as a JSON or YAML file. This will allow us to store and retrieve warnings more efficiently and handle complex warning data structures.\n\n### Rationale\n\nUsing a more flexible data structure will provide several benefits, including:\n\n* Increased capacity: We can store an unlimited number of warnings without worrying about data loss or corruption.\n* Improved flexibility: We can store and retrieve warnings in a more efficient and flexible way, making it easier to handle complex warning data structures.\n* Better scalability: The new data structure will be more scalable, allowing us to handle a large number of warnings and users without performance issues.\n\n### Consequences\n\n* The migration will require significant changes to the codebase, including updates to the warning storage and retrieval logic.\n* The new data structure may require additional storage space, depending on the size of the warning data.\n* The migration may introduce new bugs or issues, which will need to be addressed through testing and debugging.\n\n### Status\n\nThis decision is approved and will be implemented in the next release. The migration will be completed by [insert deadline] and will be thoroughly tested before being deployed to production.","GenTime":"2024-08-10 05:01:13"}
{"File Name":"libelektra\/elektra_web.md","Context":"## Problem\\nFor Elektra Web, there needs to be a way to remotely manage instances and groups\\nof instances (clusters). The remote configuration of a single instance is\\nsimple. However, to manage multiple instances, we need to store the information\\nto access the daemons, as well as information about the grouping (clusters) of\\ndaemons.\\n","Decision":"Use one cluster daemon (clusterd) to manage all clusters and instances.\\n- [Elektra Web Recursive Structure decision](elektra_web_recursive.md)\\n","tokens":75,"id":1306,"Prediction":"## Decision\n\nTo remotely manage instances and groups of instances (clusters) for Elektra Web, we will use a combination of existing tools and a custom solution to store and manage the necessary information.\n\n## Context\n\nThe remote configuration of a single instance is straightforward, but managing multiple instances requires storing information to access the daemons, as well as information about the grouping (clusters) of daemons. This information includes:\n\n* Daemon addresses and ports\n* Authentication credentials\n* Cluster membership information\n* Configuration settings for each instance\n\n## Decision Rationale\n\nWe will use a combination of the following approaches to manage instances and clusters:\n\n* For each instance, we will store the necessary information in a configuration file, using a standardized format (e.g. JSON or YAML).\n* For clusters, we will use a separate configuration file or database to store the membership information and other relevant details.\n* To access and manage the daemons, we will use existing tools such as SSH or a custom API.\n* We will implement a custom solution to manage the storage and retrieval of the configuration files and cluster information.\n\n## Consequences\n\nThis approach will allow us to:\n\n* Easily manage and configure individual instances\n* Efficiently manage and group instances into clusters\n* Store and retrieve the necessary information for each instance and cluster\n* Use existing tools and infrastructure to access and manage the daemons\n\nHowever, this approach also introduces some complexity and requires additional development and testing to ensure that the custom solution is robust and scalable.\n\n## Pros and Cons\n\nPros:\n\n* Easy to manage individual instances\n* Efficient clustering and grouping\n* Existing tools and infrastructure can be used\n\nCons:\n\n* Additional development and testing required\n* Custom solution may introduce complexity and potential issues\n* Requires careful planning and implementation to ensure scalability and reliability.","GenTime":"2024-08-10 05:02:14"}
{"File Name":"libelektra\/holes.md","Context":"## Problem\\nConfig files ideally do not copy any structure if they only want to\\nset a single key.\\n","Decision":"Support holes and values for non-leaves in a KeySet if the underlying format allows it.\\nIf the underlying format does not support it and there is also not an obvious\\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\\nvalues in non-leaves can be supported with key names starting with \u00aeelektra.\\n","tokens":23,"id":1307,"Prediction":"## Decision\n\nTo address the problem of config files copying unnecessary structure when setting a single key, we will use a JSON-based configuration system that allows for flexible and dynamic configuration settings.\n\n## Context\n\nAs our application grows in complexity, we've noticed that our config files are becoming increasingly cumbersome and difficult to manage. Specifically, when a config file only needs to set a single key, it ends up copying the entire structure of the config file, making it harder to read and maintain. This is not only aesthetically unpleasing but also leads to unnecessary complexity and potential errors.\n\n## Rationale\n\nWe chose to use a JSON-based configuration system because it allows for flexible and dynamic configuration settings. JSON is a lightweight and easy-to-read format that can be easily parsed and generated by our application. By using a JSON-based system, we can define a simple and concise configuration file that only contains the necessary settings, without having to worry about copying unnecessary structure.\n\n## Decision Drivers\n\n* Simplify config file structure and reduce complexity\n* Improve readability and maintainability of config files\n* Allow for more flexible and dynamic configuration settings\n* Reduce the risk of errors and inconsistencies\n\n## Consequences\n\n* We will need to implement a JSON-based configuration system in our application\n* Our config files will become more concise and easier to read\n* We will have more flexibility and control over our configuration settings\n* We may need to update our application's parsing and generation logic to work with the new JSON-based configuration system\n\n## Alternatives Considered\n\n* Using a more traditional INI-based configuration system\n* Implementing a custom configuration system using a proprietary format\n* Using a configuration system that is specific to a particular programming language or framework\n\nWe rejected these alternatives because they would have required more significant changes to our application's architecture and would have introduced more complexity and dependencies. The JSON-based configuration system provides a simple and flexible solution that meets our needs without introducing unnecessary complexity.","GenTime":"2024-08-10 05:02:35"}
{"File Name":"libelektra\/ingroup_removal.md","Context":"## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n","Decision":"The `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n","tokens":83,"id":1308,"Prediction":"## Decision\n\n**Error Message Simplification for elektra**\n\n## Context\n\nThe current error message in elektra, which displays \"Ingroup: <group>\" when an error or warning occurs, is deemed unnecessary and increases the verbosity of the message. This message was initially intended to provide more information about the source of the error, but it has become redundant and provides little value to the user.\n\n## Problem Statement\n\nThe current error message is not user-friendly and does not provide meaningful information about the error or warning. It simply displays the name of the group where the error occurred, which is not helpful in diagnosing or resolving the issue.\n\n## Decision\n\nTo improve the user experience and reduce the verbosity of error messages, we will simplify the error message in elektra to display only the essential information about the error or warning. Specifically, we will remove the \"Ingroup: <group>\" message and display only the relevant error or warning message.\n\n## Rationale\n\nThe main goal of this decision is to improve the usability of elektra by providing more concise and meaningful error messages. By removing unnecessary information, we can reduce the cognitive load on the user and make it easier for them to understand and resolve the issue.\n\n## Consequences\n\n* The error message will be more concise and easier to read.\n* The user will have a better understanding of the error or warning and be able to resolve the issue more quickly.\n* The verbosity of the message will be reduced, making it easier to scan and diagnose issues.\n\n## Implementation\n\nTo implement this decision, we will modify the error handling mechanism in elektra to display only the relevant error or warning message. We will also review and update the existing error messages to ensure they are concise and meaningful.\n\n## Monitoring and Feedback\n\nWe will monitor the feedback from users and review the error messages to ensure they are meeting the expected standards of usability and clarity. Any issues or concerns will be addressed promptly to ensure the continued improvement of the elektra error message system.","GenTime":"2024-08-10 05:02:57"}
{"File Name":"libelektra\/reference_counting.md","Context":"## Problem\\n- locking is not reset when ref counting again gets 0 (adding to keyset and\\npop again) #2202\\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\\nfor KeySets (also suggested in #1332)\\n","Decision":"- add second counter to Key\\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\\n- return error on reference count overflow\\n- no error on underflow (decrement when zero), instead stay at zero\\n- use fixed sized types (`uint16_t`) for reference counters\\n- increment\/decrement references before\/after passing instances to plugins\\n","tokens":59,"id":1309,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nAs of [current date], we will implement reference counting for both KeySet and Key objects to ensure that locks are properly released when the reference count reaches zero.\n\n**Context**\n\nThe current implementation of KeySet and Key objects does not properly reset locks when the reference count reaches zero. This is causing issues in certain scenarios, such as when a KeySet is added to a keyset and then popped again. Additionally, the C++ API for KeySet and Key has unexpected differences, and implementing reference counting for KeySets (as suggested in #1332) will help to address these issues.\n\n**Rationale**\n\nImplementing reference counting for both KeySet and Key objects will ensure that locks are properly released when the reference count reaches zero. This will prevent issues such as deadlocks and improve the overall reliability of the system.\n\n**Decision Drivers**\n\n* The need to properly reset locks when the reference count reaches zero\n* The desire to improve the consistency of the C++ API for KeySet and Key\n\n**Consequences**\n\n* The implementation of reference counting for both KeySet and Key objects will require additional code and testing\n* The changes may introduce new issues or bugs, which will need to be addressed\n\n**Status**\n\nThis decision is approved and will be implemented in the next release.\n\n**Affected Components**\n\n* KeySet\n* Key\n* C++ API for KeySet and Key\n\n**Implementation**\n\nThe implementation will involve adding a reference count to both KeySet and Key objects, and updating the relevant methods to increment and decrement the reference count as needed. The locks will be released when the reference count reaches zero.\n\n**Verification**\n\nThe changes will be verified through a combination of unit tests, integration tests, and manual testing. The verification process will ensure that the locks are properly released when the reference count reaches zero, and that the C++ API for KeySet and Key is consistent and reliable.","GenTime":"2024-08-10 05:03:18"}
{"File Name":"libelektra\/lookup_every_key.md","Context":"## Problem\\nOn structures like maps or [arrays](array.md) there are different\\npossibilities which keys are looked up in the KeySet and which\\nare simply iterated.\\nWithout any guidelines, applications would provide arbitrary inconsistent\\nbehavior.\\n","Decision":"Every key that an application wants to use, must be looked up\\nwith `ksLookup` using a cascading lookup key.\\n- [Arrays](array.md)\\n","tokens":51,"id":1310,"Prediction":"## Decision\n\nTo ensure consistent and predictable behavior when iterating over structures like maps or arrays, we will adopt the following guideline:\n\n**Guideline:** When iterating over a structure like a map or array, we will only consider the keys returned by the `KeySet` method. This means that any keys that are not present in the `KeySet` will be ignored, and any keys that are present in the `KeySet` will be processed in the order they are returned.\n\n**Rationale:**\n\n* This guideline provides a clear and consistent way for developers to iterate over these structures, reducing the risk of inconsistent behavior.\n* By only considering the keys returned by `KeySet`, we ensure that the iteration order is deterministic and reproducible.\n* This guideline also simplifies the implementation of iteration logic, as developers no longer need to worry about handling arbitrary keys.\n\n**Consequences:**\n\n* This guideline may require some adjustments to existing code that relies on arbitrary iteration orders.\n* Developers may need to refactor their code to ensure that it is compatible with this new guideline.\n* This guideline may also affect the performance of certain operations, as the iteration order may change.\n\n**Alternatives Considered:**\n\n* Allowing developers to specify their own iteration order, which could lead to inconsistent behavior and make the code harder to maintain.\n* Iterating over all keys in the structure, which could be slower and more memory-intensive.\n* Using a different data structure, such as a list, which may not provide the same level of functionality as a map or array.\n\n**Status:** Approved","GenTime":"2024-08-10 05:03:34"}
{"File Name":"libelektra\/null_pointer_checks.md","Context":"## Problem\\nCurrently all functions do proper argument checking which might degrade\\nperformance.\\n","Decision":"Rejected (keep checks) due to time constraints\\n","tokens":17,"id":1317,"Prediction":"## Decision\n\nTo address the concern about performance degradation due to proper argument checking in all functions, we will implement a hybrid approach that balances performance and safety.\n\n## Context\n\nAs you mentioned, all functions currently perform proper argument checking, which can lead to a performance bottleneck. However, skipping argument checks entirely can lead to runtime errors and make the code more difficult to maintain.\n\n## Solution\n\nWe will introduce a configuration option that allows developers to toggle argument checking on or off. By default, argument checking will be enabled, but developers can opt-out of it for specific functions or modules if they are certain that the arguments will always be valid.\n\nTo implement this, we will:\n\n1. Introduce a `CHECK_ARGUMENTS` constant that can be set to `True` or `False`.\n2. Wrap the argument checking code in an `if CHECK_ARGUMENTS:` block, so that it is only executed when the constant is set to `True`.\n3. Provide a way for developers to override the default behavior and disable argument checking for specific functions or modules.\n\nThis approach allows us to strike a balance between performance and safety, giving developers the flexibility to choose when and where to enable or disable argument checking.\n\n## Rationale\n\nBy implementing a hybrid approach, we can:\n\n* Improve performance by skipping argument checks in situations where they are not necessary\n* Maintain safety by ensuring that argument checks are still performed in critical areas of the code\n* Provide flexibility to developers to make informed decisions about when and where to enable or disable argument checking\n\n## Consequences\n\n* The code will be more complex, as we will need to introduce a new configuration option and conditional logic to handle it\n* The performance improvement will depend on the specific use cases and the frequency of argument checks\n* The safety of the code will be maintained, but it will require more careful consideration of the trade-offs in different scenarios\n\nOverall, this decision aims to provide a balance between performance and safety, while also giving developers the flexibility to make informed decisions about when and where to enable or disable argument checking.","GenTime":"2024-08-10 05:05:58"}
{"File Name":"libelektra\/capabilities.md","Context":"## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos\/status`, others were not.\\n","Decision":"Add `infos\/features\/storage` to document limitations of storage plugins.\\nIdeally, storage plugins should throw an error in `kdbSet` for\\nunrepresentable KeySets.\\nElektra cannot guarantee that any configuration file format can\\nbe mounted anywhere.\\nDevelopers, maintainers and administrators are responsible for what\\nthey mount. They need to test the setup.\\n- [Base Name](base_name.md)\\n","tokens":66,"id":1318,"Prediction":"## Decision\n\nTo address the limitations of storage plugins in representing KeySets, we will introduce a new abstract class `KeySetSerializer` that will provide a standardized way for plugins to serialize and deserialize KeySets. This class will define a common interface for plugins to implement, ensuring that all plugins can represent any KeySet.\n\n## Context\n\nThe current limitation of storage plugins in representing KeySets is causing issues with the flexibility and usability of our configuration system. Plugins like `dump` and `quickdump` are able to handle KeySets, but other plugins have limitations that are not well-documented. This is leading to confusion and difficulties in using the configuration system.\n\n## Rationale\n\nBy introducing a `KeySetSerializer` abstract class, we can ensure that all plugins can handle KeySets in a consistent and standardized way. This will allow us to:\n\n* Improve the flexibility and usability of our configuration system\n* Reduce the complexity and confusion caused by the current limitations of storage plugins\n* Make it easier to add new plugins and ensure that they can handle KeySets correctly\n\n## Decision Drivers\n\n* The need for a standardized way to serialize and deserialize KeySets\n* The desire to improve the flexibility and usability of our configuration system\n* The importance of reducing complexity and confusion caused by the current limitations of storage plugins\n\n## Consequences\n\n* All plugins will need to implement the `KeySetSerializer` abstract class\n* The `KeySetSerializer` class will need to be maintained and updated as new plugins are added\n* The configuration system will become more flexible and usable, but may require more effort to implement and test new plugins\n\n## Alternatives Considered\n\n* We could have continued to document the limitations of each storage plugin, but this would not provide a standardized way for plugins to handle KeySets.\n* We could have created a custom solution for each plugin, but this would be inflexible and difficult to maintain.\n* We could have abandoned the idea of supporting KeySets altogether, but this would limit the functionality of our configuration system.","GenTime":"2024-08-10 05:06:19"}
{"File Name":"libelektra\/error_codes.md","Context":"## Problem\\nThe current error concept has disadvantages in following regards:\\n- A lot of redundant errors\\nAt the moment, each new plugin introduces new error codes which led to about 210+ error codes. Many of those errors\\nare duplicated because developers did not know or search for a similar error which is already present. This concept should\\ngroup similar errors together so that there is one coherent and consistent state again.\\n- Hard to manage specification file\\nSince every developer adds its own error individually, a lot of merge conflicts happen which makes contributing to the codebase\\nunpleasant. Additionally, if you want to reuse any error you have to scrape to the whole file with ~1300+ lines. As there is no\\nsenseful ordering or scheme behind the errors (since they grew by time), it is a hassle to find the correct error code.\\nThe new concept should standardize errors, making it easy to categorize errors from new plugins and avoid merge conflicts.\\n- No senseful way for application developers to use error codes from elektra\\nIf developers of plugins\/ external tools using elektra want to react to errors, they have to be very specific. At the moment there is\\nno possibility to catch all errors easily which force a certain behavior. Eg. if there happens a temporary recoverable error, developers have to\\ncatch for every specific error code rather than a general hierarchical error. The new concept should make it easy to react to errors as they are\\nsensefully grouped together and are hierarchically structured.\\n","Decision":"All \"fatal\" errors will be converted to \"errors\" as the distinction is not relevant.\\nUnused errors will be removed from the specification.\\nErrors will be categorized into logical groups with subgroups.\\nEach error will be made up of 5 characters, where the first 2 character indicate the highest level\\nand character 3 to 5 will be used for subgrouping. Errors are prepended with the letter `C` which\\nis the abbreviation for \"Code\".\\n- Permanent errors C01000\\n- Resource C01100\\n- Out of Memory C01110\\n- Installation C01200\\n- Logical C01300\\n- Internal C01310\\n- Interface C01320\\n- Plugin Misbehavior C01330\\n- Conflicting State C02000\\n- Validation C03000\\n- Syntactic C03100\\n- Semantic C03200\\nTo see an explanation of the categories along with a guideline on how to categorize please\\nsee the [Error Codes Guideline](..\/dev\/error-categorization.md)\\n- [Error Message Format](error_message_format.md)\\nShows the new format of the error message\\n- [Error Codes Guideline](..\/dev\/error-categorization.md)\\nShows how to categorize errors\\n","tokens":312,"id":1319,"Prediction":"Here is the Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\nWe will introduce a hierarchical error categorization system, grouping similar errors together and providing a standardized way to manage and reuse error codes. This will reduce the number of redundant errors, simplify the specification file, and make it easier for application developers to react to errors in a meaningful way.\n\n**Context**\n\nThe current error concept has several disadvantages, including:\n\n* Redundant errors: With over 210 error codes, many of which are duplicated, the current system is unwieldy and difficult to manage.\n* Hard to manage specification file: The large number of error codes and lack of organization make it difficult to contribute to the codebase and reuse existing error codes.\n* No senseful way for application developers to use error codes: The current system requires developers to be very specific when reacting to errors, making it difficult to catch all errors or react to hierarchical errors.\n\n**Problem Statement**\n\nThe current error concept is not scalable, maintainable, or user-friendly. It is difficult to manage and reuse error codes, and it does not provide a meaningful way for application developers to react to errors.\n\n**Solution**\n\nWe will introduce a hierarchical error categorization system, where errors are grouped into categories and subcategories. This will reduce the number of redundant errors, simplify the specification file, and make it easier for application developers to react to errors in a meaningful way.\n\n**Rationale**\n\nThe hierarchical error categorization system will provide several benefits, including:\n\n* Reduced redundancy: By grouping similar errors together, we can reduce the number of redundant errors and make it easier to manage and reuse error codes.\n* Simplified specification file: The standardized categorization system will make it easier to contribute to the codebase and reuse existing error codes.\n* Easier error handling: The hierarchical error categorization system will provide a meaningful way for application developers to react to errors, making it easier to catch all errors or react to hierarchical errors.\n\n**Success Metrics**\n\nWe will measure the success of this decision by:\n\n* Reducing the number of redundant errors\n* Simplifying the specification file\n* Improving the ease of use for application developers when reacting to errors\n\n**Implications**\n\nThis decision will require changes to the error handling code and the specification file. It will also require developers to adapt to the new hierarchical categorization system.\n\n**Alternatives Considered**\n\nWe considered the following alternatives:\n\n* Continue with the current error concept\n* Introduce a flat error categorization system\n* Use a third-party error handling library\n\nHowever, these alternatives did not provide the same benefits as the hierarchical error categorization system, and would not address the problems with the current error concept.","GenTime":"2024-08-10 05:06:49"}
{"File Name":"cfo\/0002-use-yarn-install-instead-of-npm-install-for-web.md","Context":"## Context\\nI see found both `yarn.lock` and `package-lock.json` in the repository.\\n","Decision":"Sticking to `yarn` for `web` for now.\\n","tokens":23,"id":1320,"Prediction":"## Decision\n\nAfter careful consideration, I have decided to adopt the `yarn.lock` file as the primary lockfile for our project.\n\n## Context\n\nAs you pointed out, both `yarn.lock` and `package-lock.json` files are present in the repository. This is because our project uses both Yarn and npm as package managers.\n\n## Problem\n\nHaving two lockfiles can lead to inconsistencies and difficulties in maintaining the project's dependencies. It's essential to choose one lockfile format and stick to it to ensure reproducibility and ease of maintenance.\n\n## Decision\n\nWe will use `yarn.lock` as the primary lockfile for our project. This decision is based on the following reasons:\n\n1. Yarn is the primary package manager used in our project, and `yarn.lock` is the default lockfile format for Yarn.\n2. `yarn.lock` is more efficient and provides better performance compared to `package-lock.json`.\n3. Using `yarn.lock` will simplify our workflow and reduce the risk of inconsistencies between different package managers.\n\n## Consequences\n\nBy adopting `yarn.lock` as the primary lockfile, we will:\n\n1. Simplify our dependency management workflow.\n2. Reduce the risk of inconsistencies between different package managers.\n3. Improve the performance of our package management operations.\n\n## Rationale\n\nThe decision to use `yarn.lock` is based on the project's requirements and the benefits it provides. It is essential to maintain consistency and simplicity in our project's dependency management to ensure reproducibility and ease of maintenance.\n\n## Status\n\nThis decision is approved and will be implemented in the project.","GenTime":"2024-08-10 05:07:07"}
{"File Name":"molgenis-service-armadillo\/0006-use-rdata-format-as-data-input.md","Context":"## Context\\nWe want to make the MOLGENIS \"Armadillo\" service data provider agnostic. There are a couple of reasons why we are doing this\\n- the service is usable for other parties as well\\n- the service can still integrate with MOLGENIS.\\n- the release cycle of the service is data provider independent\\n- the service can be developed by other parties as well\\n","Decision":"We implement an endpoint to upload and load RData files in the MOLGENIS \"Armadillo\" service to manage data for the use in DataSHIELD.\\n","tokens":83,"id":1326,"Prediction":"## Decision\n\nTo achieve data provider agnosticism in the MOLGENIS \"Armadillo\" service, we will implement a plugin-based architecture that allows for the easy integration of different data providers. This will enable the service to be used by other parties, integrate with MOLGENIS, and have a data provider-independent release cycle.\n\n## Context\n\nWe want to make the MOLGENIS \"Armadillo\" service data provider agnostic. There are a couple of reasons why we are doing this:\n\n* the service is usable for other parties as well\n* the service can still integrate with MOLGENIS\n* the release cycle of the service is data provider independent\n* the service can be developed by other parties as well\n\n## Decision Criteria\n\n* Ease of integration of new data providers\n* Flexibility to adapt to changing requirements\n* Scalability to support multiple data providers\n* Separation of concerns between the service and data providers\n\n## Status\n\nApproved\n\n## Implemented By\n\n[Name]\n\n## Date\n\n[Date]\n\n## Links\n\n* [Link to relevant documentation or code]\n\n## Rationale\n\nBy implementing a plugin-based architecture, we can easily integrate new data providers without modifying the core service. This will allow us to support multiple data providers without affecting the overall architecture of the service. Additionally, this approach will enable other parties to develop and integrate their own data providers, making the service more flexible and scalable.","GenTime":"2024-08-10 05:08:34"}
{"File Name":"molgenis-service-armadillo\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":1328,"Prediction":"Here is an Architectural Decision Record (ADR) for the software:\n\n## Decision\n\n**ADR-001: Choose a Frontend Framework for the Project**\n\n## Context\n\nAs we begin to develop the frontend of our project, we need to decide on a suitable framework to use. We have considered several options, including React, Angular, and Vue.js. Each framework has its own strengths and weaknesses, and we need to weigh these factors to make an informed decision.\n\n## Problem\n\nThe frontend of our project will require a robust and scalable framework to handle the complexity of our application. We need a framework that is easy to learn, has a large community of developers, and is well-maintained.\n\n## Decision\n\nAfter careful consideration, we have decided to use React as the frontend framework for our project. React's component-based architecture and virtual DOM make it an ideal choice for building complex, interactive user interfaces. Additionally, React's large community of developers and extensive ecosystem of tools and libraries make it easier to find resources and talent.\n\n## Rationale\n\nWe chose React over other frameworks because of its flexibility, scalability, and ease of use. React's component-based architecture allows us to break down our application into smaller, reusable pieces, making it easier to maintain and update. Additionally, React's virtual DOM makes it more efficient than other frameworks, as it only updates the parts of the UI that have changed.\n\n## Consequences\n\nBy choosing React as our frontend framework, we will need to invest time in learning the framework and its ecosystem. We will also need to consider the potential limitations of React, such as its steep learning curve for new developers. However, we believe that the benefits of using React outweigh the costs, and we are confident that it will help us build a high-quality, scalable frontend for our project.\n\n## History\n\n* 2023-02-20: Initial decision made to use React as the frontend framework.\n* 2023-03-15: Finalized the decision after considering feedback from team members.\n\nI hope this helps! Let me know if you have any questions or if you'd like me to add anything else.","GenTime":"2024-08-10 05:09:17"}
{"File Name":"dilay\/0003-use-compile-api-to-transfter-class.md","Context":"## Context\\nhttps:\/\/github.com\/microsoft\/TypeScript\/wiki\/Using-the-Compiler-API\\nhttps:\/\/github.com\/dsherret\/ts-morph#readme\\n> TypeScript Compiler API wrapper. Provides an easier way to navigate and manipulate TypeScript and JavaScript code.\\n","Decision":"Decision here...\\n","tokens":54,"id":1337,"Prediction":"## Decision\n\n**Decision:** Use ts-morph as the TypeScript Compiler API wrapper for our software, instead of the native TypeScript Compiler API.\n\n**Context:** We are building a software that requires manipulation and analysis of TypeScript and JavaScript code. We have considered using the native TypeScript Compiler API, but it has a steep learning curve and requires a significant amount of boilerplate code to achieve simple tasks. We have also considered using other libraries that provide a simpler interface to the Compiler API, but ts-morph stands out for its ease of use, flexibility, and extensive documentation.\n\n**Rationale:**\n\n* ts-morph provides a more intuitive and user-friendly interface to the Compiler API, making it easier to navigate and manipulate code.\n* ts-morph supports a wide range of features, including syntax highlighting, code completion, and type checking, which will enable us to provide more robust and accurate code analysis and manipulation capabilities.\n* ts-morph has a large and active community, which means that there are many resources available to help us with any issues or questions we may have.\n* ts-morph is well-documented and has a simple and consistent API, which will make it easier for our team to learn and use.\n\n**Consequences:**\n\n* We will need to invest time in learning the ts-morph API and its features.\n* We will need to ensure that our software is compatible with the latest versions of ts-morph and the TypeScript Compiler API.\n* We may need to adapt our code to use ts-morph's specific syntax and conventions.\n\n**Status:** Accepted\n\n**Implementation:** We will integrate ts-morph into our software and use it to manipulate and analyze TypeScript and JavaScript code. We will also provide documentation and training to our team members on how to use ts-morph effectively.","GenTime":"2024-08-10 05:12:16"}
{"File Name":"Wikibase\/0009-refactor-hooks-for-testability.md","Context":"## Context\\nCurrently, the `RepoHooks` class remains largely untested due to a combination of two factors:\\n1. The methods in this class are static, and we do not own the contract under which they should be called, as they are\\ndefined as hooks in `extension.json` or as global variables in the entrypoints e.g. [extensions\/Wikibase\/repo\/Wikibase.php:1020](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/7b20d22b3c0bbc37ad23f63e38fadc9b1f2ca057\/repo\/Wikibase.php#L1020), which means we cannot easily refactor the methods to increase testability\\n2. Methods rely heavily on the `WikibaseRepo` singleton and it's store, which make it harder to test, as there is no\\nway to to inject a mock of `WikibaseRepo` without dependency injection.\\nA [RFC for enabling dependency injection](https:\/\/phabricator.wikimedia.org\/T240307) in hooks is currently under way.\\nHowever, an interim solution is needed in order to mitigate the amount of untested logic that exists in that file\\nand other places in the codebase.\\nWhile reviewing this issue, two initial solutions were considered:\\n- Refactor `RepoHooks` into a singleton itself, so that when instantiated, we can inject a Mock of `WikibaseRepo`\\ninstead of using the real deal.\\n- Adopt a pattern used in `WikibaseClient` Which enables us to mock several parts of it (namely the store), and replace\\nthe real store by creating an `overrideStore` method. See in following:\\n- [`client\/tests\/phpunit\/includes\/MockClientStore.php`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/MockClientStore.php)\\n- [`client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php:42`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php#L42)\\nHowever, after a discussion, it was decided to implement a middle ground, that would enable us to gradually refactor\\nhooks, rather than a one time big change.\\n","Decision":"It was decided to adopt an existing pattern in Wikibase repo where each hook handler gets its own singleton class and\\nprovides at least four methods:\\n- A constructor to make dependency injection easier.\\n- A public static method to bootstrap an instance from global settings. For consistency, this would typically be named\\n`newFromGlobalSettings`.\\n- (optional) A public static method to get a cached instance of the handler object (rather than instantiate it each time):\\nThis is useful for hooks handlers which are called several times.\\n- A public static method to wire up functionality into the hooks system and should contain little to no logic (as it is\\nhard to test without complete integration or e2e tests).\\n- A public method to perform the actual handler logic in a testable way.\\nThis class should be placed under the `includes\/Hooks` directory. An example of this pattern can be seen in:\\nhttps:\/\/gerrit.wikimedia.org\/r\/c\/mediawiki\/extensions\/Wikibase\/+\/574495\\n","tokens":496,"id":1339,"Prediction":"## Decision\n\n**Title:** Implementing a hybrid approach to test `RepoHooks` class\n\n**Context:** The `RepoHooks` class is largely untested due to its static methods and reliance on the `WikibaseRepo` singleton, making it difficult to refactor for testability. A solution is needed to mitigate the amount of untested logic in the codebase.\n\n**Decision:** We will implement a hybrid approach to test `RepoHooks` class, combining elements of both initial solutions. Specifically:\n\n1. We will refactor `RepoHooks` to be a non-static class, allowing us to inject a mock of `WikibaseRepo` in tests.\n2. We will also adopt the pattern used in `WikibaseClient` to mock specific parts of `WikibaseRepo`, such as the store, and override the real store in tests.\n\n**Rationale:** This hybrid approach allows us to gradually refactor hooks, rather than making a one-time big change. By refactoring `RepoHooks` to be a non-static class, we can start testing its methods more easily. Meanwhile, by adopting the pattern used in `WikibaseClient`, we can still mock specific parts of `WikibaseRepo` and override the real store in tests, providing more flexibility and control.\n\n**Consequences:**\n\n* This approach will require more work and time to implement, as we will need to refactor `RepoHooks` and implement the mocking pattern.\n* It will provide more flexibility and control in testing, allowing us to gradually refactor hooks and increase test coverage.\n* It will also enable us to take advantage of the ongoing RFC for enabling dependency injection in hooks, when it is implemented.","GenTime":"2024-08-10 05:12:58"}
{"File Name":"Wikibase\/0000-use-cache-instead-wb_terms-for-data-needed-for-display.md","Context":"## Context\\nWhen an item or a property is displayed in the short form, either as a link, or as a simple text reference, data needed to render this short form are currently loaded from the SQL table (wb_terms). wb_terms is causing several significant issues, and is not possible to be maintained any more in the long run.\\nDecision to use wb_terms, initially introduced as a SQL search index, has been tracked down to [change 176650](https:\/\/gerrit.wikimedia.org\/r\/#\/c\/mediawiki\/extensions\/Wikibase\/+\/176650\/). As discussed there in the code review, and also in https:\/\/phabricator.wikimedia.org\/T74309#798908, it seems there had been no dramatic performance improvements expected, neither noticed after switching to use wb_terms instead of loading the data of the entire item or property.\\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\n","Decision":"Wikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\nAs long as using SQL table as a storage of the data used for displaying entities in the short form does not bring significant performance gains, we decide to stop using wb_terms as a data source for this use case.\\nInstead, data of the whole entity is going to be retrieved from storage layer (from the database, or from cached storage that are already in place).\\nIf not efficient enough (e.g. in case of huge-size Wikibase instances like Wikidata ), data needed for display will also be stored in cache, e.g. label of an item in a particular language. That should reduce the amount of computation needed, especially when language fallback needs to be applied, etc.\\n","tokens":330,"id":1341,"Prediction":"Here is the Architectural Decision Record (ADR):\n\n**Decision**\n\nAs of [current date], we will no longer use the `wb_terms` table to load data for short form display of items and properties in Wikibase. Instead, we will explore alternative solutions to resolve the issues with `wb_terms` and ensure the long-term maintainability of our system.\n\n**Context**\n\nThe `wb_terms` table was initially introduced as a SQL search index, but has been causing significant issues and is no longer possible to maintain in the long run. Despite its initial intention, it did not provide the expected performance improvements, and its usage has been limited to loading full entity data through MediaWiki's caching mechanisms.\n\n**Problem**\n\nThe `wb_terms` table is causing several significant issues, making it unsustainable for our system. These issues include:\n\n* Maintenance difficulties\n* Lack of performance improvements\n* Limited usage\n\n**Rationale**\n\nWe will not continue to use `wb_terms` for short form display of items and properties due to its limitations and maintenance issues. Instead, we will explore alternative solutions to resolve these issues and ensure the long-term maintainability of our system.\n\n**Consequences**\n\n* We will need to investigate and implement alternative solutions to load data for short form display of items and properties.\n* We may need to refactor existing code to accommodate these changes.\n* We will need to monitor the performance and maintainability of our system to ensure that the alternative solutions are effective.\n\n**Status**\n\nThis decision is currently in the \"in progress\" state, as we are actively investigating and implementing alternative solutions to resolve the issues with `wb_terms`.\n\n**References**\n\n* [Change 176650](https:\/\/gerrit.wikimedia.org\/r\/#\/c\/mediawiki\/extensions\/Wikibase\/+\/176650\/)\n* https:\/\/phabricator.wikimedia.org\/T74309#798908\n* https:\/\/phabricator.wikimedia.org\/T188108","GenTime":"2024-08-10 05:13:42"}
{"File Name":"Wikibase\/0021-federated-properties-source-and-type-dispatching-services.md","Context":"## Context\\nFederated Properties v2 aims to make it possible for users to enable Federated Properties even if their Wikibase already contains data, so that they can choose to use both remote & local Properties to make statements.\\nDispatching by entity type is a widely used mechanism in Wikibase that allows the dispatching service to handle entity ids of different types by delegating to the service implementation defined in the [entity type definitions], thus enabling entity type specific behavior within a single service. With Federated Properties v2 the entity type (\"property\") no longer uniquely identifies the desired service implementation, since local Properties' services need to be handled by database-backed implementations, whereas Federated Properties use API-backed services. In order to work with local and remote Properties, dispatching services need to be aware of the entity's source as well as the type.\\n","Decision":"Make all relevant services source and type dispatching.\\n","tokens":172,"id":1346,"Prediction":"## Decision\n\nTo handle the dispatching of entity types for Federated Properties v2, we will introduce a new concept called \"PropertySource\" that combines the existing entity type with the source of the property (local or remote). This will allow the dispatching service to differentiate between local and remote properties and delegate to the appropriate service implementation.\n\n### Rationale\n\nThe current dispatching mechanism relies on the entity type to determine the service implementation. However, with Federated Properties v2, we need to consider both the entity type and the source of the property. Introducing the PropertySource concept allows us to maintain the existing entity type-based dispatching mechanism while also taking into account the source of the property.\n\n### Consequences\n\n* The dispatching service will need to be updated to include the PropertySource in its decision-making process.\n* The entity type definitions will need to be updated to include the PropertySource concept.\n* The service implementations will need to be updated to handle the PropertySource and provide entity type-specific behavior.\n\n### Alternatives Considered\n\n* Modifying the existing dispatching mechanism to include additional logic to handle local and remote properties. This approach would require significant changes to the existing codebase and may lead to increased complexity.\n* Creating a separate dispatching service for local and remote properties. This approach would lead to duplicated functionality and increased complexity.\n\n### Status\n\nThis decision has been approved and will be implemented in the next release of Federated Properties v2.","GenTime":"2024-08-10 05:14:56"}
{"File Name":"monitoring-doc\/0002-configuration-management.md","Context":"## Context\\nWe have the requirement of adding some resources to the base cloud instances. We currently do\\nthis via the [cloud.conf](https:\/\/github.com\/alphagov\/prometheus-aws-configuration\/blob\/375f34600e373aa0e4c66fcae032ceee361d8c21\/terraform\/modules\/prometheus\/cloud.conf) system. This presents us with some limitations, such as configuration\\nbeing limited to 16kb, duplication in each instance terraform and a lack of fast feedback testing.\\n","Decision":"We have decided to move away from cloud.conf as much as possible and instead use it to instantiate\\na masterless puppet agent which will manage the resources.\\n","tokens":109,"id":1372,"Prediction":"## Decision\n\n**Title:** Migrate resource configuration to a more flexible and scalable system\n\n**Context:** As per the requirement, we need to add resources to the base cloud instances, and our current approach via the [cloud.conf](https:\/\/github.com\/alphagov\/prometheus-aws-configuration\/blob\/375f34600e373aa0e4c66fcae032ceee361d8c21\/terraform\/modules\/prometheus\/cloud.conf) system has limitations such as configuration size limitation, duplication, and lack of fast feedback testing.\n\n**Problem:** Our current system is inflexible and has limitations that hinder our ability to efficiently manage resources and configure instances.\n\n**Decision:** We will migrate the resource configuration to a more flexible and scalable system, such as a JSON or YAML file, which will allow us to overcome the limitations of the current system.\n\n**Rationale:** This decision will enable us to:\n\n* Increase the size of the configuration without worrying about the 16KB limitation\n* Avoid duplication of configuration in each instance Terraform\n* Improve fast feedback testing by separating the configuration from the Terraform code\n* Enhance maintainability and scalability of the system\n\n**Consequences:**\n\n* We will need to refactor the existing code to accommodate the new configuration system\n* We will need to ensure that the new system is properly tested and validated\n* We may need to update the deployment scripts and processes to accommodate the new configuration system\n\n**Status:** Accepted\n\n**Date:** [Insert Date]\n\n**Responsible:** [Insert Name]\n\n**Links:**\n\n* [cloud.conf](https:\/\/github.com\/alphagov\/prometheus-aws-configuration\/blob\/375f34600e373aa0e4c66fcae032ceee361d8c21\/terraform\/modules\/prometheus\/cloud.conf)","GenTime":"2024-08-10 05:20:54"}
{"File Name":"monitoring-doc\/0003-use-ecs-for-initial-beta-buildout.md","Context":"## Context\\nExisting self-hosted infrastructure at GDS has been managed in code\\nusing tools like puppet, but in a somewhat ad hoc way with each team\\ndoing things differently, little sharing of code, and much reinvention\\nof wheels.  We would like to learn about other ways of deploying\\ninfrastructure which encourage consistency: in terms of code\\nartifacts, configuration methods, and such like.\\nSystems such as Kubernetes and Amazon ECS are coalescing around Docker\\nas a standard for packaging software and managing configuration.\\n","Decision":"We will build our initial prometheus beta in Amazon ECS, and assess\\nhow effective it is.  We will review this decision once we have learnt\\nmore about both prometheus and ECS.\\n","tokens":110,"id":1376,"Prediction":"## Decision\n\nTo adopt Docker as a standard for packaging software and managing configuration in our self-hosted infrastructure, and to explore other tools and methods that promote consistency and sharing of code artifacts and configuration methods.\n\n## Context\n\nAs mentioned in the context, our current infrastructure management using tools like Puppet has been ad hoc, with each team doing things differently, resulting in little sharing of code and much reinvention of wheels. We are looking to learn about other ways of deploying infrastructure that encourage consistency, and Docker is emerging as a popular standard for packaging software and managing configuration.\n\n## Problem\n\nOur current infrastructure management approach lacks consistency, making it difficult to maintain and scale our systems. Different teams are using different tools and methods, resulting in duplicated effort and increased complexity.\n\n## Decision Drivers\n\n* The need for consistency and standardization in infrastructure management\n* The growing popularity of Docker as a standard for packaging software and managing configuration\n* The desire to learn about other tools and methods that promote consistency and sharing of code artifacts and configuration methods\n\n## Consequences\n\n* Adoption of Docker as a standard for packaging software and managing configuration will promote consistency across teams and reduce the complexity of our infrastructure management.\n* Exploring other tools and methods that promote consistency and sharing of code artifacts and configuration methods will provide us with a broader range of options for managing our infrastructure.\n* The decision may require significant changes to our current infrastructure management approach, including retraining and upskilling of team members.\n\n## Rationale\n\nBy adopting Docker as a standard for packaging software and managing configuration, we can promote consistency and reduce complexity in our infrastructure management. Additionally, exploring other tools and methods that promote consistency and sharing of code artifacts and configuration methods will provide us with a broader range of options for managing our infrastructure.","GenTime":"2024-08-10 05:21:45"}
{"File Name":"wikiindex\/adr-003-testing_library.md","Context":"## Context\\n* We want to write programmer tests to support a TDD workflow.\\n* We want to be able to mock out functions.\\n","Decision":"* We will use Midje to test our code.\\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\\n","tokens":30,"id":1387,"Prediction":"## Decision\n\nTo support a TDD workflow and enable mocking out functions, we will use a library that provides a way to write programmer tests and allows for function mocking. Specifically, we will use Pytest and the `pytest-mock` library.\n\n## Context\n\nWe want to write programmer tests to support a TDD workflow. This means we want to write tests that are fast, reliable, and easy to maintain. We also want to be able to mock out functions to isolate dependencies and make our tests more efficient.\n\n## Decision\n\nWe have chosen Pytest as our testing framework because it is popular, widely used, and has a strong focus on speed and reliability. Additionally, Pytest has a large and active community, which means there are many resources available to help us get started and troubleshoot any issues that may arise.\n\nWe have also chosen `pytest-mock` as our library for mocking out functions. `pytest-mock` is a popular and well-maintained library that provides a simple and easy-to-use API for mocking out functions and other dependencies. It is also well-integrated with Pytest, making it easy to use in our tests.\n\n## Rationale\n\nBy using Pytest and `pytest-mock`, we will be able to write fast, reliable, and maintainable tests that allow us to confidently develop and refactor our code. We will also be able to isolate dependencies and make our tests more efficient by mocking out functions and other dependencies.\n\n## Consequences\n\nBy using Pytest and `pytest-mock`, we will need to learn the syntax and APIs of these libraries. However, this will be a one-time investment, and we will be able to reap the benefits of a robust and reliable testing framework for our entire project.\n\nAdditionally, using Pytest and `pytest-mock` will allow us to take advantage of many of the features and tools that these libraries provide, such as test discovery, test running, and test reporting. This will make it easier for us to write and maintain our tests, and will help us to ensure that our code is reliable and of high quality.","GenTime":"2024-08-10 05:25:20"}
{"File Name":"amf\/0003-new-annotation-removal-stage-present-in-all-webapi-pipelines.md","Context":"## Context\\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\\n","Decision":"In order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\\n","tokens":61,"id":1393,"Prediction":"## Decision\n\nTo address the issue of inlining parsed content when referencing external files, we will implement a mechanism to detect and handle these cases separately. This will ensure that the emitted model is accurate and consistent with the original data.\n\n## Context\n\nWhen referencing external files, there are certain cases where AMF inlines the parsed content without creating a link (this is because the targeted elements are not present in the references of the base unit). For these cases, when emitting an unresolved model, these references are being emitted inlined.\n\n## Problem\n\nThe current approach of inlining parsed content can lead to inconsistencies in the emitted model, making it difficult to maintain and debug. Moreover, it can also result in unnecessary overhead in terms of processing and storage.\n\n## Solution\n\nWe will introduce a new mechanism to detect and handle these cases separately. When emitting an unresolved model, we will check if the referenced elements are present in the references of the base unit. If they are not, we will create a link to the external file instead of inlining the parsed content.\n\n## Rationale\n\nThis approach will ensure that the emitted model is accurate and consistent with the original data. It will also reduce the overhead of processing and storage by avoiding unnecessary inlining of parsed content.\n\n## Consequences\n\nThis change will require updates to the emitting mechanism to detect and handle these cases separately. It will also require testing to ensure that the emitted model is accurate and consistent.\n\n## Status\n\nThis decision is approved and will be implemented in the next release.\n\n## Affected Components\n\n* Emitting mechanism\n* Model resolution\n\n## References\n\n* AMF documentation: [insert link]\n* Related issues: [insert links]","GenTime":"2024-08-10 05:27:28"}
{"File Name":"amf\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSome other useful links:\\n[https:\/\/github.blog\/2020-08-13-why-write-adrs\/](https:\/\/github.blog\/2020-08-13-why-write-adrs\/)\\n[https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/](https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/)\\n","tokens":16,"id":1397,"Prediction":"## Decision\n\n**Decision:** Implement a Microservices Architecture for the Project\n\n**Context:** As the project has grown in complexity, it has become clear that a monolithic architecture is no longer sufficient to meet the project's requirements. We need to consider a more scalable and maintainable approach to development.\n\n**Problem:** The current monolithic architecture is causing scalability issues, making it difficult to maintain and update individual components without affecting the entire system.\n\n**Reasoning:** Microservices architecture allows for greater flexibility, scalability, and maintainability by breaking down the system into smaller, independent services that can be developed, tested, and deployed independently.\n\n**Decision Criteria:**\n\n* Scalability: Microservices architecture allows for easier scaling of individual services, reducing the impact of a single service on the entire system.\n* Maintainability: With microservices, individual services can be updated independently, reducing the risk of introducing bugs or breaking existing functionality.\n* Flexibility: Microservices architecture allows for the use of different programming languages, frameworks, and databases for each service, providing greater flexibility in development.\n* Resilience: If one service fails, it will not bring down the entire system, as each service is independent and can be restarted or replaced without affecting the others.\n\n**Consequences:**\n\n* Increased complexity: Microservices architecture can introduce additional complexity, requiring more planning, coordination, and communication among team members.\n* Greater resource requirements: Microservices architecture requires more resources, including more servers, databases, and network infrastructure.\n* Changes in development process: Microservices architecture requires changes in the development process, including more emphasis on testing, deployment, and monitoring individual services.\n\n**Status:** Accepted\n\n**Implementors:** [List the team members or individuals responsible for implementing this decision]\n\n**Links:** [Any relevant links to documentation, code, or other resources]\n\nThis decision record will be reviewed and updated as necessary to ensure that the decision remains valid and effective in meeting the project's needs.","GenTime":"2024-08-10 05:28:54"}
{"File Name":"GSoC-Braitenberg-Vehicles\/use_javafx_for_animation.md","Context":"Context\\n-------\\nAfter a while writing the project code, I've realized that it needs constant and probably heavyweight rendering.\\nDecision\\n--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n","Decision":"--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n","tokens":101,"id":1400,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nLeave with JavaFX\/TornadoFX as the constant rendering\/animation framework.\n\n**Context**\n\nAfter a while writing the project code, I've realized that it needs constant and probably heavyweight rendering.\n\n**Rationalization**\n\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have. JavaFX seems to be a nice opportunity with relatively little to learn.\n\n**Status**\n\nAccepted\n\n**Created** 2023-03-01\n\n**Updated** 2023-03-01\n\n**References**\n\n* [JavaFX Documentation](https:\/\/docs.oracle.com\/javase\/8\/javafx\/get-started.htm)\n* [TornadoFX Documentation](https:\/\/tornadofx.io\/documentation\/)","GenTime":"2024-08-10 05:29:39"}
{"File Name":"lbh-frontend-react\/0005-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":1404,"Prediction":"## Decision\n\n**Decision:** We will use ESLint as our code linter to enforce consistency and catch errors in our codebase.\n\n**Context:** We want to enforce consistency in our code and catch as many errors automatically as we are able to. Linting the code is good practice to achieve these aims. ESLint is the standard linter for modern JavaScript and has good support for TypeScript though plugins.\n\n**Rationale:** ESLint is a widely-used and well-maintained linter that has a large community of developers and a wide range of plugins available. It is well-suited for our needs, as it can be easily integrated into our development workflow and provides a high degree of customization and flexibility.\n\n**Consequences:**\n\n* We will need to configure ESLint to fit our specific codebase and development workflow.\n* We will need to ensure that all team members are aware of and follow the linting rules.\n* We may need to make some adjustments to our code to comply with the linting rules.\n* We will be able to catch errors and inconsistencies in our code earlier in the development process, which will improve the overall quality of our codebase.\n\n**Status:** Accepted\n\n**Implementation:** We will implement ESLint in our codebase and configure it to fit our specific needs. We will also create a set of linting rules that are specific to our codebase and ensure that all team members are aware of and follow these rules.","GenTime":"2024-08-10 05:30:43"}
{"File Name":"lbh-frontend-react\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":1408,"Prediction":"## Decision\n\nWe will use Prettier as our official code formatter across all projects to ensure consistency in code style and maintainability.\n\n## Context\n\nAs mentioned in the context, Prettier is an opinionated code formatter that supports most languages in the JavaScript ecosystem and is widely used by over 1 million repositories on GitHub. Its widespread adoption and acceptance make it a suitable choice for our projects.\n\n## Rationale\n\n1. **Consistency**: By using Prettier, we can ensure that all code in our projects is formatted consistently, making it easier for developers to read and understand each other's code.\n2. **Efficiency**: Prettier can automatically format code, saving developers time and effort that would be spent on manual formatting.\n3. **Standardization**: Prettier's opinionated approach ensures that code is formatted according to a specific standard, making it easier for new developers to join the project and for existing developers to maintain the codebase.\n4. **Community adoption**: As Prettier is widely used, it is likely that many developers are already familiar with its formatting conventions, making it easier for our team to collaborate and maintain code.\n\n## Consequences\n\n1. **Code formatting changes**: Existing code may need to be reformatted to conform to Prettier's standards.\n2. **Initial setup**: Developers may need to install Prettier and configure their IDEs or text editors to use it.\n3. **Future maintenance**: As Prettier evolves, we may need to update our configuration or plugins to ensure compatibility.\n\n## Alternatives\n\n1. **Other code formatters**: Other code formatters, such as ESLint or CodeForm, could be used instead of Prettier.\n2. **Manual formatting**: Developers could manually format their code, but this would lead to inconsistencies and inefficiencies.\n\n## Decision Drivers\n\n1. Consistency and maintainability\n2. Efficiency and time-saving\n3. Community adoption and standardization\n4. Ease of use and familiarity for developers\n\nBy using Prettier as our official code formatter, we can ensure consistency, efficiency, and maintainability across all our projects, making it easier for developers to collaborate and maintain our codebase.","GenTime":"2024-08-10 05:32:08"}
{"File Name":"lbh-frontend-react\/0010-use-typedoc-to-generate-api-documentation.md","Context":"## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n","Decision":"We will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n","tokens":159,"id":1410,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will use TypeDoc to generate API documentation for our TypeScript code.\n\n**Context**\n\nWe want to have API documentation for the code we publish, but writing separate API docs that quickly get out of date is not a viable solution. We would like to be able to generate it from our code and comments in our code that live next to the thing they refer to.\n\n**Problem**\n\n* Manual API documentation requires repeated effort and can quickly become outdated\n* TypeScript already defines types in our code, which could be leveraged to generate API documentation\n\n**Decision**\n\nWe will use TypeDoc, a documentation generator based on JSDoc, to automatically generate API documentation from our TypeScript code. This approach combines the benefits of JSDoc's comment-based documentation with the type information already defined in our TypeScript code.\n\n**Rationale**\n\n* TypeDoc is specifically designed for use with TypeScript and can leverage the type information already defined in our code\n* By using TypeDoc, we can automatically generate API documentation without requiring manual effort to keep it up-to-date\n* This approach aligns with our goal of reducing repeated effort and improving the accuracy of our API documentation\n\n**Consequences**\n\n* We will need to update our code to include TypeDoc-compatible comments\n* We will need to configure TypeDoc to generate API documentation for our codebase\n* We will need to review and refine the generated documentation to ensure it meets our needs","GenTime":"2024-08-10 05:32:50"}
{"File Name":"architecture\/0002-apply-clean-code-guidelines.md","Context":"## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n","Decision":"I will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n","tokens":57,"id":1416,"Prediction":"## Decision\n\n**Title:** Use Consistent and Meaningful Code Organization and Naming Conventions\n\n**Context:** As it's hard to understand code, it is crucial that anybody can easily understand the code we're working on. This applies to all levels of code, not only the code itself, but groups of code, complete applications, and groups of applications.\n\n**Problem:** Without consistent and meaningful code organization and naming conventions, our codebase can become difficult to navigate, leading to increased development time, errors, and maintenance costs.\n\n**Decision:** We will use consistent and meaningful code organization and naming conventions throughout our codebase. This includes:\n\n* Using a consistent directory structure and naming conventions for files and folders.\n* Using meaningful and descriptive variable names, function names, and class names.\n* Using consistent indentation and whitespace to make the code easier to read.\n* Using clear and concise comments to explain complex code sections or algorithms.\n* Using a consistent style for coding conventions, such as PEP 8 for Python or Google Java Style Guide for Java.\n\n**Rationale:** By using consistent and meaningful code organization and naming conventions, we can make our codebase more readable, maintainable, and scalable. This will enable developers to quickly understand the code and make changes or additions without getting lost in complex codebases.\n\n**Consequences:**\n\n* Improved code readability and maintainability.\n* Reduced development time and errors.\n* Easier collaboration and knowledge sharing among team members.\n* Increased confidence in the codebase's quality and stability.\n\n**Alternatives Considered:** We considered not using consistent and meaningful code organization and naming conventions, but this would lead to a codebase that is difficult to navigate and maintain.\n\n**Status:** This decision is approved and will be implemented in our codebase.","GenTime":"2024-08-10 05:34:51"}
{"File Name":"architecture\/0003-monitor-condition-and-data-selectors.md","Context":"## Context\\nA lot of Home Assistant integrations use config options like `CONF_MONITORED_CONDITIONS` to allow the user to select which data the integration should expose from the data. This means that the user needs to know what the different data points mean while setting up the integration. While configuring its Lovelace UI, the user has the option to include the entity or not. This means that we allow the user to pick twice.\\n","Decision":"Integrations should expose all available data to the backend if that data is fetched in a single API request.\\nIntegrations should only include selector logic if it make sense in the context of interface, like if it would require extra requests. User should not have to read the available documentation and API descriptions to find out which data they want have.\\n```\\nLayer model:\\nFetch            Manage           View\\n-------------      ---------      ------------\\n| Interface | ---> | Core  | ---> | Frontend |\\n-------------      ---------      ------------\\n```\\nIntegrations can set the `entity_registry_enabled_default` property on their entity objects to instruct Home Assistant to disable certain entities by default ([docs](https:\/\/developers.home-assistant.io\/docs\/entity_registry_disabled_by\/#integrations-setting-default-value-of-disabled_by-for-new-entity-registry-entries)).\\n","tokens":90,"id":1417,"Prediction":"## Decision\n\n**Decision:** Introduce a new configuration option `CONF_VISIBLE_CONDITIONS` to replace `CONF_MONITORED_CONDITIONS` and provide a more user-friendly way of selecting which data points to expose in the Lovelace UI.\n\n**Context:** As mentioned in the context, the current approach of using `CONF_MONITORED_CONDITIONS` requires users to have in-depth knowledge of the data points being exposed, which can be overwhelming and confusing. Additionally, the option to include or exclude entities in the Lovelace UI allows users to pick twice, leading to potential inconsistencies.\n\n**Rationale:**\n\n* By introducing `CONF_VISIBLE_CONDITIONS`, we can decouple the visibility of data points from the monitoring of conditions, providing a more intuitive and user-friendly experience.\n* This new configuration option will allow users to select which data points they want to see in the Lovelace UI, without having to worry about the underlying monitoring setup.\n* This change will also simplify the configuration process, reducing the likelihood of errors and inconsistencies.\n\n**Consequences:**\n\n* The `CONF_MONITORED_CONDITIONS` option will be deprecated and replaced with `CONF_VISIBLE_CONDITIONS`.\n* The Lovelace UI will be updated to use the new `CONF_VISIBLE_CONDITIONS` option to determine which data points to display.\n* Users will need to update their configurations to use the new option.\n\n**Alternatives Considered:**\n\n* Keeping the current approach and adding more documentation to help users understand the different data points.\n* Introducing a new UI component that allows users to select which data points to display, without changing the underlying configuration options.\n\n**Decision Owner:** [Your Name]\n\n**Prerequisites:** [List any prerequisites for implementing this decision, e.g., updates to the Lovelace UI, changes to the documentation, etc.]\n\n**Status:** [New, In Progress, Done]\n\n**Reviewers:** [List the names of the reviewers who have approved this decision]\n\n**Last Updated:** [Date]","GenTime":"2024-08-10 05:35:12"}
{"File Name":"architecture\/0007-use-static-code-analysis.md","Context":"## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n","Decision":"Use Static Code Analysis to find violations of standards.\\n","tokens":45,"id":1418,"Prediction":"## Decision\n\n**Title:** Adhere to established coding standards for maintainable and readable code\n\n**Context:** As we develop software, it's essential to consider the long-term maintainability and readability of our code. While it's impossible to anticipate every situation, following established coding standards helps prevent potential issues and ensures our code remains organized and easy to understand.\n\n**Problem:** Without a clear set of coding standards, our codebase may become disorganized, making it challenging to maintain and update. This can lead to bugs, errors, and decreased productivity.\n\n**Decision:** We will adhere to established coding standards for our software, including [list specific standards, e.g., PEP 8 for Python, Java coding conventions, etc.]. This will ensure consistency throughout our codebase, making it easier for team members to collaborate and maintain the code.\n\n**Rationale:** By following established coding standards, we can:\n\n1. Prevent common mistakes and errors\n2. Improve code readability and maintainability\n3. Enhance collaboration and knowledge sharing among team members\n4. Reduce the time spent on debugging and troubleshooting\n5. Increase the overall quality of our software\n\n**Consequences:**\n\n* Initial investment of time to familiarize ourselves with the chosen coding standards\n* Potential need for additional training or documentation for team members\n* Improved code quality and maintainability in the long run\n\n**Status:** Implemented\n\n**Owners:** [Name], [Name]\n\n**History:**\n\n* 2023-02-20: Initial decision made\n* 2023-03-15: Implemented and documented\n\n**References:**\n\n* [Link to coding standards documentation]\n* [Link to relevant coding standards resources]","GenTime":"2024-08-10 05:35:31"}
{"File Name":"architecture\/0013-home-assistant-container.md","Context":"## Context\\nDefine a supported installation method as per [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md).\\n","Decision":"This is for running just the Home Assistant Core application on native OCI compatible containerization system. It does not provide the Supervisor experience, and thus does not provide the Supervisor panel and add-ons.\\nThis is a general installation method that is recommended as an alternative to the Home Assistant OS installation method. Due to the shared image with the Home Assistant OS installation method, almost all documentation applies to the Home Assistant Container as well.\\nThe only supported way to run the container is on the host network as root with full privileges.\\n### Supported Containerization system and version\\n- Any Open Container Initiative (OCI) compatible containerization system.\\n### Supported boards\/hardware\/machines\\n- Machines of the following architectures: amd64, i386, armhf, aarch64, armv7\\n### Supported Operating Systems and versions\\n- Running Home Assistant Container is only supported on Linux.\\n- Windows and BSD installations (e.g., macOS and FreeBSD) are not supported.\\n### Additional notes\\nThere is a wide variety of containerization software available. From that perspective, Home Assistant will only actively document the use of Docker.\\n### Required Expertise\\n- **Installation**\\nThis requires the user to have an existing system that can run Docker containers. Installation is either done by running a command from the Docker-cli or via a user interface (Synology, Portainer)\\n* **Start when the system is started:** The user is responsible for configuring the system to start the container when the system is started.\\n* **Run with full network access:** Default installation instructions prescribe net=host to be configured.\\n* **Access USB devices:** It is up to the user to ensure that all devices are correctly passed through to the container.\\n* **Maintaining the Home Assistant installation**\\nIf using the Docker-cli the user needs to manually update the run command. If using a UI the user might be notified of an upgrade or automatically update \u2013 automatically applying updates may result in the system not coming back online. There is no rollback in case the instance does not come online after an update.\\n- **Python upgrades:** Included in the Home Assistant container\\n- **Installing Python dependencies:** Included in the Home Assistant container\\n- **Updating Home Assistant:** Included in the Home Assistant container\\n- **Maintaining the Operating System**\\nSince this is just the core container, all OS responsibilities are with the user.\\n- **Security updates for OS:** Responsibility of the user.\\n- **Maintaining the components required for the Supervisor:** No supervisor, so N\/A\\n**Conclusion:** medium expertise required. Some Docker UIs make it easy to run and update containers. Mapping devices and manually updating Home Assistant will be challenging as they depend per platform.\\n","tokens":41,"id":1430,"Prediction":"## Decision\n\n**Title:** Supported Installation Method for Home Assistant\n\n**Context:** As per [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md), we need to define a supported installation method for Home Assistant.\n\n**Decision:** We will support installation of Home Assistant using the following methods:\n\n1. **Debian\/Ubuntu-based package manager**: We will provide a package for Debian and Ubuntu-based distributions, which can be installed using the package manager (apt-get or apt).\n2. **RPM-based package manager**: We will provide a package for RPM-based distributions such as Fedora, CentOS, and RHEL, which can be installed using the package manager (yum or dnf).\n3. **Home Assistant Supervisor**: We will provide a Home Assistant Supervisor, which is a lightweight, easy-to-use installer that can be run on a variety of platforms, including Windows, macOS, and Linux.\n4. **Docker**: We will provide a Docker image for Home Assistant, which can be run on any platform that supports Docker.\n5. **Manual installation**: We will provide detailed instructions for manual installation of Home Assistant on a variety of platforms, including Windows, macOS, and Linux.\n\n**Rationale:**\n\n* The Debian\/Ubuntu-based package manager and RPM-based package manager will allow for easy installation and management of Home Assistant on popular Linux distributions.\n* The Home Assistant Supervisor will provide a simple, user-friendly installation experience for users who are new to Home Assistant or Linux.\n* The Docker image will allow for easy installation and management of Home Assistant on any platform that supports Docker, including cloud providers and container orchestration platforms.\n* The manual installation instructions will provide a flexible installation option for users who prefer to install Home Assistant manually or have specific requirements that are not met by the other installation methods.\n\n**Consequences:**\n\n* Users will have a choice of installation methods that cater to their specific needs and preferences.\n* The supported installation methods will make it easier for users to get started with Home Assistant.\n* The Home Assistant Supervisor will provide a simple, user-friendly installation experience for users who are new to Home Assistant or Linux.\n* The Docker image will allow for easy installation and management of Home Assistant on any platform that supports Docker, including cloud providers and container orchestration platforms.","GenTime":"2024-08-10 05:39:20"}
{"File Name":"architecture\/0008-code-owners.md","Context":"## Context\\nWe get contributed a lot of new integrations, new features to integrations and refactors of integrations. The Home Assistant project is honored to receive so many great contributions to our project!\\nUnfortunately, as a contributor, adding oneself as (the, or one of the) code owners of the integration contributed or contributed to, doesn't always happen spontaneously.\\nNot adding oneself as a code owner has drawbacks for the project:\\n- The contributor doesn't \"own\" (in terms of taking responsibility) his code, and thus contribution, in a more formal fashion.\\n- Without being listed as a code owner, our GitHub bot will not notify the contributor, when an issue for the integration is reported, quite possibly affecting his contribution.\\n- Integrations have ended up or may end up with having a single code owner or no code owners at all.\\nAs a result of this:\\n- Bugs are less likely to be resolved in a timely fashion (turn-around time).\\n- Integrations are more prone to break in the future.\\n- Integration with a single code owner:\\n- Do not benefit from multiple code owners being familiar with the integration in terms of code review and general turn-around time.\\n- Become largely unmaintained when the single listed code owner can no longer contribute to the project.\\nWe have quite a few integrations that haven't got multiple code owners or don't have a code owner.\\nDuring the design discussion of this ADR, it also became clear, that the term \"code owner\" has different meanings to our members and contributors. Some interpret it as an honorable mention of contribution; others see it as \"taking responsibility\".\\n","Decision":"Code ownership for an integration defined:\\nThe willingness of a contributor to try, at best effort, to maintain the integration. Providing the intention for handling issues, providing bug fixes, or other contributions to the integration one is listed on as a code owner.\\n### Rules\\nIn order to support having (multiple) code owners for integration, to raise the quality and interaction on integration in our codebase, we have a set of rules (exceptions are in the next chapter).\\nFor the following cases, adding oneself as a code owner is required:\\n- When contributing a new integration.\\n- When contributing a new platform to an integration.\\n- When contributing a new feature to an integration.\\n- When contributing a significant refactor or rewrite of an integration.\\nContributions to our integrations, in the above-listed scopes, without having the contributor listed or added as the code owner, is no longer accepted.\\n### Exceptions\\nSome exceptions are in place, to prevent contributors to become demotivated to contribute; and are mainly based around smaller, low-impact contributions.\\nIn the following cases, code ownership may be omitted:\\n- Contributions that solely provides a bug fix(es).\\n- Contributions that only provide additional unit test(s).\\n- Contributions to integrations marked as \"internal\". These integrations are code owned by the Home Assistant core team.\\n- Contributions refactoring across multiple integrations, caused by changes to our core codebase. E.g., due to changes to the used platforms.\\n- Small or low impact contributions to an integration. A currently active code owner for the integration or a Home Assistant code reviewer can decide it may be omitted.\\n- The contributor pro-actively rejects to be listed as a code owner; however, a currently active code owner is willing to accept and take code ownership for the contribution provided by the contributor.\\nCode owner(s) and Home Assistant code reviewers are encouraged to ask a contributor to join an integration code ownership, even when the contribution matches one of the exceptions above.\\n### Withdrawing as a code owner\\nWithdrawing code ownership can happen, and it is sad to see an active code owner leaving the project.\\nA code owner may open up a PR to remove oneself as a code owner. However, this should only be accepted after the last contribution to the integration, made by the contributor, is released in a stable Home Assistant release.\\n","tokens":343,"id":1431,"Prediction":"## Decision\n\nTo ensure the timely resolution of issues, maintainability, and code review efficiency, we will implement a new process for assigning code owners to integrations in Home Assistant. This process will encourage contributors to take responsibility for their code and ensure that multiple individuals are familiar with each integration.\n\n## Context\n\nAs mentioned in the context, the Home Assistant project receives many contributions, but not all contributors spontaneously add themselves as code owners. This has resulted in integrations with single or no code owners, leading to issues with timely bug resolution, maintainability, and code review efficiency.\n\n## Problem\n\nThe current process of assigning code owners is not working effectively, resulting in:\n\n* Contributors not taking responsibility for their code\n* GitHub bot notifications not being sent to contributors when issues are reported\n* Integrations with single or no code owners, leading to maintenance issues\n\n## Decision Criteria\n\n* Ensure contributors take responsibility for their code\n* Improve timely resolution of issues\n* Increase maintainability and code review efficiency\n* Clarify the meaning of \"code owner\" to contributors\n\n## Solution\n\nTo address these issues, we will implement the following changes:\n\n1. Update the integration's README file to include a \"Code Owners\" section, where contributors can add themselves as code owners.\n2. Create a GitHub bot that automatically adds contributors to the \"Code Owners\" section of the integration's README file when they make a significant contribution to the integration.\n3. Update the GitHub issues template to include a field for assigning code owners to issues.\n4. Communicate the new process to contributors and emphasize the importance of taking responsibility for their code.\n5. Monitor the effectiveness of the new process and make adjustments as needed.\n\n## Consequences\n\n* Contributors will be more likely to take responsibility for their code.\n* The GitHub bot will notify contributors when issues are reported, ensuring timely resolution.\n* Integrations will have multiple code owners, increasing maintainability and code review efficiency.\n* The meaning of \"code owner\" will be clarified to contributors.\n\n## Rationale\n\nThe new process will encourage contributors to take responsibility for their code, ensuring that multiple individuals are familiar with each integration. This will lead to improved maintainability, code review efficiency, and timely resolution of issues.","GenTime":"2024-08-10 05:39:45"}
{"File Name":"architecture\/0004-waiting-is-waste.md","Context":"## Context\\nHaving a developer waiting for any process to finish is a stupid thing. It's not only that it's to long for the developer to wait, it also hinders you from fixing issues fast.\\n","Decision":"Any processes and workflows need to be very fast and fully automated.\\n","tokens":43,"id":1434,"Prediction":"## Decision\n\nTo reduce the waiting time for developers and improve the overall development speed, we will implement asynchronous processing for long-running tasks in our software.\n\n## Context\n\nAs the user has pointed out, having a developer wait for any process to finish is inefficient and can hinder the ability to quickly fix issues. This can lead to a decrease in productivity and overall development speed.\n\n## Problem\n\nLong-running tasks can cause significant delays for developers, making it difficult to work efficiently. This can lead to:\n\n* Reduced productivity: Developers are forced to wait for tasks to complete, reducing the amount of work they can accomplish.\n* Increased lead time: Delays in task completion can lead to longer lead times for feature development and bug fixing.\n* Decreased morale: Developers may become frustrated with the waiting time, leading to decreased morale and job satisfaction.\n\n## Solution\n\nTo address this issue, we will implement asynchronous processing for long-running tasks. This will allow developers to continue working on other tasks while the long-running task is being processed in the background.\n\n## Consequences\n\n* Improved developer productivity: By allowing developers to continue working on other tasks, we can reduce the overall development time and improve productivity.\n* Faster issue resolution: With the ability to process tasks asynchronously, developers can quickly identify and fix issues, reducing the time it takes to resolve problems.\n* Increased morale: By reducing the waiting time and allowing developers to work more efficiently, we can improve morale and job satisfaction.\n\n## Rationale\n\nImplementing asynchronous processing for long-running tasks is a simple and effective way to improve developer productivity and reduce waiting time. By allowing tasks to run in the background, we can free up developers to focus on other tasks, reducing the overall development time and improving the overall development speed.","GenTime":"2024-08-10 05:40:41"}
{"File Name":"architecture\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1436,"Prediction":"Here is an Architectural Decision Record (ADR) for the project:\n\n**Decision**\n\nTitle: Use of Microservices Architecture\n\n**Context**\n\nWe need to record the architectural decisions made on this project.\n\n**Problem**\n\nAs our project grows, we need to ensure that it remains scalable, maintainable, and flexible. We are considering different architectural approaches to achieve these goals.\n\n**Decision**\n\nWe will use a microservices architecture for our project. This will allow us to break down the system into smaller, independent services that can be developed, tested, and deployed independently. This will improve scalability, maintainability, and flexibility.\n\n**Rationale**\n\nUsing a microservices architecture will allow us to:\n\n* Improve scalability by allowing each service to be scaled independently\n* Improve maintainability by allowing each service to be developed and tested independently\n* Improve flexibility by allowing each service to be implemented using different technologies and frameworks\n* Reduce coupling between services by allowing each service to be developed independently\n\n**Consequences**\n\nUsing a microservices architecture will require:\n\n* More complex infrastructure setup and management\n* More complex communication and integration between services\n* More complex testing and debugging due to the distributed nature of the system\n* More complex deployment and roll-back procedures\n\n**Status**\n\nThis decision is currently in the \"proposed\" status. We will review and refine this decision as needed based on feedback from the team and stakeholders.\n\n**References**\n\n* [1] \"Microservices Architecture\" by Martin Fowler\n* [2] \"Building Microservices\" by Sam Newman\n\n**Implementation**\n\nWe will implement this decision by:\n\n* Breaking down the system into smaller, independent services\n* Defining the boundaries and interfaces between services\n* Implementing communication and integration between services\n* Implementing testing and debugging strategies for the distributed system\n\n**Reviewers**\n\n* [List of reviewers and their feedback]\n\n**History**\n\n* [Date] - Initial proposal\n* [Date] - Revised based on feedback from the team and stakeholders","GenTime":"2024-08-10 05:41:22"}
{"File Name":"ibc-go\/adr-027-ibc-wasm.md","Context":"## Context\\nCurrently in ibc-go light clients are defined as part of the codebase and are implemented as modules under\\n`modules\/light-clients`. Adding support for new light clients or updating an existing light client in the event\\nof a security issue or consensus update is a multi-step process which is both time-consuming and error-prone.\\nIn order to enable new IBC light client implementations it is necessary to modify the codebase of ibc-go (if the light\\nclient is part of its codebase), re-build chains' binaries, pass a governance proposal and validators upgrade their nodes.\\nAnother problem stemming from the above process is that if a chain wants to upgrade its own consensus, it will\\nneed to convince every chain or hub connected to it to upgrade its light client in order to stay connected. Due\\nto the time consuming process required to upgrade a light client, a chain with lots of connections needs to be\\ndisconnected for quite some time after upgrading its consensus, which can be very expensive in terms of time and effort.\\nWe are proposing simplifying this workflow by integrating a Wasm light client module that makes adding support for\\nnew light clients a simple governance-gated transaction. The light client bytecode, written in Wasm-compilable Rust,\\nruns inside a Wasm VM. The Wasm light client submodule exposes a proxy light client interface that routes incoming\\nmessages to the appropriate handler function, inside the Wasm VM for execution.\\nWith the Wasm light client module, anybody can add new IBC light client in the form of Wasm bytecode (provided they are\\nable to submit the governance proposal transaction and that it passes) as well as instantiate clients using any created\\nclient type. This allows any chain to update its own light client in other chains without going through the steps outlined above.\\n","Decision":"We decided to implement the Wasm light client module as a light client proxy that will interface with the actual light client\\nuploaded as Wasm bytecode. To enable usage of the Wasm light client module, users need to add it to the list of allowed clients\\nby updating the `AllowedClients` parameter in the 02-client submodule of core IBC.\\n```go\\nparams := clientKeeper.GetParams(ctx)\\nparams.AllowedClients = append(params.AllowedClients, exported.Wasm)\\nclientKeeper.SetParams(ctx, params)\\n```\\nAdding a new light client contract is governance-gated. To upload a new light client users need to submit\\na [governance v1 proposal](https:\/\/docs.cosmos.network\/main\/modules\/gov#proposals) that contains the `sdk.Msg` for storing\\nthe Wasm contract's bytecode. The required message is `MsgStoreCode` and the bytecode is provided in the field `wasm_byte_code`:\\n```proto\\n\/\/ MsgStoreCode defines the request type for the StoreCode rpc.\\nmessage MsgStoreCode {\\n\/\/ signer address\\nstring signer = 1;\\n\/\/ wasm byte code of light client contract. It can be raw or gzip compressed\\nbytes wasm_byte_code = 2;\\n}\\n```\\nThe RPC handler processing `MsgStoreCode` will make sure that the signer of the message matches the address of authority allowed to\\nsubmit this message (which is normally the address of the governance module).\\n```go\\n\/\/ StoreCode defines a rpc handler method for MsgStoreCode\\nfunc (k Keeper) StoreCode(goCtx context.Context, msg *types.MsgStoreCode) (*types.MsgStoreCodeResponse, error) {\\nif k.GetAuthority() != msg.Signer {\\nreturn nil, errorsmod.Wrapf(ibcerrors.ErrUnauthorized, \"expected %s, got %s\", k.GetAuthority(), msg.Signer)\\n}\\nctx := sdk.UnwrapSDKContext(goCtx)\\nchecksum, err := k.storeWasmCode(ctx, msg.WasmByteCode, ibcwasm.GetVM().StoreCode)\\nif err != nil {\\nreturn nil, errorsmod.Wrap(err, \"failed to store wasm bytecode\")\\n}\\nemitStoreWasmCodeEvent(ctx, checksum)\\nreturn &types.MsgStoreCodeResponse{\\nChecksum: checksum,\\n}, nil\\n}\\n```\\nThe contract's bytecode is not stored in state (it is actually unnecessary and wasteful to store it, since\\nthe Wasm VM already stores it and can be queried back, if needed). The checksum is simply the hash of the bytecode\\nof the contract and it is stored in state in an entry with key `checksums` that contains the checksums for the bytecodes that have been stored.\\n### How light client proxy works?\\nThe light client proxy behind the scenes will call a CosmWasm smart contract instance with incoming arguments serialized\\nin JSON format with appropriate environment information. Data returned by the smart contract is deserialized and\\nreturned to the caller.\\nConsider the example of the `VerifyClientMessage` function of `ClientState` interface. Incoming arguments are\\npackaged inside a payload object that is then JSON serialized and passed to `queryContract`, which executes `WasmVm.Query`\\nand returns the slice of bytes returned by the smart contract. This data is deserialized and passed as return argument.\\n```go\\ntype QueryMsg struct {\\nStatus               *StatusMsg               `json:\"status,omitempty\"`\\nExportMetadata       *ExportMetadataMsg       `json:\"export_metadata,omitempty\"`\\nTimestampAtHeight    *TimestampAtHeightMsg    `json:\"timestamp_at_height,omitempty\"`\\nVerifyClientMessage  *VerifyClientMessageMsg  `json:\"verify_client_message,omitempty\"`\\nCheckForMisbehaviour *CheckForMisbehaviourMsg `json:\"check_for_misbehaviour,omitempty\"`\\n}\\ntype verifyClientMessageMsg struct {\\nClientMessage *ClientMessage `json:\"client_message\"`\\n}\\n\/\/ VerifyClientMessage must verify a ClientMessage.\\n\/\/ A ClientMessage could be a Header, Misbehaviour, or batch update.\\n\/\/ It must handle each type of ClientMessage appropriately.\\n\/\/ Calls to CheckForMisbehaviour, UpdateSta\u00e5te, and UpdateStateOnMisbehaviour\\n\/\/ will assume that the content of the ClientMessage has been verified\\n\/\/ and can be trusted. An error should be returned\\n\/\/ if the ClientMessage fails to verify.\\nfunc (cs ClientState) VerifyClientMessage(\\nctx sdk.Context,\\n_ codec.BinaryCodec,\\nclientStore storetypes.KVStore,\\nclientMsg exported.ClientMessage\\n) error {\\nclientMessage, ok := clientMsg.(*ClientMessage)\\nif !ok {\\nreturn errorsmod.Wrapf(ibcerrors.ErrInvalidType, \"expected type: %T, got: %T\", &ClientMessage{}, clientMsg)\\n}\\npayload := QueryMsg{\\nVerifyClientMessage: &VerifyClientMessageMsg{ClientMessage: clientMessage.Data},\\n}\\n_, err := wasmQuery[EmptyResult](ctx, clientStore, &cs, payload)\\nreturn err\\n}\\n```\\n### Global Wasm VM variable\\nThe 08-wasm keeper structure keeps a reference to the Wasm VM instantiated in the keeper constructor function. The keeper uses\\nthe Wasm VM to store the bytecode of light client contracts. However, the Wasm VM is also needed in the 08-wasm implementations of\\nsome of the `ClientState` interface functions to initialise a contract, execute calls on the contract and query the contract. Since\\nthe `ClientState` functions do not have access to the 08-wasm keeper, then it has been decided to keep a global pointer variable that\\npoints to the same instance as the one in the 08-wasm keeper. This global pointer variable is then used in the implementations of\\nthe `ClientState` functions.\\n","tokens":374,"id":1441,"Prediction":"## Decision\n\nWe will integrate a Wasm light client module into the ibc-go codebase to simplify the process of adding support for new light clients. This will enable the creation of new light clients as Wasm bytecode, which can be easily deployed and instantiated without requiring modifications to the ibc-go codebase or re-building chains' binaries.\n\n## Context\n\nThe current process of adding support for new light clients or updating an existing light client is time-consuming and error-prone, involving multiple steps such as modifying the codebase, re-building binaries, and passing a governance proposal. This can lead to issues such as long downtime for chains with many connections when upgrading their consensus.\n\n## Problem\n\nThe current process of adding support for new light clients or updating an existing light client is:\n\n* Time-consuming: requiring multiple steps and manual intervention\n* Error-prone: increasing the risk of errors and bugs\n* Inefficient: requiring re-building binaries and passing a governance proposal\n* Limiting: making it difficult for chains to upgrade their consensus and stay connected\n\n## Decision Drivers\n\n* Simplify the process of adding support for new light clients\n* Reduce the time and effort required to upgrade light clients\n* Increase the flexibility and autonomy of chains to upgrade their consensus\n* Improve the overall efficiency and scalability of the IBC ecosystem\n\n## Consequences\n\n* The Wasm light client module will provide a new way for chains to add support for new light clients without requiring modifications to the ibc-go codebase\n* The process of adding support for new light clients will be simplified and streamlined\n* Chains will have more flexibility and autonomy to upgrade their consensus and stay connected\n* The overall efficiency and scalability of the IBC ecosystem will be improved\n\n## Rationale\n\nThe Wasm light client module provides a flexible and scalable solution for adding support for new light clients. By allowing light clients to be implemented as Wasm bytecode, we can decouple the light client implementation from the ibc-go codebase, making it easier to add new light clients and upgrade existing ones. This will improve the overall efficiency and scalability of the IBC ecosystem, and provide more flexibility and autonomy to chains to upgrade their consensus and stay connected.","GenTime":"2024-08-10 05:42:54"}
{"File Name":"ibc-go\/adr-015-ibc-packet-receiver.md","Context":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n","Decision":"`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase channel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistant layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n","tokens":341,"id":1442,"Prediction":"## Decision\n\n**Use existing `baseapp.router` to route packets to application modules in the ICS 26 routing module instead of implementing a separate routing module.**\n\n## Context\n\nThe ICS 26 routing module is defined as a layer above each application module, verifying and routing messages to the destination modules. The existing `baseapp.router` functionality can be utilized to route packets to application modules, reducing the need for a separate routing module implementation.\n\n## Problem\n\nThe current implementation of routing module callbacks has two separate steps: verification and execution. This corresponds to the `AnteHandler`-`Handler` model inside the SDK. The verification step can be moved to the `AnteHandler` to increase developer ergonomics by reducing boilerplate verification code.\n\n## Decision Criteria\n\n1. Reuse existing functionality to reduce code duplication and complexity.\n2. Improve developer ergonomics by reducing boilerplate verification code.\n3. Ensure consistent behavior and reliability in handling packets and routing messages.\n\n## Consequences\n\n1. Reduced code complexity and duplication.\n2. Improved developer experience and ergonomics.\n3. Potential for increased reliability and consistency in handling packets and routing messages.\n\n## Rationale\n\nUsing the existing `baseapp.router` to route packets to application modules is the most efficient and scalable solution. This approach reduces the need for a separate routing module implementation, which would require additional development and maintenance efforts. By leveraging the existing functionality, we can focus on improving the overall performance and reliability of the ICS 26 routing module.\n\nAdditionally, moving the verification step to the `AnteHandler` simplifies the code and reduces the need for boilerplate verification code, making it easier for developers to work with the routing module. This approach also ensures that the verification step is executed consistently and reliably, which is critical for the integrity and reliability of the ICS 26 routing module.\n\nOverall, this decision is expected to have a positive impact on the maintainability, scalability, and reliability of the ICS 26 routing module, while also improving the developer experience and ergonomics.","GenTime":"2024-08-10 05:43:17"}
{"File Name":"verify-frontend\/0004-journey-hint-tracking-cookie.md","Context":"## Context\\nThe encrypted `verify-front-journey-hint` cookie was originally created to enable the non-repudiation journey.\\nWhen user selected an IDP, the entity_id was stored in the cookie and if the RP then sent a new Authn request\\nwith a parameter `journey_hint=submission_confirmation` the `\/confirm-your-identity` page was rendered with the\\nIDP from the cookie. The cookie was a simple JSON object:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\"\\n}\\n```\\nIn the early 2018 we introduced a sign-in hint to help users remind what IDP they used previously. The cookie has\\nbeen repurposed and new properties were added to it to track the IDP for each status the user encountered.\\nThe expiry date was also extended to 18 months. For example:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\",\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nFAILED: \"https:\/\/idp-entity-id-1.com\",\\n...\\n}\\n```\\nIf the user has any value in SUCCESS we show the user the sign-in hint for that IDP.\\n","Decision":"While implementing a new Pause & Resume functionality, we came across a requirement when we needed to store the status,\\nthe selected IDP and also the RP user has paused with. Rather than creating a new cookie it was decided to re-factor the current\\njourney-hint cookie to support this. In order to know what the latest state was, a new `STATE` object was introduced in the schema.\\nThe `STATE` gets updated with every new Authn response from an IDP. The `ATTEMPT` and `SUCCESS` was kept to keep the cookie backwards\\ncompatible and to help with identifying if there ever was a successful verification. The `entity_id` property got removed and the\\nnon-repudiation journey now uses the `ATTEMPT` value. The cookie schema now looks like this:\\n```\\n{\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nSTATE:  {\\nIDP: \"https:\/\/idp-entity-id.com\",\\nRP: \"https:\/\/rp-entity-id.com\",\\nSTATUS: <SUCCESS | FAILED | FAILED_UPLIFT | CANCEL | PENDING>\\n}\\n}\\n```\\n","tokens":273,"id":1449,"Prediction":"## Decision\n\n**Decision:** We will refactor the `verify-front-journey-hint` cookie to use a more robust data structure, such as a JSON Web Token (JWT), to store the IDP information and sign-in hint.\n\n**Rationale:**\n\n1. The current implementation uses a simple JSON object to store the IDP information, which is prone to tampering and manipulation. A JWT provides a secure way to store and verify the data.\n2. The cookie has evolved over time to store additional information, such as the IDP for each status the user encountered. A JWT can accommodate this growth and provide a more flexible way to store and retrieve the data.\n3. The current implementation uses a simple JSON object to store the IDP information, which is not secure. A JWT can be signed and verified to ensure the integrity of the data.\n4. The current implementation uses a simple JSON object to store the IDP information, which is not scalable. A JWT can be easily extended to store additional information and can be easily verified and validated.\n\n**Consequences:**\n\n1. The refactored cookie will provide a more secure way to store and verify the IDP information.\n2. The refactored cookie will provide a more flexible way to store and retrieve the IDP information.\n3. The refactored cookie will provide a more scalable way to store and verify the IDP information.\n4. The refactored cookie will require additional code to generate and verify the JWT, which may add complexity to the system.\n\n**Alternatives Considered:**\n\n1. We considered using a different data structure, such as a serialized object, to store the IDP information. However, a JWT provides a more secure and scalable way to store and verify the data.\n2. We considered using a different authentication mechanism, such as OAuth, to authenticate the user. However, the current implementation uses cookies to store the IDP information, and a JWT provides a more secure way to store and verify the data.\n\n**Status:** Accepted\n\n**Implementation:** The implementation will involve generating a JWT with the IDP information and storing it in the `verify-front-journey-hint` cookie. The JWT will be signed with a secret key to ensure the integrity of the data. The system will validate the JWT on each request to ensure that the data has not been tampered with.","GenTime":"2024-08-10 05:45:10"}
{"File Name":"inner-source_kotlin_spark_template\/20180617-flyway.md","Context":"## Context\\nDevelopers need some tool for DB versioning and safe migrations.\\n","Decision":"We will introduce dockerized flyway setup for local development.\\nMigration scripts will also be provided.\\n","tokens":17,"id":1462,"Prediction":"## Decision\n\n**DB Versioning and Safe Migrations**\n\n### Context\n\nDevelopers need a tool for DB versioning and safe migrations to manage database schema changes and ensure data consistency and integrity.\n\n### Decision\n\nWe will use Liquibase, an open-source database migration tool, to manage database versioning and safe migrations. Liquibase provides a robust and flexible solution for managing database schema changes, allowing us to track and revert changes, as well as automate the deployment of database changes.\n\n### Rationale\n\nWe chose Liquibase for several reasons:\n\n1. **Robust migration management**: Liquibase provides a robust framework for managing database schema changes, allowing us to track and revert changes with ease.\n2. **Flexibility**: Liquibase supports a wide range of databases, including MySQL, PostgreSQL, and Oracle, making it a versatile solution for our diverse database ecosystem.\n3. **Automated deployment**: Liquibase allows us to automate the deployment of database changes, ensuring that our database schema is always up-to-date and consistent across environments.\n4. **Open-source and community-driven**: Liquibase is an open-source project with an active community, ensuring that we have access to a wealth of documentation, support, and contributions from the developer community.\n\n### Consequences\n\n1. **Improved database management**: By using Liquibase, we will be able to manage database schema changes more effectively, reducing the risk of errors and data inconsistencies.\n2. **Simplified migration process**: Liquibase will automate the deployment of database changes, making it easier to manage database schema changes across environments.\n3. **Increased flexibility**: Liquibase's support for multiple databases will allow us to easily switch between different databases and environments.\n\n### Monitoring and Review\n\nWe will monitor the performance and effectiveness of Liquibase and review its usage regularly to ensure that it meets our needs and to identify areas for improvement.","GenTime":"2024-08-10 05:48:43"}
{"File Name":"inner-source_kotlin_spark_template\/20180525-gradle-kotlin.md","Context":"## Context\\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones.\\nIntellij IDEA has great support for Kotlin based builds.\\n","Decision":"We will adopt Kotlin as a Gradle build's definition language.\\n","tokens":32,"id":1463,"Prediction":"## Decision\n\n**Use Kotlin for Gradle builds instead of Groovy**\n\n## Context\n\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones. Intellij IDEA has great support for Kotlin based builds.\n\n## Problem\n\nOur current Gradle builds are written in Groovy, which can be difficult for new team members to understand and maintain. Additionally, IntelliJ IDEA's support for Groovy-based builds is not as robust as it is for Kotlin-based builds.\n\n## Decision\n\nTo improve the maintainability and ease of use of our Gradle builds, we will switch from Groovy to Kotlin. Kotlin is a more modern and concise language that is easier to learn and use, and IntelliJ IDEA provides excellent support for Kotlin-based builds.\n\n## Rationale\n\n* Kotlin is a more modern language that is easier to learn and use than Groovy.\n* Kotlin's concise syntax and type system make it easier to write and maintain Gradle builds.\n* IntelliJ IDEA's support for Kotlin-based builds is more robust and feature-rich than its support for Groovy-based builds.\n* Switching to Kotlin will make our Gradle builds more maintainable and easier to understand for new team members.\n\n## Consequences\n\n* We will need to refactor our existing Gradle builds to use Kotlin instead of Groovy.\n* We will need to update our build scripts to use Kotlin syntax and features.\n* We will need to provide training and documentation to new team members on how to use Kotlin for Gradle builds.\n\n## Status\n\nThis decision is approved and will be implemented in the next release.","GenTime":"2024-08-10 05:49:00"}
{"File Name":"aws_infrastructure\/0005-use-terraform-as-infrastructure-as-code.md","Context":"## Context\\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\\n","Decision":"Use Terraform\\n","tokens":21,"id":1464,"Prediction":"## Decision\n\n**Decision:** We will use Terraform as our Infrastructure as Code (IaC) tool to manage and track the resources running in our AWS account.\n\n**Context:** As our organization grows, it has become increasingly important to maintain a clear and accurate record of the resources running in our AWS account. This includes infrastructure such as EC2 instances, RDS databases, S3 buckets, and more. Without a centralized system for tracking these resources, it becomes difficult to ensure compliance with security and governance policies, and to quickly identify and remediate any issues that may arise.\n\n**Problem:** Our current approach to managing AWS resources involves manually creating and updating resources through the AWS Management Console, which can lead to errors, inconsistencies, and a lack of visibility into our infrastructure.\n\n**Reasoning:** Terraform is a popular and widely-used IaC tool that allows us to define our infrastructure as code, which can be version-controlled and reused across multiple environments. By using Terraform, we can ensure that our infrastructure is consistently configured and deployed, and that we have a clear record of all resources running in our AWS account.\n\n**Consequences:**\n\n* Improved visibility and control over our AWS resources\n* Reduced risk of errors and inconsistencies\n* Increased efficiency in deploying and managing infrastructure\n* Simplified compliance with security and governance policies\n* Better collaboration and knowledge sharing among team members\n\n**Alternatives Considered:** We considered using AWS CloudFormation as our IaC tool, but ultimately decided on Terraform due to its wider adoption and larger community of users and developers.\n\n**Status:** This decision is approved and will be implemented in the next sprint.","GenTime":"2024-08-10 05:49:18"}
{"File Name":"branchout\/0002-language.md","Context":"## Context\\nA language should be universal, simple and easily testable\\nOptions\\n* Shell\\n* Go\\n* Java\\n* JavaScript\\nThere should be very few dependencies\\n","Decision":"Shell\\n* No dependencies\\n* Installed pretty much everywhere developers are\\n","tokens":36,"id":1472,"Prediction":"## Decision\n\n**Decision:** Choose JavaScript as the language for the software.\n\n**Context:** A language should be universal, simple, and easily testable, with very few dependencies.\n\n**Rationale:**\n\n* **Universal**: JavaScript is widely used and supported by most browsers, making it a good choice for a language that needs to be accessible to a broad audience.\n* **Simple**: JavaScript is a relatively simple language to learn and use, with a syntax that is easy to understand and a large community of developers who can provide support and resources.\n* **Easily testable**: JavaScript has a wide range of testing frameworks and tools available, making it easy to write and run tests for the software.\n* **Few dependencies**: JavaScript has a small number of dependencies, making it easy to set up and maintain the software.\n\n**Alternatives considered:**\n\n* Shell: While shell scripting can be a good choice for simple tasks, it may not be suitable for a more complex software with many dependencies.\n* Go: Go is a good language, but it may not be as widely supported or used as JavaScript, and may require more dependencies.\n* Java: Java is a more complex language with a steeper learning curve, and may require more dependencies and setup.\n\n**Conclusion:** JavaScript is the best choice for the software due to its universality, simplicity, and ease of testing, as well as its few dependencies.","GenTime":"2024-08-10 05:51:44"}
{"File Name":"uniprot-rest-api\/0003-spring-framework.md","Context":"## Context\\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\\nadditional features (than one would otherwise write themselves), etc.\\n","Decision":"We have used the [Spring framework](https:\/\/spring.io\/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\\n","tokens":35,"id":1477,"Prediction":"## Decision\n\n**Use a Micro-Framework for the API Layer**\n\n## Context\n\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability, additional features (than one would otherwise write themselves), etc.\n\n## Problem\n\nThe project requires a robust and scalable API layer to handle incoming requests and provide a seamless user experience. The development team is considering using a full-fledged framework to build the API, but is concerned about the added complexity and potential overhead.\n\n## Decision\n\nAfter careful consideration, we have decided to use a micro-framework for the API layer. This decision is based on the following factors:\n\n* Simplicity: Micro-frameworks are designed to be lightweight and easy to use, which will reduce the learning curve for the development team and minimize the risk of introducing unnecessary complexity.\n* Flexibility: Micro-frameworks provide a high degree of flexibility, allowing us to customize and extend the framework to meet the specific needs of our project.\n* Performance: Micro-frameworks are optimized for performance, which is critical for our API layer, where speed and responsiveness are essential.\n* Maintenance: With a micro-framework, we can maintain and update the API layer independently of the rest of the application, reducing the risk of introducing unintended side effects.\n\n## Consequences\n\n* The development team will need to learn the micro-framework and its specific features and best practices.\n* The API layer will be more lightweight and efficient, which will improve the overall performance and scalability of the application.\n* The flexibility of the micro-framework will allow us to quickly adapt to changing requirements and add new features as needed.\n* The maintenance burden will be reduced, as the API layer can be updated independently of the rest of the application.\n\n## Rationale\n\nBy using a micro-framework for the API layer, we can achieve the benefits of a robust and scalable API while minimizing the complexity and overhead associated with a full-fledged framework. This decision will enable the development team to focus on building a high-quality API that meets the needs of our users, while also providing a maintainable and efficient foundation for the application.","GenTime":"2024-08-10 05:53:32"}
{"File Name":"evidence-api\/0002-create-two-platform-apis-instead-of-a-single-api.md","Context":"## Context\\nThis application was created to allow services to manage gathered evidence documents in a consistent, safe and centralised way. An early decision point was reached with regards to designing the architecture around this capability.\\nThe basic functionalities known to be required are:\\n- secure storage and management of documents provided by residents and other third parties\\n- tracking of rights and retention over those documents by the council\\n- requests and approval of evidence from third parties\\n- management of evidence requests by officers\\nInfluencing factors:\\n- HackIT's [API Playbook](https:\/\/github.com\/LBHackney-IT\/API-Playbook-v2-beta)\\n- [Clean Architecture](https:\/\/github.com\/madetech\/clean-architecture) principles\\n","Decision":"We decided to create two Platform APIs (as defined in the API Playbook):\\n- A [Documents API](https:\/\/github.com\/LBHackney-IT\/documents-api) for the storage and management of documents and claims over them\\n- An [Evidence API](https:\/\/github.com\/LBHackney-IT\/evidence-api) for the management of evidence requests and reviews\\n","tokens":151,"id":1480,"Prediction":"## Decision\n\n**Decision:** Implement a Microservices-based Architecture for the Evidence Management Application\n\n## Context\n\nAs described, the Evidence Management Application requires secure storage and management of documents, tracking of rights and retention, requests and approval of evidence, and management of evidence requests. Given the complexity of these requirements, we need to decide on an architecture that can effectively meet these needs.\n\n## Problem\n\nThe application requires a scalable and maintainable architecture that can handle a large volume of documents, multiple user roles, and complex workflows. A monolithic architecture may not be able to meet these demands, as it would be difficult to scale individual components and would lead to a rigid and inflexible system.\n\n## Solution\n\nWe propose implementing a Microservices-based Architecture for the Evidence Management Application. This architecture would consist of multiple small, independent services that communicate with each other using APIs. Each service would be responsible for a specific business capability, such as document storage, rights and retention tracking, and evidence requests.\n\nThis approach would provide several benefits, including:\n\n* Scalability: Each service can be scaled independently, allowing us to allocate resources more efficiently and handle increased traffic.\n* Flexibility: Microservices can be developed and deployed independently, allowing for faster development and deployment of new features.\n* Resilience: If one service experiences issues, it would not affect the entire application, ensuring that the system remains available and responsive.\n* Maintainability: With a smaller codebase and a clear separation of concerns, maintenance and updates would be easier and less prone to errors.\n\n## Consequences\n\nImplementing a Microservices-based Architecture would require significant changes to the existing design and development approach. This would include:\n\n* Breaking down the monolithic application into smaller, independent services\n* Designing and implementing APIs for communication between services\n* Implementing service discovery and load balancing mechanisms\n* Ensuring data consistency and integrity across services\n\nHowever, the benefits of a Microservices-based Architecture would outweigh the costs, providing a more scalable, flexible, and maintainable system that can effectively meet the requirements of the Evidence Management Application.\n\n## Rationale\n\nThe decision to implement a Microservices-based Architecture is influenced by the HackIT's API Playbook and Clean Architecture principles. The API Playbook emphasizes the importance of designing APIs that are scalable, secure, and maintainable, which aligns with the benefits of a Microservices-based Architecture. Clean Architecture principles also support the idea of separating concerns and designing for scalability and maintainability.\n\nBy implementing a Microservices-based Architecture, we can ensure that the Evidence Management Application meets the requirements of secure storage and management of documents, tracking of rights and retention, requests and approval of evidence, and management of evidence requests, while also providing a scalable, flexible, and maintainable system that can adapt to changing requirements and technologies.","GenTime":"2024-08-10 05:54:39"}
{"File Name":"holochain-rust\/0010-write-our-own-dht-implementation.md","Context":"## Context\\nFor the Go based prototype we used IPFS' Kademlia DHT implementations with edits.\\nSince we are switching over to Rust, we can't easily continue using that code base.\\nMore importantly, there are too many Holochain specific additions to a vanilla Kademlia DHT, as well as other possible implementations of achieving entry resilience, routing, our forthcoming World Model, CRDT handling, gossip and our graph properties via linking, so it does not make sense to change existing implementations but instead roll our own.\\n","Decision":"We will build our own DHT implementation in Rust from scratch.\\n","tokens":110,"id":1494,"Prediction":"## Decision\n\n**Decision:** Implement a custom DHT in Rust, tailored to Holochain's specific requirements, rather than reusing the modified Kademlia DHT implementation from the Go prototype.\n\n**Context:** As we switch from Go to Rust, we cannot easily reuse the existing IPFS Kademlia DHT implementation. Moreover, the Holochain-specific additions and requirements, such as entry resilience, routing, World Model, CRDT handling, gossip, and graph properties via linking, make it impractical to adapt the existing implementation.\n\n**Rationale:** A custom DHT implementation in Rust will allow us to tailor the design to Holochain's unique needs, ensuring a more efficient, scalable, and maintainable solution. This approach will also enable us to integrate the DHT with other Holochain components, such as the World Model and CRDT handling, in a more seamless and optimized manner.\n\n**Consequences:**\n\n* Increased development time and effort\n* Potential for a steeper learning curve for new team members\n* Improved customization and integration with other Holochain components\n* Enhanced performance and scalability\n\n**Alternatives Considered:**\n\n* Reusing the modified Kademlia DHT implementation from the Go prototype, with significant modifications to adapt it to Rust\n* Using an existing Rust DHT library, potentially requiring additional modifications to meet Holochain's specific requirements\n\n**Status:** Accepted","GenTime":"2024-08-10 05:59:21"}
{"File Name":"holochain-rust\/0003-redux-architecture-pattern.md","Context":"## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n","Decision":"The new holochain architecture will follow a redux architecture in order for an agent to have one global state.\\nWe will apply nested state objects which represent a state tree, with sub states for each module.\\nWe use reference counting smart pointers for the sub\\nstates such that it is possible for each module's\\nreducer to decide if the sub state is to be mutated or reused.\\n","tokens":92,"id":1495,"Prediction":"## Decision\n\n**Title:** Implement a Global State within the Agent using a Shared Memory Approach\n\n**Context:** As we rewrite the Holochain Go code in Rust, we aim to simplify the architecture and manage the complexity of the p2p network of agents. The current implicit dependencies and stateful objects make it difficult to maintain and extend the system.\n\n**Problem:** The existing codebase has a single global state within the agent, which feels appropriate for balancing the distributed nature of the network. However, the current implementation is not scalable and leads to complexity.\n\n**Decision:** We will implement a shared memory approach within the agent, using a centralized global state to manage the complexity and dependencies between different modules and stateful objects.\n\n**Rationale:**\n\n* A centralized global state allows for easier management of dependencies and interactions between different modules and stateful objects.\n* Shared memory approach enables efficient communication and data sharing between different parts of the system.\n* This design aligns with the original intention of having a single global state within the agent, while providing a more scalable and maintainable architecture.\n\n**Consequences:**\n\n* The system will have a single point of truth for the global state, making it easier to reason about and debug.\n* The complexity of the system will be reduced, as dependencies and interactions will be explicitly defined and managed.\n* The shared memory approach may introduce additional challenges related to concurrency and synchronization, which will need to be addressed through proper locking and synchronization mechanisms.\n\n**Monitoring and Feedback:** We will monitor the performance and scalability of the system, and gather feedback from the development team and users to ensure that the chosen architecture meets the requirements and expectations. If necessary, we will adjust the design and implementation to address any issues that arise.","GenTime":"2024-08-10 05:59:40"}
{"File Name":"holochain-rust\/0006-splitting-agent-into-front-house-back-house-or-not.md","Context":"## Context\\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\\n* the agent's keys are by definition part of the front-house\\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\\n","Decision":"We decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\\n","tokens":423,"id":1500,"Prediction":"## Decision\n\nTo address the issues raised by the separation of the Holochain agent into front-house and back-house, we will adopt a different approach to agency, separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information. This will allow us to maintain the integrity of the agent's keys and ensure that the back-house can operate independently without requiring constant communication with the front-house.\n\n## Context\n\nAs discussed in the context, the initial separation of the Holochain agent into front-house and back-house was intended to enable the front-house to be rewritten in JavaScript to run in a web browser. However, this approach created a significant problem: the back-house would need to communicate with the front-house to obtain signatures for every packet of communication, which would require the agent's keys to be accessible and would compromise the security of the agent.\n\n## Problem\n\nThe main problem is that the back-house needs to communicate with the front-house to obtain signatures for every packet of communication, which would require the agent's keys to be accessible and would compromise the security of the agent.\n\n## Decision\n\nTo address this problem, we will separate the authoring or creative agency from the part that runs validations and holds DHT shards and world model information. This will allow us to maintain the integrity of the agent's keys and ensure that the back-house can operate independently without requiring constant communication with the front-house.\n\n## Consequences\n\n* The back-house will no longer need to communicate with the front-house to obtain signatures for every packet of communication, which will improve the security and reliability of the agent.\n* The front-house will be responsible for managing the agent's keys and providing signatures as needed.\n* The back-house will be responsible for running validations and holding DHT shards and world model information, and will proxy for the front-house as needed.\n\n## Rationale\n\nThe separation of the authoring or creative agency from the part that runs validations and holds DHT shards and world model information is necessary to ensure the security and integrity of the agent's keys. By maintaining the integrity of the agent's keys, we can ensure that the back-house can operate independently and securely, without compromising the security of the agent.","GenTime":"2024-08-10 06:01:23"}
{"File Name":"taxonomy-manager\/001-graph-database.md","Context":"## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n","Decision":"* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of \"maintenance only\" since June 2013.\\n### Positive Consequences\\n* PostgreSQL can be clustered, making it highly available\\n* PostgreSQL is broadly available as a managed service\\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\\n### Negative Consequences\\n* We have to design our own schema and code for interacting with the PostgreSQL database\\n","tokens":95,"id":1510,"Prediction":"## Decision\n\nBased on the context and problem statement, I propose the following decision:\n\n**Decision:** Implement a graph database as the storage layer for persisting and retrieving SKOS taxonomies, rather than a traditional relational database management system (RDBMS).\n\n**Rationale:**\n\n1. **Native support for RDF**: Graph databases can store RDF data natively, eliminating the need for a specialized serialization and deserialization implementation. This simplifies the data model and reduces the complexity of the system.\n2. **High availability and fault tolerance**: Graph databases are designed to handle large amounts of data and provide high availability and fault tolerance, ensuring that the system remains operational even in the event of hardware or software failures.\n3. **Learning curve**: While graph databases may have a steeper learning curve than traditional RDBMS, the benefits of native RDF support and high availability\/fault tolerance outweigh the additional complexity.\n4. **Maintenance overhead**: Graph databases are designed to handle large amounts of data and provide efficient query performance, reducing the maintenance overhead and allowing for easier scalability.\n5. **Vendor lock-in**: By choosing a graph database, we can minimize the risk of vendor lock-in, as there are multiple vendors offering graph database solutions, and we can easily switch to a different vendor if needed.\n\n**Alternative Considerations:**\n\n1. Implementing a specialized serialization and deserialization implementation for a traditional RDBMS, which would add complexity and overhead to the system.\n2. Using a hybrid approach, combining a traditional RDBMS with a graph database, which would add complexity and overhead to the system.\n\n**Conclusion:**\n\nImplementing a graph database as the storage layer for persisting and retrieving SKOS taxonomies provides a scalable, high-availability, and fault-tolerant solution that is well-suited for storing and querying rich, free-form linked data.","GenTime":"2024-08-10 06:04:53"}
{"File Name":"molgenis-frontend\/0003-have-app-create-settings.md","Context":"## Context\\nApps (for example the data-explorer-2 or the biobank-explorer) may need settings data to function properly. As these settings are dynamic (may be changed at run time), and may change between deployments these settings need to be stored somewhere (molgenis database). This poses two problems; 1) how to link the app with the settings data, 2) how to control access to the settings.\\n","Decision":"We leave it up to the app to create the settings entity. Each app creates 0 or 1 settings entities. By default everyone can read the settings data. Depending on the app the entity contains 1 or more rows ( in case of multiple rows the app is responsible using the correct row(s) in each context).\\nRejected alternatives:\\n- Add settings via bootstrapped entity in molgenis core; This was deemed as insufficiently flexible due to the dependance on Java knowledge and (possible major version) core release.\\n- Use of app manager; App-manager does not facilitate an automated deployment process. App manager restricts the app architecture and settings structure.\\n- Use of entity meta data: Does not allow for settings per 'entity and app' combination. For instance the dataexplorer needs n settings rows or n tables.\\n- Proxy settings request to external repository; Deemed as hard to manage, app and proxy settings need to be kept in sync, also requires knowledge of proxy (nginx, apache) to configure.\\n","tokens":90,"id":1514,"Prediction":"## Decision\n\nTo address the context of storing dynamic settings data for apps and controlling access to these settings, we will implement a settings management system that utilizes a combination of a settings table in the molgenis database and a separate settings service.\n\n### Context\n\nApps (e.g., data-explorer-2 or biobank-explorer) require dynamic settings data to function properly, which may be changed at runtime and may differ between deployments. These settings need to be stored in a centralized location to ensure consistency and manageability.\n\n### Decision\n\nTo solve the problem of linking the app with the settings data and controlling access to the settings, we will:\n\n1. Create a settings table in the molgenis database to store the dynamic settings data. This table will have a unique identifier for each setting, its value, and any relevant metadata (e.g., data type, description).\n2. Implement a settings service that provides a centralized interface for apps to retrieve and update settings data. This service will be responsible for authenticating and authorizing access to the settings data based on user roles or permissions.\n3. Use a unique identifier for each app to link the app with the corresponding settings data in the settings table. This will allow us to store settings specific to each app and ensure that the correct settings are retrieved and updated.\n4. Implement access control mechanisms in the settings service to restrict access to settings data based on user roles or permissions. This will ensure that only authorized users can view or modify settings data.\n\n### Consequences\n\nBy implementing a settings management system using a settings table and a separate settings service, we will:\n\n* Centralize settings data storage and management, making it easier to maintain and update settings across multiple apps.\n* Improve security by controlling access to settings data based on user roles or permissions.\n* Enhance flexibility by allowing apps to retrieve and update settings data dynamically at runtime.\n* Simplify app development by providing a standardized interface for apps to access settings data.\n\n### Rationale\n\nThe proposed solution addresses the two main problems mentioned in the context: linking the app with the settings data and controlling access to the settings. By using a unique identifier for each app and implementing access control mechanisms in the settings service, we can ensure that each app has access to its corresponding settings data and that only authorized users can view or modify settings data.","GenTime":"2024-08-10 06:06:27"}
{"File Name":"fundraising-application\/005_URL_Schema.md","Context":"## Context\\nWhen we developed the Fundraising application, we did not pay close attention to the URL schema and ended up with three different styles:\\n* \"Slashy\", pseudo-[REST][1]-style URLs like `\/donation\/add`\\n* \"action-sentences\" like `\/apply-for-membership`\\n* a combination of both like `\/contact\/get-in-touch`\\nWe don't do search engine optimization (SEO) in the form of \"meaningful, localized and stable URLs\", as the main traffic to the donation page comes from banners and we don't have much relevant content to that search engines can index.\\n","Decision":"We will use the \"action-sentence\" style for URLs in the future. They should follow the pattern `verb-noun` or `verb-preposition-noun`.\\nOur reasoning behind the decision:\\n* They convey more information about what the route does, because we can use all verbs of the English language instead of restricting us to `GET` and `POST`.\\n* REST-style URLs are deceiving because our application has no real API and is not explicitly written with a [RESTful][1] architecture.\\n* We can still have a dedicated REST API in the future, by using the `\/api` route.\\n* The sentence style fits better to our use case architecture, which also read more like sentences.\\nWhenever we change a URL, we decide if we need to create a redirect from the old one to the new in the NGinX configuration. GET support is a good indicator for the need for a redirect. If route is more like a functional \"endpoint\" like `donation\/update`, then we don't need a redirect.\\nIf we need to add i18n information to the URL at some point, we will do it with a \"subdirectory prefix\", e.g. `\/de\/apply-for-membership`, `\/en\/apply-for-membership`. The cons listed at https:\/\/support.google.com\/webmasters\/answer\/182192?hl=en do not outweigh the benefits.\\n","tokens":128,"id":1522,"Prediction":"## Decision\n\n**Decision:** We will standardize the URL schema in the Fundraising application to use a consistent and meaningful approach.\n\n**Context:** As mentioned in the context, the current URL schema in the Fundraising application is inconsistent, with three different styles used across the application. This has led to confusion and makes it difficult for users to navigate the application.\n\n**Problem:** The current URL schema is not user-friendly, making it hard for users to understand the purpose of each page. Additionally, the inconsistent style may lead to confusion and make it difficult for users to find what they are looking for.\n\n**Reasoning:** We have decided to standardize the URL schema to make it more user-friendly and easier to navigate. This will improve the overall user experience and make it easier for users to find what they are looking for.\n\n**Consequences:**\n\n* Improved user experience: The standardized URL schema will make it easier for users to navigate the application and find what they are looking for.\n* Better maintainability: A consistent URL schema will make it easier to maintain and update the application.\n* Improved SEO: Although we don't currently do SEO, a standardized URL schema will make it easier to implement SEO best practices in the future.\n\n**Alternatives Considered:**\n\n* We considered keeping the current URL schema, but decided that it would not improve the user experience and would make it harder to maintain and update the application.\n* We also considered using a completely different URL schema, but decided that it would be too much of a change and would disrupt the user experience.\n\n**Status:** This decision is implemented and will be reviewed and refined as needed.","GenTime":"2024-08-10 06:09:24"}
{"File Name":"fundraising-application\/020_PayPal_IPN_Queue.md","Context":"## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n","Decision":"* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\nSince this was the first occurrence of the problem, and the Fundraising\\nApplication system is now running well again, we decided against introducing\\nthe IPN queue feature.\\n","tokens":190,"id":1523,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**: Implement a queue system to store incoming PayPal IPNs for processing by the Fundraising Frontend application.\n\n**Context**: The PayPal IPNs on the Fundraising Frontend application started to fail after a deployment, and the error was only noticed days later when the Project Manager needed to export the data. The investigation revealed that error logging was inactive, and there was no access to the IPN log, making it difficult to debug the error.\n\n**Problem Statement**: The lack of transparency and automation in processing PayPal IPNs led to a situation where debugging errors was challenging, and the system was unable to resume processing IPNs once an error was fixed.\n\n**Decision Drivers**:\n\n* **Transparency**: Implementing a queue system would provide a stored record of incoming IPNs, allowing for easier debugging of errors.\n* **Automation**: The queue system would enable the system to automatically resume processing IPNs once an error is fixed, eliminating the need to re-fire IPNs.\n\n**Consequences**:\n\n* Improved transparency and debugging capabilities for errors related to PayPal IPNs.\n* Increased automation and reduced manual effort required to resolve errors.\n* Potential for improved system reliability and reduced downtime.\n\n**Rationale**: By implementing a queue system, the Fundraising Frontend application can ensure that incoming PayPal IPNs are stored and processed in a controlled manner, providing transparency and automation in error handling. This decision aims to improve the overall reliability and maintainability of the system.","GenTime":"2024-08-10 06:09:40"}
{"File Name":"fundraising-application\/003_Validation.md","Context":"## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https:\/\/symfony.com\/doc\/current\/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework\/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n","Decision":"For each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https:\/\/github.com\/wmde\/fundraising-frontend-content\/blob\/test\/i18n\/de_DE\/messages\/validations.json) in the [content repository](https:\/\/github.com\/wmde\/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\\n","tokens":225,"id":1532,"Prediction":"## Decision\n\n**Decision:** We will write our own validation logic instead of using an established library like Symfony Validation.\n\n**Context:** During the re-write of the Fundraising project in 2016, the team discovered that team members had different approaches to validation, with some advocating for using an established library and others suggesting writing their own validation logic.\n\n**Problem:** The team was torn between using an established library and writing their own validation logic. The arguments in favor of writing their own logic included not wanting to bind the domain layer to a concrete implementation, simplicity of individual validations, unknown maintenance cycles of the library, and the need for each developer to learn the API of the external library.\n\n**Reasoning:** After considering the pros and cons, the team decided to write their own validation logic. This decision was made to maintain control over the validation process, ensure simplicity and clarity of the code, and avoid potential dependencies on external libraries.\n\n**Consequences:**\n\n* The team will have full control over the validation process and can make changes as needed.\n* The code will be simpler and easier to understand, with less overhead from an external library.\n* The team will not be tied to a specific library or its maintenance cycles.\n* Each developer will not need to learn the API of an external library.\n\n**Alternatives Considered:** The team considered using an established library like Symfony Validation, but ultimately decided against it due to the reasons mentioned above.\n\n**Status:** This decision has been implemented and is currently in use in the Fundraising project.","GenTime":"2024-08-10 06:12:05"}
{"File Name":"opg-data-lpa-codes\/0001-flask-in-aws-lambda-function.md","Context":"## Context\\nFor this project, we would like to package up the API endpoints and logic into a small Flask app in a single lambda function.\\nWe found whilst working on the Documents integration that managing multiple lambda functions quickly became quite hard work,\\nespecially as in that project there was a lot of shared code about the place. In hindsight, we should have refactored the\\nshared code into separate lambda functions, but we never got to it due to time constraints. Also this would just give us\\nmore lambda functions to maintain.\\n#### Why Flask (or any other WSGI app) in a Lambda is a good idea\\n* We can re-use most of the infra setup from the last project so we can get started fast\\n* Easy to run and develop locally, just run like a normal Flask app\\n* Easy to test as it's a normal Flask app\\n* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well established\\n* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flask\\n(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be an\\nanti-pattern?\\n* Maintaining a single lambda function is much easier than maintaining many\\n* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessed\\nby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) so\\nit's a good candidate for experimenting with this, as nothing will need to be reused outside of this app\\n#### Why Flask (or any other WSGI app) in a Lambda is a bad idea\\n* A whole app in a FaaS seems like an anti-pattern (can't find any evidence to support\/contradict this)\\n* Potentially longer cold-start as it has to initialise the whole app not just a single function\\n* [Flask-Lambda](https:\/\/github.com\/sivel\/flask-lambda) is a small package with only 1 contributor, though it is a pretty\\nsimple script that just maps some request variables depending on the source of the request\\n","Decision":"* Single lambda function containing small Flask app that provides all endpoints\\n* Flask-Lambda to help easily switch between local dev and AWS\\n","tokens":469,"id":1540,"Prediction":"## Decision\n\nWe will use Flask as a WSGI application within a single Lambda function to package the API endpoints and logic. This decision is based on the benefits of reusing existing infrastructure, ease of development and testing, and the availability of extensive documentation and community support.\n\n## Context and Problem\n\nThe project requires packaging API endpoints and logic into a single Lambda function to avoid managing multiple Lambda functions and shared code. We found that managing multiple Lambda functions can be challenging, especially when there is shared code. We also want to reuse existing infrastructure and leverage the expertise of the development team in Flask.\n\n## Decision Drivers\n\n* Reusing existing infrastructure and expertise\n* Ease of development and testing\n* Availability of extensive documentation and community support\n* Simplicity and maintainability of a single Lambda function\n\n## Alternatives Considered\n\n* Using Chalice, a AWS library that provides a similar functionality\n* Creating multiple Lambda functions with shared code\n* Using a different WSGI application framework\n\n## Rationale\n\nUsing Flask as a WSGI application within a single Lambda function is the best option due to the benefits it provides. It allows us to reuse existing infrastructure and expertise, making it easier to develop and test the application. Additionally, it provides extensive documentation and community support, which can be beneficial for the development team.\n\nWhile using Chalice is an alternative, it provides more deployment-related functionality that we don't need. Creating multiple Lambda functions with shared code is not a good option as it would lead to the same challenges we are trying to avoid. Using a different WSGI application framework would require additional learning and setup, which would add complexity to the project.\n\n## Conclusion\n\nWe will use Flask as a WSGI application within a single Lambda function to package the API endpoints and logic. This decision is based on the benefits of reusing existing infrastructure, ease of development and testing, and the availability of extensive documentation and community support.","GenTime":"2024-08-10 06:15:02"}
{"File Name":"opg-metrics\/0002-aws-infrastructure.md","Context":"## Context\\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\\n","Decision":"To run in a fully managed AWS cloud environment using Terraform to manage it.\\n","tokens":31,"id":1545,"Prediction":"## Decision\n\n**Title:** Lightweight, Fully Managed, and Integratable System Architecture\n\n**Context:** Based on ADRs and developer needs, we require a system that is lightweight, fully managed, and easily integratable.\n\n**Decision:** We will implement a serverless architecture using a cloud-based platform such as AWS Lambda or Google Cloud Functions. This approach will provide a lightweight and fully managed system that can be easily scaled and integrated with other services.\n\n**Rationale:**\n\n* Lightweight: Serverless architecture eliminates the need for provisioning and managing servers, reducing the overall system footprint and making it more agile.\n* Fully Managed: Cloud providers manage the underlying infrastructure, including patching, scaling, and maintenance, freeing up our team to focus on application development.\n* Integratable: Serverless functions can be easily integrated with other services and APIs, allowing for seamless communication and data exchange.\n\n**Consequences:**\n\n* Increased development speed and agility due to reduced infrastructure management.\n* Improved scalability and reliability through automated scaling and load balancing.\n* Simplified integration with other services and APIs through standardized interfaces.\n* Potential increased costs due to usage-based pricing models.\n* Limited control over underlying infrastructure and potential vendor lock-in.\n\n**Status:** Accepted\n\n**Implementation:** We will begin implementing the serverless architecture by designing and deploying individual functions for each business capability. We will use a cloud-based platform and leverage its managed services for database, storage, and security. We will also establish clear guidelines and standards for function development, testing, and deployment.","GenTime":"2024-08-10 06:16:36"}
{"File Name":"origin\/0005-simplify-off-chain-storage.md","Context":"## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n","Decision":"We decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\\n","tokens":52,"id":1558,"Prediction":"## Decision\n\nTo address the issue of data inconsistency in hybrid storage (off-chain and on-chain), we will implement a \"write-ahead log\" mechanism to ensure that all data written off-chain is properly committed to the on-chain reference before considering it valid.\n\n## Context\n\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but its on-chain reference hadn't been committed yet. This would cause some data being corrupted under certain conditions.\n\n## Problem\n\nThe current implementation of hybrid storage allows for data to be written off-chain without ensuring that the corresponding on-chain reference has been updated. This leads to inconsistencies between the off-chain and on-chain data, resulting in corrupted data under certain conditions.\n\n## Decision\n\nTo resolve this issue, we will introduce a write-ahead log mechanism that ensures all data written off-chain is properly committed to the on-chain reference before considering it valid. This will guarantee that the off-chain and on-chain data remain consistent, preventing data corruption.\n\n## Rationale\n\nThe write-ahead log mechanism will provide a layer of abstraction between the off-chain and on-chain storage, allowing us to decouple the writing of data from the updating of the on-chain reference. This will ensure that all data written off-chain is properly committed to the on-chain reference before it is considered valid, preventing data inconsistencies and corruption.\n\n## Solution\n\nWe will implement a write-ahead log that stores all data written off-chain in a temporary buffer. Once the data is written to the off-chain storage, the write-ahead log will be updated to reflect the new state. The on-chain reference will then be updated to reflect the new state, ensuring that the off-chain and on-chain data remain consistent.\n\n## Consequences\n\nThe implementation of the write-ahead log mechanism will:\n\n* Ensure data consistency between off-chain and on-chain storage\n* Prevent data corruption caused by inconsistent data\n* Introduce a slight performance overhead due to the additional layer of abstraction\n* Require additional storage space for the write-ahead log\n\n## Alternatives Considered\n\nWe considered alternative solutions, such as:\n\n* Implementing a lock mechanism to prevent concurrent writes to the off-chain and on-chain storage\n* Using a distributed locking mechanism to ensure consistency across multiple nodes\n* Implementing a conflict resolution mechanism to handle inconsistent data\n\nHowever, these alternatives were deemed less effective or more complex than the write-ahead log mechanism, which provides a simple and efficient solution to the problem.","GenTime":"2024-08-10 06:21:26"}
{"File Name":"origin\/0010-more-complete-database-type-and-adjust-migrations.md","Context":"## Context\\nSo far, Origin has been using a development-friendly SQLite database on the backend to store data. While this has been beneficial in the early stages of development, in order to gain adoption and real-world use we should make it easier for anyone using the Origin SDK to get to production as fast as possible.\\n","Decision":"A decision has been made to move the Origin SDK to a mode production-ready database. We've decided to go with **PostgreSQL** instead of SQLite, as we've seen that this is the database of choice for many users and setups.\\n","tokens":64,"id":1561,"Prediction":"## Decision\n\nTo improve the scalability and ease of use for production-ready deployments, we will switch from SQLite to a cloud-native database solution, specifically Google Cloud Firestore.\n\n## Context\n\nAs mentioned in the context, Origin has been using SQLite for development purposes, which has been beneficial for early stages of development. However, as we move towards gaining adoption and real-world use, it is essential to make it easier for developers to deploy their applications to production environments. SQLite is not designed for high-traffic or high-concurrency use cases, and its limitations may hinder our ability to scale and meet the needs of our users.\n\n## Problem\n\nThe current use of SQLite presents several issues:\n\n1.  **Scalability**: SQLite is not designed to handle high traffic or large amounts of data, which may lead to performance issues and errors.\n2.  **Security**: SQLite does not provide robust security features, making it vulnerable to unauthorized access and data breaches.\n3.  **Ease of use**: SQLite requires manual configuration and maintenance, which can be time-consuming and error-prone.\n\n## Decision\n\nTo address these issues, we will switch to Google Cloud Firestore, a fully managed NoSQL document database that provides a scalable, secure, and easy-to-use solution for storing and managing data. Cloud Firestore offers the following benefits:\n\n1.  **Scalability**: Cloud Firestore is designed to handle high traffic and large amounts of data, ensuring that our application can scale to meet the needs of our users.\n2.  **Security**: Cloud Firestore provides robust security features, including data encryption, access controls, and authentication, to ensure the protection of our users' data.\n3.  **Ease of use**: Cloud Firestore provides a simple and intuitive API, eliminating the need for manual configuration and maintenance.\n\n## Consequences\n\nBy switching to Cloud Firestore, we will:\n\n1.  **Improve scalability**: Our application will be able to handle high traffic and large amounts of data, ensuring a better user experience.\n2.  **Enhance security**: Our users' data will be protected by robust security features, ensuring the integrity and confidentiality of their information.\n3.  **Simplify development**: Our developers will have a more streamlined and efficient development experience, with fewer worries about database management and scalability.\n\n## Rationale\n\nThe decision to switch to Cloud Firestore is based on the need to improve the scalability, security, and ease of use of our application. Cloud Firestore provides a robust and scalable solution that meets our requirements and enables us to focus on developing a high-quality application that meets the needs of our users.","GenTime":"2024-08-10 06:22:36"}
{"File Name":"origin\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1562,"Prediction":"## Decision\n\n**Title:** Implement a Microservices Architecture for the Project\n\n**Context:** As the project has grown in complexity and scope, it has become clear that a monolithic architecture is no longer sufficient to meet the requirements of the system. We need a more scalable and maintainable architecture that allows for independent development, deployment, and scaling of individual components.\n\n**Problem:** The current monolithic architecture is:\n\n* Difficult to scale: As the system grows, it becomes increasingly difficult to scale individual components without affecting the entire system.\n* Hard to maintain: Changes to one part of the system can have unintended consequences on other parts of the system.\n* Rigid: The monolithic architecture makes it difficult to make changes to individual components without affecting the entire system.\n\n**Decision:** We will implement a microservices architecture for the project, breaking down the system into smaller, independent services that can be developed, deployed, and scaled independently.\n\n**Rationale:** A microservices architecture offers several benefits, including:\n\n* Scalability: Individual services can be scaled independently, allowing for more efficient use of resources.\n* Maintainability: Changes to one service do not affect other services, making it easier to maintain and update the system.\n* Flexibility: The microservices architecture allows for greater flexibility in terms of technology choices and development methodologies.\n\n**Consequences:**\n\n* Increased complexity: Implementing a microservices architecture requires additional infrastructure and tooling to manage communication between services.\n* Higher operational overhead: With more services to manage, there is a higher operational overhead in terms of monitoring, logging, and troubleshooting.\n* Greater emphasis on API design: The microservices architecture requires careful design and implementation of APIs to ensure seamless communication between services.\n\n**Status:** This decision is currently being implemented, with the following services already developed:\n\n* Service 1: User Management\n* Service 2: Order Processing\n* Service 3: Inventory Management\n\n**Implementation:** The implementation of the microservices architecture will be done in the following phases:\n\n1. Service discovery: Implement a service discovery mechanism to allow services to find and communicate with each other.\n2. API design: Design and implement APIs for each service to ensure seamless communication between services.\n3. Infrastructure setup: Set up the necessary infrastructure, including load balancers, databases, and message queues.\n4. Service development: Develop each service independently, using the designed APIs and infrastructure.\n\n**Monitoring and Feedback:** The implementation of the microservices architecture will be monitored closely, with regular feedback and reviews to ensure that the architecture is meeting the requirements of the project.","GenTime":"2024-08-10 06:23:03"}
{"File Name":"mediawiki-extensions-WikibaseLexeme\/0003-adr-with-all-team-members.md","Context":"## Context\\nWe use architectural decision records (ADR) on this project.\\nThe project's code base is owned by the team, the team organizes itself (see Journey model) into smaller, short lived \"journey\" sub-units to - amongst others - increase focus. Architecture decisions affect the project for a long time, and will likely soon be faced by developers that were not part of the journey at the time. Consequently their feedback about the architectural decision is inevitable. Additionally, given the intentionally small size of an individual journey's group, the amount of opinions concerning any given ADR could be as small as one or two, should the ADR be voted upon by members of the same journey exclusively. To avoid a flood of ADRs trying to unwrite each other and to increase the standing of ADRs in general and the quality of the individual ADR they should be vetted (RFC) by the entire team.\\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\\n","Decision":"We put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.\\n","tokens":214,"id":1563,"Prediction":"Here is the Architectural Decision Record:\n\n## Decision\n\nTo ensure the quality and longevity of architectural decisions, we will implement a vetting process for all architectural decision records (ADRs) that requires review and feedback from the entire team, rather than just the team members involved in the specific \"journey\" sub-unit.\n\n## Context\n\nAs mentioned in the context, the project's code base is owned by the team, and the team organizes itself into smaller, short-lived \"journey\" sub-units to increase focus. Architecture decisions affect the project for a long time and will likely soon be faced by developers who were not part of the journey at the time. Additionally, given the intentionally small size of an individual journey's group, the amount of opinions concerning any given ADR could be as small as one or two, should the ADR be voted upon by members of the same journey exclusively. To avoid a flood of ADRs trying to unwrite each other and to increase the standing of ADRs in general and the quality of the individual ADR, we need to ensure that they are vetted by the entire team.\n\n## Problem\n\nWithout a vetting process, ADRs may not be thoroughly reviewed and may not take into account the perspectives of all team members. This could lead to ADRs that are not well-suited for the project's long-term goals or that may cause unintended consequences.\n\n## Decision Criteria\n\n1. Ensure that ADRs are reviewed and feedback is gathered from the entire team.\n2. Prevent a flood of ADRs trying to unwrite each other.\n3. Increase the standing of ADRs in general and the quality of the individual ADR.\n\n## Decision\n\nWe will implement a vetting process for all ADRs that requires review and feedback from the entire team. This will ensure that ADRs are thoroughly reviewed and take into account the perspectives of all team members. To avoid long-running feedback loops that block the individual journey team from fulfilling their commitments, we will also limit the time for RFCs (Requests for Comments).\n\n## Consequences\n\n1. ADRs will be more thoroughly reviewed and will take into account the perspectives of all team members.\n2. The standing of ADRs in general and the quality of the individual ADR will increase.\n3. The risk of ADRs causing unintended consequences or not being well-suited for the project's long-term goals will decrease.\n\n## Rationale\n\nThe vetting process will ensure that ADRs are reviewed and feedback is gathered from the entire team, which will increase the quality of the ADRs and reduce the risk of ADRs causing unintended consequences. The time-limited RFCs will also ensure that the feedback loop is not too long-running and will not block the individual journey team from fulfilling their commitments.","GenTime":"2024-08-10 06:23:34"}
{"File Name":"mlmc\/0002-use-mongodb-for-operation.md","Context":"## Context and Problem Statement\\nOperation API need some data storage to be able to list\/commission\/decommission\/launch missiles.\\n## Decision Drivers\\n* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\n","Decision":"* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\nDecided to use MongoDB since\\n* The product growths without strict requirements, so entitites are changed very often. In this circumstances schemaless approach is better than predefined schema approach\\n* No need to use RDBMS just to store some random missiless on a stock\\n* I wanted to get some experience in MongoDB\\n### Positive Consequences\\n* Implementation will be faster and easier since schema is going to be changed lot of times\\n### Negative Consequences\\n* n\/a\\n","tokens":48,"id":1574,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Use a NoSQL Document-Oriented Database (e.g. MongoDB) for storing missile-related data.\n\n**Context:** Operation API needs a data storage solution to support listing, commissioning, decommissioning, and launching missiles.\n\n**Decision Drivers:**\n\n* **Easy to use storage**: NoSQL databases are known for their flexibility and ease of use, allowing for rapid development and deployment of the API.\n* **Fast storage**: Document-oriented databases are optimized for fast data retrieval and insertion, making them suitable for high-performance applications like missile operations.\n* **Dynamic and\/or schemaless storage**: NoSQL databases do not require a fixed schema, allowing for flexible data modeling and easy adaptation to changing requirements.\n\n**Consequences:**\n\n* Simplified data modeling and reduced complexity in the API codebase\n* Faster data retrieval and insertion, enabling real-time support for missile operations\n* Flexibility to adapt to changing requirements and schema changes without significant rework\n* Potential for increased scalability and performance\n\n**Rationale:** The chosen NoSQL database will provide a flexible and scalable solution for storing missile-related data, allowing the API to efficiently support the required operations. The ease of use and fast storage capabilities will enable rapid development and deployment, while the dynamic schema will accommodate changing requirements and schema changes without significant rework.","GenTime":"2024-08-10 06:27:21"}
{"File Name":"mlmc\/0003-use-rabbit-mq-for-eda.md","Context":"## Context and Problem Statement\\nDifferent services are going to push different kind of messages so there is a need in EDA architecture and some messaging server.\\n## Decision Drivers\\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\n","Decision":"* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\nDecided to use RabbitMQ since\\n* It provides exact functionality that is neded for the product (publish-subscribe event messaging)\\n* It is easy to learn and use and I already had some knowledge of how to use it\\n* It has official container on Docker Hub\\n* It seems to be popular and trending these days\\n### Positive Consequences\\n* I will be able to setup EDA messaging in the product.\\n### Negative Consequences\\n* n\/a\\n","tokens":53,"id":1575,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use Apache Kafka as the messaging server in our EDA architecture. This decision is driven by the need for a scalable, fault-tolerant, and highly performant messaging system that can handle different types of messages from various services.\n\n## Rationale\n\nApache Kafka's ability to handle high-throughput and scalable architecture makes it an ideal choice for our EDA architecture. Its distributed architecture and partitioning mechanism ensure that messages are processed efficiently and reliably. Additionally, Kafka's support for multiple data formats, such as JSON, Avro, and Protobuf, makes it easy to integrate with different services and data formats.\n\nKafka's ease of use and learning curve are also significant factors in our decision. Its RESTful API and command-line interface make it easy to manage and interact with the Kafka cluster. Furthermore, Kafka's extensive documentation and community support ensure that we can quickly resolve any issues that may arise.\n\nLastly, Kafka's support for Docker containers makes it easy to run and manage the Kafka cluster in a containerized environment. This allows us to take advantage of the benefits of containerization, such as ease of deployment, scalability, and portability.\n\n## Consequences\n\nBy choosing Apache Kafka as our messaging server, we will be able to:\n\n* Handle high-throughput and scalable architecture\n* Integrate with different services and data formats\n* Easily manage and interact with the Kafka cluster\n* Run the Kafka cluster in a containerized environment\n\nHowever, this decision also means that we will need to invest time and resources in learning Kafka and integrating it with our existing infrastructure. Additionally, we will need to ensure that our services are properly configured to produce and consume messages from the Kafka cluster.","GenTime":"2024-08-10 06:27:40"}
{"File Name":"SoundCloudVisualizer\/0004-use-gulp-to-build-source-code.md","Context":"## Context\\nThe application's JS code was loaded via `script` tags in `index.html`. As the project was migrated from Vanilla JS to AngularJS the number of source files that needed to be included grew, and this approach became unwieldy.\\nI looked into how we could bundle our code. At the time of development Grunt and Gulp were the two main task runners available.\\n","Decision":"[Gulp](https:\/\/gulpjs.com\/) will be used to bundle client-side code.\\n","tokens":81,"id":1576,"Prediction":"## Decision\n\nTo improve the maintainability and scalability of the application's JavaScript code, we will adopt a bundling strategy using Gulp as our task runner. This decision is based on the following factors:\n\n* The increasing number of source files and the need for a more efficient way to manage dependencies and concatenation.\n* The maturity and popularity of Gulp as a task runner, which provides a wide range of plugins and community support.\n* The ability to easily integrate Gulp with our existing development workflow and tools.\n\n## Context\n\nThe application's JS code was loaded via `script` tags in `index.html`. As the project was migrated from Vanilla JS to AngularJS the number of source files that needed to be included grew, and this approach became unwieldy. I looked into how we could bundle our code. At the time of development Grunt and Gulp were the two main task runners available.\n\n## Rationale\n\nBy using Gulp to bundle our code, we can:\n\n* Reduce the number of HTTP requests and improve page load times by concatenating and minifying our JavaScript files.\n* Simplify the process of managing dependencies and reducing the risk of errors caused by missing or incorrect file inclusion.\n* Improve the maintainability of our codebase by providing a clear and consistent way of organizing and building our application.\n\n## Consequences\n\n* We will need to learn and adapt to the Gulp ecosystem, including writing tasks and plugins.\n* We will need to update our build process and tooling to incorporate Gulp.\n* We will need to consider the trade-offs between different bundling strategies, such as UglifyJS vs. Rollup, and choose the best approach for our specific use case.\n\n## Status\n\nThis decision has been implemented and is currently in use in our project.","GenTime":"2024-08-10 06:27:59"}
{"File Name":"iampeterbanjo.com\/0009-place-tests-next-to-source-files.md","Context":"## Context\\nWhile migrating to Typescript, fixing tests means switching between the source file and their test files. Where the test files are in another folder, the journey is longer. If they were side-by-side then once I had the source or test, finding the related file would be much easier\\n","Decision":"Moving test files next to the source code makes it much easier to switch between source and tests. It's also easier to see which test files do not have any unit tests.\\n","tokens":60,"id":1588,"Prediction":"## Decision\n\n**Decision:** Implement a test runner with a feature to automatically switch between the source file and its corresponding test file, allowing developers to easily navigate between the two.\n\n**Context:** While migrating to TypeScript, fixing tests requires frequent switching between the source file and its test file, which can be time-consuming and tedious, especially when the test files are located in a separate folder. Having the test files side-by-side with the source files would greatly improve navigation and reduce the time spent on testing.\n\n**Rationale:**\n\n* This decision aims to improve the developer experience by reducing the time and effort spent on navigating between source and test files.\n* By implementing a test runner with automatic file switching, developers can focus more on writing and debugging tests, rather than navigating between files.\n* This feature can also help reduce errors and improve code quality by ensuring that developers are working with the correct test file.\n\n**Consequences:**\n\n* Implementing this feature will require additional development effort to create the necessary test runner functionality.\n* The feature may introduce new technical debt, such as increased complexity and potential bugs.\n* However, the benefits of improved developer experience and reduced testing time will outweigh the costs.\n\n**Alternatives:**\n\n* Do nothing: Leave the current testing workflow and continue to manually switch between source and test files.\n* Implement a separate IDE plugin: Create a separate plugin for the IDE to provide the file switching feature, rather than integrating it with the test runner.\n\n**Status:** Accepted\n\n**Implementation:**\n\n* Research and implement a test runner with automatic file switching functionality.\n* Integrate the feature with the existing testing workflow.\n* Test and refine the feature to ensure it meets the requirements and is user-friendly.","GenTime":"2024-08-10 06:31:16"}
{"File Name":"iampeterbanjo.com\/0012-parallel-database-tests-mongodb-server.md","Context":"## Context\\nJest tests are fast because they can be run in parallel. If we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. There are two ways to decouple tests:\\n- [define databases in tests][defined-test-database]\\n- [randomly create databases for each test][random-test-database]\\n","Decision":"In the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the `beforeAll` and `afterAll` hooks because this is more flexible and is less coupled to Jest's idiosyncracies.\\n","tokens":80,"id":1589,"Prediction":"## Decision\n\nTo mitigate the risk of race conditions in Jest tests due to the use of the same database for every test, we will implement a solution that randomly creates a new database for each test. This approach will ensure that each test has its own isolated environment, reducing the likelihood of conflicts and making our tests more reliable.\n\n## Context\n\nAs you mentioned, Jest tests are fast because they can be run in parallel. However, if we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. This can lead to unpredictable test results and make it difficult to debug issues.\n\n## Decision Rationale\n\nWe considered two options to decouple tests: defining databases in tests and randomly creating databases for each test. While defining databases in tests can provide some isolation, it may not be sufficient to prevent all race conditions. Randomly creating databases for each test provides a higher level of isolation, ensuring that each test has its own unique environment.\n\n## Pros and Cons\n\nPros:\n\n* Each test has its own isolated environment, reducing the likelihood of conflicts and making tests more reliable.\n* Randomly creating databases for each test provides a higher level of isolation than defining databases in tests.\n\nCons:\n\n* Creating a new database for each test may increase the overall test execution time.\n* We will need to implement a mechanism to clean up the test databases after each test to prevent data accumulation.\n\n## Conclusion\n\nWe will implement a solution that randomly creates a new database for each test. This approach will provide a higher level of isolation and make our tests more reliable. We will also implement a mechanism to clean up the test databases after each test to prevent data accumulation.","GenTime":"2024-08-10 06:31:34"}
{"File Name":"iampeterbanjo.com\/0008-use-ava-test-runner.md","Context":"## Context\\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\\n","Decision":"Jest seems like the way to go:\\n- Typescript support\\n- Large community\\n- Familiar\\n","tokens":91,"id":1594,"Prediction":"## Decision\n\n**Decision:** We will use Jest with TypeScript for code coverage reporting, instead of Lab.\n\n**Context:** As we are experiencing issues with Lab's code coverage reporting due to its lack of support for TypeScript, and considering the small community and limited integration with other tools, we need to find an alternative solution.\n\n**Rationale:** Jest is a widely used and well-maintained testing framework that supports TypeScript out of the box. It also has a large community and is well-integrated with many other development tools. Additionally, Jest has a strong focus on code coverage reporting, making it a suitable replacement for Lab.\n\n**Consequences:**\n\n* We will need to set up Jest with our existing TypeScript project, which may require some configuration and adaptation.\n* We may need to update our testing strategy and code structure to take advantage of Jest's features and best practices.\n* We will need to monitor and test our code coverage reporting to ensure it is accurate and reliable.\n\n**Alternatives Considered:** We considered using Ava, but the performance issues reported in the review we read made it a less attractive option. We also considered sticking with Lab, but the lack of TypeScript support and limited community integration made it a less viable choice.\n\n**Status:** Implemented\n\n**Owner:** [Your Name]\n\n**Created:** [Date]\n\n**Updated:** [Date]","GenTime":"2024-08-10 06:33:23"}
{"File Name":"iampeterbanjo.com\/0007-module.exports-vs-export-default.md","Context":"## Context\\nWhen HapiJS plugins are registered they expect an object like this:\\n```JavaScript\\n{\\nname: 'plugin-name',\\nversion: '1.0.0',\\nregister: (server, options) => {}\\n}\\n```\\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\\n```JavaScript\\nimport plugin from '.\/plugin';\\nexport default {\\nplugin,\\n}\\n```\\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\\nSo let's use `module.exports` instead of `export default`.\\n","Decision":"Rejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\\n","tokens":202,"id":1596,"Prediction":"## Decision\n\n**Use `module.exports` instead of `export default` for HapiJS plugin registration**\n\n## Context\n\nWhen registering HapiJS plugins, they expect an object with a specific structure. However, using `export default` in TypeScript can lead to issues due to CommonJS's behavior of wrapping the exported object in a `default` property. This makes it difficult to maintain a consistent convention for registering plugins.\n\n## Problem\n\nThe current use of `export default` in the plugin registration process can lead to confusion and errors when using the plugin in different environments. This is because HapiJS uses CommonJS, which does not support the `default` property in ES6 module exports.\n\n## Decision\n\nTo avoid these issues and maintain a consistent convention, we will use `module.exports` instead of `export default` for registering HapiJS plugins. This will ensure that the plugin registration object is exported as expected by HapiJS, without the need to worry about CommonJS's behavior.\n\n## Consequences\n\n* All plugins will use `module.exports` instead of `export default` for registration.\n* This decision will simplify the process of registering plugins and reduce the likelihood of errors due to differences in module export behavior.\n* Developers will not need to worry about using `export default` or `module.exports` depending on the environment.\n\n## Rationale\n\nThe decision to use `module.exports` is based on the need to maintain a consistent convention for registering HapiJS plugins. By using `module.exports`, we can ensure that the plugin registration object is exported as expected by HapiJS, without the need to worry about CommonJS's behavior. This will simplify the process of registering plugins and reduce the likelihood of errors.","GenTime":"2024-08-10 06:34:06"}
{"File Name":"iampeterbanjo.com\/0011-artist-images-from-spotify.md","Context":"## Context\\nKorin would look much better with artist images. Unfortunately LastFm [removed artist images][lastfm-images-gone] from their API and have replaced it with a placeholder. There are some options to get artist images which are:\\n- [Music story][music-story-api]: The documentation is not easy to understand and the SDK is designed for web browsers. It also uses a custom ID to associate artists, not IMDB. One unique feature is that it has lyrics too.\\n- [Music brainz][music-brainz-api]: Creative commons but has everything except artist images.\\n- [Spotify][spotify-api]: Has artist information including images but uses a custom ID, not IMDB.\\n","Decision":"Spotify is the only viable alternative.\\n","tokens":148,"id":1598,"Prediction":"## Decision\n\n### Decision: Integrate with Music Story API for Artist Images\n\n### Context\n\nAs LastFm has removed artist images from their API and replaced them with a placeholder, we need to find an alternative solution to fetch artist images for Korin. After considering the options, we have decided to integrate with Music Story API.\n\n### Problem\n\nThe main problem is that we need a reliable source to fetch artist images for Korin. LastFm's removal of artist images from their API has left us without a default solution.\n\n### Reasoning\n\nWe have evaluated the following options:\n\n* Music Story API: Although the documentation is not easy to understand, the API has a unique feature of providing lyrics along with artist images. This could be a valuable addition to Korin.\n* Music Brainz API: Although it has a creative commons license, it lacks artist images, which is a crucial feature for Korin.\n* Spotify API: Although it has artist information including images, it uses a custom ID, not IMDB, which might not be compatible with our existing system.\n\nConsidering these options, Music Story API seems to be the best choice due to its provision of lyrics and artist images.\n\n### Decision Criteria\n\n* Availability of artist images\n* Compatibility with existing system (IMDB ID)\n* Additional features (lyrics)\n\n### Conclusion\n\nWe will integrate with Music Story API to fetch artist images for Korin. This decision will provide a reliable source of artist images and additional features like lyrics, enhancing the overall user experience.","GenTime":"2024-08-10 06:34:39"}
{"File Name":"iampeterbanjo.com\/0002-blog-engine.md","Context":"## Context\\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\\n### [11ty][4]\\nPros\\n- Easy setup\\n- Predictable static file output\\n- Relatively small codebase\\n- Supporst different template engines\\nCons\\n- The frontmatter parser [didn't work as I expected][7]\\n- Template errors were not reported and would only fail on build\\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\\n### [Gatsby][5]\\nPros\\n- Modern [JAMstack][8] project with React\\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\\n- Opportunity to use GraphQL\\nCons\\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\\n- Developing static files based on API was difficult because of caching and unexpected error handling\\n- Relatively complex project with separate build, develop steps, config, plugins etc\\n### [Hugo][6]\\nPros\\n- Fast and mature static site generator\\n- Small footprint\\nCons\\n- Written in Go - I prefer fullstack JavaScript for this site\\n","Decision":"In the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\\n","tokens":300,"id":1599,"Prediction":"Here is the Architectural Decision Record (ADR) based on the context you provided:\n\n**Decision**\n\nWe will use Hugo as the static site generator for our website blog, despite its limitations, due to its fast and mature nature, small footprint, and ease of use.\n\n**Context**\n\nWe have evaluated three static site generators (11ty, Gatsby, and Hugo) for our website blog project, considering factors such as ease of setup, predictability, and features. While 11ty was easy to set up, its frontmatter parser did not work as expected, and template errors were not reported. Gatsby, on the other hand, is a modern JAMstack project with React, but it relies on client-side JavaScript and has complex build and development processes.\n\n**Problem**\n\nWe need a reliable and efficient way to generate static files for our website blog, while also considering the limitations of each static site generator.\n\n**Decision Criteria**\n\n1. Ease of use and setup\n2. Predictability and reliability\n3. Features and capabilities\n4. Performance and scalability\n\n**Decision**\n\nWe will use Hugo as the static site generator for our website blog due to its fast and mature nature, small footprint, and ease of use. Although it is written in Go, we are willing to compromise on this aspect in favor of its other benefits.\n\n**Consequences**\n\n1. We will need to learn and adapt to Hugo's syntax and configuration options.\n2. We will need to consider the limitations of Hugo, such as its Go-based architecture, and find workarounds or compromises as needed.\n3. We will need to ensure that our project is optimized for performance and scalability using Hugo.\n\n**Rationale**\n\nWhile 11ty was easy to set up, its limitations in frontmatter parsing and error reporting make it less suitable for our project. Gatsby, although modern and feature-rich, has complex build and development processes that may be challenging to manage. Hugo, on the other hand, offers a good balance of ease of use, reliability, and features, making it the best choice for our project.","GenTime":"2024-08-10 06:35:02"}
{"File Name":"dogma\/0018-projection-compaction.md","Context":"## Context\\nMany projections produce data that is only required for a limited period of\\ntime.\\nAn application developer needs to consider how long projection data will be\\nretained and ideally implement measures to remove any unnecessary data.\\nOften such data can be removed when some future event occurs. However, in some\\ncases there is no future event that appropriately indicates the data is no\\nlonger required.\\nWe have encountered a genuine example of this when implementing an SQL\\nprojection that inserts a row whenever a user performs a specific action. The\\ndata is queried in order to enforce a \"cool down\" that prevents the user from\\nrepeating that action again within a specific time frame.\\nThe insert is triggered by the occurrence of an event, but the data becomes\\nunnecessary whenever the \"cool down\" time has elapsed.\\nIn this particular use case the \"cool down\" was not part of the business logic,\\nbut rather an API level restriction. Hence, processes\/timeout messages were not\\nthe appropriate solution.\\n","Decision":"We have decided to add a `Compact()` method to `ProjectionMessageHandler`.\\nThe implementation of `Compact()` can modify the projection's data by whatever\\nmeans is appropriate such that unnecessary data is removed but the projection\\nstill serves its purpose.\\n","tokens":210,"id":1606,"Prediction":"## Decision\n\n**Title:** Handling Temporarily Required Projection Data\n\n**Context:** Many projections produce data that is only required for a limited period of time, and it is essential to consider how long projection data will be retained and implement measures to remove unnecessary data.\n\n**Problem:** In some cases, there is no future event that indicates the data is no longer required, making it challenging to determine when to remove the data.\n\n**Decision:** We will use a combination of time-based retention and a custom \"expiration\" mechanism to manage temporarily required projection data.\n\n**Rationale:** The time-based retention approach will allow us to automatically remove data after a certain period, reducing storage needs and improving performance. The custom \"expiration\" mechanism will enable us to specify a specific event or condition that marks the data as no longer required, providing a more flexible and accurate way to manage temporary data.\n\n**Consequences:**\n\n* Improved data retention and management\n* Reduced storage needs and improved performance\n* Increased flexibility in managing temporary data\n* Potential for more accurate and timely removal of unnecessary data\n\n**Alternatives Considered:**\n\n* Using processes\/timeout messages to manage data expiration, but this approach may not be suitable for API-level restrictions.\n* Implementing a complex event-driven system to manage data expiration, which may be overkill for this use case.\n\n**Status:** Implemented\n\n**Owners:** [List the names of the owners or the team responsible for implementing and maintaining this decision]\n\n**History:**\n\n* [Date]: Initial decision made\n* [Date]: Decision reviewed and updated","GenTime":"2024-08-10 06:37:27"}
{"File Name":"dogma\/0008-location-of-testing-features.md","Context":"## Context\\nWe need to decide whether Dogma's testing features should reside in the `dogma`\\nmodule itself, or a separate `dogmatest` module.\\n### Benefits to separate module\\n1. They can be versioned separately. A `dogma` release can be made without\\nhaving to update the testing tools. This is a _pro_ for the releaser, but\\npossibly a _con_ for the users.\\n1. Users that opt not to test, or not to test using our tools do not need to\\ndownload `dogmatest` at all. This is not a particularly strong argument.\\n1. We can make BC breaking changes to `dogmatest`, without having to make\\na new major release of `dogma`. We would need to document clearly which\\nmajor `dogma` versions are supported by which major `dogmatest` versions.\\n### Benefits to a single module\\n1. The user does not need to determine which `dogmatest` version to use with\\ntheir application; compatible tools are always available right from `dogma`. If\\nwe want to encourage users to test their domain logic using these utilities;\\nthey should be easy to access.\\n1. As mentioned in [#16](github.com\/dogmatiq\/dogma), splitting the testing\\ntools would mean that the example code would also need to be moved elsewhere.\\nHowever, we have since already decided to move the example code to a separate\\nmodule in [ADR-7](0007-location-of-examples.md).\\n","Decision":"For the same reasons [ADR-7](0007-location-of-examples.md) we've decided to\\nimplement testing tools in their own module.\\n","tokens":324,"id":1608,"Prediction":"## Decision\n\nAfter considering the benefits of having the testing features in a separate `dogmatest` module versus having them in the `dogma` module itself, we have decided to keep the testing features within the `dogma` module.\n\n## Rationale\n\nWhile having a separate `dogmatest` module would provide benefits such as versioning independence and the ability to make BC breaking changes without affecting the main `dogma` module, we believe that the convenience and ease of use provided by having the testing features within the `dogma` module outweigh these benefits.\n\nBy keeping the testing features within `dogma`, users do not need to worry about determining which `dogmatest` version to use with their application, and the testing utilities are always available right from the `dogma` module. This makes it easier for users to access and use the testing features, which is an important consideration for encouraging users to test their domain logic using these utilities.\n\nAdditionally, the example code is already planned to be moved to a separate module, as mentioned in ADR-7, so the concern about splitting the testing tools and example code is mitigated.\n\n## Consequences\n\n* The testing features will be included in the `dogma` module and will be versioned together with the main `dogma` module.\n* Users will not need to worry about determining which `dogmatest` version to use with their application.\n* The testing utilities will be easily accessible from the `dogma` module, making it easier for users to test their domain logic.\n* The example code will be moved to a separate module, as planned.","GenTime":"2024-08-10 06:38:10"}
{"File Name":"dogma\/0011-message-timing-information.md","Context":"## Context\\nWe need to decide whether message timing information should be exposed via the\\nAPI. In this context \"timing information\" refers to important points in time\\nthroughout the lifecycle of a message.\\nThe initial rationale for *not* exposing these timestamps was that any business\\nlogic that depends on time in some way should explicitly include any timing\\ninformation within the message itself. We call such logic \"time-based\" and the\\napproach of including explicit timing information \"modeling time\".\\n","Decision":"The sections below focus on each of the message roles, their respective\\ntimestamps of interest, and the decisions made in each case.\\n### Command Messages\\nWe believe the existing requirement that the application \"model time\" is still\\nappropriate for command messages. The time at which the command message is\\ncreated or enqueued is irrelevant; any time information relevant to the domain\\nlogic should be included in the message itself.\\n**We have decided not to expose the command creation time.**\\n### Event Messages\\nThe time at which an event is recorded is a fundamental property of the event\\nitself. Put another way, every event occurs at some time regardless of whether\\nthe domain is time-based.\\nFurthermore, the time at which the event occurs may be relevant to some\\nancillary domain logic that is *triggered* by the event, even if the aggregate\\nthat *produced* the event has no time-based logic.\\nThe inclusion of the \"occurred time\" as a fundamental property of the event is\\nsupported by [Implementing Domain Driven\\nDesign](https:\/\/www.amazon.com\/Implementing-Domain-Driven-Design-Vaughn-Vernon\/dp\/0321834577),\\nChapter 8, in the \"Modeling Events\" section.\\n**We have decided to include a `RecordedAt()` method on `ProcessEventScope` and `ProjectionEventScope`.**\\nIn actuality, a `Time()` method had already been added to `ProjectionEventScope`\\nwithout any supporting ADR, this method is to be renamed.\\n### Timeout Messages\\nThe time at which a timeout message is scheduled to be handled is a fundamental\\nproperty of the timeout concept.\\nBy definition, the use of a timeout message indicates that there is time-based\\nlogic. It seems like an unnecessary imposition to require the application\\ndeveloper to include the scheduled time in the message.\\n**We have decided to include a `ScheduledFor()` method on `ProcessTimeoutScope`.**\\n","tokens":101,"id":1610,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to expose message timing information via the API.\n\n## Context\n\nAs mentioned in the initial context, the initial rationale for not exposing these timestamps was that any business logic that depends on time in some way should explicitly include any timing information within the message itself. This approach is referred to as \"modeling time\". However, after further analysis, we have determined that exposing message timing information via the API can provide several benefits.\n\n## Problem\n\nThe initial approach of not exposing timing information may lead to the following problems:\n\n* Increased complexity in the client-side logic, as the client would need to parse and interpret the message to extract the necessary timing information.\n* Reduced flexibility, as the client would be tightly coupled to the specific timing information included in the message.\n* Difficulty in debugging and troubleshooting, as the timing information would be scattered throughout the message and not easily accessible.\n\n## Rationale\n\nExposing message timing information via the API can provide the following benefits:\n\n* Simplified client-side logic, as the client can easily access the necessary timing information without having to parse and interpret the message.\n* Increased flexibility, as the client can easily modify or extend the timing information as needed.\n* Improved debugging and troubleshooting, as the timing information is easily accessible and can be used to quickly identify and resolve issues.\n\n## Decision Criteria\n\nThe decision to expose message timing information via the API was based on the following criteria:\n\n* Simplicity and ease of use: Exposing timing information via the API simplifies the client-side logic and makes it easier to use.\n* Flexibility and extensibility: Exposing timing information via the API provides the client with the flexibility to modify or extend the timing information as needed.\n* Debugging and troubleshooting: Exposing timing information via the API makes it easier to debug and troubleshoot issues related to timing.\n\n## Conclusion\n\nIn conclusion, we have decided to expose message timing information via the API. This decision is based on the benefits of simplicity, flexibility, and improved debugging and troubleshooting. We believe that this approach will provide a more robust and maintainable solution for our users.","GenTime":"2024-08-10 06:38:52"}
{"File Name":"dogma\/0015-routing-unrecognized-messages.md","Context":"## Context\\n[ADR-14](0014-apply-historical-events-to-aggregates.md) relaxed the\\nspecification such that `AggregateRoot.ApplyEvent()` implementations were no\\nlonger required to panic with an `UnrecognizedMessage` value when passed an\\nunexpected message type.\\nPrompted by this requirement, we relaxed the requirement for ALL handler\\nmethods, which was likely too broad of a change.\\nSpecifically, unlike when handling a message, the routing methods\\n`AggregateMessageHandler.RouteCommandToInstance()` and\\n`ProcessMessageHandler.RouteEventToInstance()` do not have the option of \"doing\\nnothing\" when passed an unexpected message type.\\n","Decision":"Reinstate the hard requirement that the handlers MUST panic with\\n`UnexpectedMessage` when asked to route a message type that was not configured\\nas being consumed by that handler.\\n","tokens":138,"id":1612,"Prediction":"## Decision\n\nTo maintain consistency and clarity in the handling of unexpected message types, we will require `AggregateMessageHandler.RouteCommandToInstance()` and `ProcessMessageHandler.RouteEventToInstance()` to throw an exception when passed an unexpected message type.\n\n## Context\n\nAs mentioned in ADR-14, the requirement for `AggregateRoot.ApplyEvent()` implementations to panic with an `UnrecognizedMessage` value when passed an unexpected message type was relaxed. This change has led to a broader relaxation of the requirement for all handler methods, which may have been too broad a change.\n\n## Problem\n\nThe current requirement for `AggregateMessageHandler.RouteCommandToInstance()` and `ProcessMessageHandler.RouteEventToInstance()` to not throw an exception when passed an unexpected message type is inconsistent with the behavior of `AggregateRoot.ApplyEvent()`. This inconsistency can lead to confusion and errors in the handling of unexpected message types.\n\n## Decision Criteria\n\n1. Consistency: The decision is based on the principle of consistency in the handling of unexpected message types. If `AggregateRoot.ApplyEvent()` implementations are required to throw an exception, then `AggregateMessageHandler.RouteCommandToInstance()` and `ProcessMessageHandler.RouteEventToInstance()` should also throw an exception.\n2. Clarity: The decision aims to provide clarity in the handling of unexpected message types. By requiring an exception to be thrown, it is clear that an unexpected message type has been encountered, and the handler can take appropriate action.\n3. Error handling: The decision takes into account the importance of error handling in the system. By throwing an exception, the handler can notify the caller of the error and allow it to take appropriate action.\n\n## Consequences\n\n1. Consistency: The decision ensures consistency in the handling of unexpected message types across the system.\n2. Clarity: The decision provides clarity in the handling of unexpected message types, making it easier to understand and debug the system.\n3. Error handling: The decision improves error handling by allowing the system to handle unexpected message types in a consistent and predictable manner.\n\n## Rationale\n\nThe decision is based on the principle of consistency and the importance of clarity and error handling in the system. By requiring `AggregateMessageHandler.RouteCommandToInstance()` and `ProcessMessageHandler.RouteEventToInstance()` to throw an exception when passed an unexpected message type, we ensure that the system handles unexpected message types in a consistent and predictable manner, providing clarity and improving error handling.","GenTime":"2024-08-10 06:39:41"}
{"File Name":"dogma\/0004-adr-process.md","Context":"## Context\\nWe need a documented process for proposing, discussing and ultimate accepting or\\nrejecting ADRs via pull requests.\\n","Decision":"We have decided to adopt a process that favours rapid changes, at least while\\nthe project is in its infancy. To this end, we will allow ADRs in the `proposed`\\nstatus to be merged to `main` via PRs. The PRs may contain related code changes.\\nWhenever a `proposed` ADR is merged to `main`, a GitHub issue is created to\\ncapture discussion about the ADR. Such issues are given the `adr` label.\\nAny `proposed` ADRs remaining on `main` must be resolved either by approving\\nthe ADR, or by rejecting it and reverting any associated code changes.\\n","tokens":27,"id":1614,"Prediction":"## Decision\n\n**ADR Process**\n\nTo ensure a structured and transparent process for proposing, discussing, and accepting or rejecting Architectural Decision Records (ADRs), we will establish a documented process for ADRs via pull requests.\n\n### Context\n\nAs our software grows and evolves, it is essential to have a clear and consistent process for documenting and discussing architectural decisions. This process will help ensure that all stakeholders are informed and aligned on the direction of the software, and that we can effectively manage and maintain our architecture over time.\n\n### Decision\n\nWe will establish the following process for proposing, discussing, and accepting or rejecting ADRs via pull requests:\n\n1. **Proposing an ADR**: Any team member can propose an ADR by creating a new pull request with a markdown file containing the ADR template.\n2. **Discussion and Review**: The proposed ADR will be reviewed by the team and discussed in the pull request comments. The reviewer will provide feedback and suggestions for improvement.\n3. **Acceptance or Rejection**: The ADR will be accepted or rejected based on the feedback and discussion. If accepted, the pull request will be merged. If rejected, the pull request will be closed, and the proposer will be notified of the reasons for rejection.\n4. **Documentation**: Accepted ADRs will be added to the ADR repository, where they will be stored and easily accessible for future reference.\n\n### Rationale\n\nThis process will help ensure that all architectural decisions are documented, reviewed, and agreed upon by the team. It will also provide a clear and transparent record of the decision-making process, which will help to reduce misunderstandings and conflicts.\n\n### Consequences\n\nThis process will require team members to be more proactive in proposing and discussing ADRs, and to be more mindful of the impact of their decisions on the software architecture. It will also require more time and effort to review and discuss ADRs, but this will be offset by the benefits of having a clear and consistent process for documenting and managing architectural decisions.\n\n### Alternatives\n\nWe considered alternative approaches, such as using a separate document or wiki for ADRs, or relying on informal discussions and email threads. However, we believe that a pull request-based process will provide a more structured and transparent approach, and will help to ensure that all stakeholders are informed and aligned on the direction of the software.\n\n### History\n\nThis ADR was proposed by [Name] on [Date]. It was reviewed and accepted by [Name] on [Date].","GenTime":"2024-08-10 06:40:36"}
{"File Name":"dogma\/0012-identifier-comparison.md","Context":"## Context\\nIdentifiers (the names and keys used to identify applications and handlers) must\\nbe compared by engines to determine if two such entities are to be considered\\nequivalent.\\nThe documentation specifies that such keys must be non-empty UTF-8 strings\\nconsisting of printable characters without whitespace, but it did not previously\\nspecify how such strings would be compared.\\nThese identifiers are either mostly or entirely immutable and generated as part\\nof the source code. They do not need to be parsed and validated from user input.\\n","Decision":"In keeping with current behavior, we've decided to specify byte-wise comparison\\nsemantics for identifiers.\\n","tokens":109,"id":1618,"Prediction":"## Decision\n\n**Title:** String Comparison for Identifiers\n\n**Context:** Identifiers (the names and keys used to identify applications and handlers) must be compared by engines to determine if two such entities are to be considered equivalent. The documentation specifies that such keys must be non-empty UTF-8 strings consisting of printable characters without whitespace, but it did not previously specify how such strings would be compared.\n\n**Decision:** We will use a case-sensitive, byte-by-byte comparison for string identifiers. This approach is simple, efficient, and consistent with the existing requirements for the format of the identifiers.\n\n**Rationale:**\n\n* Since the identifiers are generated as part of the source code and are mostly or entirely immutable, we can assume that they are correctly formatted and do not need to be parsed or validated from user input.\n* A case-sensitive comparison is necessary because the documentation specifies that the identifiers must be non-empty UTF-8 strings consisting of printable characters without whitespace. This means that \"A\" and \"a\" would be considered different identifiers.\n* A byte-by-byte comparison is efficient and easy to implement, and it ensures that the comparison is consistent across different engines and platforms.\n\n**Consequences:**\n\n* This decision may result in some identifiers being considered non-equivalent that would be considered equivalent under a case-insensitive comparison. However, this is consistent with the existing requirements for the format of the identifiers.\n* This decision may require some changes to the existing code that handles identifier comparisons. However, these changes should be minimal and straightforward.\n\n**Alternatives Considered:**\n\n* We considered using a case-insensitive comparison, but this would require additional logic to handle the conversion of the identifiers to a standard case (e.g., lowercase). This would add complexity and potentially introduce bugs.\n* We considered using a more sophisticated comparison algorithm, such as a levenshtein distance or a fuzzy matching algorithm. However, these algorithms are more complex and may not be necessary for this specific use case.","GenTime":"2024-08-10 06:42:01"}
{"File Name":"admin-react-components\/0003-component-bundling.md","Context":"## Context\\nWhen bundling components for usage in consuming applications, should we require consuming applications to have the same dependencies as this repository? Or should exported components be packaged assuming no dependencies?\\n","Decision":"We'll package components with their own inline styles. No external dependencies needed.\\n","tokens":38,"id":1631,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to require consuming applications to have the same dependencies as this repository when bundling components for usage. This decision is based on the following rationale:\n\n* **Consistency**: By requiring the same dependencies, we ensure consistency across all consuming applications. This makes it easier to maintain and update the components, as well as reduces the risk of compatibility issues.\n* **Easier debugging**: When a consuming application encounters an issue, having the same dependencies as the repository makes it easier to identify and debug the problem.\n* **Improved reliability**: By using the same dependencies, we can ensure that the components are used in the same environment and configuration, which reduces the risk of unexpected behavior or errors.\n* **Simplified maintenance**: If a dependency is updated or changed, we only need to update the repository, and all consuming applications will automatically benefit from the change.\n\nOn the other hand, packaging exported components assuming no dependencies would lead to:\n\n* **Inconsistent environments**: Consuming applications may use different versions or configurations of dependencies, leading to compatibility issues and increased maintenance complexity.\n* **Debugging challenges**: Debugging issues in consuming applications would be more difficult, as the dependencies may not be the same as those used in the repository.\n* **Reliability concerns**: The risk of unexpected behavior or errors increases, as the components may not be used in the same environment and configuration.\n\nOverall, requiring consuming applications to have the same dependencies as this repository ensures consistency, ease of debugging, improved reliability, and simplified maintenance.","GenTime":"2024-08-10 06:46:22"}
{"File Name":"linshare-mobile-android-app\/0011-upload-document.md","Context":"## Context\\nAt this time, we used `android.net.Uri` to extract information and open an `InputStream` to upload.\\nBut, an `Uri` has `UriPermission` protect the data which is represented.\\nThis permission will be revoked by shared application when the received Activity no-longer run.\\nIt is not critical to Android 7, but with Android 9 we always get `Permission Denied` when deliver Uri to Worker to execute.\\n","Decision":"- We extract all requirement information of the Document at the time we receive the Intent.\\n- Instead of using Uri directly, we create a temporary file to store the file which Uri represent\\n- We deliver temporary file path to Worker\\n","tokens":94,"id":1643,"Prediction":"## Decision\n\nTo address the issue of `UriPermission` being revoked by shared applications and causing `Permission Denied` errors when delivering Uri to a worker to execute, we will use the `FileProvider` to expose the file to the worker instead of using `android.net.Uri`.\n\n## Context\n\nAs mentioned in the context, using `android.net.Uri` to extract information and open an `InputStream` to upload can lead to issues with `UriPermission` being revoked by shared applications, resulting in `Permission Denied` errors on Android 9 and later. This is not a critical issue on Android 7, but it is a significant problem on newer versions of the operating system.\n\n## Problem\n\nThe current implementation of using `android.net.Uri` to extract information and open an `InputStream` to upload is not robust and can lead to issues with permissions being revoked, resulting in errors when delivering Uri to a worker to execute.\n\n## Solution\n\nTo address this issue, we will use the `FileProvider` to expose the file to the worker instead of using `android.net.Uri`. This will allow us to provide the worker with a file path that can be used to access the file, rather than relying on the Uri and its associated permissions.\n\n## Rationale\n\nUsing `FileProvider` to expose the file to the worker is a more robust and reliable solution than using `android.net.Uri`. This is because `FileProvider` allows us to provide the worker with a file path that can be used to access the file, rather than relying on the Uri and its associated permissions. This will prevent issues with permissions being revoked and will ensure that the worker can access the file even when the Uri is no longer valid.\n\n## Consequences\n\nBy using `FileProvider` to expose the file to the worker, we will need to implement additional logic to handle the file path and ensure that it is properly exposed to the worker. This may require additional code and testing to ensure that the solution is robust and reliable. However, this will provide a more robust and reliable solution than using `android.net.Uri`.","GenTime":"2024-08-10 06:50:35"}
{"File Name":"linshare-mobile-android-app\/0008-download-with-downloadmanager-service.md","Context":"## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n","Decision":"Instead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\\n","tokens":99,"id":1647,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Use the `DownloadManager` service to handle background downloads instead of implementing custom downloading mechanisms.\n\n**Context:** We have some ways to perform downloading stably in the background, but the system exposed a service called `DownloadManager`.\n\n**Problem:** Our current implementation of background downloads may not be robust enough to handle various scenarios such as network failures, connectivity changes, and system reboots.\n\n**Decision:** Using the `DownloadManager` service provides a more robust and reliable way to handle background downloads. The service is designed to handle these scenarios and provide a more consistent experience for our users.\n\n**Rationale:** By using the `DownloadManager` service, we can leverage its built-in functionality to handle complex download scenarios, such as retrying downloads after failures, and notify our app of download progress, failure, or completion.\n\n**Consequences:**\n\n* Our app will benefit from the robustness and reliability of the `DownloadManager` service, providing a better user experience.\n* We will reduce the complexity of our code by not having to implement custom download mechanisms.\n* We will need to register a broadcast receiver to handle download events, which may require additional configuration and testing.\n\n**Status:** Approved\n\n**Date:** [Insert date]\n\n**Implementors:** [Insert names]\n\n**References:** [Insert relevant documentation or code references]","GenTime":"2024-08-10 06:51:53"}
{"File Name":"opg-lpa\/0006-modernise-the-code-base.md","Context":"## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https:\/\/pypl.github.io\/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and\/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n","Decision":"We have decided to modernise the code base to make it easier to work with and better\\naligned with modern web architecture and standards. This is not a small job, but\\nthe guiding principles we've decided on, shown below, should help us achieve our aims.\\n(\"Modernising the code base\" is not to be confused with \"modernising LPAs\". Here\\nwe're just talking about modernising the code base for the Make an LPA tool.)\\n* **Don't rewrite everything at once**\\nWhere possible, migrate part of an application to a new\\ncomponent and split traffic coming into the domain so that some paths are diverted to that\\ncomponent. This will typically use nginx in dev, but may be done at the AWS level if\\nappropriate (e.g in a load balancer or application gateway).\\nThis is challenging, but means that we don't have to do a \"big bang\" release of the new\\nversion of the tool. Our aim is to gradually replace existing components with new\\nones, which are (hopefully) simpler, future-proofed, more efficient, and don't rely on PHP.\\n* **Use Python for new work**\\nWe considered golang, but don't have the experience in the team to build applications with it.\\nWe felt that learning a new language + frameworks would only reduce our ability to deliver, with\\nminimal benefits: our application is not under heavy load and responds in an\\nacceptable amount of time, so golang's super efficiency isn't essential.\\nWe feel that we could scale horizontally if necessary and have not had any major issues\\nwith capacity in the past.\\n* **Choose containers or lambdas as appropriate**\\nUse a container for components which stay up most of the time, and lambdas for\\n\"bursty\" applications (e.g. background processes like PDF generation, daily statistics aggregation).\\n* **Choose the right lambda for the job**\\nUse \"pure\" lambdas where possible. This is only the case where an application has simple dependencies\\nwhich don't require unusual native libraries outside the\\n[stock AWS Docker images for lambdas](https:\/\/gallery.ecr.aws\/lambda\/python)).\\nIf a component is problematic to run as a pure lambda, use a lambda running a Docker image based\\non one of the stock AWS Docker images for lambdas.\\n* **Choose the right Docker image**\\nWhen using Docker images, prefer the following:\\n* Images based on AWS Lambda images (if writing a component which will run as a Docker lambda).\\n* Images based on Alpine (for other cases).\\n* Images based on a non-Alpine foundation like Ubuntu, but only if an Alpine image is not available.\\n* **Use Flask and gunicorn**\\nUse [Flask](https:\/\/flask.palletsprojects.com\/) for new Python web apps, fronted by\\n[gunicorn](https:\/\/gunicorn.org\/) for the WSGI implementation.\\n* **Use the latest Design System**\\nUse the [Government Design System](https:\/\/design-system.service.gov.uk\/) guidelines for new UI. In\\nparticular, use the\\n[Land Registry's Python implementation of the design system](https:\/\/github.com\/LandRegistry\/govuk-frontend-jinja),\\nwritten as [Jinja2 templates](https:\/\/jinja.palletsprojects.com\/).\\nOur aim should be to utilise it without modification as far as possible, so that we can easily upgrade\\nif it is changed by developers at the Land Registry.\\n* **Migrate legacy code to PHP 8**\\nWhere possible, upgrade PHP applications to PHP 8, when supported by [Laminas](https:\/\/getlaminas.org\/).\\nAt the time of writing, Laminas support for PHP 8 is only partial, so we are stuck with PHP 7 for now,\\nas large parts of our stack are implemented on top of Laminas.\\n* **Specify new APIs with OpenAPI**\\nSpecify new APIs using [OpenAPI](https:\/\/swagger.io\/specification\/). Ideally, use tooling\\nwhich enables an API to be automatically built from an OpenAPI specification, binding to\\ncode only when necessary, to avoid repetitive boilerplate.\\n* **Controlled, incremental releases**\\nProvision new infrastructure behind a feature flag wherever possible. This allows us to\\nwork on new components, moving them into the live environment as they are ready, but hidden\\nfrom the outside world. When ready for delivery, we switch the flag over to make that piece\\nof infrastructure live.\\n* **Follow good practices for web security**\\nBe aware of the [OWASP Top Ten](https:\/\/owasp.org\/www-project-top-ten\/) and code to avoid those\\nissues. Use tools like [Talisman](https:\/\/github.com\/GoogleCloudPlatform\/flask-talisman) to\\nimprove security.\\n* **Be mindful of accessibility**\\nConsider accessibility requirements at every step of the design and coding phases. Aim to\\ncomply with [WCAG 2.1 Level AA](https:\/\/www.w3.org\/WAI\/WCAG22\/quickref\/) as a minimum. While the\\nDesign System helps a lot with this, always bear accessibility in mind when building workflows\\nand custom components it doesn't cover.\\n* **Be properly open source**\\nMake the code base properly open source. While our code is open, there are still barriers to entry\\nfor developers outside the Ministry of Justice, such as the requirement to have access to AWS secrets,\\nS3, postcode API, the Government payment gateway and SendGrid for the system to work correctly. We\\nwill work towards removing these barriers so that onboarding of new developers (internally and\\nexternally) is seamless, and to enable potentially anyone to fully contribute to the project.\\n* **Improve test coverage everywhere**\\nAs we work on the code, be aware of gaps in testing and plug them as they arise. Don't wait for\\nan opportunity to fix everything at once: make refactoring and adding unit tests part of the\\nwork on an issue (unless it's going to take longer than working on the issue!).\\nWhere a whole category of testing is missing, add it (for example, we\\nhave recently implemented the foundations for load testing; see\\n[0004-implement-load-testing](.\/0004-implement-load-testing.md)).\\n* **Automate code quality metrics**\\nIntroduce tools to lint and scan code as we go, to ensure consistent, easy-to-follow code. See\\n[0003-linting-and-scanning](.\/0003-linting-and-scanning.md)) for a starting point.\\n* **Peer review everything**\\nAll commits to the code base must go through peer review before merging. No lone wolf developers.\\n* **Be pragmatic**\\nSee the [pragmatic quick reference](https:\/\/www.ccs.neu.edu\/home\/lieber\/courses\/csg110\/sp08\/Pragmatic%20Quick%20Reference.htm)\\nfor a summary. These are generally good principles for software engineering.\\n","tokens":306,"id":1652,"Prediction":"## Decision\n\nIn light of the context provided, we decide to gradually transition the legacy code base from PHP to Python, while also modernizing the codebase and adopting the latest Design System guidelines.\n\n## Context\n\nAs mentioned in the context, the PHP language is on a downward trend, and it may become increasingly difficult to find good PHP developers in the future. Additionally, the codebase is showing its age, with a mix of styles and approaches, and a lack of modernization. The current design system is not being followed, making it difficult to make changes and updates.\n\n## Decision Drivers\n\n* The decreasing popularity of PHP as a language, making it harder to find skilled developers\n* The age and complexity of the codebase, making it resistant to change and prone to breaking\n* The need to modernize the codebase to make it more maintainable and scalable\n* The desire to adopt the latest Design System guidelines to improve the user experience and consistency\n\n## Decision Criteria\n\n* Gradual transition to Python to minimize disruption and risk\n* Prioritization of high-risk or high-impact components first\n* Identification and removal of unused and\/or broken code\n* Adoption of modern coding practices and design patterns\n* Integration of the latest Design System guidelines\n* Training and upskilling of existing developers to work with Python\n\n## Decision Consequences\n\n* Short-term: Increased complexity and workload for developers, potential disruption to existing workflows\n* Medium-term: Improved maintainability, scalability, and reliability of the codebase\n* Long-term: Reduced risk of finding skilled PHP developers, improved consistency and user experience, and a more modern and sustainable codebase\n\n## Rationale\n\nBy transitioning to Python, we can take advantage of its popularity, scalability, and maintainability. This will also give us the opportunity to modernize the codebase, adopt the latest Design System guidelines, and improve the overall user experience. The gradual transition will minimize disruption and risk, and the prioritization of high-risk or high-impact components will ensure that the most critical parts of the codebase are addressed first.","GenTime":"2024-08-10 06:53:46"}
{"File Name":"opg-lpa\/0002-custom-save-handler-in-service-front.md","Context":"## Context\\nThe service-front component, written in PHP, uses the default Redis save\\nhandler for persisting session data. In certain situations, the\\napplication may request a resource *A* which takes significant time to deliver,\\nsuch as LPA statuses via the Sirius data API. If resource *A*\\nis requested via an Ajax request, it's possible that the client\\nwill request a new resource *B* before *A* is fully processed. If processing for\\n*B* then completes before processing for *A*, the process for *A* can erroneously\\noverwrite session data added by *B*, resulting in loss of session data required\\nby *A*.\\nThis causes particular problems for CSRF tokens, as shown by this typical sequence\\non service-front:\\n1.  dashboard page loads in browser, triggering client-side Ajax request to statuses controller\\n2.  statuses controller reads session data **S** and initiates (slow) request to Sirius\\nAPI to get LPA statuses\\n3.  meanwhile, user goes to replacement-attorney page; the Ajax request is now redundant, as the\\nuser isn't on the dashboard page any more, but the statuses controller doesn't know this\\n4.  replacement-attorney controller reads session data **S**\\n5.  statuses controller continues processing Sirius response, unaware of new data about to be added to\\nsession by replacement-attorney...\\n6.  replacement-attorney adds CSRF data to session, creating **S'**\\n7.  replacement-attorney page renders form with CSRF token, associated with data in **S'**\\n8.  replacement-attorney writes **S'** to session, including CSRF data\\n9.  statuses page finishes processing, unaware of **S'**; it assumes it has\\nthe correct data **S** and writes it to the session, losing the delta between\\n**S** and **S'** (including the CSRF token!)\\n10. user submits form to replacement-attorney controller with CSRF token in the form\\n11. replacement-attorney controller loads again, but retrieves **S** from session (just written by\\nstatuses controller in 9); this doesn't have the CSRF token (which was in **S'**)\\nfor comparison with the value in the form submitted by the user; CSRF validation fails!\\n","Decision":"Use a custom save handler to prevent certain Ajax requests from writing data to the session.\\nThis will still use Redis as the storage back-end.\\nThe approach is to send some Ajax requests with a custom `X-SessionReadOnly: true` header,\\nimplying that the controller they invoke should only read from the session and never write to it.\\nThe save handler inspects the header on the incoming request and ignores any requests to write\\nthe session if accompanied by this header.\\nPHP 7+ provides a mechanism to only read from the session, via:\\n```\\nsession_start(array('read_and_close' => true))\\n```\\nHowever, the complexity of the processing in the Laminas stack, which does its own session\\nmanagement, overrides any attempts to call this function. Consequently, the pragmatic\\nsolution is to move down the stack to the lower-level save handler, and implement the read-only\\nbehaviour there for requests we know to be problematic.\\n","tokens":490,"id":1654,"Prediction":"## Decision\n\nTo mitigate the issue of session data loss and ensure the integrity of CSRF tokens, we will implement a custom Redis session handler that uses a lock mechanism to prevent concurrent modifications to session data. This will ensure that only one request can modify the session data at a time, preventing the overwrite of session data and ensuring that CSRF tokens are correctly validated.\n\n## Context\n\nThe service-front component uses the default Redis save handler for persisting session data, which can lead to session data loss and CSRF token validation failures in certain situations. The current implementation allows concurrent modifications to session data, which can result in the overwrite of session data and loss of CSRF tokens.\n\n## Problem\n\nThe current implementation of the Redis session handler does not provide a mechanism to prevent concurrent modifications to session data, which can lead to session data loss and CSRF token validation failures. This is particularly problematic in situations where multiple requests are made to the application, and the session data needs to be updated accordingly.\n\n## Decision Criteria\n\n1. Ensure the integrity of CSRF tokens by preventing concurrent modifications to session data.\n2. Prevent session data loss by ensuring that only one request can modify the session data at a time.\n3. Minimize the impact on the existing application code and infrastructure.\n\n## Solution\n\nWe will implement a custom Redis session handler that uses a lock mechanism to prevent concurrent modifications to session data. This will ensure that only one request can modify the session data at a time, preventing the overwrite of session data and ensuring that CSRF tokens are correctly validated.\n\n## Rationale\n\nThe custom Redis session handler will use a lock mechanism to prevent concurrent modifications to session data. This will ensure that only one request can modify the session data at a time, preventing the overwrite of session data and ensuring that CSRF tokens are correctly validated. This solution will also minimize the impact on the existing application code and infrastructure.\n\n## Consequences\n\n1. The custom Redis session handler will provide a mechanism to prevent concurrent modifications to session data, ensuring the integrity of CSRF tokens and preventing session data loss.\n2. The implementation of the custom Redis session handler will require additional code and testing, which may require additional resources and time.\n3. The solution will ensure that the application is more robust and reliable, reducing the risk of session data loss and CSRF token validation failures.","GenTime":"2024-08-10 06:54:39"}
{"File Name":"DunkMe\/0001-front-end.md","Context":"#### Context and Problem Statement\\nA front end, client facing technology stack \/ language is needed.\\n#### Considered Options\\n- ASP.NET MVC (C#)\\n- ASP.NET Web Forms (C#)\\n- Angular (with TypeScript)\\n#### Decision Outcome\\nChosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n","Decision":"Chosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n","tokens":93,"id":1659,"Prediction":"## Decision\n\n**Front-end Technology Stack: Angular**\n\n## Context and Problem Statement\n\nA front-end, client-facing technology stack\/language is needed.\n\n## Considered Options\n\n* ASP.NET MVC (C#)\n* ASP.NET Web Forms (C#)\n* Angular (with TypeScript)\n\n## Decision Outcome\n\nChosen option: \"Angular\", because:\n\n* This is in-line with Trade Me's technical stack trajectory.\n* In line with what is considered industry standard for green field projects.\n\nThis decision ensures that Trade Me's front-end technology stack aligns with their existing technical stack trajectory and industry standards, making it easier to maintain and scale the project in the future.","GenTime":"2024-08-10 06:56:17"}
{"File Name":"DunkMe\/0002-data-access-api.md","Context":"#### Context and Problem Statement\\nA technology stack is required for the projects data access application programming interface (API).\\n#### Considered Options\\n- ASP.NET Web API (C#)\\n#### Decision Outcome\\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","Decision":"Chosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","tokens":111,"id":1660,"Prediction":"Here is the Architectural Decision Record (ADR) based on the provided context and decision outcome:\n\n**Decision:** Use ASP.NET Web API (C#) as the technology stack for the projects data access application programming interface (API).\n\n**Context:** The project requires a technology stack for the data access API.\n\n**Problem Statement:** The project needs a suitable technology stack for the data access API.\n\n**Considered Options:**\n\n* ASP.NET Web API (C#)\n\n**Decision Outcome:** We chose ASP.NET Web API (C#) because:\n\n* It can target.NET Core 2, allowing for flexibility and scalability.\n* Dapper Object Relational Mapper (ORM) can be used, leveraging existing skills within Trade Me.\n* A repository pattern can be applied, facilitating unit testing and promoting maintainability.\n\n**Status:** [Accepted]\n\n**Implementors:** [List of implementors]\n\n**Links:** [Any relevant links or references]\n\n**Notes:** [Any additional notes or comments]\n\nThis ADR provides a clear and concise record of the decision-making process, including the context, problem statement, considered options, and decision outcome. It also provides a reference for future developers to understand the reasoning behind the chosen technology stack.","GenTime":"2024-08-10 06:56:31"}
{"File Name":"DunkMe\/0003-relational-database.md","Context":"#### Context and Problem Statement\\nA relational database is required to persist the applications data.\\n#### Considered Options\\n- Microsoft SQL\\n- MySQL\\n- PostgreSQL\\n- SQLite\\n#### Decision Outcome\\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n","Decision":"Chosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n","tokens":78,"id":1661,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Use Microsoft SQL as the relational database for persisting the application's data.\n\n**Context and Problem Statement:** A relational database is required to persist the application's data.\n\n**Considered Options:**\n\n* Microsoft SQL\n* MySQL\n* PostgreSQL\n* SQLite\n\n**Decision Outcome:** Chosen option: \"Microsoft SQL\", because:\n\n* This is in-line with Trade Me's technical stack.\n* Microsoft SQL is already widely and almost exclusively used at Trade Me.\n\n**Rationale:** By choosing Microsoft SQL, we are aligning with Trade Me's existing technical stack and leveraging the familiarity and expertise of the team. This decision ensures a seamless integration with the existing infrastructure and reduces the complexity of implementing and maintaining the database.","GenTime":"2024-08-10 06:56:39"}
{"File Name":"ODT_OnlineBank\/ARD-0001.md","Context":"## Context\\n*This section describes the forces at play, including technological, political, social, and project local.\\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\\nIt is simply describing facts.*\\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and\\nprocesses. ODT will be initialy focus on Green Field Developent. The Online Bank is a sample app used in the ODT University classes.  The Purpose of theapplication is to provide a\\nsample app as a context for the training courses.  We believe most people are familiar with online\\nbanking, and won't need to spend a lot of time learning the problem domain.\\nThe purpose of this document is to determine the technology stack for the sample application.\\n**NOTE:** While we are initially focused on a Java Web stack, we may elect to develop out an example in the Microsoft Stack.\\nIf you would like to help, please contact us.\\n","Decision":"*This section describes our response to these forces. It is stated in full sentences, with active voice.\\n\"We will ...\"*\\nWe will be focusing on the modern web java stack.\\n### UI Technology Stack:\\n- Angular 2\\n- Bootstrap\\n- protractor\\n- jasmine\\n### Server Side Technology Stack:\\n- Java 1.8+\\n- SpringBoot 1.4+\\n- Restful\\n- MySQL\\n- Spring Data JPA (Hibernate)\\n- JUnit 5 \/ Spock\\n### Development Technology Stack:\\n- Local Development\\n- SCM: GIT\\n- Intellij\\n- Favorite Editor\\n- Gradle\\n- Angular Quickstart\\n- Concourse CI (CI\/CD)\\n- Jira\\n- Sonar\\n#### Deployment:\\n- Pivotal Cloud Foundry (Java and Static Build Packs)\\n","tokens":220,"id":1665,"Prediction":"## Decision\n\n**Technology Stack for the Online Bank Sample Application**\n\n## Context\n\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and processes. The Online Bank is a sample app used in the ODT University classes, aiming to provide a familiar context for training courses. The purpose of this document is to determine the technology stack for the sample application.\n\nThe ODT is initially focused on a Java Web stack, but may also consider developing an example in the Microsoft Stack in the future. The goal is to provide a sample app that can be used as a context for training courses, with the understanding that most people are familiar with online banking and won't need to spend a lot of time learning the problem domain.\n\n## Decision\n\nAfter careful consideration, we have decided to use a Java-based technology stack for the Online Bank sample application. This decision is based on the following factors:\n\n* The initial focus of ODT on Java Web stack\n* The familiarity of most people with online banking, reducing the need for extensive problem domain learning\n* The ability to use Java-based technologies to demonstrate lean, agile, and devOps principles and practices\n\nWe will use the following technologies:\n\n* Java 11 as the programming language\n* Spring Boot as the web framework\n* MySQL as the relational database management system\n* Apache Tomcat as the web server\n* Maven as the build tool\n\nThis technology stack will allow us to create a robust and scalable sample application that can be used for training purposes. We will also consider developing an example in the Microsoft Stack in the future, as mentioned earlier.\n\n## Conclusion\n\nThe decision to use a Java-based technology stack for the Online Bank sample application is based on the initial focus of ODT and the familiarity of most people with online banking. We believe this technology stack will allow us to create a sample application that can be used effectively for training purposes.","GenTime":"2024-08-10 06:57:38"}
{"File Name":"govuk-docker\/0004-use-versioned-database-services.md","Context":"## Context\\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\\nMongoDB and Elasticsearch. These have their data persisted in\\n[Docker volumes][volume] defined [for each service][defined-volumes].\\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\\nbegan receiving cryptic errors when trying to run applications that depended\\non MongoDB. This was because they had a volume with data structured for MongoDB\\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\\nWe felt that this upgrade path was more confusing and difficult than it needed\\nbe and wanted to improve it for the future.\\n[volume]: https:\/\/docs.docker.com\/storage\/volumes\/\\n[defined-volumes]: https:\/\/github.com\/alphagov\/govuk-docker\/blob\/cb124a3a1d3353e777d4f777d77f03f93415d415\/docker-compose.yml#L5-L10\\n[mongodb-update]: https:\/\/github.com\/alphagov\/govuk-docker\/pull\/356\\n","Decision":"We decided that we would rename database services, and their respective\\nvolumes, to reflect the software version number. For example the `mongo`\\nservice, and namesake volume, have been renamed to `mongo-3.6`.\\nThe precision of the version number is chosen based on the backwards\\ncompatibility between versions of the software. For example to upgrade from\\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\\nhave specified the service and volume with respective to a major version:\\n`elasticsearch-6`.\\n","tokens":238,"id":1666,"Prediction":"## Decision\n\nTo improve the upgrade path for databases in GOV.UK Docker, we will use Docker's built-in support for database migration by using Docker Compose's `volumes` feature with a consistent naming convention and a separate directory for each database.\n\n## Context\n\nGOV.UK Docker has services defined for various databases, such as PostgreSQL, MongoDB, and Elasticsearch. These have their data persisted in Docker volumes defined for each service. When GOV.UK Docker updated from MongoDB 2.4 to 3.6, users began receiving cryptic errors when trying to run applications that depended on MongoDB. This was because they had a volume with data structured for MongoDB 2.4 whereas MongoDB was expecting this to be structured for 3.6.\n\n## Problem\n\nThe current approach of using separate volumes for each database service makes it difficult to manage and upgrade the databases. The upgrade path from MongoDB 2.4 to 3.6 was confusing and difficult, resulting in errors for users.\n\n## Decision Criteria\n\n1. Improve the upgrade path for databases in GOV.UK Docker.\n2. Simplify the management of database data.\n3. Reduce errors caused by database version upgrades.\n\n## Decision\n\nWe will use Docker Compose's `volumes` feature to define a consistent naming convention for each database volume. We will create a separate directory for each database, and each directory will contain the relevant data for that database. This will allow us to easily manage and upgrade the databases, and reduce the risk of errors caused by version upgrades.\n\n## Consequences\n\n1. Improved upgrade path for databases in GOV.UK Docker.\n2. Simplified management of database data.\n3. Reduced errors caused by database version upgrades.\n\n## Rationale\n\nUsing Docker Compose's `volumes` feature with a consistent naming convention and a separate directory for each database will improve the upgrade path for databases in GOV.UK Docker. This approach will simplify the management of database data and reduce the risk of errors caused by version upgrades.","GenTime":"2024-08-10 06:58:01"}
{"File Name":"govuk-docker\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\nThe [original govuk-docker repo](https:\/\/github.com\/benthorner\/govuk-docker) was written outside of GOV.UK. Major architectural decisions, as well as more minor decisions, were written as [documentation in the repo][DECISIONS], together with their associated user [NEEDS]. While these documents have historical value, they are not being maintained and increasingly differ from the current state of the repo. As part of adopting an ADR approach, we should clearly deprecate these historical documents to avoid confusion.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":127,"id":1669,"Prediction":"## Decision\n\n**Title:** Deprecate Historical Architectural Decision Records in the govuk-docker Repo\n\n**Context:** As we adopt an ADR (Architectural Decision Record) approach, we need to clearly deprecate the historical architectural decision records in the govuk-docker repo to avoid confusion and ensure consistency.\n\n**Problem:** The original govuk-docker repo was written outside of GOV.UK, and major architectural decisions were documented in the repo, along with their associated user needs. However, these documents are not being maintained and increasingly differ from the current state of the repo.\n\n**Decision:** We will deprecate the historical architectural decision records in the govuk-docker repo and focus on creating new ADRs that accurately reflect the current state of the project.\n\n**Rationale:** Deprecating the historical documents will help to:\n\n* Avoid confusion by removing outdated and potentially conflicting information\n* Ensure consistency by focusing on creating new ADRs that accurately reflect the current state of the project\n* Simplify maintenance by reducing the number of outdated documents that need to be updated or removed\n\n**Consequences:**\n\n* Historical documents will no longer be maintained or updated\n* New ADRs will be created to reflect the current state of the project\n* The deprecation of historical documents may cause some disruption to users who rely on the information, but this will be minimized by providing clear documentation and support\n\n**Status:** This decision is approved and will be implemented as part of our ADR adoption process.\n\n**Related Decisions:** [Insert links to related ADRs, if any]\n\n**Links:** [Insert links to the historical documents being deprecated, if any]","GenTime":"2024-08-10 06:58:19"}
{"File Name":"raster-foundry\/adr-0004-static-asset-pipeline.md","Context":"## Context\\nIn selecting a static asset pipeline, there are several factors that are important to us.\\nWe don\u2019t want to lose features that speed up developer feedback, such as a dev server with linting, auto-gen, and browser reload.\\nThe solution needs to support tests run with a single command similar to what we\u2019ve had in previous projects.\\nThe solution needs to be fast.\\nSome team members have expressed dissatisfaction with the stability of the NPM ecosystem, but pretty much everything relies on it to some extent, so the issue is probably best addressed elsewhere (possibly using a lazy NPM cache).\\nIn the past, our team has used Grunt to manage and build static assets. Various team members \/ designers have expressed discontent with the idea of going forward using Grunt, primarily due to inherent factors such as long compile times and configuration complexity.\\nOther teams in Azavea have had good experiences with Webpack. Webpack is not a complete replacement for Grunt, but of the difference in functionality, it is highly likely that NPM tasks will cover it if used in conjunction. Despite scattered complaints of complexity and poor documentation found while researching, I was able to set up pipeline using npm and webpack overnight without too many issues. Significant features which are built in include a dev server capable of supporting hot reloading, very fast feedback on changes, and support for all the tools we commonly use in our applications. In addition to quality of life features, Webpack also gives us a dependency graph which includes static assets and bundle splitting. Combined with NPM, it appears to be a good solution.\\nA final option that we have to consider is Gulp, which has been used on a couple of projects by the Urban Apps team. Where grunt uses configuration files, Gulp uses code. Gulp\u2019s main advantage is speed: streaming operations mean that files never touch disk between chained tasks. Other than the method of configuration and speed of operating, the basic functionality of Gulp and Grunt seem to be fairly aligned.\\n","Decision":"Given the good experiences that others in the company have had with Webpack, we\u2019ve decided to run the static asset pipeline using a combination of npm and webpack. Npm will be used to manage dependencies and initiating tasks, while webpack will be used to run the dev server and build static assets.\\n","tokens":408,"id":1680,"Prediction":"## Decision\n\nAfter careful consideration of the requirements and options, we have decided to use Webpack as our static asset pipeline solution. This decision is based on the following factors:\n\n* Webpack's ability to provide a dev server with linting, auto-gen, and browser reload, which is essential for speeding up developer feedback.\n* Its support for tests run with a single command, similar to previous projects.\n* Its fast performance, which is critical for our team's productivity.\n* The stability of the NPM ecosystem, despite some concerns, is a necessary evil for our project's dependencies.\n* Webpack's ability to provide a dependency graph and bundle splitting, which will help us manage and optimize our static assets.\n* Its compatibility with NPM, which will allow us to leverage the existing ecosystem and tools.\n\nWe have also considered Gulp as an alternative, but its configuration style and speed advantages do not outweigh the benefits of using Webpack. While Grunt is an option, its limitations and complexity make it less desirable compared to Webpack.\n\n## Rationale\n\nWe chose Webpack because it provides a comprehensive set of features that meet our requirements, including a dev server, fast feedback, and support for tests and NPM. While Gulp is a viable option, its configuration style and lack of built-in features for dependency graph and bundle splitting make it less suitable for our needs. Grunt's complexity and limitations also make it a less desirable choice.\n\n## Consequences\n\n* We will use Webpack as our static asset pipeline solution, which will provide a dev server with linting, auto-gen, and browser reload, support for tests run with a single command, and fast performance.\n* We will need to learn and adapt to Webpack's configuration and usage.\n* We will need to consider the trade-offs and limitations of using Webpack, such as its potential complexity and steep learning curve.\n* We will need to ensure that our project's dependencies are managed effectively using NPM and Webpack.","GenTime":"2024-08-10 07:00:07"}
{"File Name":"raster-foundry\/adr-0013-spark-execution-environment.md","Context":"## Context\\nPart of the Raster Foundry processing workflow requires a source imagery ingest into a GeoTrellis Catalog. GeoTrellis depends on Apache Spark to parallelize work during an ingest, and that work typically occurs on an Apache Spark enabled compute cluster.\\nPrior iterations of Raster Foundry attempted ingests with an Apache YARN managed compute cluster via Amazon Elastic MapReduce (EMR) on a per-request basis. Unfortunately, that didn't perform well due to the overhead of bootstrapping an EMR cluster.\\nOur goal for the this iteration aims to keep some Spark cluster components active at all times through a combination of spot pricing and usage based cluster auto-scaling. This approach should help minimize cluster bootstrapping durations and keep cost-incurring cluster resources at a minimum.\\n","Decision":"Given that we are still early in Raster Foundry's product development, the desire to keep costs low and ship quickly carry significant weight. In order to meet those objectives, but still minimize cluster bootstrapping overhead, the first pass at having Spark enabled resources on standby will consist of a shared (across staging and production) Amazon EMR cluster.\\nThe overall Raster Foundry infrastructure will be split across three Amazon Virtual Private Clouds (VPCs):\\n- Staging (`10.0.0.0\/18`, 16382 IPs)\\n- Spark (`10.0.64.0\/18`, 16382 IPs)\\n- Production (`10.0.128.0\/18`, 16382 IPs)\\nThe `Staging` and `Production` VPCs will be peered with the `Spark` VPC to allow direct private network access, if necessary. Regardless, public facing APIs such as the Amazon EMR's Step API, or the Raster Foundry API can be used to pass information between systems.\\nDespite having thought long and hard about the consequences, I suspect that the section below does not cover all of the issues that lie ahead of us. The hope is that at least these core ideas remain in future iterations of the Spark execution environment:\\n- Shared cluster resources between environments\\n- Isolated environments at the VPC level\\n- Build on top of the Amazon EMR platform\\n- Use spot pricing and auto-scaling aggressively to keep costs low\\n","tokens":162,"id":1681,"Prediction":"## Decision\n\n**Decision:** We will utilize a combination of spot pricing and usage-based cluster auto-scaling to keep some Spark cluster components active at all times, minimizing cluster bootstrapping durations and keeping cost-incurring cluster resources at a minimum.\n\n**Context:** As part of the Raster Foundry processing workflow, we need to ingest source imagery into a GeoTrellis Catalog, which relies on Apache Spark for parallelization. In previous iterations, we used an Apache YARN managed compute cluster via Amazon Elastic MapReduce (EMR) on a per-request basis, but this approach was inefficient due to the overhead of bootstrapping an EMR cluster.\n\n**Rationale:** By keeping some Spark cluster components active at all times, we can minimize the time it takes to bootstrap a cluster and reduce the cost of incurring cluster resources. Spot pricing and usage-based cluster auto-scaling will allow us to dynamically adjust the cluster size based on actual usage, ensuring that we only pay for the resources we need.\n\n**Consequences:**\n\n* Improved performance: By keeping some Spark cluster components active at all times, we can reduce the time it takes to ingest source imagery into the GeoTrellis Catalog.\n* Cost savings: By minimizing the need for cluster bootstrapping and only paying for resources based on actual usage, we can reduce the overall cost of processing.\n* Increased scalability: With the ability to dynamically adjust the cluster size based on usage, we can handle increased processing demands without incurring additional costs.\n\n**Alternatives Considered:**\n\n* Continuing to use an Apache YARN managed compute cluster via Amazon EMR on a per-request basis, which would likely result in slower performance and higher costs.\n* Using a cloud-based managed Spark service, which would likely result in higher costs and limited customization options.\n\n**Monitoring and Feedback:** We will monitor the performance and cost-effectiveness of this approach and adjust as needed to ensure optimal results. We will also gather feedback from users and stakeholders to refine the approach and identify areas for improvement.","GenTime":"2024-08-10 07:00:29"}
{"File Name":"raster-foundry\/adr-0006-workflow-manager.md","Context":"## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n","Decision":"Raster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\\ndatabase and a redis cache.\\n","tokens":353,"id":1688,"Prediction":"## Decision\n\nWe will use Airflow as the workflow tool for managing asynchronous tasks and workflows in Raster Foundry.\n\n## Context\n\nAs described in the context, we require a tool to manage asynchronous tasks and workflows in Raster Foundry, which involves features such as monitoring task progress, retrying on failures, and notifying personnel if necessary. We have evaluated four options: Celery, SWF (botoflow), Luigi, and Airflow. While Celery and SWF are familiar to us, we have decided to choose Airflow due to its maturity, stability, and excellent UI.\n\n## Problem\n\nThe problem we are trying to solve is the need for a workflow tool that can manage asynchronous tasks and workflows in Raster Foundry. We require a tool that can monitor task progress, retry on failures, and notify personnel if necessary.\n\n## Decision Criteria\n\nWe evaluated the four options based on the following criteria:\n\n* Familiarity: We considered the familiarity of our team with each tool, as well as the availability of documentation and support.\n* Stability: We evaluated the stability of each tool, including the maturity of the project and the number of users.\n* Execution Model: We considered the execution model of each tool, including its ability to handle ad hoc tasks, scheduled tasks, and workflows.\n* UI: We evaluated the user interface of each tool, including its ease of use and ability to provide real-time feedback.\n* Integration: We considered the ease of integration with our existing infrastructure and tools.\n\n## Decision\n\nBased on our evaluation, we have decided to use Airflow as the workflow tool for managing asynchronous tasks and workflows in Raster Foundry. Airflow meets our requirements for familiarity, stability, execution model, and UI. While it requires Celery for what we want to do, and requires managing the scheduler and a cache, we believe the benefits of using Airflow outweigh the additional complexity.\n\n## Consequences\n\nThe consequences of this decision are:\n\n* We will need to invest time in learning Airflow and integrating it with our existing infrastructure and tools.\n* We will need to manage the scheduler and cache for Airflow.\n* We will need to modify our workflows and tasks to use Airflow instead of Celery or SWF.\n\nOverall, we believe that Airflow is the best choice for managing asynchronous tasks and workflows in Raster Foundry, and we are confident that it will meet our requirements and provide a high level of stability and reliability.","GenTime":"2024-08-10 07:02:08"}
{"File Name":"simple-server\/011-rollup-summary-tables-for-dashboards.md","Context":"## Context\\nEarlier we went with a [materialized view approach](https:\/\/github.com\/simpledotorg\/simple-server\/blob\/master\/doc\/arch\/008-materialized-views-for-dashboards.md) for dashboard reporting data. This approach has worked out well for the current reports but has limitations in terms of scaling and complexity.\\nWe currently have a need for a new set of dashboard reports that rely on similar sets of data, and anticipate more needs for summary data for monthly and quarterly reports. We'd like to try to find a simpler, more stable, and more scalable approach to making that data easily accessible.\\n","Decision":"We will create rollup summary tables for to capture the \"most recent blood pressure per patient per period\". These rollup tables will be updated in real-time as blood pressures are captured, removing the need for recurring batch jobs. We will denormalize as much data as we need to to make reporting queries simple and cheap to execute, to prevent the need for any complicated SQL queries or additional data layer caching.\\nOnce the current period has completed, we will treat the data in these rollup tables for that period as immutable. This will let us and our users have more confidence in reports from past periods, as they will not change due to patients moving to different facilities or changes in how we calculate things.\\nWe will focus on the District controlled blood pressure trend report as a first test case for this sort of approach, as it is a key report and one we'd like to gather feedback on as quickly as possible.\\n","tokens":123,"id":1701,"Prediction":"## Decision\n\n**Decision:** Implement a Data Warehousing approach using a dimensional modeling strategy to store and manage dashboard reporting data.\n\n**Context:** As previously discussed, our materialized view approach has been effective for our current reports but has limitations in terms of scaling and complexity. With the need for new reports and summary data for monthly and quarterly reports, we require a more scalable and stable solution.\n\n**Problem:** The materialized view approach is not designed to handle the increasing complexity and volume of data required for our growing set of reports. This approach also lacks the ability to easily manage and maintain large amounts of data.\n\n**Reasoning:** A Data Warehousing approach using a dimensional modeling strategy offers a more scalable, stable, and maintainable solution for our dashboard reporting data needs. This approach allows us to separate our reporting data into fact tables and dimension tables, making it easier to manage and analyze large datasets. Additionally, this approach enables us to use SQL-based queries to retrieve data, making it more efficient and flexible.\n\n**Consequences:**\n\n* We will need to invest time in designing and implementing a dimensional model for our reporting data.\n* We will need to migrate our existing reports to use the new data warehousing approach.\n* We will need to ensure that our data ingestion and processing pipelines are designed to populate the data warehouse efficiently and accurately.\n* We will need to monitor and optimize the performance of our data warehouse to ensure it can handle the increasing volume of data.\n\n**Pros:**\n\n* Improved scalability and performance for our reporting data\n* Easier management and maintenance of large datasets\n* Increased flexibility and efficiency in querying and analyzing data\n* Better support for complex reporting and analytics requirements\n\n**Cons:**\n\n* Initial investment of time and resources in designing and implementing the dimensional model\n* Potential disruption to existing reports and processes during migration\n* Ongoing monitoring and optimization of the data warehouse to ensure performance and scalability.\n\nBy adopting a Data Warehousing approach using a dimensional modeling strategy, we can create a more scalable, stable, and maintainable solution for our dashboard reporting data needs, enabling us to efficiently and effectively support our growing set of reports and analytics requirements.","GenTime":"2024-08-10 07:06:25"}
{"File Name":"simple-server\/014-region-level-sync.md","Context":"## Context\\n[PRD](https:\/\/docs.google.com\/document\/d\/1Cflct0Y-44IRUVw_5-NptcnNSX1UgAPBiXqoXHq22io\/edit)\\nUsers in large districts reported that the Simple app was running very slow, making the app near-unusable.\\nThe slowdown was caused by the volume of patient data synced to the user\u2019s phone. We realised that the amount of data\\nbeing stored on the device had to be reduced for better long-term performance.\\nCurrently we sync the entire district's records to a user's phone. Some of the large districts have upto 50,000 patients,\\nwhich can amount to 400-500 MB of data. On lower-end phones we noticed the app started slowing down when the DB size grew beyond 250 MB.\\nA district typically has between 1-20 blocks. From trends in IHCI, we found it's uncommon for patients to visit facilities across blocks.\\nPatients that have a BP taken in more than 1 block is around 2%, with the exceptions of:\\nSindhudurg (9.8%), Hoshiarpur (5.3%), Bathinda (3.1%).\\nThis means that we can sync only a block's data to the user's phone and be reasonably confident about finding patients on the app.\\n","Decision":"- The server will sync records from the user's block instead of the entire district.\\nSpecifically the following patients will be synced:\\n- patients that registered at a facility in the same block,\\n- patients that are assigned to a facility in the same block, and\\n- patients that have an appointment scheduled at a facility in the same block.\\n- All other sync resources will return records belonging to these patients only.\\n- The sync mechanism should support the ability to adjust the sync radius to any sync region.\\nThis is important in case we need to change the kind of records that are synced to the app in the future.\\nSee the [wiki entry on Region level sync](..\/wiki\/adjusting-sync-boundaries.md) for how it works.\\n### On the app\\n- Users can continue selecting any facility in their district when switching facilities.\\n- Users can continue selecting any facility in their district when scheduling a patient\u2019s next visit or preferred facility.\\n- It is possible that a patient will visit a facility outside their block and their record will not be found on the user\u2019s device. In this case the user should\\n- Scan the patient\u2019s BP passport if they have one.\\n- Register the patient again, as if they were new. Make sure to attach their existing BP passport to the registration.\\n- The duplicate patient records will be merged by the Simple team later.\\n","tokens":282,"id":1708,"Prediction":"## Decision\n\n**Decision:** Implement a block-level data synchronization approach to reduce the amount of patient data stored on the user's phone, improving the app's performance and usability.\n\n## Context\n\nThe Simple app is experiencing performance issues due to the large volume of patient data being synced to the user's phone, particularly in large districts with over 50,000 patients. This is causing the app to slow down and become unusable. To address this issue, we need to reduce the amount of data being stored on the device.\n\n## Problem\n\nThe current approach of syncing the entire district's records to the user's phone is causing performance issues, especially on lower-end phones. The large amount of data being stored on the device is leading to slow app performance and making the app unusable.\n\n## Constraints\n\n* Patients rarely visit facilities across blocks, with only 2% of patients having a BP taken in more than one block, except in a few exceptional cases.\n* Districts typically have between 1-20 blocks.\n* We need to find a solution that balances performance and usability while still providing a useful app experience.\n\n## Decision Criteria\n\n* Improve app performance and usability\n* Reduce the amount of data being stored on the user's phone\n* Ensure the app remains useful for users\n\n## Solution\n\nImplement a block-level data synchronization approach, where only the data from the user's assigned block is synced to their phone. This will reduce the amount of data being stored on the device, improving performance and usability. This approach is supported by the fact that patients rarely visit facilities across blocks, making it reasonable to assume that syncing only a block's data will be sufficient.\n\n## Rationale\n\nBy syncing only a block's data, we can reduce the amount of data being stored on the user's phone, which will improve app performance and usability. This approach also aligns with the typical district structure, where patients are usually associated with a specific block. The exceptions noted in the context, where patients visit facilities across blocks, can be handled through additional logic or exceptions to ensure that the app remains useful for these users.\n\n## Conclusion\n\nImplementing a block-level data synchronization approach will improve the performance and usability of the Simple app, making it more usable for users. This decision is based on the analysis of the performance issues, constraints, and decision criteria, and is supported by the data on patient behavior and district structure.","GenTime":"2024-08-10 07:08:35"}
{"File Name":"simple-server\/001-synchronization.md","Context":"## Context\\nNetwork connectivity on phones in rural areas can be low, and\\npatchy. So, the app needs to work offline, as much as possible. The\\nsync mechanism exists to allow sharing records of patients, blood\\npressures, etc across devices.\\nWe need to accommodate cases where patients, and nurses move across\\nfacilities multiple times during a week.\\n","Decision":"### Mechanism\\n1. Send records from device to server\\nThe device needs to keep track of records that need to be\\nsynced. These can be new records, or records that have one or more\\nfields updated. These records need to be formatted into the payload\\nschemas as defined in the individual APIs below. The API will not\\ndifferentiate between new and updated records; this is handled by the\\nserver.  These payloads then need to be sent in batches, where each\\nbatch is inherently independent of the previous one. However, it is\\nimportant to _send_ all records that need syncing, before fetching\\nrecords from the server.\\n2. Fetch records from server to device\\nWhen fetching records for the first time, the `first_time` query\\nparameter should be set. The server will then send back a number of\\nrecords as defined by the `number_of_records` query parameter. This is\\nessentially the \\\"batch_size\\\". The first response also includes a\\n`latest_record_timestamp`, that needs to be sent with the next\\nrequest, in order to retrieve more records from the server. If there\\nare no more records to be sent from the server, it returns an empty\\nlist.\\n3. Merge records on device\\nAfter receiving records from the server, the device will match the\\nrecords in the local database using the ID field of the entity, and\\nupdate them to the server's version. If the local record in the\\ndatabase is pending a sync, it will not update it. The merging of\\nrecords will be handled only by the server.\\nThe server may send back the same record multiple times, so updating\\nrecords on the device needs to be idempotent.\\n### Levers\\nPhysical resource constraints such as battery life, or network are not\\nunder our control. In order for the app to adapt well to the\\nconstraints, we need to have knobs on:\\n1. Batch size: how many records to send or fetch\\n2. Sync frequency: how often sending or fetching happens\\nPreferably, these levers are modifiable at run-time, per device.\\n","tokens":77,"id":1712,"Prediction":"## Decision\n\nTo ensure the app works offline as much as possible and accommodates cases where patients and nurses move across facilities, we will implement a hybrid approach to data storage and synchronization.\n\n## Context\n\nAs mentioned, network connectivity in rural areas can be low and patchy, making it essential to allow the app to function offline. Additionally, patients and nurses may move across facilities multiple times during a week, requiring a robust synchronization mechanism to ensure data consistency across devices.\n\n## Problem\n\nWithout a hybrid approach, the app would either require a constant internet connection, which may not be feasible in rural areas, or store all data locally, which would lead to inconsistencies when data is synced across devices.\n\n## Solution\n\nWe will implement a hybrid approach that combines local storage with cloud-based storage. The app will store critical data locally on the device, allowing it to function offline. When a stable internet connection is available, the app will sync the locally stored data with the cloud-based storage, ensuring data consistency across devices.\n\n## Consequences\n\n* The app will be able to function offline, reducing the impact of poor network connectivity on rural areas.\n* The hybrid approach will ensure data consistency across devices, even when patients and nurses move across facilities.\n* The app will require a more complex data storage and synchronization mechanism, which may increase development time and complexity.\n* The app will require a reliable internet connection to sync data, which may not always be available in rural areas.\n\n## Rationale\n\nThe hybrid approach allows us to balance the need for offline functionality with the need for data consistency across devices. By storing critical data locally, we can ensure the app remains functional even in areas with poor network connectivity. When a stable internet connection is available, we can sync the data with the cloud-based storage, ensuring consistency across devices.\n\n## Alternatives Considered\n\n* Storing all data locally, which would lead to inconsistencies when data is synced across devices.\n* Requiring a constant internet connection, which may not be feasible in rural areas.\n* Implementing a more complex data storage mechanism, such as a peer-to-peer network, which may add unnecessary complexity to the app.\n\n## Status\n\nThis decision is approved and will be implemented in the next development cycle.","GenTime":"2024-08-10 07:09:38"}
{"File Name":"viplab-websocket-api\/0004-use-asymmetric-jwt-signing.md","Context":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n","Decision":"* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","tokens":77,"id":1721,"Prediction":"## Decision\n\nWe will use a public\/private key pair using RSA or ECDSA to sign and verify JSON Web Tokens, rather than a secret key with the HMAC algorithm.\n\n## Context\n\nWhen implementing JWTs, we need to decide which method to use for generating signatures and verifying them. JWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\n\n## Problem Statement\n\nWhen using JSON Web Tokens generating signatures and verifying them is an important task. JWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA. When implementing JWTs one must decide which method to use.\n\n## Decision Drivers\n\n* Multi-tenant support with own keys for each tenant\n\n## Decision Rationale\n\nWe have chosen to use a public\/private key pair using RSA or ECDSA to sign and verify JSON Web Tokens for several reasons:\n\n* Multi-tenant support: By using a public\/private key pair, we can provide each tenant with their own key pair, ensuring that their data is secure and cannot be accessed by other tenants.\n* Security: Public\/private key pairs provide a higher level of security compared to secret keys, as the private key is not shared with anyone and the public key can be shared without compromising the security of the system.\n* Flexibility: RSA and ECDSA are widely supported algorithms, and we can choose the one that best suits our needs.\n\n## Consequences\n\n* We will need to generate and manage public\/private key pairs for each tenant.\n* We will need to implement RSA or ECDSA algorithms to sign and verify JWTs.\n* We will need to ensure that the private keys are securely stored and protected.\n\n## Pros and Cons\n\nPros:\n\n* Provides a higher level of security compared to secret keys.\n* Allows for multi-tenant support with own keys for each tenant.\n* Flexibility in choosing the algorithm.\n\nCons:\n\n* Requires generating and managing public\/private key pairs for each tenant.\n* Requires implementing RSA or ECDSA algorithms.\n* Requires ensuring that the private keys are securely stored and protected.","GenTime":"2024-08-10 07:12:38"}
{"File Name":"external-service-operator\/0004-ports-are-never-stored-as-named-ports.md","Context":"## Context\\nActually it does not make sense to store ports as strings, as those Backends are not in the cluster so they don't have names and can only be referenced by a port number. Nevertheless the Kuberenetes API makes it possible to store ports as string which will not be used at the endpoints generated by the External Service Operator.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":70,"id":1735,"Prediction":"## Decision\n\nDecision: Store ports as integers in the database and only convert them to strings when interacting with the Kubernetes API.\n\n## Context\n\nAs you mentioned, storing ports as strings may seem convenient, but since the Backends are not in the cluster and can only be referenced by a port number, it's more practical to store them as integers. This approach also aligns with the expected behavior of port numbers in the Kubernetes API.\n\n## Rationale\n\nStoring ports as integers ensures that the data is consistent and accurate, and it's easier to perform arithmetic operations on port numbers if needed. Additionally, converting ports to strings only when interacting with the Kubernetes API minimizes the overhead of unnecessary string conversions.\n\n## Consequences\n\n* The database will store port numbers as integers, which may require additional logic to convert them to strings when interacting with the Kubernetes API.\n* The External Service Operator will need to handle the conversion from integer to string when generating endpoints.\n* The system will be more efficient and accurate in its handling of port numbers.\n\n## Status\n\nApproved\n\n## Implemented By\n\n[Your Name]\n\n## Date\n\n[Date]","GenTime":"2024-08-10 07:17:23"}
{"File Name":"digitalrig-metal-aws\/0007-build-and-deploy-pipelines-separate.md","Context":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":1739,"Prediction":"## Decision\n\n**Decision:** Introduce a separate configuration management pipeline to decouple configuration changes from the build and deploy pipeline.\n\n**Context:** As mentioned in the previous iteration, the current pipeline has consolidated build and deploy into one pipeline, resulting in a full rebuild being required for any configuration change. This approach can lead to inefficiencies, especially for projects with long-running build steps.\n\n**Problem:** The current approach can cause delays and slow down the development process, as any configuration change requires a full rebuild and redeployment of the application.\n\n**Rationale:** To improve the development workflow and reduce the time-to-market, we need to separate the configuration management from the build and deploy process. This will allow developers to make changes to the configuration without requiring a full rebuild and redeployment.\n\n**Decision Drivers:**\n\n* Improve development efficiency and reduce delays\n* Enhance the overall development workflow\n* Allow for more flexibility and scalability in the configuration management process\n\n**Consequences:**\n\n* Additional complexity in the pipeline management\n* Increased overhead in terms of pipeline maintenance and monitoring\n* Potential for additional errors or bugs in the configuration management process\n\n**Alternatives Considered:**\n\n* Continue with the current approach and live with the inefficiencies\n* Introduce a separate pipeline for configuration changes, but make it a part of the existing pipeline\n* Use a configuration management tool that can handle changes independently of the build and deploy process\n\n**Implementation:**\n\n* Create a new pipeline specifically designed for configuration management\n* Implement a mechanism to trigger the configuration management pipeline automatically when changes are detected in the configuration files\n* Integrate the configuration management pipeline with the existing build and deploy pipeline to ensure seamless deployment of the application\n\n**Monitoring and Review:**\n\n* Monitor the performance and efficiency of the new pipeline\n* Review the effectiveness of the configuration management process and make adjustments as needed\n* Continuously evaluate the trade-offs between the benefits of the new pipeline and the additional complexity it introduces.","GenTime":"2024-08-10 07:18:31"}
{"File Name":"digitalrig-metal-aws\/0005-use-aws-rds-aurora-mysql-for-database-persistence.md","Context":"## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL\/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted\/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor\/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny\/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n","Decision":"Use AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\\n","tokens":238,"id":1740,"Prediction":"## Decision\n\nWe have decided to use Amazon RDS with MySQL as our persistence mechanism for Bookit. This decision is based on the following factors:\n\n* A SQL\/RDBMS approach was deemed more suitable for the application, as opposed to a NoSQL solution.\n* The need for a hosted\/PaaS solution led us to consider Amazon RDS, which is a popular choice for managed relational databases.\n* Given our presence in AWS, RDS was a natural choice, and we can leverage its integration with other AWS services.\n* We prioritized open-source and free database solutions, which led us to consider MySQL and PostgreSQL.\n* Amazon RDS offers a MySQL implementation that provides better performance and up-to-the-minute backups at a low additional cost, making it an attractive option.\n* Aurora, a MySQL-compatible database engine, offers high availability and a hands-off management experience, which aligns with our requirements.\n\nWe have chosen to skip the Aurora implementation for PostgreSQL at this time, as it is still in preview and we prioritize stability and reliability in our production environment.\n\n## Status\n\nThis decision has been made and implemented.\n\n## Affected Components\n\n* Bookit's persistence layer\n* Amazon RDS configuration\n\n## Rationale and Trade-Offs\n\nThe decision to use Amazon RDS with MySQL was driven by the need for a managed relational database solution that provides high availability and performance. While PostgreSQL is a popular choice, the preview status of the Aurora implementation and the additional cost of the MySQL implementation on RDS made MySQL a more attractive option at this time.\n\n## Open Questions\n\n* How will we handle database migrations and schema changes in the future?\n* What is the plan for handling database backups and restores?\n\n## References\n\n* AWS RDS documentation\n* Amazon Aurora documentation\n* MySQL documentation","GenTime":"2024-08-10 07:18:51"}
{"File Name":"digitalrig-metal-aws\/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md","Context":"## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":36,"id":1741,"Prediction":"## Decision\n\nTo address the limitations of Travis CI and provide a hosted\/PaaS CI\/CD solution, we will adopt CircleCI as our primary CI\/CD platform.\n\n## Context\n\nAs you mentioned, Travis CI has limitations on the stages that can be used to create a pipeline, which hinders our ability to implement a comprehensive CI\/CD workflow. We require a more flexible and customizable solution that can accommodate our specific needs.\n\n## Decision Rationale\n\nCircleCI offers a more extensive range of features and flexibility compared to Travis CI, allowing us to create complex workflows and integrate with various tools and services. Its cloud-based architecture and scalable infrastructure ensure high availability and reliability. Additionally, CircleCI provides a more user-friendly interface and extensive documentation, making it easier for our team to manage and maintain our CI\/CD pipelines.\n\n## Trade-Offs\n\nBy adopting CircleCI, we will need to invest time in learning its features and workflows, which may require additional training and documentation. However, this investment will pay off in the long run by providing a more robust and customizable CI\/CD solution that meets our specific needs.\n\n## Consequences\n\n* We will migrate our existing Travis CI pipelines to CircleCI, ensuring minimal disruption to our development workflow.\n* We will take advantage of CircleCI's advanced features, such as support for multiple languages, custom workflows, and integration with third-party services, to create a more comprehensive CI\/CD solution.\n* We will provide training and documentation to ensure that our team is equipped to manage and maintain our CircleCI pipelines effectively.\n\n## Reevaluation\n\nWe will reevaluate our CI\/CD solution in six months to assess its effectiveness and identify areas for improvement. This will enable us to make any necessary adjustments and ensure that our CI\/CD pipeline continues to meet our evolving needs.","GenTime":"2024-08-10 07:19:10"}
{"File Name":"digitalrig-metal-aws\/0006-create-reference-implementation-repository.md","Context":"## Context\\nThe rig defined at [Bookit Infrastructure](https:\/\/github.com\/buildit\/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n","Decision":"Create a digitalrig-metal-aws repo (https:\/\/github.com\/buildit\/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording\/concepts.\\n","tokens":162,"id":1742,"Prediction":"## Decision\n\nTo address the complexity and inflexibility of the current AWS Bare Metal Rig setup, we will implement a modular and extensible architecture for the rig, utilizing a combination of configuration files and a centralized repository.\n\n## Context\n\nAs described in the context, the current AWS Bare Metal Rig is specific to Bookit's needs, but it lacks flexibility and scalability. The rig is defined in a single repository, which makes it difficult to maintain and update. The use of branches to capture different component choices is not ideal, as it can be hard to discover and manage.\n\n## Problem\n\nThe current architecture of the AWS Bare Metal Rig has the following problems:\n\n* It is specific to Bookit's needs, making it difficult to adapt to changes or new requirements.\n* It lacks flexibility, making it hard to add or remove components.\n* It is not scalable, as it becomes difficult to maintain and update as instances of the rig diverge.\n* It is not easy to discover and manage the different component choices.\n\n## Decision\n\nTo address these problems, we will implement a modular and extensible architecture for the AWS Bare Metal Rig. This will be achieved by:\n\n* Creating a centralized repository for the rig's configuration files.\n* Using a configuration management tool to manage and version the configuration files.\n* Breaking down the rig into smaller, independent components, each with its own configuration file.\n* Creating a way to easily discover and manage the different component choices.\n* Implementing a process for updating and maintaining the rig, including a way to instantiate a new rig with the latest features.\n\n## Consequences\n\nThis decision will have the following consequences:\n\n* The rig will become more flexible and scalable, making it easier to adapt to changes or new requirements.\n* The rig will be easier to maintain and update, as changes can be made in a centralized repository.\n* The rig will be more discoverable, as the different component choices will be easily accessible.\n* The process for updating and maintaining the rig will be streamlined, making it easier to instantiate a new rig with the latest features.\n\n## Rationale\n\nThe decision to implement a modular and extensible architecture for the AWS Bare Metal Rig is based on the following rationale:\n\n* The rig's current architecture is inflexible and difficult to maintain, making it necessary to implement a more scalable and maintainable solution.\n* A centralized repository for the rig's configuration files will make it easier to manage and update the rig.\n* Breaking down the rig into smaller, independent components will make it easier to add or remove components and will increase the rig's flexibility.\n* Discoverability and manageability are critical aspects of the rig's architecture, and the proposed solution will address these concerns.\n\n## Alternatives\n\nThe alternatives considered for this decision were:\n\n* Continuing to use the current architecture and trying to adapt to changes or new requirements.\n* Implementing a completely new architecture for the rig, which would require significant resources and effort.\n* Using a different configuration management tool or process, which may not be compatible with the existing infrastructure.\n\nThe chosen solution is the most feasible and effective way to address the problems with the current architecture and to implement a more scalable and maintainable solution.","GenTime":"2024-08-10 07:19:45"}
{"File Name":"digitalrig-metal-aws\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1744,"Prediction":"Here is an Architectural Decision Record (ADR) for the software project:\n\n## Decision\n\n**ADR-001: Use of a Microservices Architecture**\n\n## Context\n\nAs our project grows in complexity and scale, we need to consider the best approach to designing and implementing our software architecture. We have been discussing the pros and cons of various architecture styles, including monolithic, service-oriented, and microservices. After careful evaluation, we have decided to adopt a microservices architecture for our project.\n\n## Problem\n\nOur project has multiple features that require independent development, deployment, and scaling. A monolithic architecture would make it difficult to maintain and update individual features without affecting the entire system. A service-oriented architecture would introduce unnecessary complexity and overhead. A microservices architecture, on the other hand, allows us to break down the system into smaller, independent services that can be developed, tested, and deployed independently.\n\n## Decision\n\nWe will design and implement our software as a collection of microservices, each responsible for a specific business capability or functionality. Each microservice will be developed, tested, and deployed independently, using a service registry and API gateway to manage communication between services.\n\n## Consequences\n\n* Pros:\n\t+ Easier maintenance and updates of individual features\n\t+ Improved scalability and fault tolerance\n\t+ Increased flexibility and adaptability to changing requirements\n* Cons:\n\t+ Increased complexity and overhead due to additional infrastructure and communication layers\n\t+ Potential for inconsistent data and errors due to distributed system\n\n## Status\n\nThis decision is currently in the \"proposed\" status, pending further evaluation and refinement.\n\n## References\n\n* [Martin Fowler - Microservices](https:\/\/martinfowler.com\/articles\/microservices.html)\n* [Sam Newman - Building Microservices](https:\/\/books.google.com\/books?id=7dKCAwAAQBAJ)\n\nNote: This is just a sample ADR, and you should adapt it to your specific project and needs.","GenTime":"2024-08-10 07:20:24"}
{"File Name":"TOSCAna\/0018-cloudfoundry-no-deployment-in-runtime.md","Context":"## Problem\\nMost of the credentials\/information of the environment (like a service address) are only available as soon the application is deployed.\\n","Decision":"* Chosen Alternative: creating scripts\\n","tokens":29,"id":1745,"Prediction":"## Decision\n\nTo address the problem of accessing environment-specific credentials and information only available during deployment, we will implement a configuration management system that separates environment-specific configuration from the application code.\n\n## Context\n\nAs the application is deployed to different environments (e.g., development, staging, production), the credentials and information required to interact with these environments are only available during deployment. This makes it challenging to manage and maintain the application's configuration in a way that is consistent across all environments.\n\n## Decision Rationale\n\nBy separating environment-specific configuration from the application code, we can:\n\n* Decouple the application logic from the environment-specific details\n* Easily switch between environments without modifying the application code\n* Manage and maintain the configuration in a centralized and consistent manner\n* Improve the overall maintainability and scalability of the application\n\n## Consequences\n\n* The application will require a configuration management system that can handle environment-specific configuration\n* The configuration management system will need to be integrated with the deployment process to ensure that the correct configuration is applied to each environment\n* The application code will need to be modified to use the configuration management system instead of hardcoding environment-specific details\n\n## Alternatives Considered\n\n* Hardcoding environment-specific details into the application code: This approach would make the application difficult to maintain and scale, as changes to the environment-specific details would require modifications to the application code.\n* Using environment variables: While environment variables can be used to pass environment-specific details to the application, this approach would still require the application code to be modified to handle the variables, and would not provide a centralized management system for the configuration.\n* Using a separate configuration file for each environment: This approach would require multiple configuration files, which would be difficult to manage and maintain, and would not provide a centralized management system for the configuration.","GenTime":"2024-08-10 07:20:43"}
{"File Name":"CAFE5\/0002-calculate-prior-from-user-input.md","Context":"## Context\\nA prior distribution needs to be specified to accurately calculate probabilities. The prior can\\nbe calculated in many different ways. Hopefully this decision reflects the least surprising\\nresults for the user.\\n","Decision":"The prior will be calculated as follows:\\n* if -p specified on command line: calculate a Poisson distribution with the specified lambda\\n* if -f specified on command line: The user has specified a root distribution. Use that. Issue a warning if the user has ALSO specified a Poisson lambda\\n* if -i specified on command line: Estimate a Poisson distribution from the families provided.\\n* Otherwise, use a uniform distribution. Issue a warning as this is not a very reasonable prior.\\n","tokens":40,"id":1747,"Prediction":"## Decision\n\n**Title:** Prior Distribution Selection\n\n**Context:** A prior distribution needs to be specified to accurately calculate probabilities. The prior can be calculated in many different ways. Hopefully this decision reflects the least surprising results for the user.\n\n**Decision:** We will use a uniform prior distribution as the default option. This decision is based on the principle of maximum entropy, which states that the uniform prior distribution is the least informative prior distribution that is consistent with the available information.\n\n**Rationale:**\n\n* The uniform prior distribution is a simple and intuitive choice that does not make any strong assumptions about the underlying distribution of the data.\n* It is a robust choice that is less sensitive to the choice of prior distribution than other methods, such as a normal prior distribution.\n* It is a good default choice because it does not favor any particular hypothesis or model over others.\n* It is easy to implement and understand, which makes it a good choice for a wide range of users.\n\n**Alternatives Considered:**\n\n* Normal prior distribution: This is a common choice for many statistical models, but it can be sensitive to the choice of mean and variance.\n* Informed prior distribution: This can be a good choice if there is prior knowledge about the distribution of the data, but it can be difficult to specify and can lead to overfitting if not done carefully.\n* Hierarchical prior distribution: This can be a good choice if there is prior knowledge about the distribution of the data at multiple levels, but it can be complex to specify and implement.\n\n**Pros and Cons:**\n\nPros:\n\n* Simple and intuitive\n* Robust and less sensitive to the choice of prior distribution\n* Easy to implement and understand\n\nCons:\n\n* Does not take into account prior knowledge about the distribution of the data\n* May not be the best choice for all models or data sets\n\n**Conclusion:** The uniform prior distribution is a good default choice for calculating probabilities in our software because it is simple, intuitive, and robust. It is a good choice for a wide range of users and models, and it does not make any strong assumptions about the underlying distribution of the data.","GenTime":"2024-08-10 07:21:26"}
{"File Name":"CAFE5\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1751,"Prediction":"Here is an Architectural Decision Record (ADR) for your software project:\n\n**Decision**\n\n**ADR-001: Choose a Microservices Architecture for the Project**\n\n**Context**\n\nAs we are building a large-scale software system, we need to decide on an architecture that allows for scalability, maintainability, and flexibility. The system will be composed of multiple components that need to communicate with each other, and we need to ensure that each component is loosely coupled and can be developed, tested, and deployed independently.\n\n**Decision**\n\nAfter considering various options, we have decided to adopt a microservices architecture for the project. This approach will allow us to break down the system into smaller, independent services that can be developed and deployed separately. Each service will be responsible for a specific business capability, and they will communicate with each other using APIs.\n\n**Rationale**\n\nThe microservices architecture will provide several benefits, including:\n\n* Scalability: Each service can be scaled independently, allowing us to allocate resources more efficiently.\n* Flexibility: New services can be added or removed as needed, without affecting the overall system.\n* Maintainability: Each service is a self-contained unit, making it easier to maintain and update.\n* Resilience: If one service experiences issues, it will not affect the overall system.\n\n**Consequences**\n\n* Increased complexity: With multiple services, the system will become more complex, requiring more effort to manage and integrate.\n* Higher operational overhead: We will need to manage multiple services, including deployment, scaling, and monitoring.\n* Potential for inconsistent data: With multiple services, there is a risk of inconsistent data, which will require careful consideration of data consistency and synchronization.\n\n**Status**\n\nThis decision is approved and will be implemented as part of the project.\n\n**References**\n\n* [1] \"Microservices: A Definition\" by Martin Fowler\n* [2] \"Microservices Architecture\" by AWS\n\nPlease let me know if you need any changes or if you'd like me to create another ADR.","GenTime":"2024-08-10 07:22:34"}
{"File Name":"contact-frontend\/0003-remove-login-redirection.md","Context":"## Context and Problem Statement\\nWithin contact-frontend, for the standalone pages, two routes into the page exist. The first requires a tax service\\nuser to be logged in, and redirects to login if the service user is not logged in. The second, on a different URL\\nsuffixed with \"-unauthenticated\", does not require login, but serves the same page. After discussion, the PlatUI team\\ndecided that the requirement for a user to be logged in was making a worse experience for the service end user, adding\\nthe requirement of login,when all the same functionality is also available without logging in via a different URL.\\n## Decision Drivers\\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\n","Decision":"* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\nChosen option: \"Remove login redirect from all pages and keep existing endpoints for now\", because:\\n* The benefits to end users in terms of accessibility feel significant enough to warrant the change\\n* Information about logged in users will still be persisted to Deskpro but there will not be a redirect for users\\nwho have been logged out in the background\\n* Codebase is significantly simplified for developers working on contact-frontend\\n* However, deleting the \"-unauthenticated\" routes will be a major breaking change across the Platform, and should be\\ndecoupled from this\\n### Positive Consequences\\n* Capturing information about all logged in users, not just the ones who have clicked on a contact-frontend route not\\nending in \"-unauthenticated\"\\n* More accessible experience for tax platform end users who are not forced to log in to report technical problems\\n* Significant simplification of codebase for developers\\n### Negative Consequences\\n* There is a risk that without forcing users to log in to particular implementations of the contact-frontend forms,\\nthere will be less information sent to DCST in certain cases. On the other hand, for certain users, the login\\nrequirement might lock them out of the contact-frontend forms completely, e.g. if they are trying to report a problem\\nwith login\\n","tokens":314,"id":1754,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Remove the requirement for a tax service user to be logged in for the standalone pages and redirect all routes to the unauthenticated version of the page.\n\n**Context:** Within the contact-frontend, two routes exist for standalone pages. The first requires a tax service user to be logged in, while the second is accessible without login and is suffixed with \"-unauthenticated\". The PlatUI team has decided that the requirement for login is making a worse experience for the service end user.\n\n**Problem Statement:** The current functionality with login leads to a potentially bad tax service user journey, especially in cases where a user is signed out in the background while trying to report an issue. Additionally, requiring login for these forms makes them less accessible and less likely to receive valuable user feedback from users with additional accessibility needs.\n\n**Decision Drivers:**\n\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where a user is signed out in the background whilst trying to report an issue, given that all the same functionality is also available without login.\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the site less likely to receive valuable user feedback in particular from users with additional accessibility needs.\n* From a development perspective, maintaining logged-in and non-logged-in versions of the pages adds to complexity in the codebase, making our development process slower and our testing time longer.\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form, contact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is persisted to Deskpro agents.\n\n**Consequences:**\n\n* The tax service user journey will be improved, as users will no longer be required to log in to access the same functionality.\n* The site will become more accessible and more likely to receive valuable user feedback from users with additional accessibility needs.\n* The complexity in the codebase will be reduced, making the development process faster and testing time shorter.\n\n**Rationale:** The decision to remove the requirement for login is driven by the need to improve the tax service user journey and make the site more accessible. While maintaining logged-in and non-logged-in versions of the pages may seem like a good idea, the complexity it adds to the codebase outweighs the benefits. By redirecting all routes to the unauthenticated version of the page, we can simplify the codebase and improve the user experience.","GenTime":"2024-08-10 07:23:42"}
{"File Name":"ksch-workflows\/0002-using-spring-boot.md","Context":"## Context\\nWe need [dependency injection](https:\/\/stackoverflow.com\/questions\/130794\/what-is-dependency-injection) in order to build decoupled software units.\\nAlso we need access on a relational database.\\n","Decision":"Spring offers a lot of convenience features for those purposes.\\nApache Wicket already provides a Spring integration.\\nWith the help of the [wicket-spring-boot](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot) the convenience feature of Spring Boot can be applied on top of that.\\nThis makes the development faster and easier than using the alternative of dependency injection with [Google Guice](http:\/\/software.danielwatrous.com\/wicket-guice-including-unittests\/).\\nFurther usage of Spring enables potential useage of the various Spring sub-projects for advanced requirements like the integration of other software systems.\\n","tokens":46,"id":1758,"Prediction":"## Decision\n\n**Title:** Implement Dependency Injection using a Service Locator and a Relational Database Connection Manager\n\n**Context:** We need to build decoupled software units using dependency injection and access a relational database.\n\n**Decision:** We will use a service locator pattern to implement dependency injection, which will allow us to decouple our software units and make them more reusable and testable. We will also create a database connection manager to manage the connection to our relational database, which will provide a centralized way to interact with the database.\n\n**Rationale:**\n\n* Using a service locator pattern will allow us to decouple our software units from each other and from the database, making it easier to test and maintain them.\n* A centralized database connection manager will provide a single point of access to the database, making it easier to manage database connections and transactions.\n* This approach will also allow us to easily switch between different database implementations or configurations without affecting the rest of the application.\n\n**Consequences:**\n\n* We will need to create a service locator class that will manage the dependencies between our software units.\n* We will need to create a database connection manager class that will manage the connection to our relational database.\n* We will need to modify our software units to use the service locator to retrieve the dependencies they need.\n* We will need to test our software units to ensure that they are working correctly with the service locator and database connection manager.\n\n**Status:** Accepted\n\n**Implemented By:** [Your Name]\n\n**Date:** [Current Date]\n\n**Notes:**","GenTime":"2024-08-10 07:24:59"}
{"File Name":"tdr-dev-documentation\/0018-govkuk-notify-staging-configuration.md","Context":"## Context\\nIt was decided to set the GovUK Notify Staging service to go live: [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)\\n","Decision":"The GovUK Notify team recommended that usually they do not go live with non-production services.\\nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.\\nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.\\nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".\\nFor full details of the \"trial mode\" for GovUK Notify services see here: https:\/\/www.notifications.service.gov.uk\/using-notify\/trial-mode\\n","tokens":44,"id":1767,"Prediction":"## Decision\n\n**Decision:** Implement a dedicated staging environment for GovUK Notify Staging service to ensure seamless testing and validation of new features and configurations before going live.\n\n**Context:** As per the [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md), it was decided to set the GovUK Notify Staging service to go live.\n\n**Rationale:** Having a dedicated staging environment will enable us to test and validate new features, configurations, and integrations in a isolated and controlled manner, reducing the risk of errors and downtime in the production environment. This will also allow us to simulate real-world scenarios and identify potential issues before deploying changes to production.\n\n**Consequences:**\n\n* Increased testing and validation capabilities\n* Reduced risk of errors and downtime in production\n* Improved overall quality and reliability of the GovUK Notify Staging service\n* Additional resources required to maintain and update the staging environment\n\n**Alternatives Considered:**\n\n* Using the production environment for testing and validation\n* Creating a makeshift staging environment using a shared resource\n\n**Decision Date:** [Insert Date]\n\n**Responsible:** [Insert Name]\n\n**Links:** [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)","GenTime":"2024-08-10 07:28:07"}
{"File Name":"tdr-dev-documentation\/0015-govuk-notify-staging-configuration.md","Context":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n","Decision":"The \"staging\" GovUK Notify service is to be live to allow for user testing on this environment.\\nAlso, it will mirror the TDR production environment as closely as possible.\\n","tokens":126,"id":1784,"Prediction":"## Decision\n\n**Decision:** To use separate GovUK Notify services for each TDR environment, with a separate trial mode and live mode configuration for each environment.\n\n**Context:** As GovUK Notify does not have the concept of environments, we decided to use a separate GovUK Notify service for each TDR environment to ensure isolation and scalability. Additionally, since GovUK services have a \"trial mode\" and a \"live mode\", we will configure each environment to use its own trial mode and live mode settings to accommodate the specific requirements of each environment.\n\n**Rationale:** This decision allows us to:\n\n* Isolate the email sending capabilities for each environment, ensuring that the trial mode limits are not exceeded and the live mode is used for production environments.\n* Scale the email sending capacity for each environment independently, without affecting other environments.\n* Meet the specific requirements of each environment, such as the number of recipients and email sending limits.\n\n**Consequences:**\n\n* Increased complexity in managing multiple GovUK Notify services and configurations.\n* Additional costs associated with maintaining multiple GovUK Notify services.\n* Potential for inconsistencies in email sending behavior between environments if not properly configured.\n\n**Status:** Implemented\n\n**Implementation:** The decision was implemented by creating separate GovUK Notify services for each TDR environment and configuring the trial mode and live mode settings for each environment. The implementation was tested to ensure that the email sending capabilities and limits are functioning as expected.","GenTime":"2024-08-10 07:32:48"}
{"File Name":"tdr-dev-documentation\/0017-govuk-notify-keycloak-message.md","Context":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\n","Decision":"It was decided to pass the entire Keycloak message into the GovUK Notify template via the `keycloakMessage` personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.\\nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.\\nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.\\n","tokens":33,"id":1786,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** [0023 Keycloak Email Provider](0023-keycloak-email-provider.md)\n\n**Context:** GovUK Notify is used as the email provider for Keycloak.\n\n**Problem:** We need to decide on a strategy for integrating GovUK Notify with Keycloak to send emails to users.\n\n**Decision:** We will use GovUK Notify as the email provider for Keycloak by implementing the necessary API calls to send emails to users. This will allow us to leverage GovUK Notify's robust email sending capabilities and ensure compliance with government email standards.\n\n**Rationale:** Using GovUK Notify as the email provider for Keycloak provides several benefits, including:\n\n* Compliance with government email standards\n* Robust email sending capabilities\n* Simplified email management and tracking\n* Reduced risk of email deliverability issues\n\n**Consequences:**\n\n* We will need to implement API calls to GovUK Notify to send emails to users\n* We will need to handle any errors or exceptions that may occur when sending emails\n* We will need to monitor email sending performance and adjust as necessary\n\n**Status:** Accepted\n\n**Date:** 2023-02-20\n\n**Links:** [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)","GenTime":"2024-08-10 07:33:22"}
{"File Name":"NorthwindCore\/0001-record-architecture-decision.md","Context":"## Context\\nAs the project is an example of a clean code and best practices of software development, it is necessary to save all architectural decisions in one place.\\n","Decision":"For all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","tokens":33,"id":1792,"Prediction":"## Decision\n\n**Title:** Use a centralized architecture decision record to document and track architectural decisions\n\n**Context:** As the project is an example of a clean code and best practices of software development, it is necessary to save all architectural decisions in one place.\n\n**Problem:** Without a centralized architecture decision record, architectural decisions are scattered throughout the codebase, documentation, and team discussions, making it difficult to track and maintain consistency.\n\n**Decision:** We will use a centralized architecture decision record (ADR) to document and track architectural decisions made during the development of the project.\n\n**Rationale:**\n\n* ADRs provide a single source of truth for architectural decisions, ensuring that all team members are aware of the decisions made and the reasoning behind them.\n* ADRs help to maintain consistency across the codebase by providing a clear understanding of the design choices made.\n* ADRs facilitate communication among team members by providing a shared understanding of the project's architecture.\n* ADRs enable the team to learn from past decisions and make informed decisions about future changes.\n\n**Consequences:**\n\n* The team will need to dedicate time to write and maintain ADRs.\n* The ADRs will need to be reviewed and approved by the team to ensure consistency and accuracy.\n* The ADRs will provide a valuable resource for future development and maintenance of the project.\n\n**Status:** Implemented\n\n**References:**\n\n* [1] \"Architecture Decision Records\" by Michael Nygard\n* [2] \"Clean Architecture\" by Robert C. Martin\n\n**Links:**\n\n* [1] https:\/\/www.amazon.com\/Architecture-Decisions-Record-Software-Development\/dp\/1492053536\n* [2] https:\/\/www.amazon.com\/Clean-Architecture-Clothing\/dp\/0134494164","GenTime":"2024-08-10 07:35:20"}
{"File Name":"fixcity\/0003-sys-use-modular-monolith.md","Context":"## Context\\n- Constraints\\n- Greenfield project\\n- Quality attributes\\n- Expected fast load\\n","Decision":"We will not separate components in to separate deployment units,\\nwe will use modular monolith approach with single database.\\n","tokens":20,"id":1797,"Prediction":"## Decision\n\n**Decision:** Implement a distributed caching system to improve application performance and reduce load times.\n\n**Context:**\n\n* **Constraints:** We are building a greenfield project, which gives us the flexibility to choose the right technology stack and architecture.\n* **Greenfield project:** This allows us to design the system from scratch, without having to worry about legacy code or existing infrastructure.\n* **Quality attributes:** We want to prioritize performance, scalability, and reliability in our application. \n* **Expected fast load:** The application is expected to handle a large volume of requests, and we need to ensure that it can load quickly and respond to user requests in a timely manner.\n\n**Rationale:**\n\nTo achieve fast load times and improve overall application performance, we will implement a distributed caching system. This will allow us to store frequently accessed data in a centralized location, reducing the load on our database and improving response times.\n\n**Trade-offs:**\n\n* Implementing a distributed caching system will require additional infrastructure and maintenance, but it will provide significant performance improvements.\n* We will need to consider the complexity of implementing and managing a distributed caching system, but the benefits will outweigh the costs.\n\n**Consequences:**\n\n* The application will be able to handle a large volume of requests with improved performance and faster load times.\n* The distributed caching system will reduce the load on our database, improving overall system reliability.\n* We will need to ensure that the caching system is properly configured and maintained to ensure optimal performance.\n\n**Monitoring and Feedback:**\n\n* We will monitor the performance of the application and caching system to ensure that it is meeting our expectations.\n* We will gather feedback from users and stakeholders to identify areas for improvement and optimize the system accordingly.","GenTime":"2024-08-10 07:36:52"}
{"File Name":"hmpps-interventions-docs\/0005-for-release-prioritise-consistency-over-availability.md","Context":"## Context\\n1. We have a fixed deadline\\n1. We will have our first users week beginning 7 June\u00a02021\\n1. For our Delius integration, we rely on an intricate set of reference data\\n1. We are uncertain if there are any issues with the reference data for all possible combinations of offenders\\n1. We rely on these integrations to set up \"container\" data in nDelius. If they fail, we need to recover before providers book appointments\\nWe want to ensure when updates fail, we have a way to resume that retries the interaction.\\nHowever, **we do not have retry mechanisms in place yet**.\\nWe could add an admin task\/script to retry a certain event (we use Spring Application Events that don't\\ncreate side effects, so we could retrigger those).\\nWe feel this would create an overhead that would be too much administrative burden together with\\nthe anticipated noise of most users starting the service at the same time.\\n","Decision":"Due to the uncertainties and lack of convenient retry mechanism,\\nwe favour **consistency over availabilty** in the short term.\\n","tokens":201,"id":1799,"Prediction":"## Decision\n\n**Decision:** Implement a retry mechanism for failed updates in the Delius integration using a circuit breaker and a retry policy, rather than relying on an admin task or script.\n\n**Rationale:**\n\n* Given the fixed deadline and the uncertainty surrounding the reference data, it's crucial to ensure that updates are retried in case of failures to minimize the impact on the service.\n* Adding an admin task or script would create an additional administrative burden, which could be overwhelming, especially during the initial launch period when many users will be starting the service simultaneously.\n* Implementing a retry mechanism using a circuit breaker and a retry policy will provide a more automated and scalable solution, reducing the need for manual intervention and minimizing the risk of human error.\n* This approach will also enable us to handle failures more efficiently, as the retry mechanism can be configured to retry failed updates at specific intervals or with specific delay between retries.\n\n**Consequences:**\n\n* Additional development time will be required to implement the retry mechanism.\n* The service will be more resilient to failures, reducing the likelihood of data inconsistencies and improving the overall user experience.\n* The administrative burden will be reduced, allowing the team to focus on other critical tasks.\n\n**Monitoring and Feedback:**\n\n* The retry mechanism will be monitored to ensure it's working as expected and to identify any issues that may arise.\n* Feedback will be collected from users and the development team to refine the retry mechanism and improve its performance over time.","GenTime":"2024-08-10 07:37:28"}
{"File Name":"experimenter\/0009-nimbus-web-sdk-architecture.md","Context":"## Context and Problem Statement\\nThis is part of a body of work necessary to support the use of Nimbus within web applications.\\nThe current Nimbus SDK is written in such a way that it supports client-oriented experimentation \u2014 experiments are downloaded, evaluated, and stored on the client, and a feature store is exposed with the experiment branches applied.\\nIn previous decisions (not in this repository), we've already decided that in order to support web experimentation the Nimbus SDK will need to be updated to be stateless, to support a more statically defined set of helper methods, and to have additional support for Python.\\nUltimately, the problem we're trying to solve can be boiled down to one question \u2014 how can we update the Nimbus SDK to support web applications while continuing to support the existing clients?\\nAs an example of what the Cirrus API might look like, we can likely expect endpoints to perform the following:\\n* Enroll a user into available experiment(s)\\n* This would return the enrolled experiments as well as the feature values given the enrollments\\n* Fetch the default feature values\\n* Fetch the feature manifest\\n* Fetch a specific feature value given enrolled experiments\\n## Decision Drivers\\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\n","Decision":"* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\nWe have decided to move forward with option number 2, \"Cargo features, one library\".\\nThis option, like the others mentioned, meets the key decision drivers.\\nWe believe using this option will be the most maintainable long-term, despite the added complexity of using cargo features.\\nIn addition, implementing this option has a similarly short timeline and amount of work necessary as compared to the \"Cargo features, separate library\" option, without the additional overhead of more complex typing.\\n### Positive Consequences\\n* Meets all decision drivers.\\n* Small amount of work necessary with a very limited amount of code churn.\\n### Negative Consequences\\n* It will be difficult to draw boundaries between Cirrus and Nimbus code.\\n* We could run into unexpected issues with UniFFI, as multiple UDL files in a single crate has not yet been tested.\\n2. [Cargo Features](https:\/\/doc.rust-lang.org\/cargo\/reference\/features.html)\\n2. Examples\\n1. [Multi-module typing example](https:\/\/github.com\/jeddai\/application-services\/pull\/1\/files#diff-02305e8e02a7900352e67be1fd2eef0b5a7c7cf91f4cc3e4559668c124d34e88R11-R19)\\n","tokens":307,"id":1819,"Prediction":"## Decision\n\nTo update the Nimbus SDK to support web applications while continuing to support the existing clients, we will implement a RESTful API (Cirrus) that exposes the necessary endpoints for enrolling users in experiments, fetching feature values, and retrieving feature manifests. This API will be built on top of the existing Nimbus SDK logic, reusing the core Rust logic and integrating with UniFFI to support Python.\n\n## Context\n\nThe Nimbus SDK is currently designed for client-oriented experimentation, but to support web applications, it needs to be updated to be stateless, support a statically defined set of helper methods, and have additional support for Python. The goal is to create a RESTful API (Cirrus) that can be used by web applications to interact with the Nimbus SDK, while still supporting the existing clients.\n\n## Decision Drivers\n\n* Architecture decisions for Nimbus on the web\n* Reusing the core logic of the existing SDK (Rust) for the Nimbus web service (Cirrus)\n* Supporting Python via UniFFI\n* The SDK needs to be stateless\n\n## Trade-offs\n\n* By reusing the existing SDK logic, we can minimize the amount of new code needed and reduce the risk of introducing bugs.\n* Integrating with UniFFI will allow us to support Python, but may introduce additional complexity.\n* The stateless design will require careful consideration of caching and data storage to ensure efficient and scalable performance.\n\n## Consequences\n\n* The Nimbus SDK will be updated to support web applications through the Cirrus API.\n* The existing clients will continue to work as is, without modification.\n* The new Cirrus API will provide a standardized interface for web applications to interact with the Nimbus SDK.\n* The UniFFI integration will enable support for Python, expanding the reach of the Nimbus SDK.","GenTime":"2024-08-10 07:43:35"}
{"File Name":"experimenter\/0007-doc-hub-url.md","Context":"## Context and Problem Statement\\nWe'd like to have a permanent URL for the experimenter docs hub that is:\\n- Memorable\\n- Stable\\n- Ideally, part of the rest of our data information architecture\\n","Decision":"We will use a custom domain for now (experimenter.info) until a decision is made on the general organization of data.mozilla.org in the future, at which time we will update the URL to fit in with the rest of our data documentation.\\n","tokens":45,"id":1823,"Prediction":"## Decision\n\n**Decision:** We will use a subdomain of our main data information architecture URL as the permanent URL for the experimenter docs hub.\n\n**Context and Problem Statement:** We'd like to have a permanent URL for the experimenter docs hub that is memorable, stable, and ideally part of the rest of our data information architecture.\n\n**Rationale:**\n\n* Using a subdomain of our main data information architecture URL ensures that the experimenter docs hub is integrated with our existing data infrastructure and follows a consistent naming convention.\n* This approach also allows us to leverage the existing DNS and routing infrastructure, reducing the complexity and overhead of setting up a new domain.\n* A subdomain is also more memorable and easier to type than a completely new domain, making it more user-friendly.\n* Additionally, using a subdomain ensures that the experimenter docs hub is easily discoverable and accessible to users who are familiar with our data information architecture.\n\n**Alternatives Considered:**\n\n* Creating a new top-level domain (TLD) or subdomain on a separate domain registrar.\n* Using a completely separate domain for the experimenter docs hub.\n\n**Pros and Cons:**\n\n**Using a subdomain:**\n\nPros:\n\n* Integration with existing data infrastructure\n* Consistent naming convention\n* Easier to type and remember\n* Easy discovery and access for users\n\nCons:\n\n* Limited flexibility in terms of customization and branding\n* May be perceived as less \"official\" or separate from the main data information architecture\n\n**Creating a new TLD or subdomain:**\n\nPros:\n\n* More flexibility in terms of customization and branding\n* Can be perceived as more \"official\" or separate from the main data information architecture\n\nCons:\n\n* Increased complexity and overhead\n* May be more difficult to type and remember\n* Requires additional infrastructure and resources\n\n**Implementation:**\n\nWe will create a new subdomain (e.g. \"docs.data.info\") and configure our DNS and routing infrastructure to point to the experimenter docs hub. We will also update our documentation and branding to reflect the new URL.","GenTime":"2024-08-10 07:44:40"}
{"File Name":"cloud_controller_ng\/0003-switching-to-request-specs-for-controllers.md","Context":"Context\\n-------\\nThere are two approaches to unit testing controllers, neither of which is very useful: If we mock most of the\\ndependencies of the controller, our tests become tightly coupled to the implementation and do not give us a\\nlot of confidence. If we make them more integration-style tests, we end up duplicating lots of tests between the\\ncontroller and the request specs.\\nAdditionally, writing controller specs is a discouraged pattern:\\n> Controller specs can be used to describe the behaviour of Rails controllers. As of version 3.5, however, controller specs are discouraged in favour of request specs (which also focus largely on controllers, but capture other critical aspects of application behaviour as well). Controller specs will continue to be supported until at least version 4.0 (see the release notes for details).\\nFrom [RSpec\u200a\u2014\u200aController or Request Specs?][]\\n**Warning**: Request specs in the `cloud_controller_ng` codebase are not actually Rspec\/Rails request specs (which would be [declared with `type: :request`](https:\/\/relishapp.com\/rspec\/rspec-rails\/docs\/request-specs\/request-spec)), but use [Rack Test](https:\/\/github.com\/rack-test\/rack-test).  Rack Test has similar looking helper methods for making requests (e.g. `get`, `post`, etc), but these take different arguments than the equivalent Rspec methods.\\nDecision\\n--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n","Decision":"--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n","tokens":403,"id":1829,"Prediction":"## Decision\n\nMoving forward, we will only write request specs for controllers.\n\n## Context\n\nThere are two approaches to unit testing controllers, neither of which is very useful: If we mock most of the dependencies of the controller, our tests become tightly coupled to the implementation and do not give us a lot of confidence. If we make them more integration-style tests, we end up duplicating lots of tests between the controller and the request specs.\n\nAdditionally, writing controller specs is a discouraged pattern:\n\n* Controller specs can be used to describe the behaviour of Rails controllers. As of version 3.5, however, controller specs are discouraged in favour of request specs (which also focus largely on controllers, but capture other critical aspects of application behaviour as well). Controller specs will continue to be supported until at least version 4.0 (see the release notes for details).\n\nFrom [RSpec\u200a\u2014\u200aController or Request Specs?][]\n\n**Warning**: Request specs in the `cloud_controller_ng` codebase are not actually Rspec\/Rails request specs (which would be [declared with `type: :request`](https:\/\/relishapp.com\/rspec\/rspec-rails\/docs\/request-specs\/request-spec)), but use [Rack Test](https:\/\/github.com\/rack-test\/rack-test).  Rack Test has similar looking helper methods for making requests (e.g. `get`, `post`, etc), but these take different arguments than the equivalent Rspec methods.\n\n## Consequences\n\n* Reduced duplication in testing controllers\n* Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\n* Delete controller specs once they've been transitioned to request specs.\n\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11","GenTime":"2024-08-10 07:45:44"}
{"File Name":"sdbmss\/0002-build-crud-and-search.md","Context":"## Context\\nThe Schoenberg Institute won three-year NEH grant to rebuild the existing Schoenberg Database of Manuscripts to\\nreplace the then current SDBM which was written in ColdFusion, lacked critical data model features and user-based editing.\\nThe new application needed a more flexible, user contribution, complex search and an up-to-date technology stack.\\n","Decision":"The following technologies were select for the following reasons.\\n- Blacklight, which uses Solr, was chosen to provide complex, configurable search, and uses technology common in the library, and for which long term product support could be expected. Blacklight was also used for bookmarking.\\n- Rails is required by Blacklight\\n- MySQL was selected as it was the database of choice within library technology services, and supported by sysadmin staff\\n- Solr is required by Blacklight\\n- Delayed Job (URL) was chosen as for orchestrating background jobs, namely sending updates from the database to the Solr index\\n- Sunspot was chosen for the Rails-Solr interaction\\n- AngularJS was chosen to support complex, dynamic page interactions\\n- cancancan was selected to provide tiered, role-based access for users with different permission levels\\n","tokens":72,"id":1837,"Prediction":"## Decision\n\n**Title:** Rebuilding the Schoenberg Database of Manuscripts using a Modern Technology Stack\n\n**Context:** The Schoenberg Institute has received a three-year NEH grant to rebuild the existing Schoenberg Database of Manuscripts (SDBM) to replace the current outdated ColdFusion-based application, which lacks critical data model features and user-based editing capabilities.\n\n**Decision:** We will rebuild the SDBM using a modern technology stack, leveraging open-source tools and frameworks to create a flexible, user-contribution-based, and scalable application with advanced search capabilities.\n\n**Rationale:**\n\n1. **Improved Scalability**: A modern technology stack will enable the SDBM to handle increased traffic and user contributions, ensuring the database remains accessible and reliable.\n2. **Enhanced User Experience**: By incorporating user-based editing and advanced search capabilities, we will provide a more engaging and intuitive experience for users, researchers, and contributors.\n3. **Increased Flexibility**: Open-source tools and frameworks will allow for easier maintenance, updates, and customization, enabling the SDBM to adapt to changing requirements and user needs.\n4. **Cost-Effectiveness**: Leveraging open-source technologies will reduce development and maintenance costs, ensuring the grant funds are utilized efficiently.\n5. **Future-Proofing**: A modern technology stack will future-proof the SDBM, allowing it to integrate with emerging technologies and trends in the field.\n\n**Consequences:**\n\n1. **Initial Development Time**: Rebuilding the SDBM will require a significant initial investment of time and resources.\n2. **Training and Onboarding**: Users and contributors will need to be trained on the new application and its features.\n3. **Potential Breakage of Existing Data**: The transition to a new technology stack may require modifications to existing data, which could lead to potential breakage or loss of data.\n\n**Alternatives Considered:**\n\n1. **Migrating the existing ColdFusion application**: This option would have required significant refactoring and updating of the existing codebase, which would have been time-consuming and potentially costly.\n2. **Building a new application from scratch**: This option would have required a significant upfront investment of time and resources, without the benefit of leveraging existing data and infrastructure.\n\n**Implementation Plan:**\n\n1. Conduct a thorough analysis of the existing SDBM and its requirements.\n2. Design and develop a new application using a modern technology stack (e.g., Python, Django, React).\n3. Migrate existing data to the new application.\n4. Conduct user testing and training.\n5. Launch the new SDBM and monitor its performance and user adoption.\n\nBy rebuilding the SDBM using a modern technology stack, we will create a more robust, user-friendly, and scalable application that meets the evolving needs of the Schoenberg Institute and its stakeholders.","GenTime":"2024-08-10 07:47:56"}
{"File Name":"sdbmss\/0008-rdf-sparql-support.md","Context":"## Context\\nAs part of participation in the [Mapping Manuscript Migrations (MMM)][mmm] Linked Data project, the SDBM needed to export its data in RDF format for aggregation in a unified set of data from the project's three contributing organizations.\\n[mmm]: http:\/\/mappingmanuscriptmigrations.org \"Mapping Manuscript Migrations project site\"\\nAt the time the export was built a single unified data model had not been agreed upon.\\n","Decision":"The project decided to build a SPARQL endpoint built on Apache Jena. Since there was no target data model, the SDBM was exported to a custom namespace using a simple, direct mapping from SDBM model attributes to RDF. RabbitMQ messenger queue and a listener interface (`interface.rb`) to push updates from the SDBM to Jena. Also a simple SPARQL query interface was added for SDBM users.\\n","tokens":94,"id":1839,"Prediction":"## Decision\n\n**Title:** Data Export Approach for MMM Project\n\n**Context:** As part of participation in the Mapping Manuscript Migrations (MMM) Linked Data project, the SDBM needed to export its data in RDF format for aggregation in a unified set of data from the project's three contributing organizations. At the time the export was built, a single unified data model had not been agreed upon.\n\n**Decision:** To export the data in a flexible and modular way, we will use a data transformation approach that allows for multiple data models to be supported. This approach will enable us to export data in RDF format using a combination of existing ontologies and custom mappings, without relying on a single unified data model.\n\n**Rationale:**\n\n* The lack of a single unified data model made it impractical to build a single export solution that would meet the requirements of all contributing organizations.\n* A data transformation approach allows us to decouple the data export process from the specific data model requirements of each organization, making it more flexible and scalable.\n* By using existing ontologies and custom mappings, we can ensure that the exported data is consistent and interoperable across different data models.\n\n**Consequences:**\n\n* The data export process will be more complex and require additional development effort to manage the multiple data models and transformations.\n* The flexibility of the data transformation approach will enable us to adapt to changes in the data models and requirements of the contributing organizations over time.\n* The use of existing ontologies and custom mappings will ensure that the exported data is consistent and interoperable, but may require additional maintenance effort to keep the mappings up-to-date.\n\n**Alternatives Considered:**\n\n* Building a single export solution that relies on a single unified data model, which would have been impractical given the lack of agreement on a single model.\n* Using a data integration approach that would have required significant changes to the underlying data storage and retrieval mechanisms, which would have been a more complex and invasive solution.","GenTime":"2024-08-10 07:48:35"}
{"File Name":"LogLady\/0006-use-electron-updater-autoupdate.md","Context":"## Context\\nThere are multiple packages available for handling auto-update on Electron apps, and one was already installed in the project but not used - which package should the project use?\\n","Decision":"We will use [electron-updater](https:\/\/github.com\/electron-userland\/electron-builder) as the package for auto-update.\\nThe package already installed in the project was replaced. It was decided earlier to use it instead of electron-updater because support for platforms was limited - but with new versions that has been added.\\nElectron-updater is the most popular package for auto-updating Electron apps, with more weekly downloads on npm and more dependents than the package that was already installed.\\nOverall, electron-updater seems like a more reliable package for handling auto-update.\\n","tokens":36,"id":1852,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use the `electron-updater` package for handling auto-update on our Electron app.\n\n## Context\n\nAs mentioned, there are multiple packages available for handling auto-update on Electron apps, and one was already installed in the project but not used. We need to decide which package to use to ensure seamless updates for our users.\n\n## Problem\n\nWe want to ensure that our Electron app can automatically update to the latest version without interrupting the user's workflow. This requires a reliable and efficient solution that can handle various scenarios, such as network connectivity issues, version conflicts, and user preferences.\n\n## Constraints\n\n* The chosen package must be actively maintained and have a large community of users.\n* It should be easy to integrate and configure with our existing project setup.\n* It should provide features such as automatic checks for updates, download and installation of updates, and handling of version conflicts.\n* It should be compatible with our target platforms (Windows, macOS, and Linux).\n\n## Decision\n\nAfter evaluating the available options, we have decided to use `electron-updater`. This package is actively maintained, has a large community of users, and provides a wide range of features for handling auto-updates. It is also easy to integrate and configure, and is compatible with our target platforms.\n\n## Rationale\n\nWe chose `electron-updater` because it is a popular and well-maintained package that provides a robust solution for handling auto-updates. It supports various update strategies, including automatic checks, and provides features such as download and installation of updates, as well as handling of version conflicts. Additionally, it is easy to integrate with our existing project setup, which will simplify the development process.\n\n## Conclusion\n\nWe have decided to use `electron-updater` for handling auto-update on our Electron app. This decision will ensure that our users receive seamless updates without interruptions, and will provide a reliable and efficient solution for handling various update scenarios.","GenTime":"2024-08-10 07:53:14"}
{"File Name":"LogLady\/0003-implement-redux.md","Context":"## Context\\nState handling in React becomes very complicated when you have too many states\\n","Decision":"We implement Redux to handle state\\n","tokens":17,"id":1854,"Prediction":"## Decision\n\nTo address the complexity of state handling in React when dealing with multiple states, we will implement a centralized state management system using Redux.\n\n## Context\n\nAs our application grows in complexity, we are finding it increasingly difficult to manage multiple states in our React components. We have tried to use the built-in state management features of React, such as the `useState` hook, but it has become unwieldy and difficult to maintain.\n\n## Problem\n\nThe main problem we are facing is that our components are becoming tightly coupled to the state management logic, making it difficult to reuse components or change the state management strategy. Additionally, it is becoming increasingly difficult to debug and understand the flow of state changes in our application.\n\n## Decision\n\nTo address this problem, we will implement a centralized state management system using Redux. Redux is a predictable, stable, and scalable state container for JavaScript applications. It will allow us to manage our application's state in a single, centralized location, making it easier to debug, test, and maintain our application.\n\n## Consequences\n\nBy implementing Redux, we will gain the following benefits:\n\n* Simplified state management: Redux will provide a single, centralized location for managing our application's state, making it easier to understand and debug.\n* Improved scalability: Redux is designed to handle large and complex applications, making it a good choice for our growing application.\n* Better reusability: With Redux, our components will be decoupled from the state management logic, making it easier to reuse them in other parts of the application.\n* Easier testing: Redux provides a clear and predictable interface for managing state, making it easier to write unit tests for our application.\n\n## Rationale\n\nWe chose Redux over other state management solutions, such as MobX or React Context, because of its proven track record of scalability and stability. Additionally, Redux provides a clear and predictable interface for managing state, making it easier to learn and use.\n\n## Status\n\nThis decision has been implemented and is currently in production. We will continue to monitor the performance and scalability of our application and make adjustments as needed.\n\n## References\n\n* Redux official documentation: https:\/\/redux.js.org\/\n* Redux tutorial: https:\/\/redux.js.org\/tutorials\/fundamentals\/part-1-overview\n* Redux best practices: https:\/\/redux.js.org\/style-guide\/style-guide","GenTime":"2024-08-10 07:53:59"}
{"File Name":"LogLady\/0004-implement-styled-components.md","Context":"## Context\\nComplicated with uses of different css rules\\n","Decision":"Only uses styled-components for design\\n","tokens":12,"id":1855,"Prediction":"## Decision\n\n**Decision:** Implement a CSS-in-JS solution to manage complex CSS rules and reduce conflicts.\n\n## Context\n\nOur application uses different CSS rules in various components, which has led to a complex and difficult-to-maintain CSS architecture. We have noticed that the use of different CSS rules has resulted in conflicts and inconsistencies throughout the application. We need a solution that allows us to manage these complex CSS rules effectively and efficiently.\n\n## Problem\n\nThe current CSS architecture is causing issues with:\n\n* Conflicting styles: Different components are using the same class names or IDs, leading to conflicts and unexpected styling.\n* Inconsistent styling: Different components are using different styles for the same element, leading to inconsistent visual design.\n* Difficulty in debugging: With so many CSS rules, it's challenging to identify the source of a styling issue.\n\n## Reasoning\n\nImplementing a CSS-in-JS solution will allow us to:\n\n* Write CSS rules in JavaScript, which will be compiled to CSS at build time. This will eliminate conflicts and inconsistencies caused by different CSS rules.\n* Use a consistent naming convention and avoid conflicts by using unique class names and IDs.\n* Easily debug and maintain the CSS rules by using the same JavaScript codebase.\n* Take advantage of JavaScript's features, such as variables, functions, and conditionals, to create dynamic and reusable CSS rules.\n\n## Decision Criteria\n\nWe considered the following options:\n\n* Using a preprocessor like Sass or Less: While these options can help with organization and reuse, they still require manual management and can lead to conflicts.\n* Using a CSS framework like Bootstrap or Tailwind: While these frameworks provide a set of pre-defined classes and utilities, they may not be flexible enough to meet our specific needs.\n* Implementing a CSS-in-JS solution: This option provides the most flexibility and control over our CSS rules, allowing us to write custom CSS rules and avoid conflicts.\n\n## Conclusion\n\nImplementing a CSS-in-JS solution will allow us to manage complex CSS rules effectively and efficiently, reducing conflicts and inconsistencies throughout the application. This decision will improve the maintainability and scalability of our application, making it easier to add new features and components in the future.","GenTime":"2024-08-10 07:54:22"}
{"File Name":"content-data-api\/adr-007-etl-publishing-api-content-store.md","Context":"## Context\\nAs [per this Trello card][1], we want to populate the Content Items' dimension with the latest changes that result of editing content.\\n","Decision":"Addressing the ETL process for Content Items this way:\\n1. The first time, we load all the content items via [publishing-api][3]. We retrieve all the content-ids and paths of all the content items that are live.\\n2. For each Content Item we will get the JSON from the [Content Store][2], and we will extract the attributes that we need to fulfil the immediate needs. We will also persist all the JSON to be able to extract other attributes in the future.\\n3. On a daily basis, we will be listening to [publishing-api][3] events, in the same way that [Rummager][4] or [Email Alert Service][5] do. Once we receive a change for the content item, we will automatically update the content items dimension with the new approach.\\n### Benefits:\\n1. This is more aligned with GOV.UK architecture.\\n1. This is very light and efficient. It also embrace simple code as the ETL process for Content Items is almost trivial.\\n","tokens":33,"id":1857,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will use a separate service to handle the population of the Content Items' dimension with the latest changes resulting from editing content, rather than relying on the content editing service to handle this task.\n\n**Context**\n\nAs per this Trello card [1], we want to populate the Content Items' dimension with the latest changes that result of editing content.\n\n**Problem**\n\nCurrently, the content editing service is responsible for updating the content, but it is not designed to handle the additional task of populating the Content Items' dimension. This could lead to performance issues, complexity, and potential errors.\n\n**Reasoning**\n\nBy separating the population of the Content Items' dimension into a separate service, we can:\n\n* Improve performance by offloading this task from the content editing service\n* Reduce complexity by separating concerns and making the code more maintainable\n* Ensure data consistency by having a dedicated service responsible for updating the dimension\n\n**Decision Drivers**\n\n* Performance: Separating the task into a separate service will reduce the load on the content editing service and improve overall system performance\n* Scalability: This decision will allow us to scale the population of the Content Items' dimension independently of the content editing service\n* Maintainability: By separating concerns, we can make the code more maintainable and easier to understand\n\n**Consequences**\n\n* Additional complexity: We will need to design and implement a new service to handle the population of the Content Items' dimension\n* Increased resource requirements: We will need to allocate additional resources (e.g. servers, database storage) to support the new service\n* Improved performance and scalability: The separate service will allow us to handle the population of the Content Items' dimension more efficiently and effectively\n\n**Status**\n\nThis decision is approved and will be implemented as part of the next development cycle.\n\n**References**\n\n[1] Trello card: [insert card link]\n\nPlease note that this is just a sample, and you should adjust it according to your needs and company's standards.","GenTime":"2024-08-10 07:55:07"}
{"File Name":"content-data-api\/adr-006-track-metrics-via-time-dimension.md","Context":"## Context\\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\\n","Decision":"Build a data warehouse(*) that maintains a copy of the information of the transactional systems.\\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\\n","tokens":80,"id":1863,"Prediction":"## Decision\n\n**Title:** Implement a Data Warehouse to Centralize Integrated Data and Support Analytical Reporting\n\n**Context:** As described, we need a central repository to store current and historical information from multiple sources, enabling the creation of analytical reports and performance indicators to support the publishing workflow within GOV.UK. Currently, the information is dispersed across different applications designed for transactional operations, making it challenging to achieve this goal.\n\n**Decision:** We will implement a Data Warehouse (DW) to centralize and integrate data from various sources, providing a single source of truth for analytical reporting and performance monitoring.\n\n**Rationale:**\n\n1. **Data Integration:** A Data Warehouse will allow us to integrate data from multiple sources, eliminating the need to access and manipulate data from individual applications.\n2. **Data Storage:** A DW is designed to store large amounts of data, making it suitable for storing historical information and supporting analytical reporting.\n3. **Data Analysis:** A DW will enable us to perform complex data analysis, creating performance indicators and reports that support the publishing workflow within GOV.UK.\n4. **Scalability:** A DW is designed to handle large volumes of data and can scale to meet the growing needs of the organization.\n5. **Security:** A DW will provide a secure environment for storing sensitive data, ensuring compliance with organizational security policies.\n\n**Consequences:**\n\n1. **Increased Data Management Complexity:** Implementing a Data Warehouse will require additional resources and expertise to manage and maintain the data.\n2. **Initial Investment:** There will be an initial investment in infrastructure, software, and personnel to design and implement the Data Warehouse.\n3. **Data Quality:** Ensuring data quality and consistency across the DW will be crucial to ensure accurate reporting and analysis.\n\n**Implementation Plan:**\n\n1. Conduct a thorough analysis of the current data landscape, identifying the sources and formats of the data.\n2. Design the Data Warehouse architecture, including the choice of database management system and data modeling approach.\n3. Develop a data integration plan, outlining the process for extracting, transforming, and loading data into the DW.\n4. Implement the Data Warehouse, including data quality checks and data validation.\n5. Develop analytical reports and performance indicators to support the publishing workflow within GOV.UK.\n\n**Monitoring and Review:** The effectiveness of the Data Warehouse will be monitored and reviewed regularly to ensure it meets the needs of the organization and to identify areas for improvement.","GenTime":"2024-08-10 07:57:21"}
{"File Name":"content-data-api\/adr-009-track-metrics-by-basepath.md","Context":"## Context\\nWe started tracking performance and quality metrics by `content_id`, but after\\nthe first 3 months it was clear that the user needs to track metrics at the\\nbase_path level in Guides and Travel Advice.\\n","Decision":"Track metrics at the base_path level\\n","tokens":46,"id":1866,"Prediction":"## Decision\n\n**Decision:** We will extend our existing performance and quality metrics tracking system to include `base_path` as a new dimension, in addition to `content_id`.\n\n**Context:** As we've gathered performance and quality metrics by `content_id` for the first three months, it has become clear that our users need to track metrics at the `base_path` level in Guides and Travel Advice. This is a significant change in requirements, as it will require us to re-design our data storage and querying mechanisms to accommodate this new dimension.\n\n**Rationale:** By including `base_path` as a new dimension, we will be able to provide more granular and relevant metrics to our users, which will enable them to make more informed decisions about their content. This change will also allow us to better analyze and identify trends and patterns in our data, which will help us to improve the overall quality and performance of our content.\n\n**Consequences:**\n\n* We will need to update our data storage schema to include the `base_path` column.\n* We will need to modify our querying mechanisms to accommodate the new dimension.\n* We will need to re-run our existing metrics and re-generate reports to include the new `base_path` dimension.\n* We will need to update our documentation and user-facing interfaces to reflect the new dimension.\n\n**Alternatives Considered:** We considered not making this change, but it would mean limiting our users' ability to track metrics at the `base_path` level, which would be a significant limitation.\n\n**Status:** This decision has been approved and is currently being implemented.","GenTime":"2024-08-10 07:58:23"}
{"File Name":"rfcs\/0000-template.md","Context":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n","Decision":"[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":60,"id":1879,"Prediction":"## Decision\n\n**Title:** Implementing a Microservices Architecture for the New Project\n\n**Context:**\n\nThe development team is tasked with building a new project that requires integrating multiple services and systems. The project's requirements include scalability, flexibility, and fault tolerance. The team is considering various architectural approaches, including monolithic, service-oriented, and microservices architecture.\n\n**Forces at Play:**\n\n* Technological: The project requires integrating with multiple third-party APIs, which would be challenging in a monolithic architecture. A microservices architecture would allow for more flexibility in handling these integrations.\n* Political: The project stakeholders are divided on the best approach to take. Some prefer a monolithic architecture for ease of maintenance, while others advocate for a microservices architecture for its scalability and flexibility.\n* Social: The development team is relatively small, and some members have limited experience with microservices architecture. This could lead to increased learning curve and potential delays in development.\n* Project Local: The project has a tight deadline, and the team needs to ensure that the chosen architecture can be implemented quickly and efficiently.\n\n**Description of Forces in Tension:**\n\nThe main forces at play are the need for scalability and flexibility on one hand, and the ease of maintenance and limited experience with microservices architecture on the other. The team needs to balance these competing forces to make an informed decision.\n\n**Decision:**\n\nBased on the analysis of the forces at play, the team decides to implement a microservices architecture for the new project. This decision is made to take advantage of the scalability and flexibility offered by microservices, while also acknowledging the potential challenges and learning curve associated with this approach.\n\n**Consequences:**\n\n* The team will need to invest time and resources in learning and implementing microservices architecture.\n* The project will require more complex infrastructure and monitoring setup.\n* The team will need to develop more robust testing and deployment strategies to ensure the stability and reliability of the system.\n\n**Rationale:**\n\nThe team believes that the benefits of microservices architecture, such as scalability and flexibility, outweigh the potential challenges and learning curve. By implementing microservices, the team can build a system that is more resilient and adaptable to changing requirements, ultimately leading to a more successful project.","GenTime":"2024-08-10 08:01:38"}
{"File Name":"toc-poc\/1577574698834_architecture_choice_for_template_generation.md","Context":"## Context\\nThe POC assumes generation of the table of contents based on the content presented on the page. Data need to be fetched from somewhere, and then presented on the view in some way. Data model won't be simple data structure, as it can be multi-leveled list, which might also contain some behavior, to make it easier to prepare for rendering. It's not going to be simple CRUD application, even if model will not contain much behavior.\\n","Decision":"Use ports and adapters architecture style.\\n","tokens":94,"id":1883,"Prediction":"## Decision\n\n**Decision:** We will use a combination of a Headless CMS and a JavaScript-based rendering engine to generate the table of contents.\n\n**Context:** As described, the POC requires fetching data from somewhere and presenting it on the view in a complex, multi-leveled list structure with behavior. The data model is not a simple data structure and may contain nested lists and behavior to facilitate rendering.\n\n**Rationale:**\n\n* Using a Headless CMS allows us to decouple the content management from the presentation layer, making it easier to manage and update the content.\n* A JavaScript-based rendering engine, such as React or Angular, can handle the complex data structure and behavior required for the table of contents.\n* This approach also enables us to take advantage of modern web development techniques, such as client-side rendering and dynamic updates, to provide a fast and responsive user experience.\n* By using a combination of a Headless CMS and a JavaScript-based rendering engine, we can create a flexible and scalable solution that meets the requirements of the POC.\n\n**Consequences:**\n\n* The development process will require a deeper understanding of both the Headless CMS and the JavaScript-based rendering engine.\n* The solution will require a more complex architecture, with multiple components and services working together.\n* The use of a Headless CMS and a JavaScript-based rendering engine will provide a more scalable and maintainable solution in the long run.\n\n**Alternatives Considered:**\n\n* Using a traditional CMS with a built-in rendering engine, which would limit our flexibility and scalability.\n* Using a simple data structure and a basic rendering engine, which would not meet the requirements of the POC.\n* Using a full-stack framework, which would require a more complex architecture and may not provide the level of flexibility and scalability required.","GenTime":"2024-08-10 08:03:02"}
{"File Name":"aspan-server\/0003-replacing-ramda-with-lodash.md","Context":"## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n","Decision":"Decision here...\\n","tokens":22,"id":1890,"Prediction":"## Decision\n\n**Decision:** Implement arrow functions as the default function syntax in our JavaScript codebase.\n\n**Context:** Arrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\n\n**Rationale:**\n\n* Arrow functions provide a more concise and readable syntax, reducing visual noise and making the code easier to understand.\n* They are more expressive and flexible, allowing for more complex function definitions in a single line.\n* The lack of `this` binding and implicit `return` statement can simplify the code and reduce the risk of errors.\n\n**Consequences:**\n\n* The codebase will adopt a more consistent and concise syntax, making it easier for new developers to understand and maintain.\n* The use of arrow functions may require adjustments to existing code that relies on the `this` keyword or explicit `return` statements.\n* The code may become more expressive and easier to read, but may also become less familiar to developers who are used to traditional function syntax.\n\n**Alternatives:**\n\n* Stick with traditional function syntax to maintain consistency with existing code.\n* Use a mix of arrow functions and traditional functions to achieve a balance between readability and consistency.\n\n**Status:** Accepted\n\n**Implementation:** We will gradually replace traditional functions with arrow functions in our codebase, starting with new code and working our way back to existing code. We will also provide guidance and training to developers on the use of arrow functions.","GenTime":"2024-08-10 08:05:10"}
{"File Name":"aspan-server\/0004-info-graphql-import.md","Context":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n","Decision":"Decision here...\\n","tokens":20,"id":1893,"Prediction":"## Decision\n\n**Decision:** We will use the `type-graphql` library to define import and export definitions in GraphQL SDL, instead of relying on a custom solution.\n\n**Context:** As we are working with GraphQL SDL, we need to define import and export definitions to organize and reuse our schema. We want to find a library that can help us achieve this in a robust and scalable way.\n\n**Problem:** Without a library, we would need to implement a custom solution to manage imports and exports in our GraphQL SDL, which could lead to complexity and maintenance issues.\n\n**Reasoning:**\n\n* `type-graphql` is a popular and well-maintained library that provides a robust way to define and manage GraphQL schema.\n* It supports import and export definitions out of the box, making it easy to organize and reuse our schema.\n* Using `type-graphql` will also provide additional benefits such as type checking, validation, and error handling, which will improve the overall quality and reliability of our GraphQL API.\n\n**Consequences:**\n\n* We will need to learn and adapt to the `type-graphql` API and syntax.\n* We will need to update our existing schema to use the `type-graphql` library.\n* We will benefit from improved schema organization, reusability, and maintainability.\n\n**Rationale:** By using `type-graphql`, we can simplify the process of defining and managing imports and exports in our GraphQL SDL, and focus on building a robust and scalable GraphQL API.","GenTime":"2024-08-10 08:06:01"}
{"File Name":"talktofrank-beta\/0003-site-wide-cms-configuration.md","Context":"## Context\\nCertain elements on the Talk To Frank site need to be loaded regardless of the user entry point,\\ne.g. the current user story involves adding a warning message bar that appears on all pages.\\nThe CMS needs to be updated to support the addition of site wide 'settings'.\\n","Decision":"A new 'site settings' content model will be created.  A 'Sitewide' item will then be created to contain all sitewide settings.\\nThis would then allow for the following (future) setup,\\nContent items of type 'site settings'\\n- Sitewide\\n- Homepage - any settings that should override the sitewide values on the homepage.\\n- Drugs A-Z - any settings that should override the sitewide values on the Drugs A-Z page.\\nReferences:\\n- https:\/\/www.contentful.com\/r\/knowledgebase\/dynamic-microcopy\/\\n- https:\/\/www.contentfulcommunity.com\/t\/is-it-common-to-have-a-settings-content-model\/924\/2\\n","tokens":59,"id":1894,"Prediction":"## Decision\n\n**Decision:** Implement a centralized settings management system using a separate module or service, allowing for easy maintenance and modification of site-wide settings.\n\n## Context\n\nCertain elements on the Talk To Frank site need to be loaded regardless of the user entry point, e.g. the current user story involves adding a warning message bar that appears on all pages. The CMS needs to be updated to support the addition of site-wide'settings'.\n\n## Problem\n\nThe current CMS architecture does not provide a straightforward way to manage site-wide settings, making it difficult to implement features that require common settings across all pages. This leads to a maintenance nightmare and increases the risk of errors and inconsistencies.\n\n## Rationale\n\nImplementing a centralized settings management system will provide several benefits:\n\n1. **Easy maintenance**: Site-wide settings can be easily managed and updated through a single interface, reducing the complexity and risk of errors.\n2. **Flexibility**: The system can be designed to support different types of settings, such as text, images, or even complex configurations.\n3. **Reusability**: The settings management system can be reused across multiple pages and features, reducing code duplication and increasing efficiency.\n4. **Scalability**: The system can be easily extended to support additional settings and features as the site evolves.\n\n## Decision Criteria\n\n1. **Ease of maintenance**: A centralized settings management system will make it easier to manage and update site-wide settings.\n2. **Flexibility**: The system should be able to support different types of settings and configurations.\n3. **Reusability**: The system should be designed to be reusable across multiple pages and features.\n4. **Scalability**: The system should be able to scale with the site's growth and evolving requirements.\n\n## Consequences\n\n1. **Increased complexity**: Implementing a centralized settings management system will introduce additional complexity to the CMS architecture.\n2. **Additional development time**: Developing the settings management system will require additional development time and resources.\n3. **Potential for errors**: Introducing a new system may introduce errors and bugs that need to be addressed.\n\n## Pros and Cons\n\nPros:\n\n* Easy maintenance and updates\n* Increased flexibility and reusability\n* Improved scalability\n\nCons:\n\n* Increased complexity\n* Additional development time and resources\n* Potential for errors and bugs\n\nBy implementing a centralized settings management system, we can improve the maintainability, flexibility, and scalability of the Talk To Frank site, while reducing the risk of errors and inconsistencies.","GenTime":"2024-08-10 08:06:27"}
{"File Name":"drt-v2\/0013-use-lihaoyi-s-autowire.md","Context":"## Context\\nWe've got a Single Page app, it needs to talk to the server. Our use of scala and scalajs means we can use [lihaoyi's autowire\\nmacros](https:\/\/github.com\/lihaoyi\/autowire)\\nAlthough this is essentially a 0 on the [Richardson maturity model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nit has huge benefits in terms of speed of change. We also (at the moment) only have the one client of the SPA so we can afford the tight coupling.\\nIt doesn't preclude moving toward something more restful, as we can just add routes when we recognise a need.\\n","Decision":"Use autowire for now.\\n","tokens":147,"id":1907,"Prediction":"Here is an Architectural Decision Record (ADR) based on the context provided:\n\n**Decision**\n\nUse Autowire macros to create a tightly-coupled client-server architecture for our Single Page App (SPA), leveraging the benefits of speed of change and simplicity.\n\n**Context**\n\nOur SPA needs to communicate with a server, and we are using Scala and Scala.js, which allows us to utilize Lihaoyi's Autowire macros. Although our current architecture is at a low level on the Richardson Maturity Model, it provides significant benefits in terms of rapid development and change. Since we currently only have one client for the SPA, we can afford the tight coupling. This decision does not preclude the possibility of moving towards a more RESTful architecture in the future, as we can simply add new routes as needed.\n\n**Rationale**\n\nThe use of Autowire macros allows us to quickly and easily create a client-server architecture, without the need for complex configuration or boilerplate code. This enables us to focus on developing the features of our SPA, rather than worrying about the underlying infrastructure. The tight coupling also allows for a more streamlined development process, as changes to the server can be easily reflected in the client.\n\n**Consequences**\n\n* The client and server will be tightly coupled, making it more difficult to change or replace either component independently.\n* The architecture may not be scalable or maintainable in the long term, if the number of clients or complexity of the system increases.\n* The decision to use Autowire macros may limit our ability to adopt a more RESTful architecture in the future, if we decide to add more clients or services.\n\n**Status**\n\nThis decision is currently in effect, and we will continue to monitor the architecture and adapt as needed to ensure it meets the changing needs of our project.","GenTime":"2024-08-10 08:10:53"}
{"File Name":"drt-v2\/0006-physical-deployment-one-jvm-per-port.md","Context":"## Context\\nDRTv1 tried to normalise all ports into a single model. We frequently had requests for port specific features\\nor structures. Also it\\n","Decision":"Use a jvm per airport\\n","tokens":33,"id":1912,"Prediction":"## Decision\n\n**Port-Specific Models**\n\n## Context\n\nDRTv1 tried to normalize all ports into a single model. We frequently had requests for port-specific features or structures.\n\n## Problem\n\nThe single model approach made it difficult to accommodate port-specific requirements, leading to a lack of flexibility and maintainability in our system.\n\n## Decision\n\nWe will create separate models for each port type, allowing for greater customization and adaptability to meet specific port requirements. This decision will enable us to better support port-specific features and structures, improving the overall usability and scalability of our system.\n\n## Rationale\n\nBy creating separate models for each port type, we can:\n\n* Better accommodate port-specific requirements and features\n* Improve maintainability and flexibility in our system\n* Enhance the overall user experience by providing a more tailored solution for each port\n\n## Consequences\n\n* Increased complexity in our system, requiring more models and relationships to manage\n* Potential for duplicated code or logic across port-specific models\n* Requires careful consideration of data consistency and integrity across port-specific models\n\n## Status\n\nApproved\n\n## Date\n\n[Insert Date]\n\n## Links\n\n* [Link to relevant documentation or code changes]","GenTime":"2024-08-10 08:12:33"}
{"File Name":"SiebenApp\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1922,"Prediction":"Here is an example of an Architectural Decision Record (ADR) for a software project:\n\n**Decision**\nUse a Microkernel Architecture for the New Feature\n\n**Context**\n\nWe need to record the architectural decisions made on this project.\n\n**Problem**\n\nThe new feature we are implementing requires a high degree of modularity and scalability. Our current architecture is monolithic and may not be able to handle the increased complexity and load that this feature will bring.\n\n**Decision**\n\nWe will use a microkernel architecture for the new feature. This will allow us to break down the feature into smaller, independent components that can be developed, tested, and deployed independently.\n\n**Rationale**\n\nThe microkernel architecture will provide several benefits, including:\n\n* Improved modularity: Each component will be responsible for a specific function, making it easier to maintain and update.\n* Increased scalability: The microkernel architecture will allow us to add or remove components as needed, making it easier to scale the system.\n* Easier testing: Each component can be tested independently, making it easier to identify and fix bugs.\n\n**Consequences**\n\n* The microkernel architecture will require more complex configuration and deployment procedures.\n* The development team will need to learn new skills and technologies to work with the microkernel architecture.\n\n**Status**\n\nThis decision has been approved and is currently being implemented.\n\n**References**\n\n* [1] \"Microkernel Architecture\" by Martin Fowler\n* [2] \"Designing Scalable Systems\" by Brendan Burns\n\n**Links**\n\n* [1] https:\/\/martinfowler.com\/articles\/microkernel.html\n* [2] https:\/\/www.oreilly.com\/library\/view\/designing-scalable-systems\/9781492044446\/","GenTime":"2024-08-10 08:15:54"}
{"File Name":"green_log\/0003-decouple-generation-and-handling.md","Context":"## Context\\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\\n","Decision":"De-couple generation of log message\/entries from how they are handled.\\n* A `logger` object provides an API that can be used to generate log entries.\\n* Log \"entries\" are strongly typed structures.\\n* Log entry \"handlers\" provide a simple, consistent interface.\\n```mermaid\\nsequenceDiagram\\nApp ->> Logger:       info(\"Message\")\\nLogger ->> Handler:   <<(entry)\\n```\\n","tokens":35,"id":1924,"Prediction":"## Decision\n\nTo achieve a consistent logging API that allows for logs to be filed, forwarded, filtered, and formatted in a variety of ways, we will implement a logging framework that utilizes a pipeline architecture.\n\n## Context\n\nAs you mentioned, we want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered, and formatted in a variety of ways. This is crucial to ensure that logs are properly collected, processed, and stored for analysis and troubleshooting purposes.\n\n## Problem\n\nCurrently, our logging API is not consistent across applications, and the logging mechanisms are not flexible enough to accommodate different logging requirements. This makes it difficult to manage and analyze logs, and can lead to inconsistencies in log data and formatting.\n\n## Decision\n\nTo address this issue, we will implement a logging framework that utilizes a pipeline architecture. This will allow us to decouple the logging API from the underlying logging mechanisms, making it possible to change or replace the logging mechanisms without affecting the API.\n\nThe pipeline architecture will consist of the following components:\n\n1. **Loggers**: These will be the entry points for logging requests from applications. Loggers will be responsible for formatting log messages and sending them to the next component in the pipeline.\n2. **Filters**: These will be used to filter log messages based on various criteria, such as log level, message content, or user-defined filters.\n3. **Formatters**: These will be used to format log messages according to specific requirements, such as log file format, network protocol, or database schema.\n4. **Sink**: This will be the final component in the pipeline, responsible for storing or forwarding the formatted log messages.\n\nBy using a pipeline architecture, we will be able to:\n\n* Decouple the logging API from the underlying logging mechanisms, making it easier to change or replace the logging mechanisms.\n* Implement multiple logging mechanisms, such as file-based logging, network-based logging, or database-based logging.\n* Filter and format log messages according to specific requirements.\n* Store or forward log messages to various destinations, such as log files, databases, or message queues.\n\n## Consequences\n\nThe implementation of a pipeline-based logging framework will require additional development effort and complexity. However, the benefits of a consistent and flexible logging API will outweigh the additional complexity.\n\nThe pipeline architecture will also provide opportunities for future enhancements and integrations, such as:\n\n* Implementing additional logging mechanisms, such as cloud-based logging services or third-party logging integrations.\n* Adding support for log analytics and visualization tools.\n* Integrating with security and compliance frameworks to ensure log data is properly secured and compliant with regulatory requirements.\n\nOverall, the pipeline-based logging framework will provide a robust and flexible logging solution that meets the requirements of our applications and ensures consistent and reliable logging across the system.","GenTime":"2024-08-10 08:16:43"}
{"File Name":"green_log\/0006-use-lock-free-io.md","Context":"## Context\\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\\nBut also, we want logging to perform well.\\n","Decision":"_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\\n- avoid using of mutexes to serialise output\\n- perform atomic writes to `IO` streams (using `<<`)\\n","tokens":37,"id":1927,"Prediction":"## Decision\n\nTo achieve the goal of writing log entries without them being interleaved while also ensuring good performance, we will implement a logging mechanism that uses a buffered writer to write log entries to the desired output (file or STDOUT). This approach will allow us to batch log entries together before writing them to the output, ensuring that they are not interleaved.\n\n## Context\n\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved. But also, we want logging to perform well.\n\n## Decision\n\nWe will use a buffered writer to write log entries. A buffered writer is a writer that stores log entries in a buffer before writing them to the output. This approach has several benefits:\n\n* It allows us to batch log entries together, reducing the number of writes to the output and improving performance.\n* It ensures that log entries are not interleaved, as they are written to the output in the order they were received.\n* It allows us to implement features such as log rotation and log compression, which can help manage the volume of log data.\n\n## Status\n\nAccepted\n\n## Implemented By\n\n[Your Name]\n\n## Links\n\n* [Link to the code that implements the decision]\n* [Link to the documentation that describes the decision]\n\n## Notes\n\nThis decision was made to ensure that our logging mechanism is both performant and reliable. By using a buffered writer, we can write log entries efficiently and ensure that they are not interleaved.","GenTime":"2024-08-10 08:17:43"}
{"File Name":"site-stitcher\/001-generate-static-pages.md","Context":"## Context\\nI see so many thing broken on the web today, it is not only website speed, it is also\\nhow many tools we have accepted to use to solve a simple problem. Especially in the JS\\nworld I feel the healthy \"How much do I need?\" question is asked rarely. We also teach\\nnewcomers that it is ok to load 1GB of npm modules before we can actually build an empty page.\\nIn this repo I try to do the simplest thing possible to generate my website.\\nI am looking for dependencies that are as tiny as possible, some I just need to not use and\\nwrite the needed code itself, this also makes me aware how much of a dependency I would have needed\\nonly.\\nAlso the site that I want to ship shall have all a11y, lighthouse, etc. scores as high as possible\\ntherefore the site shall degrade gracefully be accessible and very light weight.\\nI also want the simplest hosting setup and infrastructure possible, due to using github it might\\nbe github-pages, which is just static pages.\\n","Decision":"With static pages the above can be achieved. Easily.\\n","tokens":222,"id":1931,"Prediction":"## Decision\n\n**Decision:** Use a minimalist approach to dependencies and infrastructure to ensure a lightweight, accessible, and maintainable website.\n\n**Context:** The user is concerned about the proliferation of unnecessary dependencies and tools in the JavaScript world, and wants to create a simple website that is easy to maintain and has high scores in accessibility and performance metrics.\n\n**Rationale:** By adopting a minimalist approach, we can:\n\n* Reduce the overall size and complexity of the website, making it easier to maintain and update.\n* Improve performance and accessibility by minimizing the number of dependencies and third-party libraries.\n* Simplify the hosting infrastructure by using a static site generator and GitHub Pages.\n\n**Consequences:**\n\n* The website may not have all the features and functionalities of a more complex website.\n* The user may need to write more code to implement certain features, rather than relying on third-party libraries.\n* The website may not be compatible with all browsers or devices, due to the focus on simplicity and accessibility.\n\n**Alternatives:**\n\n* Using a more complex framework or library to build the website, which may add unnecessary dependencies and complexity.\n* Relying on third-party services or libraries to implement certain features, which may compromise accessibility and performance.\n\n**Status:** This decision has been made and will be implemented in the website.\n\n**References:**\n\n* [Minimalist approach to dependencies](https:\/\/www.smashingmagazine.com\/2019\/06\/minimalist-approach-dependencies\/)\n* [Accessibility guidelines](https:\/\/www.w3.org\/TR\/WCAG21\/)\n* [Lighthouse performance metrics](https:\/\/developers.google.com\/web\/tools\/lighthouse\/audits\/performance)","GenTime":"2024-08-10 08:19:02"}
{"File Name":"tendermint\/adr-051-double-signing-risk-reduction.md","Context":"## Context\\nTo provide a risk reduction method for double signing incidents mistakenly executed by validators\\n- Validators often mistakenly run duplicated validators to cause double-signing incident\\n- This proposed feature is to reduce the risk of mistaken double-signing incident by checking recent N blocks before voting begins\\n- When we think of such serious impact on double-signing incident, it is very reasonable to have multiple risk reduction algorithm built in node daemon\\n","Decision":"We would like to suggest a double signing risk reduction method.\\n- Methodology : query recent consensus results to find out whether node's consensus key is used on consensus recently or not\\n- When to check\\n- When the state machine starts `ConsensusReactor` after fully synced\\n- When the node is validator ( with privValidator )\\n- When `cs.config.DoubleSignCheckHeight > 0`\\n- How to check\\n1. When a validator is transformed from syncing status to fully synced status, the state machine check recent N blocks (`latest_height - double_sign_check_height`) to find out whether there exists consensus votes using the validator's consensus key\\n2. If there exists votes from the validator's consensus key, exit state machine program\\n- Configuration\\n- We would like to suggest by introducing `double_sign_check_height` parameter in `config.toml` and cli, how many blocks state machine looks back to check votes\\n- <span v-pre>`double_sign_check_height = {{ .Consensus.DoubleSignCheckHeight }}`<\/span> in `config.toml`\\n- `tendermint node --consensus.double_sign_check_height` in cli\\n- State machine ignore checking procedure when `double_sign_check_height == 0`\\n","tokens":85,"id":1944,"Prediction":"## Decision\n\n**Title:** Implement a \"Recent Block Check\" mechanism to reduce the risk of double-signing incidents\n\n**Context:** To provide a risk reduction method for double signing incidents mistakenly executed by validators, where validators often mistakenly run duplicated validators to cause double-signing incident.\n\n**Decision:** We will implement a \"Recent Block Check\" mechanism in the node daemon to reduce the risk of double-signing incidents. This mechanism will check the recent N blocks before voting begins to ensure that the validator has not already signed a block.\n\n**Rationale:**\n\n* The risk of double-signing incidents is significant and can have serious consequences for the network.\n* Implementing a \"Recent Block Check\" mechanism will provide an additional layer of protection against such incidents.\n* This mechanism will be built into the node daemon, allowing it to be easily integrated with our existing validation process.\n* The \"Recent Block Check\" mechanism will be configurable, allowing us to adjust the number of blocks to check (N) based on the specific needs of our network.\n\n**Consequences:**\n\n* The \"Recent Block Check\" mechanism will add an additional step to the validation process, which may increase the time it takes for validators to validate blocks.\n* The mechanism will require additional storage to store the recent block hashes.\n* The mechanism will need to be thoroughly tested to ensure that it is functioning correctly and does not introduce any new bugs or vulnerabilities.\n\n**Alternatives:**\n\n* We could implement a different risk reduction algorithm, such as a \"Block Hash Check\" mechanism that checks the block hash against a list of known hashes.\n* We could not implement any risk reduction mechanism at all, and rely solely on the validators to ensure that they are not executing duplicate validations.\n\n**Status:** Implemented\n\n**Implemented By:** [Name]\n\n**Date:** [Date]\n\n**Reviewers:** [List of reviewers]\n\n**Changelog:** [Link to changelog]","GenTime":"2024-08-10 08:22:45"}
{"File Name":"tendermint\/adr-063-privval-grpc.md","Context":"## Context\\nValidators use remote signers to help secure their keys. This system is Tendermint's recommended way to secure validators, but the path to integration with Tendermint's private validator client is plagued with custom protocols.\\nTendermint uses its own custom secure connection protocol (`SecretConnection`) and a raw tcp\/unix socket connection protocol. The secure connection protocol until recently was exposed to man in the middle attacks and can take longer to integrate if not using Golang. The raw tcp connection protocol is less custom, but has been causing minute issues with users.\\nMigrating Tendermint's private validator client to a widely adopted protocol, gRPC, will ease the current maintenance and integration burden experienced with the current protocol.\\n","Decision":"After discussing with multiple stake holders, [gRPC](https:\/\/grpc.io\/) was decided on to replace the current private validator protocol. gRPC is a widely adopted protocol in the micro-service and cloud infrastructure world. gRPC uses [protocol-buffers](https:\/\/developers.google.com\/protocol-buffers) to describe its services, providing a language agnostic implementation. Tendermint uses protobuf for on disk and over the wire encoding already making the integration with gRPC simpler.\\n","tokens":147,"id":1949,"Prediction":"## Decision\n\n**Title:** Migrate Tendermint's Private Validator Client to gRPC for Secure Connection Protocol\n\n**Context:** Validators use remote signers to secure their keys, and the current integration path with Tendermint's private validator client is plagued with custom protocols, including Tendermint's own custom secure connection protocol (`SecretConnection`) and a raw tcp\/unix socket connection protocol.\n\n**Decision:** We will migrate Tendermint's private validator client to use gRPC as the secure connection protocol, replacing the existing custom protocols.\n\n**Rationale:**\n\n* gRPC is a widely adopted and well-established protocol that provides a secure and efficient way to communicate between services.\n* Migrating to gRPC will ease the current maintenance and integration burden experienced with the custom protocols.\n* gRPC is language-agnostic, making it easier to integrate with other languages and frameworks.\n* gRPC provides built-in support for security features such as SSL\/TLS encryption, authentication, and authorization, which will help to improve the security of the connection.\n\n**Consequences:**\n\n* The migration will require significant changes to the existing codebase, including the development of new gRPC services and the integration of these services with the existing Tendermint infrastructure.\n* The migration may introduce new dependencies and complexity, which will need to be carefully managed.\n* The migration will likely require additional testing and validation to ensure that the new gRPC protocol is functioning correctly and securely.\n\n**Status:** Accepted\n\n**Implementors:** [List of implementors responsible for the migration]\n\n**Links:** [List of relevant links, such as design documents, issue trackers, or code reviews]","GenTime":"2024-08-10 08:24:25"}
{"File Name":"tendermint\/adr-012-peer-transport.md","Context":"## Context\\nOne of the more apparent problems with the current architecture in the p2p\\npackage is that there is no clear separation of concerns between different\\ncomponents. Most notably the `Switch` is currently doing physical connection\\nhandling. An artifact is the dependency of the Switch on\\n`[config.P2PConfig`](https:\/\/github.com\/tendermint\/tendermint\/blob\/05a76fb517f50da27b4bfcdc7b4cf185fc61eff6\/config\/config.go#L272-L339).\\nAddresses:\\n- [#2046](https:\/\/github.com\/tendermint\/tendermint\/issues\/2046)\\n- [#2047](https:\/\/github.com\/tendermint\/tendermint\/issues\/2047)\\nFirst iteraton in [#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)\\n","Decision":"Transport concerns will be handled by a new component (`PeerTransport`) which\\nwill provide Peers at its boundary to the caller. In turn `Switch` will use\\nthis new component accept new `Peer`s and dial them based on `NetAddress`.\\n### PeerTransport\\nResponsible for emitting and connecting to Peers. The implementation of `Peer`\\nis left to the transport, which implies that the chosen transport dictates the\\ncharacteristics of the implementation handed back to the `Switch`. Each\\ntransport implementation is responsible to filter establishing peers specific\\nto its domain, for the default multiplexed implementation the following will\\napply:\\n- connections from our own node\\n- handshake fails\\n- upgrade to secret connection fails\\n- prevent duplicate ip\\n- prevent duplicate id\\n- nodeinfo incompatibility\\n```go\\n\/\/ PeerTransport proxies incoming and outgoing peer connections.\\ntype PeerTransport interface {\\n\/\/ Accept returns a newly connected Peer.\\nAccept() (Peer, error)\\n\/\/ Dial connects to a Peer.\\nDial(NetAddress) (Peer, error)\\n}\\n\/\/ EXAMPLE OF DEFAULT IMPLEMENTATION\\n\/\/ multiplexTransport accepts tcp connections and upgrades to multiplexted\\n\/\/ peers.\\ntype multiplexTransport struct {\\nlistener net.Listener\\nacceptc chan accept\\nclosec  <-chan struct{}\\nlistenc <-chan struct{}\\ndialTimeout      time.Duration\\nhandshakeTimeout time.Duration\\nnodeAddr         NetAddress\\nnodeInfo         NodeInfo\\nnodeKey          NodeKey\\n\/\/ TODO(xla): Remove when MConnection is refactored into mPeer.\\nmConfig conn.MConnConfig\\n}\\nvar _ PeerTransport = (*multiplexTransport)(nil)\\n\/\/ NewMTransport returns network connected multiplexed peers.\\nfunc NewMTransport(\\nnodeAddr NetAddress,\\nnodeInfo NodeInfo,\\nnodeKey NodeKey,\\n) *multiplexTransport\\n```\\n### Switch\\nFrom now the Switch will depend on a fully setup `PeerTransport` to\\nretrieve\/reach out to its peers. As the more low-level concerns are pushed to\\nthe transport, we can omit passing the `config.P2PConfig` to the Switch.\\n```go\\nfunc NewSwitch(transport PeerTransport, opts ...SwitchOption) *Switch\\n```\\n","tokens":179,"id":1956,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will refactor the `Switch` component in the p2p package to separate its concerns and remove its dependency on `config.P2PConfig`. This will improve the maintainability, scalability, and testability of the system.\n\n**Context**\n\nAs described in the context, the current architecture of the p2p package lacks a clear separation of concerns between different components. The `Switch` component is responsible for both physical connection handling and configuration management, which makes it difficult to maintain and test. The dependency on `config.P2PConfig` also creates a tight coupling between the `Switch` and the configuration layer.\n\n**Rationale**\n\nBy separating the concerns of the `Switch` component, we can:\n\n1. Improve maintainability: Each component will have a single responsibility, making it easier to understand and modify.\n2. Improve scalability: The system will be more flexible and able to handle changes in the configuration without affecting the core functionality.\n3. Improve testability: The components will be easier to test independently, reducing the complexity of the test suite.\n\n**Decision Drivers**\n\n* The need for a more maintainable and scalable architecture\n* The desire to reduce the complexity of the system\n* The importance of improving testability\n\n**Consequences**\n\n* The `Switch` component will be refactored to separate its concerns\n* The `config.P2PConfig` dependency will be removed\n* The system will require additional components to manage configuration and physical connections\n\n**Status**\n\nThis decision is currently being implemented in the first iteration of the refactoring process, as described in issue #2067.\n\n**References**\n\n* Issue #2046: \"Separate concerns in the p2p package\"\n* Issue #2047: \"Remove dependency on `config.P2PConfig` in the `Switch` component\"\n* Issue #2067: \"First iteration of refactoring the `Switch` component\"","GenTime":"2024-08-10 08:25:46"}
{"File Name":"tendermint\/adr-014-secp-malleability.md","Context":"## Context\\nSecp256k1 has two layers of malleability.\\nThe signer has a random nonce, and thus can produce many different valid signatures.\\nThis ADR is not concerned with that.\\nThe second layer of malleability basically allows one who is given a signature\\nto produce exactly one more valid signature for the same message from the same public key.\\n(They don't even have to know the message!)\\nThe math behind this will be explained in the subsequent section.\\nNote that in many downstream applications, signatures will appear in a transaction, and therefore in the tx hash.\\nThis means that if someone broadcasts a transaction with secp256k1 signature, the signature can be altered into the other form by anyone in the p2p network.\\nThus the tx hash will change, and this altered tx hash may be committed instead.\\nThis breaks the assumption that you can broadcast a valid transaction and just wait for its hash to be included on chain.\\nOne example is if you are broadcasting a tx in cosmos,\\nand you wait for it to appear on chain before incrementing your sequence number.\\nYou may never increment your sequence number if a different tx hash got committed.\\nRemoving this second layer of signature malleability concerns could ease downstream development.\\n### ECDSA context\\nSecp256k1 is ECDSA over a particular curve.\\nThe signature is of the form `(r, s)`, where `s` is a field element.\\n(The particular field is the `Z_n`, where the elliptic curve has order `n`)\\nHowever `(r, -s)` is also another valid solution.\\nNote that anyone can negate a group element, and therefore can get this second signature.\\n","Decision":"We can just distinguish a canonical form for the ECDSA signatures.\\nThen we require that all ECDSA signatures be in the form which we defined as canonical.\\nWe reject signatures in non-canonical form.\\nA canonical form is rather easy to define and check.\\nIt would just be the smaller of the two values for `s`, defined lexicographically.\\nThis is a simple check, instead of checking if `s < n`, instead check `s <= (n - 1)\/2`.\\nAn example of another cryptosystem using this\\nis the parity definition here https:\/\/github.com\/zkcrypto\/pairing\/pull\/30#issuecomment-372910663.\\nThis is the same solution Ethereum has chosen for solving secp malleability.\\n","tokens":356,"id":1957,"Prediction":"## Decision\n\nTo address the second layer of malleability in Secp256k1 signatures, we will use a technique to prevent the alteration of signatures in downstream applications. Specifically, we will use a scheme to ensure that the signature is not malleable, making it impossible to produce another valid signature for the same message from the same public key.\n\n## Context\n\nAs discussed, Secp256k1 has two layers of malleability. The first layer is due to the random nonce used by the signer, allowing them to produce multiple valid signatures. However, the second layer is more concerning, as it allows anyone who is given a signature to produce exactly one more valid signature for the same message from the same public key, without even knowing the message. This malleability can be exploited in downstream applications, such as transactions in a peer-to-peer network, where the signature can be altered and the transaction hash changed, potentially leading to unexpected consequences.\n\n## Problem\n\nThe second layer of malleability in Secp256k1 signatures can lead to issues in downstream applications, such as transactions in a peer-to-peer network. This can cause problems, such as the inability to broadcast a valid transaction and wait for its hash to be included on chain, or the potential for a different transaction hash to be committed instead.\n\n## Solution\n\nTo address this issue, we will use a technique to ensure that the signature is not malleable. This can be achieved by using a scheme that prevents the alteration of signatures, making it impossible to produce another valid signature for the same message from the same public key.\n\n## Rationale\n\nBy removing the second layer of malleability, we can ease downstream development and ensure that signatures are not altered in a way that can lead to unexpected consequences. This will also provide a more secure and reliable way of verifying signatures and transactions.\n\n## Status\n\nThis decision has been approved and will be implemented in the next release.\n\n## Implications\n\nThis decision will have the following implications:\n\n* The signature scheme will be modified to prevent malleability.\n* Downstream applications will need to be updated to use the new signature scheme.\n* The security and reliability of signatures and transactions will be improved.\n\n## Changelog\n\n* 2023-02-20: Initial decision\n* 2023-03-10: Implementation plan\n* 2023-04-15: Update on implementation progress","GenTime":"2024-08-10 08:26:12"}
{"File Name":"tendermint\/adr-015-crypto-encoding.md","Context":"## Context\\nWe must standardize our method for encoding public keys and signatures on chain.\\nCurrently we amino encode the public keys and signatures.\\nThe reason we are using amino here is primarily due to ease of support in\\nparsing for other languages.\\nWe don't need its upgradability properties in cryptosystems, as a change in\\nthe crypto that requires adapting the encoding, likely warrants being deemed\\na new cryptosystem.\\n(I.e. using new public parameters)\\n","Decision":"### Public keys\\nFor public keys, we will continue to use amino encoding on the canonical\\nrepresentation of the pubkey.\\n(Canonical as defined by the cryptosystem itself)\\nThis has two significant drawbacks.\\nAmino encoding is less space-efficient, due to requiring support for upgradability.\\nAmino encoding support requires forking protobuf and adding this new interface support\\noption in the language of choice.\\nThe reason for continuing to use amino however is that people can create code\\nmore easily in languages that already have an up to date amino library.\\nIt is possible that this will change in the future, if it is deemed that\\nrequiring amino for interacting with Tendermint cryptography is unnecessary.\\nThe arguments for space efficiency here are refuted on the basis that there are\\nfar more egregious wastages of space in the SDK.\\nThe space requirement of the public keys doesn't cause many problems beyond\\nincreasing the space attached to each validator \/ account.\\nThe alternative to using amino here would be for us to create an enum type.\\nSwitching to just an enum type is worthy of investigation post-launch.\\nFor reference, part of amino encoding interfaces is basically a 4 byte enum\\ntype definition.\\nEnum types would just change that 4 bytes to be a variant, and it would remove\\nthe protobuf overhead, but it would be hard to integrate into the existing API.\\n### Signatures\\nSignatures should be switched to be `[]byte`.\\nSpatial efficiency in the signatures is quite important,\\nas it directly affects the gas cost of every transaction,\\nand the throughput of the chain.\\nSignatures don't need to encode what type they are for (unlike public keys)\\nsince public keys must already be known.\\nTherefore we can validate the signature without needing to encode its type.\\nWhen placed in state, signatures will still be amino encoded, but it will be the\\nprimitive type `[]byte` getting encoded.\\n#### Ed25519\\nUse the canonical representation for signatures.\\n#### Secp256k1\\nThere isn't a clear canonical representation here.\\nSignatures have two elements `r,s`.\\nThese bytes are encoded as `r || s`, where `r` and `s` are both exactly\\n32 bytes long, encoded big-endian.\\nThis is basically Ethereum's encoding, but without the leading recovery bit.\\n","tokens":100,"id":1962,"Prediction":"## Decision\n\n**Standardize on a JSON-based encoding for public keys and signatures**\n\n## Context\n\nAs we currently use amino encoding for public keys and signatures on chain, primarily for ease of parsing in other languages. However, we don't require the upgradability properties of amino in our cryptosystems, and changes to the crypto that would require adapting the encoding would likely warrant being considered a new cryptosystem.\n\n## Problem\n\nThe current use of amino encoding for public keys and signatures may lead to:\n\n* Limited flexibility in case of changes to the crypto system\n* Potential compatibility issues with other languages or systems\n* Increased complexity in handling and parsing the encoded data\n\n## Decision\n\nTo standardize our method for encoding public keys and signatures on chain, we will switch to a JSON-based encoding. This approach provides a simple, human-readable, and widely supported format for encoding and decoding public keys and signatures. This decision aligns with our requirement for ease of support in parsing for other languages, while also providing a more flexible and maintainable solution.\n\n## Consequences\n\n* Simplified encoding and decoding of public keys and signatures\n* Improved compatibility with other languages and systems\n* Reduced complexity in handling and parsing the encoded data\n* Potential for future flexibility in case of changes to the crypto system\n\n## Rationale\n\nThe use of JSON-based encoding provides a straightforward and widely adopted format for encoding and decoding public keys and signatures. This approach allows us to decouple the encoding from the specific crypto system, making it easier to adapt to changes in the future. Additionally, JSON is a widely supported format, making it easier to integrate with other systems and languages.","GenTime":"2024-08-10 08:27:01"}
{"File Name":"tendermint\/adr-061-p2p-refactor-scope.md","Context":"## Context\\nThe `p2p` package responsible for peer-to-peer networking is rather old and has a number of weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilites, poor performance, custom protocols, and incorrect behavior. A refactor has been discussed for several years ([#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)).\\nInformal Systems are also building a Rust implementation of Tendermint, [Tendermint-rs](https:\/\/github.com\/informalsystems\/tendermint-rs), and plan to implement P2P networking support over the next year. As part of this work, they have requested adopting e.g. [QUIC](https:\/\/datatracker.ietf.org\/doc\/draft-ietf-quic-transport\/) as a transport protocol instead of implementing the custom application-level `MConnection` stream multiplexing protocol that Tendermint currently uses.\\nThis ADR summarizes recent discussion with stakeholders on the scope of a P2P refactor. Specific designs and implementations will be submitted as separate ADRs.\\n","Decision":"The P2P stack will be refactored and improved iteratively, in several phases:\\n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.\\n* **Phase 2:** additional transports and incremental protocol improvements.\\n* **Phase 3:** disruptive protocol changes.\\nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.\\n","tokens":227,"id":1979,"Prediction":"## Decision\n\nThe `p2p` package will be refactored to use QUIC as the transport protocol instead of the custom `MConnection` stream multiplexing protocol. This decision is made to address the weaknesses of the current implementation, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilities, poor performance, and incorrect behavior.\n\n## Context\n\nThe `p2p` package is an old and outdated implementation of peer-to-peer networking that has been discussed for several years. It has a number of weaknesses that need to be addressed. Informal Systems is building a Rust implementation of Tendermint, Tendermint-rs, and plans to implement P2P networking support over the next year. They have requested adopting QUIC as a transport protocol instead of implementing a custom application-level protocol.\n\n## Status\n\nAccepted\n\n## Date\n\n2023-02-15\n\n## Decision Drivers\n\n* The need to address the weaknesses of the current `p2p` package implementation\n* The request from Informal Systems to adopt QUIC as a transport protocol\n* The desire to improve the performance, security, and maintainability of the `p2p` package\n\n## Decision Criteria\n\n* The use of QUIC as a transport protocol will improve the performance and security of the `p2p` package\n* The use of QUIC will reduce the complexity and maintainability of the `p2p` package\n* The use of QUIC will align with the goals of Informal Systems to implement P2P networking support\n\n## Pros and Cons\n\nPros:\n\n* Improved performance and security\n* Reduced complexity and maintainability\n* Alignment with Informal Systems' goals\n\nCons:\n\n* Potential for significant changes to the `p2p` package implementation\n* Potential for compatibility issues with existing clients and servers\n\n## Future ADRs\n\nThis ADR summarizes the decision to use QUIC as a transport protocol. Future ADRs will provide more details on the specific designs and implementations for the `p2p` package refactor.","GenTime":"2024-08-10 08:33:49"}
{"File Name":"tendermint\/adr-073-libp2p.md","Context":"## Context\\nAs part of the 0.35 development cycle, the Tendermint team completed\\nthe first phase of the work described in ADRs 61 and 62, which included a\\nlarge scale refactoring of the reactors and the p2p message\\nrouting. This replaced the switch and many of the other legacy\\ncomponents without breaking protocol or network-level\\ninteroperability and left the legacy connection\/socket handling code.\\nFollowing the release, the team has reexamined the state of the code\\nand the design, as well as Tendermint's requirements. The notes\\nfrom that process are available in the [P2P Roadmap\\nRFC][rfc].\\nThis ADR supersedes the decisions made in ADRs 60 and 61, but\\nbuilds on the completed portions of this work. Previously, the\\nboundaries of peer management, message handling, and the higher level\\nbusiness logic (e.g., \"the reactors\") were intermingled, and core\\nelements of the p2p system were responsible for the orchestration of\\nhigher-level business logic. Refactoring the legacy components\\nmade it more obvious that this entanglement of responsibilities\\nhad outsized influence on the entire implementation, making\\nit difficult to iterate within the current abstractions.\\nIt would not be viable to maintain interoperability with legacy\\nsystems while also achieving many of our broader objectives.\\nLibP2P is a thoroughly-specified implementation of a peer-to-peer\\nnetworking stack, designed specifically for systems such as\\nours. Adopting LibP2P as the basis of Tendermint will allow the\\nTendermint team to focus more of their time on other differentiating\\naspects of the system, and make it possible for the ecosystem as a\\nwhole to take advantage of tooling and efforts of the LibP2P\\nplatform.\\n","Decision":"Tendermint will adopt libp2p during the 0.37 development cycle,\\nreplacing the bespoke Tendermint P2P stack. This will remove the\\n`Endpoint`, `Transport`, `Connection`, and `PeerManager` abstractions\\nand leave the reactors, `p2p.Router` and `p2p.Channel`\\nabstractions.\\nLibP2P may obviate the need for a dedicated peer exchange (PEX)\\nreactor, which would also in turn obviate the need for a dedicated\\nseed mode. If this is the case, then all of this functionality would\\nbe removed.\\nIf it turns out (based on the advice of Protocol Labs) that it makes\\nsense to maintain separate pubsub or gossipsub topics\\nper-message-type, then the `Router` abstraction could also\\nbe entirely subsumed.\\n","tokens":387,"id":1982,"Prediction":"## Decision\n\n**Use LibP2P as the basis for Tendermint's peer-to-peer networking stack**\n\n## Context\n\nAs part of the 0.35 development cycle, the Tendermint team completed the first phase of the work described in ADRs 61 and 62, which included a large-scale refactoring of the reactors and the p2p message routing. This refactoring replaced the switch and many of the other legacy components without breaking protocol or network-level interoperability and left the legacy connection\/socket handling code. Following the release, the team has reexamined the state of the code and the design, as well as Tendermint's requirements.\n\n## Problem\n\nThe current implementation of Tendermint's peer-to-peer networking stack is tightly coupled with the legacy connection\/socket handling code, making it difficult to iterate within the current abstractions. The entanglement of responsibilities between peer management, message handling, and higher-level business logic has outsized influence on the entire implementation, making it difficult to maintain interoperability with legacy systems while achieving broader objectives.\n\n## Decision\n\nAdopting LibP2P as the basis of Tendermint's peer-to-peer networking stack will allow the Tendermint team to focus more of their time on other differentiating aspects of the system, and make it possible for the ecosystem as a whole to take advantage of tooling and efforts of the LibP2P platform. This decision supersedes the decisions made in ADRs 60 and 61, but builds on the completed portions of this work.\n\n## Rationale\n\nUsing LibP2P will provide a thoroughly-specified implementation of a peer-to-peer networking stack, designed specifically for systems such as ours. This will allow Tendermint to leverage the expertise and resources of the LibP2P community, and focus on developing differentiating features and functionality.\n\n## Conclusion\n\nThe adoption of LibP2P as the basis of Tendermint's peer-to-peer networking stack is a key decision that will enable the Tendermint team to focus on other areas of the system, while also taking advantage of the expertise and resources of the LibP2P community.","GenTime":"2024-08-10 08:34:35"}
{"File Name":"tendermint\/adr-003-abci-app-rpc.md","Context":"## Context\\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\\nClients need to talk to Tendermint for proofs, unless we burden all app devs\\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\\nserver than two, but granted it makes querying a bit more kludgy since it needs\\nto be passed as a `Query`. Also, **having a very standard rpc interface means\\nthe light-client can work with all apps and handle proofs**. The only\\napp-specific logic is decoding the binary data to a more readable form (eg.\\njson). This is a huge advantage for code-reuse and standardization.\\n","Decision":"We dont expose an RPC server on any of our ABCI-apps.\\n","tokens":161,"id":1985,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nABCI-server will expose its own RPC-server and act as a proxy to Tendermint, providing a standardized interface for clients to interact with Tendermint and retrieve proofs.\n\n**Context**\n\nThe ABCI-server will serve as a proxy to Tendermint, allowing clients to communicate with Tendermint without having to expose Tendermint's proof functionality to all app developers. This approach simplifies the architecture by reducing the number of servers that need to be managed and secured, while also providing a standardized interface for clients to interact with Tendermint.\n\n**Rationale**\n\nBy exposing its own RPC-server, the ABCI-server can provide a standardized interface for clients to interact with Tendermint, making it easier to reuse code and achieve standardization across different apps. This approach also reduces the complexity of querying Tendermint, as clients only need to interact with the ABCI-server, which can then forward the request to Tendermint.\n\n**Consequences**\n\n* Clients will need to interact with the ABCI-server instead of Tendermint directly.\n* The ABCI-server will need to handle the decoding of binary data to a more readable form (e.g. JSON) for each app.\n* The architecture will be simplified by reducing the number of servers that need to be managed and secured.\n* Code reuse and standardization will be improved across different apps.\n\n**Status**\n\nApproved\n\n**History**\n\n* 2023-02-20: Decision made\n* 2023-02-25: Implementation started\n* 2023-03-05: Implementation completed","GenTime":"2024-08-10 08:35:19"}
{"File Name":"visit-plannr\/0002-dynamodb-composite-primary-key-design.md","Context":"## Context\\n\"[In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.](https:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/HowItWorks.CoreComponents.html)\"\\nThere are two types of primary key in dynamodb.\\nThe first kind is having just a partition key. The partition key is a hash and determines where on physical storage the item is placed. The partition key must be unique.\\nThe second kind is a composite primary key. It consists of a partition key and a sort key. The partition key stays the same but doesn't need to be unique in isolation. Rather the sort key\/ partition key pair must be unique.\\nIn a real system this would probably push towards StreamName as the partition key: so that events that logically live together physically live together. And Event Number in the stream as the sort key. So that the order of items as they are stored on physical media matches the order they are likely to be read.\\nThis introduces unwanted complexity at this time in the code for tracking event numbers.\\n","Decision":"For now instead of an Event Number as the sort key we will introduce a UUID EventId. And use StreamName as the HASH key. StreamNames will need to be unique anyway.\\n","tokens":260,"id":1990,"Prediction":"## Decision\n\n**Use a composite primary key with StreamName as the partition key and EventNumber as the sort key in DynamoDB**\n\n## Context\n\nAs mentioned in the DynamoDB documentation, there are two types of primary keys: partition key and composite primary key. In our system, we have a requirement to store events in a way that logically related events are physically stored together, which suggests using a composite primary key.\n\n## Problem\n\nThe current design uses a simple partition key, which does not provide the necessary flexibility for querying and storing events. Using a composite primary key with StreamName as the partition key and EventNumber as the sort key allows us to take advantage of DynamoDB's querying capabilities, such as scanning and querying by range.\n\n## Decision Rationale\n\nUsing a composite primary key with StreamName as the partition key and EventNumber as the sort key provides several benefits:\n\n* It allows us to store events in a way that logically related events are physically stored together, which improves query performance and reduces storage costs.\n* It enables us to query events by range using the sort key, which is essential for our use case.\n* It provides a unique identifier for each event, which is necessary for tracking and processing events.\n\n## Trade-offs\n\nThe main trade-off is the added complexity of tracking event numbers, which requires additional logic and infrastructure. However, this complexity is outweighed by the benefits of using a composite primary key in DynamoDB.\n\n## Consequences\n\nThe consequence of this decision is that we will need to implement additional logic to track event numbers and ensure that they are correctly assigned and updated. This will require changes to our event processing pipeline and data storage infrastructure.\n\n## Status\n\nThis decision is final and has been implemented in our system.","GenTime":"2024-08-10 08:36:55"}
{"File Name":"handbook\/0003-css.md","Context":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n","Decision":"- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","tokens":126,"id":1992,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context:\n\n**Decision:** Use a CSS-in-JS solution with a statically compiled and cached approach, leveraging a design system library and a type-safe programming language.\n\n**Context:** We're building websites and web applications that share a common design system with reusable components. We need to write CSS styles in a way that is performant, safe, and easily accessible to our design system constants.\n\n**Decision Drivers:**\n\n1. **Performance**: We want to minimize runtime overhead and leverage code splitting, caching, and other performance optimization techniques.\n2. **Accessibility**: We need easy access to our design system constants, which should be shared with JS logic.\n3. **Type Safety**: We want to catch issues when refactoring and ensure type safety in our CSS code.\n4. **Reusability**: Reusable components should be closed, not accepting arbitrary styles or classes.\n5. **Responsive Props**: We want a pattern for responsive props with atomic layout components.\n\n**Decision:** We will use a CSS-in-JS solution, such as Emotion or Styled Components, which allows us to write CSS styles in a JavaScript file. This approach provides several benefits:\n\n1. **Statically compiled**: Our CSS code will be statically compiled, which means that it will be optimized and minified before being sent to the browser, reducing runtime overhead.\n2. **Cached**: Our CSS code will be cached, which means that it will be stored in memory and re-used, reducing the number of requests to the server.\n3. **Type-safe**: We will use a type-safe programming language, such as TypeScript, to ensure that our CSS code is type-safe and catches any errors or issues when refactoring.\n4. **Reusable components**: We will use a design system library to provide reusable components that are closed and do not accept arbitrary styles or classes.\n5. **Responsive props**: We will use atomic layout components and responsive props to create a pattern for responsive design.\n\n**Consequences:**\n\n1. **Initial Learning Curve**: There may be an initial learning curve for our team members who are not familiar with CSS-in-JS solutions.\n2. **Additional Dependencies**: We will need to add additional dependencies, such as Emotion or Styled Components, to our project.\n3. **Increased Complexity**: Our project may become more complex due to the additional layers of abstraction provided by the CSS-in-JS solution.\n\n**Rationale:** The benefits of using a CSS-in-JS solution with a statically compiled and cached approach, leveraging a design system library and a type-safe programming language, outweigh the potential drawbacks. This approach will provide a performant, safe, and reusable solution for our CSS styles, while also ensuring easy access to our design system constants and type safety.","GenTime":"2024-08-10 08:37:42"}
{"File Name":"handbook\/0009-naming-files-and-directories.md","Context":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n","Decision":"* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","tokens":93,"id":1996,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to adopt the PascalCase naming convention for files and directories in the monorepo. This decision is driven by the need for consistency in navigating the codebase, and the desire to establish a naming strategy early on to avoid potential issues and confusion.\n\n## Rationale\n\nThe PascalCase naming convention is widely used in the industry and is easy to read and understand. It also aligns with the default naming conventions used by NX, which will help to reduce confusion and make it easier to maintain the codebase.\n\n## Consequences\n\n* All new files and directories created in the monorepo will follow the PascalCase naming convention.\n* Existing files and directories will be updated to conform to the new naming convention over time.\n* This decision will provide a consistent and rational way of naming files and directories, making it easier for developers to navigate the codebase.\n* This decision will also help to reduce confusion and make it easier to maintain the codebase over time.\n\n## Alternatives Considered\n\n* We considered using the camelCase naming convention, but it is not as widely used in the industry and may be more difficult to read and understand.\n* We also considered using the underscore notation, but it is not as commonly used in the industry and may be more difficult to read and understand.\n\n## Pros and Cons\n\nPros:\n\n* Consistency in naming conventions will make it easier to navigate the codebase.\n* PascalCase is widely used in the industry and is easy to read and understand.\n* Aligns with default naming conventions used by NX.\n\nCons:\n\n* May require updates to existing files and directories.\n* May require additional effort to maintain consistency in naming conventions.\n\n## Status\n\nThis decision is considered final and will be implemented in the monorepo.","GenTime":"2024-08-10 08:39:04"}
{"File Name":"super-eks\/0007-backup.md","Context":"## Context and Problem Statement\\nIn the current setup of super eks there is no backup solution installed and configured. A backup solution can be helpful for some resources that are not managed by IaC.\\n","Decision":"### Positive Consequences\\nUsers get a state of the art backup technology included with super eks, enabling them to restore a cluster after a\\ncatastropic failure.\\n### Negative Consequences\\nUsers have to think about how the backup needs to be setup and distinguish between resources that have been set up by\\nIaC and manually set up resources (e.g., secrets).\\n","tokens":41,"id":2034,"Prediction":"## Decision\n\n**Decision:** Implement a backup solution for non-IaC managed resources in Super EKS.\n\n**Context:** In the current setup, there is no backup solution installed and configured for non-IaC managed resources in Super EKS, which poses a risk to data loss and system downtime.\n\n**Problem Statement:** The lack of a backup solution for non-IaC managed resources in Super EKS makes it difficult to ensure data integrity and availability in the event of a failure or data corruption.\n\n**Reasoning:** Implementing a backup solution will provide an additional layer of protection for critical data and applications, ensuring business continuity and minimizing the risk of data loss. This decision aligns with the company's goal of maintaining high availability and reliability of its systems.\n\n**Consequences:**\n\n* Additional infrastructure and maintenance costs for the backup solution\n* Increased complexity in managing the backup solution\n* Improved data integrity and availability\n* Reduced risk of data loss and system downtime\n\n**Decision Drivers:**\n\n* Risk of data loss and system downtime\n* Business continuity and reliability\n* Compliance with regulatory requirements (if applicable)\n\n**Alternatives Considered:**\n\n* Implementing a manual backup process\n* Using a third-party backup service\n* Not implementing a backup solution at all\n\n**Rationale:** The chosen solution is a dedicated backup solution that can be easily integrated with the existing infrastructure and managed by the operations team. This approach provides a more robust and reliable backup solution compared to manual processes or third-party services.\n\n**Status:** Implemented","GenTime":"2024-08-10 08:51:30"}
{"File Name":"ReportMI-service-manual\/0018-submission-state-machine.md","Context":"## Context\\nAs outlined in [ADR-0016][adr-0016], the Data Submission Service will use\\n\"submissions\" to describe something that a supplier has submitted to us in order\\nto complete a \"task\".\\nSubmissions will exist in a state machine that outlines what is happening with\\nthem.\\nCurrently, we expect there to be 6 states:\\n* **pending** - a blank submission that is awaiting data\\n* **processing** - a submission where data is being processed\\n(eg a file is being ingested or data is being validated)\\n* **validation_failed** - the submitted data has failed the validation process\\nand needs to be corrected\\n* **validation_passed** - the submitted data has passed the validation process,\\nand the supplier now needs to review the results\\n* **supplier_rejected** - the supplier has reviewed the result of the data\\nprocessing and has rejected it (eg they have realised that their data needs to\\nbe amended)\\n* **supplier_accepted** - the supplier has reviewed the result of the data\\nprocessing and are happy that it is accurate\\nOther states may added in future to cover approval processes, and fixing\\nmistakes in returns.\\n","Decision":"The system will model the 6 states highlighted above.\\nSubmissions containing data will proceed through the states from `pending` to\\n`processing` to either `validation_failed` or `validation_passed`. If the\\nvalidation has passed, the supplier can reject the submission (move to\\n`supplier_rejected`) or accept the submission (move to `supplier_accepted`).\\nA 'no business' submission will proceed straight to `supplier_accepted` once\\nthe supplier has confirmed they wish to make this submission.\\nThis is shown in the following diagram.\\n![Submission state machine diagram](..\/diagrams\/0018-submission-states.jpg)\\n","tokens":250,"id":2039,"Prediction":"## Decision\n\n**Title:** State Machine for Submissions\n\n**Context:** As outlined in [ADR-0016][adr-0016], the Data Submission Service will use \"submissions\" to describe something that a supplier has submitted to us in order to complete a \"task\". Submissions will exist in a state machine that outlines what is happening with them.\n\n**Decision:** We will implement a state machine with the following states:\n\n* **pending** - a blank submission that is awaiting data\n* **processing** - a submission where data is being processed (e.g. a file is being ingested or data is being validated)\n* **validation_failed** - the submitted data has failed the validation process and needs to be corrected\n* **validation_passed** - the submitted data has passed the validation process, and the supplier now needs to review the results\n* **supplier_rejected** - the supplier has reviewed the result of the data processing and has rejected it (e.g. they have realised that their data needs to be amended)\n* **supplier_accepted** - the supplier has reviewed the result of the data processing and are happy that it is accurate\n\n**Rationale:** The state machine is designed to capture the different stages of the submission process, from initial submission to final acceptance or rejection. This will allow us to track the progress of each submission and provide a clear understanding of the current state of each submission.\n\n**Consequences:**\n\n* The state machine will provide a clear and concise way to describe the current state of each submission.\n* It will allow us to implement business logic and rules for each state, ensuring that submissions are processed correctly and consistently.\n* It will provide a clear understanding of the submission process for both internal and external stakeholders.\n\n**Alternatives Considered:**\n\n* We could have used a simpler approach, such as a single \"in_progress\" state, but this would not provide the same level of clarity and understanding of the submission process.\n* We could have used a more complex approach, such as a graph-based state machine, but this would add unnecessary complexity to the system.\n\n**Status:** Implemented","GenTime":"2024-08-10 08:53:23"}
{"File Name":"ReportMI-service-manual\/0011-use-auth0-for-temporary-user-authentication.md","Context":"## Context\\nOur user facing applications will need to authenticate users.\\nIn the long-term, we want to use the standard user sign-on services in CCS for\\neach appropriate user group.\\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\\nFor CCS staff it is their G-Suite account.\\nUsing a single authentication service will reduce administration overheads\\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\\nusers it will also mean they don't have to remember yet another username and\\npassword combination.\\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\\nwill take time. For now, we want a simple sign-in service that we can integrate\\nwith quickly, to enable us to test and iterate the application.\\n","Decision":"We will use [Auth0][service-auth0] for authentication on a temporary basis.\\nAuth0 is an authentication service that uses the OAuth protocol. It provides\\nand simple integration path, and is free for several thousand active users.\\nWe acknowledge that we are incurring technical debt by not integrating\\nimmediately with SRS and G-Suite, however we believe that this will let us move\\nmore quickly during development.\\nThis decision will also allow us to gain a better understanding of the user\\nneeds around authentication before a decision around the final authentication\\nsolution is made.\\nWe will replace Auth0 with the proper authentication service later in the\\ndevelopment process.\\n","tokens":179,"id":2041,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will use the Auth0 service as a temporary authentication solution for our user-facing applications, allowing for quick integration and testing, while planning for a long-term integration with the standard user sign-on services in CCS for each appropriate user group.\n\n**Context**\n\nOur user-facing applications will need to authenticate users. In the long-term, we want to use the standard user sign-on services in CCS for each appropriate user group. For suppliers, this is the Supplier Registration Service (SRS). For CCS staff, it is their G-Suite account. Using a single authentication service will reduce administration overheads because we won't have to manage a JML (Joiners Movers Leavers) process. For users, it will also mean they don't have to remember yet another username and password combination. However, we want to get moving quickly, and integration with SRS\/CCS G-Suite will take time. For now, we want a simple sign-in service that we can integrate with quickly, to enable us to test and iterate the application.\n\n**Rationale**\n\nThe decision to use Auth0 as a temporary authentication solution is based on the need for a quick and simple integration to enable testing and iteration of the application. Auth0 provides a straightforward and widely-used authentication service that can be integrated with our application quickly, allowing us to focus on developing the application's core features. Additionally, Auth0 provides features such as user management, password reset, and multi-factor authentication, which will help to simplify the authentication process for our users.\n\n**Consequences**\n\nThe use of Auth0 as a temporary authentication solution will:\n\n* Allow for quick integration and testing of the application\n* Provide a simple and straightforward authentication process for users\n* Simplify the management of user authentication, reducing the need for manual JML processes\n* Enable us to focus on developing the application's core features\n\nHowever, this decision also means that we will need to plan for a long-term integration with the standard user sign-on services in CCS for each appropriate user group, which will require additional development and testing time.","GenTime":"2024-08-10 08:54:03"}
{"File Name":"ReportMI-service-manual\/0019-store-data-validation-rules-in-json-files.md","Context":"## Context\\nEach framework has a specific description of what data must be submitted in the\\nmonthly MI return. This description outlines what fields should be submitted,\\nwhat data types are accepted and any validation rules which should be applied.\\nAt the moment, these rules are encapsulated in the Excel template which\\nsuppliers download, using drop-downs, tool-tips and sometimes macros.\\nA separate copy of these rules is configured for each framework in MISO, which\\nthe service uses to validate templates have been filed in correctly when\\nuploaded.\\n### Longer term approach\\nIn future, we want a single source for the rules for framework data submissions\\nwhich we can use for various tasks including:\\n- generating submission templates in different formats (eg Excel, ODS, CSV)\\n- validating submitted returns\\n- documenting any APIs\\nThese rules should ideally be stored in a central place and made available to\\nany services which require them via an API. This will allow new services to\\nshare the ruleset and description.\\nThe high-level principle is that there should be an artefact that describes the\\nrules that need to be applied.\\nThe artefacts need to be version controlled so they can be changed during the\\nlifetime of the framework. Services may need to access and used outdated\\nversions of the rules.\\n### Interim approach\\nFor the initial waves of on-boarding, we expect only a small number of\\nframeworks to be submitting monthly returns.\\nWhile we develop our approach, and learn more about the varied rules for each\\nframework, we will use an interim approach for storing the rules.\\nWe will create a JSON file for each framework which outlines the data structure\\nrequired and the validation rules for each field. This JSON file will be stored\\nin a GitHub repository, and used to generate templates and validate submitted\\nfiles.\\nEventually, we will develop an API to provide access to this information.\\n","Decision":"We will create a JSON file for each framework and store it in a GitHub\\nrepository.\\nThis JSON file will describe each field, it's data type and associated\\nvalidation rules.\\nWe will migrate these files to a longer-term solution once we have a better\\nunderstanding of the required rules for each framework.\\n","tokens":391,"id":2045,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will use a JSON file for each framework to store the data structure required and validation rules for each field, and store these files in a GitHub repository. This will be our interim approach for storing the rules until we develop a more robust solution.\n\n**Context**\n\nEach framework has a specific description of what data must be submitted in the monthly MI return, which outlines what fields should be submitted, what data types are accepted, and any validation rules that should be applied. Currently, these rules are encapsulated in the Excel template, but we want a single source for these rules that can be used for various tasks, including generating submission templates, validating submitted returns, and documenting APIs.\n\n**Problem**\n\nWe need a solution that allows us to store and manage the rules for each framework in a central location, while also allowing services to access and use outdated versions of the rules.\n\n**Reasoning**\n\nWe chose the interim approach because it allows us to quickly start onboarding new frameworks and learn more about the varied rules for each framework. Using a JSON file for each framework provides a simple and flexible way to store the rules, and storing them in a GitHub repository allows for version control and collaboration.\n\n**Consequences**\n\n* We will need to maintain a GitHub repository to store the JSON files for each framework.\n* We will need to develop a process for updating the JSON files and ensuring that they are consistent across all frameworks.\n* We will need to develop a solution for generating submission templates and validating submitted files using the JSON files.\n* We will need to eventually develop an API to provide access to the rules and description, which will require additional development and testing.\n\n**Status**\n\nApproved\n\n**Links**\n\n* GitHub repository for storing JSON files\n* API design document for providing access to the rules and description","GenTime":"2024-08-10 08:55:01"}
{"File Name":"ReportMI-service-manual\/0014-use-gov-uk-notify-for-sending-notifications.md","Context":"## Context\\nThe Data Submission Service will need to send notifications to suppliers at\\nvarious stages during the submission process.\\nThe notifications will, for example, include:\\n- Telling a supplier they have a task to complete\\n- Telling a supplier that a deadline is approaching\\n- Telling a supplier that their task is overdue\\n- Providing a receipt for a submission\\nInitially, the notifications will be email, but may also need to be SMS based in\\nfuture.\\nTo reduce the need for each organisation or service team to build it's own\\ninfrastructure for sending email and SMS based messages, the Government Digital\\nService (GDS) has built [GOV.UK Notify][service-notify].\\nGOV.UK Notify can send notifications either via an API call, or by uploading a\\nCSV to the website. Sending email is free.\\nThe service is already used in various parts of CCS.\\n","Decision":"We will use GOV.UK notify to send notifications for Data Submission Service.\\nFor MVP we may use the CSV upload function to send notifications, but this will\\nbe replaced by automated API calls later in development.\\n","tokens":188,"id":2050,"Prediction":"## Decision\n\nThe Data Submission Service will utilize the Government Digital Service's (GDS) GOV.UK Notify service to send notifications to suppliers at various stages during the submission process.\n\n## Context\n\nThe Data Submission Service needs to send notifications to suppliers at various stages during the submission process, including task completion, deadline approaching, task overdue, and submission receipt. Initially, notifications will be email-based, but may also need to be SMS-based in the future. To reduce the need for each organization or service team to build its own infrastructure for sending email and SMS-based messages, we will utilize the GDS's GOV.UK Notify service.\n\n## Rationale\n\nThe use of GOV.UK Notify will provide several benefits, including:\n\n* Reduced infrastructure costs and complexity by leveraging an existing service\n* Improved consistency and standardization across the organization\n* Ability to send notifications via API calls or CSV uploads, making it easy to integrate with the Data Submission Service\n* Free email sending, with the option to use SMS-based notifications in the future\n* Existing usage in various parts of CCS, ensuring familiarity and support\n\n## Consequences\n\n* Integration with GOV.UK Notify will require additional development and testing to ensure seamless communication between the Data Submission Service and the notification service.\n* The Data Submission Service will need to adhere to GOV.UK Notify's guidelines and best practices for sending notifications.\n* The use of GOV.UK Notify may require additional configuration and setup, such as setting up notification templates and testing notification flows.\n\n## Status\n\nApproved\n\n## Date\n\n[Insert Date]\n\n## Links\n\n[GOV.UK Notify](https:\/\/www.gov.uk\/government\/publications\/govuk-notify-service-manual)","GenTime":"2024-08-10 08:56:48"}
{"File Name":"ReportMI-service-manual\/0006-use-terraform-to-create-and-document-infrastructure.md","Context":"## Context\\nRunning a service like the Data Submission Service requires infrastructure of\\nvarious kinds.\\nIt's good practice to manage this infrastructure using code (Infrastructure as\\nCode), as this allows the infrastructure to be version-controlled and managed\\nlike the rest of the application code.\\nThere are various mechanisms for doing this. Each of the main cloud providers\\nhave their own solutions to manage infrastructure in code, for example Amazon\\nWeb Services (AWS) has CloudFormation, Microsoft Azure has Resource Manager etc.\\nHowever each of these are specific to the individual cloud provider.\\nIt would also be possible to do this manually by running scripts against the\\ncloud provider API. However, this would take a significant amount of time and\\nwould take effort to make it work across more than one cloud provider.\\nThere are tools available, like Terraform, which allow you to define\\ninfrastructure as code, in a standard way which can then be applied against more\\nthan one cloud provider. The tool handles the differences between providers.\\n","Decision":"We will use Terraform to create the infrastructure for the Data Submission\\nService.\\n","tokens":208,"id":2062,"Prediction":"Here is the Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\nWe will use Terraform as our Infrastructure as Code (IaC) tool to manage the infrastructure for the Data Submission Service, allowing us to define and version-control the infrastructure configuration in a standard way that can be applied across multiple cloud providers.\n\n**Context**\n\nAs described in the context, managing infrastructure using code is a good practice that allows for version-control and management of infrastructure like application code. However, each cloud provider has its own solution for managing infrastructure in code, which can lead to vendor lock-in and complexity. Using Terraform, we can define infrastructure as code in a standard way that can be applied against multiple cloud providers, handling the differences between providers.\n\n**Rationale**\n\nUsing Terraform will allow us to:\n\n* Define infrastructure configuration in a standard way that can be applied across multiple cloud providers\n* Version-control infrastructure configuration, allowing for easy tracking of changes and rollbacks\n* Reduce the complexity of managing infrastructure across multiple cloud providers\n* Improve collaboration and reuse of infrastructure configuration across teams and projects\n\n**Consequences**\n\n* We will need to learn and master Terraform syntax and configuration\n* We will need to create a Terraform configuration for each cloud provider we want to support\n* We will need to ensure that our Terraform configuration is properly version-controlled and managed\n\n**Status**\n\nThis decision is currently in the \"Proposed\" status and will be reviewed and finalized before implementation.\n\n**References**\n\n* Terraform documentation: <https:\/\/www.terraform.io\/docs\/>\n* Terraform cloud provider documentation: <https:\/\/www.terraform.io\/docs\/providers\/>","GenTime":"2024-08-10 09:00:51"}
{"File Name":"application-services\/0004-early-startup-experiments.md","Context":"## Context and Problem Statement\\nAs an experimenter, I would like to run experiments early on a user's first run of the application. However, the experiment data is only available on the second run. We would like to have that experiment data available before the user's first run.\\nFor more information: https:\/\/docs.google.com\/document\/d\/1Qw36_7G6XyHvJZdM-Hxh4nqYZyCsYajG0L5mO33Yd5M\/edit\\n## Decision Drivers\\n* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\n","Decision":"* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\nNone of the options were feasible, so for now we are sticking with option **(A) Do Nothing** until there are experiments planned that are expected to run on early startup on the first run, then we will revaluate our options.\\nThe **(B) Bundle Experiment data with app on release** option was rejected mainly due to difficulty in disabling experiments and pausing enrollments. This can create a negative user experience as it prevents us from disabling any problematic experiments. Additionally, it ties experiment creation with application release cycles.\\nThe **(C) Retrieve Experiment data on first run, and deal with delay** option was rejected due to the fact it changes the Nimbus SDK will no longer be idempotent,and the possibility of introducing undesirable UI.\\n","tokens":170,"id":2067,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context and problem statement:\n\n**Decision**: Use a caching mechanism to store experiment data and retrieve it on the first run of the application, rather than waiting for the second run.\n\n**Context**: As an experimenter, we want to run experiments early on a user's first run of the application. However, the experiment data is only available on the second run, which is not ideal.\n\n**Decision Drivers**:\n\n* Availability of experiments early on the first run: By caching the experiment data, we can make it available on the first run, providing a better user experience.\n* No impact on experimentation data analysis: The caching mechanism will not affect the analysis of the experiment data, ensuring that our analysis remains accurate.\n* Flexibility in creating experiments: Caching allows us to create experiments without worrying about the availability of data on the first run.\n* Ability to quickly disable experiments: If needed, we can quickly disable experiments by clearing the cache, without affecting the experiment data.\n* Simplicity of releases: Implementing caching will not add significant complexity to our releases, making it a low-risk decision.\n* Mobile's expectations of Nimbus (The SDK should be idempotent): Caching ensures that the SDK behaves idempotently, as the same experiment data will be retrieved on subsequent runs.\n\n**Status**: Accepted\n\n**Implementation**: We will implement a caching mechanism using a suitable caching library or framework, such as Redis or Memcached, to store the experiment data. The caching mechanism will be designed to retrieve the data on the first run and store it for future runs.\n\n**Rationale**: Caching allows us to provide a better user experience by making experiment data available on the first run, while also ensuring that our analysis remains accurate and that we can quickly disable experiments if needed.","GenTime":"2024-08-10 09:02:35"}
{"File Name":"ng-kaart\/0005-links-in-tabel-tonen.md","Context":"## Context\\nURL's worden gestyled zodat deze clickable zijn in het Identify paneel. We willen graag dat ze ook bruikbaar zijn in de tabel.\\nUit de lagen.json heb ik afgeleid dat sommige van deze links veldtype URL hebben en andere veldtype String.\\nWe willen deze URL's graag op dynamische wijze tonen in de tabel (niet gewoon de URL zelf, maar ook niet telkens gewoon LINK).\\nEen groot deel kan getoond worden op basis van de waarde in een ander veld bv dossiernummer bij AV.\\nVoor andere lagen mag er een vaste string getoond worden.\\nEn voor sommige lagen een combinatie van 2 velden als label.\\n","Decision":"Momenteel detecteren we links automatisch door naar de start van de string of naar veldtype te kijken. Als het met http of https begint of\\nveldtype 'url' heeft beelden we het af als een link.\\nWe houden voor de tabel vanaf nu rekening met 'html' veld. Indien ingevuld, dan gebruiken we de inhoud van dat html veld om de weergave af te beelden.\\nWe staan toe dat er tokens instaan om bepaalde velden dynamisch\\nin te vullen. Bvb:\\n{ \"isBasisVeld\": true, \"label\": \"Rapport\", \"naam\": \"rapport_url_1\", \"veldType\": \"url\", \"html\": \"{{periode_1_begin}} - {{periode_1_einde}}\" },\\nIndien 'html' leeg is, nemen we de waarde in 'label'.\\nMet het veld \"constante\" moet ook rekening gehouden worden, dat moet soms genomen worden ipv de waarde van het veld. Bvb:\\n{\\n\"isBasisVeld\": true,\\n\"label\": \"Open in Werf\",\\n\"naam\": \"externeurl\",\\n\"veldType\": \"string\",\\n\"html\": \"{werfid}\",\\n\"constante\": \"https:\/\/{domain.name}\/werf\/schermen\/werf\/{werfid};werf=werf%2Fapi%2Fwerf%2F{werfid}\"\\n},\\n","tokens":172,"id":2074,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will implement a custom URL rendering mechanism in the Identify panel and table that allows for dynamic and flexible URL rendering based on the type of URL and the values in other fields.\n\n**Context**\n\nWe have URLs that need to be styled to make them clickable in the Identify panel, and we also want to make them usable in the table. The URLs are stored in a JSON file with varying data types, including URL and String. We want to render these URLs in a dynamic way in the table, without simply showing the URL itself or a generic \"LINK\" text. For some layers, the URL can be rendered based on the value in another field, such as a dossier number for AV. For other layers, a fixed string can be displayed. For some layers, a combination of two fields can be used as a label.\n\n**Reasoning**\n\nImplementing a custom URL rendering mechanism will allow us to provide a more user-friendly and flexible experience for our users. By rendering URLs based on the type of URL and the values in other fields, we can provide a more tailored and relevant experience for our users. This will also allow us to handle the varying data types and requirements for each layer in a more efficient and scalable way.\n\n**Consequences**\n\n* We will need to implement a custom URL rendering mechanism that can handle the varying data types and requirements for each layer.\n* We will need to update our JSON file to include additional metadata about the URL fields, such as the type of URL and the field(s) that should be used to render the URL.\n* We will need to update our Identify panel and table components to use the custom URL rendering mechanism.\n* We may need to perform additional testing and validation to ensure that the custom URL rendering mechanism is working correctly and is not vulnerable to security risks.\n\n**Status**\n\nProposed\n\n**Decision Owner**\n\n[Your Name]\n\n**Reviewers**\n\n[List of reviewers, including at least one technical reviewer and one non-technical reviewer]\n\n**Revisions**\n\n[Revision history, including dates and descriptions of changes]\n\nPlease let me know if you want me to make any changes.","GenTime":"2024-08-10 09:04:55"}
{"File Name":"ToPS\/0001-extra-test-class-for-junit-test-with-SWI-Prolog-involved.md","Context":"## Context and Problem Statement\\nTests including Prolog queries with SWI Prolog can not be executed with CircleCI because SWI Prolog is not\\nrunning at the executing system.\\n","Decision":"Chosen option: extra test class to reduce the effort and to still enable testing SWI Prolog locally. For this the extra class is\\nannotated with @Ignore. All other tests run with CircleCI. These tests includes the core functionalities of the developed tool.\\n","tokens":38,"id":2080,"Prediction":"Here is an Architectural Decision Record (ADR) for the problem:\n\n**Decision:** Run Prolog tests using Docker containers on CircleCI\n\n**Context:** We have tests that include Prolog queries with SWI Prolog, which cannot be executed on CircleCI because SWI Prolog is not running on the executing system.\n\n**Problem Statement:** We need to find a way to execute Prolog tests on CircleCI without having to install and maintain a Prolog environment on the CircleCI server.\n\n**Decision Rationale:** Running Prolog tests using Docker containers on CircleCI provides a flexible and maintainable solution to this problem. By using Docker, we can create a Prolog environment within a container that can be easily spun up and torn down as needed, without affecting the underlying CircleCI infrastructure. This approach also allows us to use the same Prolog version across all environments, ensuring consistency and reproducibility of test results.\n\n**Decision Description:** We will create a Docker container image that includes SWI Prolog and use it to run Prolog tests on CircleCI. We will use the `docker` command in our CircleCI configuration file to spin up the container and run the tests. We will also use a `docker-compose` file to define the dependencies and environment variables required by the Prolog tests.\n\n**Consequences:**\n\n* Pros:\n\t+ Provides a consistent and reproducible Prolog environment across all environments\n\t+ Allows us to use the same Prolog version across all environments\n\t+ Easy to maintain and update the Prolog environment\n* Cons:\n\t+ Adds complexity to the CircleCI configuration file\n\t+ Requires additional resources (CPU and memory) to run the Docker container\n\n**Status:** Accepted\n\n**Date:** [Insert date]\n\n**Links:** [Insert links to related documentation or code]","GenTime":"2024-08-10 09:07:08"}
{"File Name":"portfolio\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\nEven though this project is mainly a 1 engineer side-project, recording architectural decisions allows using the records as a reminder of the \"state of mind\" we were in when making a decision, as well as the motivations behind it.\\nAdditionally, it's a good engineering practice that would be nice to try out, and this blog is a great \"guinea pig\" for this approach.\\nSee this article on [GitHub's engineering blog](https:\/\/github.blog\/2020-08-13-why-write-adrs\/) for some addition reasons why they could be a good practice.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.\\n","tokens":135,"id":2091,"Prediction":"## Decision\n\n**Use Architectural Decision Records (ADRs) to document key decisions made during the development of the project**\n\n## Context\n\nAs a single-engineer side-project, it's essential to maintain a record of the architectural decisions made during the development process. This will allow for future reference and understanding of the thought process behind each decision, as well as serve as a reminder of the \"state of mind\" at the time of the decision. Additionally, this practice is considered a good engineering habit and will provide a valuable learning experience.\n\n## Decision\n\nWe will use Architectural Decision Records (ADRs) to document key decisions made during the development of the project. This will include decisions on technology choices, design patterns, and other significant architectural choices. Each ADR will include the following information:\n\n* A unique identifier for the decision\n* A title and brief summary of the decision\n* The context and motivation behind the decision\n* The decision itself, including any alternatives considered and the rationale for the chosen approach\n* The expected impact of the decision on the project\n* The date and author of the decision\n\n## Consequences\n\nUsing ADRs will provide several benefits, including:\n\n* Improved understanding of the project's architecture and design decisions\n* Enhanced transparency and accountability for decision-making\n* A record of the thought process behind each decision, allowing for future reference and learning\n* A good engineering practice that can be applied to future projects\n\n## Rationale\n\nRecording architectural decisions is a valuable practice that can provide numerous benefits, including improved understanding of the project's architecture and design decisions, enhanced transparency and accountability for decision-making, and a record of the thought process behind each decision. By using ADRs, we can ensure that our project is well-documented and that our decisions are well-reasoned and transparent.","GenTime":"2024-08-10 09:10:46"}
{"File Name":"claim-additional-payments-for-teaching\/0004-deployment-on-heroku.md","Context":"## Context\\nDepartment for Education have a Cloud Infrastructure Program based on Azure that\\nthey would like digital services to use. Access to Azure is heavily restricted\\nfor production, and slightly restricted for lower environments.\\nWe need to be able to work quickly, particularly in the early stages of this\\nproject.\\nWe need to be able to deploy prototypes and experimental features and versions\\nof the service for user research.\\n","Decision":"We will use Heroku to deploy the application.\\nWe will use Heroku's pipeline feature to run CI and deploy the application.\\n","tokens":84,"id":2093,"Prediction":"## Decision\n\n**Decision:** To use a combination of Azure DevOps and Azure Resource Manager (ARM) templates to quickly deploy and manage infrastructure for the digital services project, while ensuring compliance with the Department for Education's Azure access restrictions.\n\n**Rationale:**\n\n* To work quickly and efficiently, we need a solution that allows us to rapidly deploy and manage infrastructure, without compromising security and compliance.\n* Azure DevOps provides a robust platform for continuous integration and continuous deployment (CI\/CD), which will enable us to automate the build, test, and deployment of our digital services.\n* ARM templates will allow us to define and deploy infrastructure as code, ensuring consistency and reproducibility across environments. This will also enable us to quickly spin up and tear down environments as needed, without requiring manual intervention.\n* By using ARM templates, we can also ensure that our infrastructure deployments are compliant with the Department for Education's Azure access restrictions, by defining and enforcing specific access controls and permissions.\n* This approach will also enable us to quickly deploy prototypes and experimental features and versions of the service for user research, without compromising the security and integrity of our production environment.\n\n**Consequences:**\n\n* We will need to invest time and resources in setting up and configuring Azure DevOps and ARM templates.\n* We will need to ensure that our team members have the necessary skills and knowledge to work with Azure DevOps and ARM templates.\n* We will need to regularly review and update our ARM templates to ensure that they remain compliant with the Department for Education's Azure access restrictions.\n\n**Status:** Approved\n\n**Implementation:** We will implement Azure DevOps and ARM templates in the next two weeks, and will provide training and support to the team to ensure a smooth transition. We will also establish a process for regularly reviewing and updating our ARM templates to ensure compliance with the Department for Education's Azure access restrictions.","GenTime":"2024-08-10 09:11:24"}
{"File Name":"claim-additional-payments-for-teaching\/0009-capture-teacher-reference-number.md","Context":"## Context\\nA claimant\u2019s eligibility is, in part, determined by their qualifications. We\\nwant to be able to validate that a claimant\u2019s qualifications match those of the\\neligibility criteria.\\n","Decision":"To aid DfE in the process of validating a claimant\u2019s qualifications, we will\\ncollect the claimant\u2019s \u2018Teacher Reference Number\u2019 or TRN.\\nWith the TRN, DfE can use the Database of Qualified Teachers\\n([DQT](https:\/\/teacherservices.education.gov.uk\/SelfService\/Login)) to validate\\na claimant\u2019s qualifications.\\n","tokens":42,"id":2098,"Prediction":"## Decision\n\n**Decision:** We will implement a separate EligibilityCriteriaQualificationValidator service to validate the claimant's qualifications against the eligibility criteria.\n\n## Context\n\nA claimant's eligibility is, in part, determined by their qualifications. We want to be able to validate that a claimant's qualifications match those of the eligibility criteria.\n\n## Problem\n\nThe current implementation of the eligibility validation process is scattered throughout the application, making it difficult to maintain and test. We need a more centralized and modular approach to ensure that the validation process is robust and scalable.\n\n## Reasoning\n\nBy introducing a separate EligibilityCriteriaQualificationValidator service, we can:\n\n* Decouple the eligibility validation logic from the rest of the application, making it easier to maintain and test.\n* Provide a clear and concise interface for validating qualifications against eligibility criteria, reducing complexity and improving code readability.\n* Allow for easier integration with future changes to the eligibility criteria or qualification requirements.\n\n## Consequences\n\n* The introduction of a new service will require additional development and testing efforts.\n* The service will need to be properly documented and maintained to ensure that it remains accurate and up-to-date.\n* The service will provide a clear and consistent interface for validating qualifications, making it easier for developers to work with and maintain the code.\n\n## Alternatives Considered\n\n* Implementing a custom validation logic within the existing application code.\n* Using a third-party library or service for eligibility validation.\n* Creating a separate module or package for eligibility validation.\n\n## Decision Criteria\n\n* Modularity and maintainability\n* Code readability and simplicity\n* Scalability and performance\n* Future-proofing and flexibility\n\nBy choosing to implement a separate EligibilityCriteriaQualificationValidator service, we can ensure that our eligibility validation process is robust, scalable, and maintainable, while also providing a clear and concise interface for developers to work with.","GenTime":"2024-08-10 09:13:05"}
{"File Name":"claim-additional-payments-for-teaching\/0003-use-rubocop-for-linting.md","Context":"## Context\\nWe need to lint our Ruby code, the team has previous experience working with\\nRuboCop.\\n","Decision":"We will use RuboCop for linting our Ruby code\\n","tokens":24,"id":2101,"Prediction":"## Decision\n\n**Linting Ruby Code with RuboCop**\n\n## Context\n\nAs we need to lint our Ruby code, the team has previous experience working with RuboCop. This decision aims to leverage this experience and utilize RuboCop for its robust features and customization capabilities.\n\n## Problem\n\nOur Ruby codebase lacks a consistent coding standard, leading to potential errors, inconsistencies, and difficulties in code maintenance. We need a reliable tool to enforce coding standards, detect errors, and improve code quality.\n\n## Decision\n\nWe will use RuboCop to lint our Ruby code, leveraging its extensive set of rules, flexibility, and customization options. This decision is based on the team's existing experience with RuboCop, which will facilitate a smoother adoption process.\n\n## Rationale\n\n1. **Familiarity**: The team is already familiar with RuboCop, which reduces the learning curve and enables a faster implementation.\n2. **Customizability**: RuboCop offers a wide range of configuration options, allowing us to tailor the linter to our specific coding standards and project requirements.\n3. **Robustness**: RuboCop is a well-maintained and widely used tool, ensuring that it will continue to support our codebase and adapt to future Ruby versions.\n4. **Error detection**: RuboCop's extensive set of rules will help detect and prevent errors, improving code quality and reducing the likelihood of bugs.\n\n## Consequences\n\n1. **Improved code quality**: RuboCop will help enforce consistent coding standards, reducing errors and improving overall code quality.\n2. **Easier code maintenance**: With RuboCop, the team will be able to quickly identify and fix errors, making code maintenance more efficient.\n3. **Customization**: The team will have the flexibility to adapt RuboCop to their specific needs, ensuring that the linter is tailored to our project requirements.\n\n## Status\n\nThis decision is approved and will be implemented in the next development cycle.","GenTime":"2024-08-10 09:14:04"}
{"File Name":"eq-author-app\/0002-routing-mk2.md","Context":"## Context\\nRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.\\nAs these conditions can be any boolean expression we need a more complex routing structure to handle\\n","Decision":"Implement a new version of the routing in the API that can handle nested expressions.\\nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2\\n### Terminology\\n- Routing - all rules etc for a page with a fallback destination when no rules match.\\n- Rule - a series of expressions that when evaluated to true will go to the destination specified.\\n- Expression - Either a binary expression or expression group\\n- Expression Group - A group of BinaryExpressions combined as either `and` or `or`.\\n- Binary Expression - The base item with a left hand side, condtion and right hand side.\\n- Left hand side - The item being compared against (e.g. a question)\\n- Condition - A way of comparing left and right e.g. `=`, `>`, `>=`, `includes`, `not_includes`\\n- Right hand side - The value being compared against a left.\\n### Example\\nOn page 1, when the user enters 5 go to page 2, when they select red or white go to page 3, otherwise go to page 4.\\n```\\npage: {\\ntitle: \"Page 1\",\\nanswers: [\\n{ id: \"answer1\", type: NUMBER, label: \"Answer\" }\\n],\\nrouting: {\\nrules: [\\n{\\nexpressionGroup: {\\noperator: \"AND\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer1\",\\ntype: CURRENCY\\n}\\n},\\ncondition: \"Equals\",\\nright: {\\nnumber: 5\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page2\"\\n}\\n}\\n},\\n{\\nexpressionGroup: {\\noperator: \"OR\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer2\",\\ntype: RADIO\\n}\\n},\\ncondition: \"OneOf\",\\nright: {\\noptions: [\\n{\\nvalue: \"red\"\\n},\\n{\\nvalue: \"white\"\\n}\\n]\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page3\"\\n}\\n}\\n}\\n],\\nelse: {\\npage: {\\nid: \"Page4\"\\n}\\n}\\n}\\n}\\n```\\n","tokens":51,"id":2111,"Prediction":"## Decision\n\n**Decision:** Implement a dynamic routing system using a graph-based approach to handle complex boolean expressions for routing conditions.\n\n## Context\n\nRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page. As these conditions can be any boolean expression, we need a more complex routing structure to handle.\n\n## Problem\n\nThe current routing system is limited to simple, predefined routes, which is not sufficient to handle complex boolean expressions. This makes it difficult to implement dynamic and flexible routing rules.\n\n## Reasoning\n\nA graph-based approach allows us to represent complex boolean expressions as nodes and edges in a graph, making it easier to evaluate and navigate the routing conditions. This approach provides more flexibility and scalability than a traditional, hardcoded routing system.\n\n## Consequences\n\n* The system will be able to handle complex boolean expressions for routing conditions.\n* The routing system will be more dynamic and flexible, allowing for easier maintenance and updates.\n* The system will require more complex data structures and algorithms to manage the graph-based routing system.\n\n## Rationale\n\nA graph-based approach provides a more natural way to represent complex boolean expressions, making it easier to evaluate and navigate the routing conditions. This approach also allows for more flexibility and scalability, making it easier to add new routing rules and conditions in the future.\n\n## Alternatives Considered\n\n* Implementing a traditional, hardcoded routing system with a large number of predefined routes.\n* Using a rule-based system with a simple syntax for defining routing conditions.\n\nBoth of these alternatives are less effective than the graph-based approach, as they are limited in their ability to handle complex boolean expressions and are less flexible and scalable.","GenTime":"2024-08-10 09:16:22"}
{"File Name":"james\/0005-distributed-task-termination-ackowledgement.md","Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n","Decision":"* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n","tokens":54,"id":2121,"Prediction":"## Decision\n\nTo enable the execution of `Task`s on any node of the cluster and to notify blocking clients of termination events, we will implement a distributed task management system using a message broker (e.g., Apache Kafka, RabbitMQ) and a distributed lock mechanism (e.g., ZooKeeper, etcd).\n\n## Context\n\nAs mentioned, the task manager needs to be switched to a distributed implementation to support executing tasks on any node of the cluster. This requires a way for nodes to be signaled of any termination event so that we can notify blocking clients.\n\n## Decision Rationale\n\nUsing a message broker and a distributed lock mechanism provides several benefits:\n\n* Decouples task execution from specific nodes, allowing tasks to be executed on any available node in the cluster.\n* Enables nodes to be notified of termination events, allowing for timely notification of blocking clients.\n* Provides a scalable and fault-tolerant solution, as the message broker and distributed lock mechanism can be horizontally scaled and replicated for high availability.\n* Allows for easy integration with existing infrastructure and tools.\n\n## Trade-Offs\n\n* Additional complexity introduced by the message broker and distributed lock mechanism.\n* Requires additional infrastructure and resources (e.g., servers, storage) to support the message broker and distributed lock mechanism.\n\n## Consequences\n\n* The distributed task management system will be more complex and require additional infrastructure and resources.\n* The system will be more scalable and fault-tolerant, allowing it to support a larger number of tasks and nodes.\n* The system will be able to notify blocking clients of termination events in a timely manner.\n\n## Status\n\nThis decision has been approved and will be implemented in the next release.\n\n## Links\n\n* Apache Kafka: <https:\/\/kafka.apache.org\/>\n* RabbitMQ: <https:\/\/www.rabbitmq.com\/>\n* ZooKeeper: <https:\/\/zookeeper.apache.org\/>\n* etcd: <https:\/\/etcd.io\/>","GenTime":"2024-08-10 09:19:19"}
{"File Name":"james\/0015-objectstorage-blobid-list.md","Context":"## Context\\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\\nHowever writing in Object storage:\\n- Takes time\\n- Is billed by most cloud providers\\nThus choosing a right strategy to avoid writing blob twice is desirable.\\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\\n","Decision":"Rely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":166,"id":2122,"Prediction":"## Decision\n\nTo address the issue of duplicated blob writes and reduce the cost and latency associated with writing to Object Storage, we will implement a cache layer in front of the Object Storage to store frequently accessed blobs. This cache layer will be implemented using Redis, a fast and scalable in-memory data store.\n\n## Context\n\nAs mentioned in the context, the mail traffic is heavily duplicated, causing the same blobs to be written multiple times. This not only increases the write latency but also incurs additional costs due to the billing model of most cloud providers. The `exist` method of OpenStack Swift, which is used to check if a blob already exists, is not efficient enough to be a real cost and performance saver.\n\n## Problem\n\nThe current implementation of BlobStore on top of Object Storage, which relies on overwriting blobs, is not optimal due to the time and cost associated with writing to Object Storage. The `exist` method of OpenStack Swift is not efficient enough to be a real cost and performance saver.\n\n## Solution\n\nWe will implement a cache layer in front of the Object Storage using Redis. This cache layer will store frequently accessed blobs, allowing us to quickly check if a blob already exists before writing it to Object Storage. This will reduce the number of writes to Object Storage, resulting in cost savings and improved performance.\n\n## Rationale\n\nThe choice of Redis as the cache layer is based on its ability to provide fast and scalable in-memory data storage. Redis is well-suited for this use case due to its high performance, low latency, and ability to handle a large number of requests.\n\n## Consequences\n\nBy implementing a cache layer in front of the Object Storage, we can expect the following consequences:\n\n* Reduced write latency to Object Storage\n* Cost savings due to reduced writes to Object Storage\n* Improved performance due to reduced latency\n* Simplified implementation of the BlobStore due to the use of a proven caching technology\n\nHowever, there may also be some consequences to consider:\n\n* Additional complexity due to the introduction of a new technology (Redis)\n* Increased memory usage due to the cache layer\n* Potential issues with cache invalidation and consistency in a distributed system.","GenTime":"2024-08-10 09:19:43"}
{"File Name":"james\/0002-make-taskmanager-distributed.md","Context":"## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n","Decision":"Create a distribution-aware implementation of `TaskManager`.\\n","tokens":102,"id":2127,"Prediction":"## Decision\n\nTo address the issue of having a distributed version of James with homogeneous task management, we will introduce a centralized `TaskRegistry` that will manage all tasks across the cluster. This registry will be responsible for maintaining a global view of all tasks, allowing for orchestration of task execution at the cluster level.\n\n## Context\n\nAs described, the current implementation of James has each node maintaining its own instance of `TaskManager`, resulting in a lack of coordination and visibility across the cluster. This makes it difficult to schedule tasks across nodes, list or access task details, and manage the overall task workflow.\n\n## Problem\n\nThe current implementation does not provide a unified way to manage tasks across the cluster, leading to:\n\n1. Lack of coordination: Tasks are scheduled and ran on the same node they are scheduled, without considering the availability or load of other nodes in the cluster.\n2. Limited visibility: There is no centralized view of all tasks in the cluster, making it difficult to monitor and manage the overall task workflow.\n3. Inability to orchestrate: The lack of coordination and visibility makes it impossible to orchestrate task execution at the cluster level, which is essential for a distributed system like James.\n\n## Decision\n\nTo address these issues, we will introduce a centralized `TaskRegistry` that will manage all tasks across the cluster. This registry will provide a global view of all tasks, allowing for:\n\n1. Coordination: The registry will allow for coordination of task execution across nodes, ensuring that tasks are scheduled and ran on the most suitable node in the cluster.\n2. Visibility: The registry will provide a centralized view of all tasks in the cluster, enabling monitoring and management of the overall task workflow.\n3. Orchestration: The registry will enable orchestration of task execution at the cluster level, allowing for more efficient and scalable task management.\n\n## Consequences\n\nThe introduction of the `TaskRegistry` will require changes to the existing `TaskManager` implementation, as well as updates to the James architecture to accommodate the new registry. This will also require changes to the way tasks are scheduled and executed, as well as the way task details are accessed and monitored.\n\n## Rationale\n\nThe `TaskRegistry` will provide a unified way to manage tasks across the cluster, enabling coordination, visibility, and orchestration of task execution. This will improve the scalability and efficiency of the James system, making it more suitable for distributed environments.","GenTime":"2024-08-10 09:21:05"}
{"File Name":"james\/0042-james-cli-based-on-webadmin.md","Context":"## Context\\nJames servers offer a command-line interface in order to interact with the server. However, it relies on the JMX protocol, which is known to be insecure. The JMX server embedded in Apache James, also used by the command line client is exposed to a java de-serialization issue according to [NVD-CVE-2017-12628 Detail](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628), and thus can be used to execute arbitrary commands.\\nBesides, the current CLI interface is also not optimal for users. It places actions in front of entities with contiguous syntax, making it harder for the user to remember the command (for example, which entity the GET action command can interact with). If we design to place the entity first and the outgoing actions can interact with that entity afterward, the user will easily imagine what he\/she can do with each entity. This creates an intuitive interface that is easier to remember.\\nWebadmin APIs use HTTP protocol, which is more secure than JMX protocol to interact with James servers.\\nWebadmin command-line interface is an upcoming replacement for the outdated, security-vulnerable JMX command-line interface.\\n","Decision":"We decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.\\n* What libraries will we use?\\n* http client: ***Feign library***. We used it as an http client in other parts of James so we continue to use it.\\n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to the follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.\\n* How will we limit breaking changes this new CLI will cause?\\n* Work on a wrapper to adapt the old CLI API.\\n* Where will we locate this cli code?\\n* server\/protocols\/webadmin-cli\\n* Write a man page.\\n* Picocli generates beautiful documentation for our CLI (HTML, PDF and Unix man pages).\\n* We decided to adopt a more modern, modular CLI syntax:\\n```\\n$ .\/james-cli [OPTION] ENTITY ACTION {ARGUMENT}\\n```\\nwhere\\nOPTION: optional parameter when running the command line,\\nENTITY: represents the entity to perform action on,\\nACTION: name of the action to perform,\\nARGUMENT: arguments needed for the action.\\n#### Examples\\nAdd a domain to the domain list.\\n```\\n$ .\/james-cli --url http:\/\/127.0.0.1:9999 domain create domainNameToBeCreated\\n```\\nIn above command-line\\nOPTION: --url http:\/\/127.0.0.1:9999\\nENTITY: domain\\nACTION: create\\nARGUMENT: domainNameToBeCreated\\n","tokens":244,"id":2133,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will replace the current JMX-based command-line interface with a new web-based interface that uses the HTTP protocol to interact with James servers. This decision is motivated by the security concerns associated with the JMX protocol, as well as the need for a more intuitive and user-friendly interface.\n\n**Context**\n\nThe current JMX-based command-line interface is vulnerable to security issues, such as the java de-serialization issue described in CVE-2017-12628. Additionally, the current interface is not optimal for users, with actions placed in front of entities with contiguous syntax, making it harder for users to remember the command.\n\n**Problem**\n\nThe current JMX-based command-line interface poses a security risk and is not user-friendly.\n\n**Reasoning**\n\nUsing the HTTP protocol, which is more secure than JMX, will provide a more secure way to interact with James servers. Additionally, placing the entity first and the outgoing actions afterward will create an intuitive interface that is easier to remember and use.\n\n**Decision Drivers**\n\n* Security concerns associated with the JMX protocol\n* Need for a more intuitive and user-friendly interface\n* Upcoming replacement for the outdated, security-vulnerable JMX command-line interface\n\n**Consequences**\n\n* The new web-based interface will be more secure and easier to use\n* The development team will need to implement the new interface and migrate existing functionality\n* Users will need to learn the new interface and commands\n\n**Pros and Cons**\n\nPros:\n\n* More secure than JMX protocol\n* More intuitive and user-friendly interface\n* Replacement for outdated and security-vulnerable interface\n\nCons:\n\n* Requires development effort to implement and migrate functionality\n* Users will need to learn the new interface and commands\n\n**Status**\n\nThis decision has been approved and is currently being implemented.","GenTime":"2024-08-10 09:22:48"}
{"File Name":"james\/0013-precompute-jmap-preview.md","Context":"## Context\\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\\n","Decision":"We should pre-compute message preview.\\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\\nWe should have a Cassandra and memory implementation.\\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore\\nis idempotent and the task can be run in live without any concurrency problem.\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":75,"id":2134,"Prediction":"## Decision\n\n**Decision:** Implement a message preview generation mechanism that extracts the first 256 characters of meaningful text from the message body without retrieving the full message body.\n\n**Context:** As described, JMAP messages have a preview property that displays the first 256 characters of meaningful text of a message. This property is frequently queried for message listing in JMAP clients. Currently, James retrieves the full message body, parses it using MIME parsers, removes HTML, and keeps meaningful text to generate the preview.\n\n**Rationale:**\n\n1. **Performance:** Retrieving the full message body and parsing it can be resource-intensive and slow. By generating the preview on the fly, we can reduce the load on the system and improve response times.\n2. **Security:** Retrieving the full message body can potentially expose sensitive information. By generating the preview without retrieving the full message body, we can improve security and reduce the risk of data leakage.\n3. **Efficiency:** The current approach requires parsing the entire message body, which can be unnecessary if the user only needs a preview. By generating the preview on the fly, we can reduce the amount of data processed and improve efficiency.\n\n**Decision Details:**\n\n1. Implement a new method that takes the message body as input and returns the first 256 characters of meaningful text.\n2. Use a library or framework that can extract meaningful text from the message body, such as a text extraction library.\n3. Integrate the new method with the existing code that generates the preview property.\n4. Test the new implementation to ensure it produces accurate and relevant previews.\n\n**Consequences:**\n\n1. Improved performance and response times.\n2. Enhanced security by reducing the amount of sensitive data exposed.\n3. Increased efficiency by reducing the amount of data processed.\n\n**Pros and Cons:**\n\nPros:\n\n* Improved performance and security\n* Increased efficiency\n* Reduced resource usage\n\nCons:\n\n* Additional complexity in implementing the new method\n* Potential for errors in extracting meaningful text\n\n**Monitoring and Feedback:**\n\n* Monitor the system's performance and response times to ensure the new implementation is effective.\n* Collect feedback from users and developers to refine the implementation and improve the accuracy of the preview generation.","GenTime":"2024-08-10 09:23:12"}
{"File Name":"james\/0004-distributed-tasks-listing.md","Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\\n","Decision":"* Read a Cassandra projection to get all `Task`s and their `Status`\\n","tokens":32,"id":2140,"Prediction":"## Decision\n\n**Decision:** We will implement a distributed task list feature that allows users to list all tasks running on the cluster using a distributed query mechanism.\n\n## Context\n\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\n\n## Problem\n\nIn a distributed system, we need to ensure that the task list feature is scalable, fault-tolerant, and provides a unified view of all tasks running on the cluster. The traditional approach of having a centralized task list may not be suitable for a distributed system, as it can become a single point of failure and may not be able to handle the increased load and complexity.\n\n## Rationale\n\nTo address the problem, we will implement a distributed query mechanism that allows us to query all nodes in the cluster and retrieve a list of tasks running on each node. This approach provides several benefits:\n\n* Scalability: The distributed query mechanism can handle a large number of nodes and tasks, making it suitable for large-scale distributed systems.\n* Fault-tolerance: If one node fails, the system can continue to function by querying other nodes in the cluster.\n* Unified view: The distributed query mechanism provides a unified view of all tasks running on the cluster, making it easier to manage and monitor tasks.\n\n## Decision Criteria\n\nWe considered the following decision criteria when making this decision:\n\n* Scalability: The distributed query mechanism allows us to scale the system horizontally by adding more nodes, making it suitable for large-scale distributed systems.\n* Fault-tolerance: The system can continue to function even if one node fails, ensuring high availability.\n* Performance: The distributed query mechanism can provide faster query results by parallelizing the query across multiple nodes.\n* Complexity: The distributed query mechanism adds complexity to the system, but it provides a scalable and fault-tolerant solution for managing tasks in a distributed system.\n\n## Consequences\n\nThe consequences of this decision are:\n\n* Increased complexity: The distributed query mechanism adds complexity to the system, requiring additional development and testing efforts.\n* Improved scalability: The system can scale horizontally by adding more nodes, making it suitable for large-scale distributed systems.\n* Improved fault-tolerance: The system can continue to function even if one node fails, ensuring high availability.\n\n## Implementation\n\nWe will implement the distributed query mechanism using a combination of technologies such as:\n\n* Distributed query language (e.g., SQL)\n* Message broker (e.g., Apache Kafka)\n* Task management system (e.g., Apache Airflow)\n* Load balancer (e.g., HAProxy)\n\nThe implementation will involve the following steps:\n\n1. Design the distributed query mechanism\n2. Implement the distributed query mechanism using the chosen technologies\n3. Test the distributed query mechanism\n4. Integrate the distributed query mechanism with the task management system\n5. Monitor and maintain the distributed query mechanism","GenTime":"2024-08-10 09:25:14"}
{"File Name":"james\/0025-cassandra-blob-store-cache.md","Context":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2145,"Prediction":"## Decision\n\nTo address the performance issues with the BlobStore API, we will implement a hybrid storage solution that combines the strengths of Object Storage and Key-Value Storage.\n\n## Context\n\nAs described in the context, the BlobStore API needs to store raw data with varying sizes and access patterns. Some blobs are small and frequently accessed, while others are larger and less frequently accessed. Object Storage is well-suited for storing large blobs, but it induces high latencies for reading small blobs, which can impact the performance of certain operations.\n\n## Problem\n\nThe current Object Storage solution is not optimized for storing small blobs, resulting in high latencies and slow performance for certain operations. We need a solution that can efficiently store and retrieve small blobs while still taking advantage of Object Storage's strengths for larger blobs.\n\n## Solution\n\nWe will implement a hybrid storage solution that uses a combination of Object Storage and Key-Value Storage. For small blobs that are frequently accessed, we will store them in a Key-Value Store, such as Cassandra, which is optimized for fast reads and writes. For larger blobs that are less frequently accessed, we will store them in Object Storage, which is optimized for storing and retrieving large amounts of data.\n\n## Rationale\n\nBy using a hybrid storage solution, we can take advantage of the strengths of both Object Storage and Key-Value Storage. Key-Value Storage will provide fast and efficient storage and retrieval of small blobs, while Object Storage will provide a scalable and cost-effective solution for storing larger blobs.\n\n## Consequences\n\n* The BlobStore API will be able to efficiently store and retrieve small blobs, reducing latency and improving performance for certain operations.\n* The hybrid storage solution will provide a scalable and cost-effective solution for storing and retrieving raw data with varying sizes and access patterns.\n* The implementation will require additional complexity and overhead to manage the hybrid storage solution, but the benefits will outweigh the costs.\n\n## Decision Owner\n\nThe decision owner for this architectural decision is [Name], the lead architect for the BlobStore API.\n\n## Affected Components\n\n* BlobStore API\n* Object Storage\n* Key-Value Storage (Cassandra)\n\n## Implementation Plan\n\n* Implement a hybrid storage solution that uses a combination of Object Storage and Key-Value Storage.\n* Configure the Key-Value Store to store small blobs that are frequently accessed.\n* Configure the Object Storage to store larger blobs that are less frequently accessed.\n* Implement logic to determine which storage solution to use based on the size and access pattern of the blob.\n\n## Reviewers\n\n* [Name], the lead architect for the BlobStore API\n* [Name], the lead developer for the Object Storage component\n* [Name], the lead developer for the Key-Value Storage component\n\n## Approval\n\nThis architectural decision has been approved by the decision owner and reviewers.","GenTime":"2024-08-10 09:27:24"}
{"File Name":"james\/0040-quality-levels-definitions.md","Context":"## Context\\nWe hereby define as an artifact compiled artifact that external people consumes. This includes:\\n- libraries\\n- Mail servers\\n- Extensions for James Mail Servers\\n- Command line tools\\nWe designate as a feature an optional, opt-in behaviour of a James server that can be configured by\\nuser willing to rely on it.\\nJames as a project delivers several artifacts, and features. In order for project users to better\\nunderstand the underlying quality of the artifact they use, as well as the level of risk associated,\\nwe need to better define some quality levels.\\n","Decision":"For a given artifact or feature, by **mature** we mean that:\\n- *interfaces* in components need a contract test suite\\n- *interfaces* have several implementations\\n- *implementation* of these interfaces need to pass this contract test suite which provides unit tests\\n- Decent integration tests coverage is needed\\n- Performance tests need to be conducted out\\n- Quality Assurance with external clients needs to be conducted out\\n- known existing production deployments\/usages\\n- usable documentation\\nThis is the maximum quality level delivered by the James project. Users should feel confident using these\\nartifacts or features.\\nBy **experimental** we designate an artifact or feature not matching yet the above requirements. However some\\nactive contributors are willing to raise the quality level of this component, and eventually make it\\nmature. Or at least are willing to support users.\\nUsers should have low expectations regarding experimental artifacts or features. They are encouraged to contribute to them\\nin order to raise its quality.\\nBy **unsupported** we mean that an artifact or feature do not match most of the *mature* quality conditions. Active\\ncontributors do not feel confident delivering support for it. This artifact or feature might be deprecated and\\nremoved from future James releases. Users are strongly encouraged to contribute to the artifact development.\\n","tokens":118,"id":2150,"Prediction":"## Decision\n\n**Artifact Quality Levels**\n\n## Context\n\nAs defined, an artifact is a compiled artifact that external people consume, including libraries, mail servers, extensions for James Mail Servers, and command-line tools. Features are optional, opt-in behaviors of a James server that can be configured by users willing to rely on them. James as a project delivers several artifacts and features, and it is essential to define quality levels to help project users better understand the underlying quality of the artifacts they use and the level of risk associated.\n\n## Problem\n\nCurrently, there is no clear definition of quality levels for James artifacts and features, making it challenging for users to assess the quality and risk of the artifacts they consume.\n\n## Decision\n\nTo address this issue, we will introduce a set of quality levels for James artifacts and features, which will provide a standardized way to evaluate and communicate the quality and risk of each artifact and feature. The quality levels will be categorized into the following tiers:\n\n1. **Tier 1: Community-Maintained**: Artifacts and features that are maintained by the James community, with a focus on stability and compatibility.\n2. **Tier 2: Project-Maintained**: Artifacts and features that are maintained by the James project team, with a focus on stability, compatibility, and security.\n3. **Tier 3: Commercial-Maintained**: Artifacts and features that are maintained by commercial entities, with a focus on stability, compatibility, security, and commercial support.\n4. **Tier 4: Experimental**: Artifacts and features that are still in the experimental phase, with a focus on innovation and testing.\n\n## Rationale\n\nThe introduction of quality levels will provide a clear and standardized way to evaluate and communicate the quality and risk of James artifacts and features. This will help users make informed decisions about which artifacts and features to use, and will also help the James project team to prioritize maintenance and development efforts.\n\n## Consequences\n\n* Users will have a clearer understanding of the quality and risk associated with each artifact and feature.\n* The James project team will have a standardized framework for evaluating and communicating the quality and risk of artifacts and features.\n* The introduction of quality levels may lead to increased adoption of high-quality artifacts and features, and decreased adoption of low-quality artifacts and features.\n\n## Status\n\nThis decision is approved and will be implemented in the next release of the James project.","GenTime":"2024-08-10 09:29:20"}
{"File Name":"stentor\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2166,"Prediction":"Here is an Architectural Decision Record (ADR) for the software:\n\n**Decision**\n\n**ADR-001: Choosing a Frontend Framework for the Project**\n\n**Context**\n\nAs we are building a web-based application, we need to decide on a frontend framework to use. The framework should provide a robust and scalable solution for building the user interface and handling user interactions.\n\n**Problem**\n\nWe have multiple options for frontend frameworks, each with its own strengths and weaknesses. We need to make a decision that balances our project requirements with the expertise of our development team.\n\n**Decision**\n\nAfter careful consideration, we have decided to use React as the frontend framework for our project. React provides a robust and scalable solution for building complex user interfaces, and its popularity and community support make it a great choice for our project.\n\n**Rationale**\n\nReact's virtual DOM and one-way data binding make it efficient and easy to manage state changes, which is important for our application's performance. Additionally, React's component-based architecture makes it easy to manage complexity and reuse code. The React community is also very active, which means there are many resources available for learning and troubleshooting.\n\n**Consequences**\n\nWe will need to invest time in learning React and its ecosystem. We will also need to consider the trade-offs of using a JavaScript-based framework, such as potential performance issues and the need for additional libraries to handle server-side rendering.\n\n**Status**\n\nThis decision is currently in implementation, and we are actively working on integrating React into our project.\n\n**Links**\n\n* [React Official Website](https:\/\/reactjs.org\/)\n* [React Documentation](https:\/\/reactjs.org\/docs\/getting-started.html)\n\nI hope this helps! Let me know if you have any further questions.","GenTime":"2024-08-10 09:35:16"}
{"File Name":"opg-data\/0005-content-structure.md","Context":"## Context\\nA consistent and well-defined document specification is required so that we may develop an API contract\\n","Decision":"Our structure closely follows the [JSON-API](https:\/\/jsonapi.org\/format\/#document-structure) document structure\\n### Document Root Level\\nAt the root level is always a JSON object.\\nThis **MUST** contain at least one of the following root-level members:\\n* data: the document's \"primary data\" resource object\\n* A single resource object is represented by a JSON object\\n* A collection or resource objects is represented by an array of objects\\n* errors: an array of error objects\\n#### Single resource object\\n```json\\n{\\n\"data\": {},\\n\"meta\": {}\\n}\\n```\\n#### Collection of resource objects\\n```json\\n{\\n\"data\": [\\n{...},\\n{...}\\n],\\n\"errors\": [],\\n\"meta\": {},\\n\"links\": {}\\n}\\n```\\nJSON-API states\\n> The members data and errors MUST NOT coexist in the same document.\\n`opg-data` standard is that if a data resource OBJECT is returned, then there **MUST be no** error member.\\nHowever there ARE certain circumstances where an array of errors **MAY** be returned alongside a data resource COLLECTION. See [0007-error-handling-and-status-codes.md#errors-in-20x](0007-error-handling-and-status-codes.md#errors-in-20x)\\nThe root JSON object **MAY** also contain the following root-level members:\\n* meta: a meta object that contains non-standard meta-information\\n* links: a links object related to the primary data (typically used for pagination links if the data returned is a collection)\\n### The Resource Object\\nSee [JSON-AP](https:\/\/jsonapi.org\/format\/#document-resource-objects)\\nNamely:\\n* As a minimum, every resource object **MUST** contain:\\n* an id member\\n* a type member. The values of the id and type members **MUST** be strings. For consistency and the avoidance of confusion, types **MUST** use PLURAL. eg \"articles\", \"people\"\\n* an array of attributes (even if empty)\\n* IF the \"data\" top level member is present, then there **MUST** be a `links` array, containing as it's minimum, a `self` member with a URL which **MUST** be callable at the API\\n* A resource object's data attributes is presented in an array named \"attributes\"\\n* A resource object's links is presented in an object named \"links\"\\n* A resource object's relationships is presented in an array named \"relationships\"\\n* A resource object's attributes and its relationships are collectively called its \"fields\"\\n```json\\n{\\n\"data\": [\\n{\\n\"type\": \"articles\",\\n\"id\": \"1\",\\n\"attributes\": {\\n\"title\": \"My First Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/1\",\\n\"next\": \"https:\/\/api.example.com\/articles\/2\"\\n},\\n\"relationships\": {\\n...\\n}\\n},\\n{\\n\"type\": \"articles\",\\n\"id\": \"2\",\\n\"attributes\": {\\n\"title\": \"Second Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/2\",\\n\"prev\": \"https:\/\/api.example.com\/articles\/1\"\\n},\\n\"relationships\": {\\n...\\n}\\n}\\n],\\n\"links\": {},\\n\"meta\": {}\\n}\\n```\\n#### Relationships\\nRelationships will be defined in a separate ADR\\n### Meta\\nSee [JSON-API](https:\/\/jsonapi.org\/format\/#document-meta)\\n### Errors\\nErrors responses will be defined in a separate ADR\\n","tokens":21,"id":2181,"Prediction":"## Decision\n\nTo ensure a consistent and well-defined document specification for developing an API contract, we will adopt the OpenAPI specification (OAS) version 3.0.0 as our standard.\n\n## Context\n\nA consistent and well-defined document specification is required so that we may develop an API contract.\n\n## Problem\n\nWithout a standardized document specification, our API contract may become inconsistent, ambiguous, and difficult to maintain. This could lead to issues such as:\n\n* Inconsistent API endpoint definitions\n* Conflicting parameter definitions\n* Inaccurate documentation\n* Difficulty in testing and debugging\n\n## Decision\n\nWe will adopt the OpenAPI specification (OAS) version 3.0.0 as our standard for defining the API contract. OAS provides a comprehensive and widely-adopted standard for describing RESTful APIs, allowing us to define the API's endpoints, parameters, request\/response bodies, and other aspects of the API in a clear and consistent manner.\n\n## Rationale\n\n* OAS is a widely-adopted standard, making it easier for our team and external developers to understand and work with the API contract.\n* OAS provides a comprehensive set of features for describing APIs, including support for multiple data formats, authentication mechanisms, and more.\n* Using OAS will help us to maintain a consistent and well-defined API contract, reducing the risk of errors and inconsistencies.\n\n## Consequences\n\n* We will need to invest time and effort in learning and implementing the OAS specification.\n* We will need to update our existing API documentation to conform to the OAS specification.\n* We will need to ensure that our API implementation conforms to the OAS specification.\n\n## Status\n\nApproved\n\n## Links\n\n* OpenAPI Specification (OAS) version 3.0.0: <https:\/\/github.com\/OAI\/OpenAPI-Specification\/blob\/master\/versions\/3.0.0.md>","GenTime":"2024-08-10 09:40:16"}
{"File Name":"opg-data\/0009-api-domain-structure.md","Context":"## Context\\nWe need to establish a domain structure which\\n* Is product agnostic\\n* Is consistent across the opg-data service\\n","Decision":"We will adopt the pattern:\\n[pull-request-id].[account-stage].[microservice-domain].api.opg.service.justive.gov.uk\\nWhere [pull-request-id] is for ephemeral dev environments.\\nWhere [account-stage] is the stage in our path to live\/accounts i.e. dev,pre and empty for prod services.\\nWhere [microservice-domain] is the scoped domain of the microservice\/integration in in question.\\n* hey\\n### examples\\n#### root:\\n* https:\/\/api.opg.service.justice.gov.uk\\n#### integration:\\n* https:\/\/deputy-reporting.api.opg.service.justice.gov.uk\\n#### environments per integration:\\n* https:\/\/pre.deputy-reporting.api.opg.service.justice.gov.uk\\n* https:\/\/dev.deputy-reporting.api.opg.service.justice.gov.uk\\n#### pr raised on an environment per integration:\\n* https:\/\/pr-1234.dev.deputy-reporting.api.opg.service.justice.gov.uk\\n","tokens":28,"id":2183,"Prediction":"## Decision\n\n**Domain Structure for Opg-Data Service**\n\n### Context\n\nWe need to establish a domain structure which is product agnostic and consistent across the opg-data service.\n\n### Decision\n\nAfter careful consideration, we have decided to adopt a modular domain structure that separates the opg-data service into distinct domains, each with its own set of entities and value objects. This approach will allow us to maintain a product-agnostic architecture, ensuring that the domain structure is not tied to a specific product or solution.\n\n### Rationale\n\nThe main benefits of this decision are:\n\n1. **Product agnosticism**: By separating the domain structure into distinct domains, we can easily adapt the opg-data service to support different products or solutions without modifying the underlying domain structure.\n2. **Consistency**: A modular domain structure ensures that the opg-data service has a consistent architecture across all products and solutions, making it easier to maintain and evolve the service over time.\n3. **Scalability**: A domain-driven design approach allows us to scale the opg-data service more efficiently, as each domain can be developed and maintained independently.\n\n### Description\n\nThe opg-data service will be organized into the following domains:\n\n* **Data Domain**: Responsible for managing and storing data, including entities such as `DataEntity`, `DataValue`, and `DataRelationship`.\n* **Business Domain**: Responsible for defining business logic and rules, including entities such as `BusinessRule`, `BusinessProcess`, and `BusinessEntity`.\n* **Infrastructure Domain**: Responsible for managing infrastructure and technical aspects, including entities such as `InfrastructureComponent`, `InfrastructureConfiguration`, and `InfrastructureService`.\n\nEach domain will have its own set of value objects and entities, and will be developed and maintained independently. This will allow us to evolve the opg-data service in a more modular and scalable way, while ensuring that the architecture remains consistent across all products and solutions.\n\n### Consequences\n\nThis decision will require us to:\n\n1. Create a separate package for each domain, with its own set of entities, value objects, and business logic.\n2. Define clear boundaries and interfaces between domains to ensure consistency and scalability.\n3. Develop and maintain each domain independently, with minimal dependencies on other domains.\n\nBy adopting a modular domain structure, we can ensure that the opg-data service is scalable, maintainable, and product-agnostic, while also providing a clear and consistent architecture for all products and solutions.","GenTime":"2024-08-10 09:41:04"}
{"File Name":"opg-data\/0008-hateoas.md","Context":"## Context\\nWe wish to build a RESTful API, and to be technically RESTful, we must implement HATEOAS. This gets us to stage 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","Decision":"Our API is to be as RESTful as it can be.\\nWe recognise that we must implement Hypermedia as the Engine of Application State.\\nThe content structure already discussed in [0005-content-structure.md](0005-content-structure.md) implements links within our resource object... this ADR is to explicitly state our aim is to implement HATEOAS.\\nFurthermore, for every endpoint on the API we will implement an OPTIONS verb, returning every action possible at that endpoint.\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","tokens":88,"id":2184,"Prediction":"## Decision\n\n**Implement Hypermedia as the Engine of Application State (HATEOAS) using JSON-LD in the RESTful API**\n\n## Context\n\nAs we aim to build a technically RESTful API, we need to implement HATEOAS to provide a complete and self-contained representation of the resources in the API. This will allow clients to navigate and interact with the API without having to hardcode URLs or use a fixed API schema.\n\n## Problem\n\nTo achieve HATEOAS, we need to include hypermedia controls in the API responses that allow clients to discover and interact with the resources. This requires a standardized way to represent the hypermedia controls and provide a clear and consistent API design.\n\n## Solution\n\nWe will implement HATEOAS using JSON-LD (JSON for Linked Data) to represent the hypermedia controls in the API responses. JSON-LD is a lightweight and flexible format that allows us to embed metadata and links in the JSON data. This will enable clients to discover and interact with the resources in a self-contained and standardized way.\n\n## Consequences\n\n* The API responses will include JSON-LD metadata that provides hypermedia controls, such as links to related resources and actions that can be performed on the resources.\n* Clients will need to parse the JSON-LD metadata to discover and interact with the resources, which may require additional processing and logic.\n* The API design will be more flexible and scalable, as clients can dynamically discover and interact with the resources without relying on fixed URLs or API schemas.\n* The use of JSON-LD will provide a clear and consistent way to represent the hypermedia controls, making it easier for clients to understand and interact with the API.\n\n## Rationale\n\nImplementing HATEOAS using JSON-LD will provide a robust and scalable solution for our RESTful API. By providing a standardized way to represent the hypermedia controls, we can ensure that clients can interact with the API in a consistent and predictable way. Additionally, using JSON-LD will allow us to take advantage of the benefits of linked data, such as improved discoverability and scalability.","GenTime":"2024-08-10 09:41:27"}
{"File Name":"macondo\/0003-remote-repositories-management.md","Context":"## Context\\nRight now this cli tool comes with a builtin set of commands that is hardcoded\\nto a gist I own somewhere. This is not ideal.\\nWe want this tool to easily access \"repositories of commands\" that are either\\nremote or local; as well as a way to easily add\/remove repositories.\\nSome ideas of repositories:\\n- HTTP urls pointing to yaml manifests with commands definitions\\n- Github repositories\\n- Local files or directories\\n","Decision":"- We won't have built-in commands or repositories\\n- We will support three types of repositories:\\n- Remote http resources pointing to yaml files\\n- Local manifest yaml files\\n- Local directories containing .mcd files\\nBecause http resources are now supported, we could make it so that the\\nrepositories with commands has CI\/CD via Jenkins, which generates a repo and\\npublishes it to artifactory.\\nThe macondo file will be simplified to only include, for now, a top-level\\n`repositories` array, which would point to the repositories.\\nA macondo update command will be added that refreshes the list of commands\\nprovided by the repositories. This is only done for http repos, whose\\ncontents could change anytime.\\nA new set of commands to be implemented:\\n- `macondo repo add` to add repositories\\n- `macondo repo remove` to remove them\\n- `macondo repo update` to update them\\n- `macondo repo list` to list them\\n","tokens":92,"id":2191,"Prediction":"## Decision\n\n**Command Repository System**\n\n### Context\n\nAs discussed, the current CLI tool has a hardcoded set of commands that are stored in a gist. This approach is inflexible and makes it difficult to manage and update commands. To address this issue, we need a system that allows the tool to easily access and manage \"repositories of commands\" from various sources, including remote and local locations.\n\n### Decision\n\nWe will implement a Command Repository System that allows the tool to dynamically load and manage commands from various sources. This system will provide a flexible and scalable way to manage commands, making it easier to add, remove, and update commands.\n\n### Description\n\nThe Command Repository System will consist of the following components:\n\n1. **Command Repository**: A central location that stores commands in a standardized format (e.g., YAML). This repository can be local or remote.\n2. **Command Loader**: A module that loads commands from the Command Repository and makes them available to the CLI tool.\n3. **Command Manager**: A module that manages the lifecycle of commands, including adding, removing, and updating commands.\n4. **Command Resolver**: A module that resolves command names to their corresponding commands in the Command Repository.\n\n### Rationale\n\nThis system will provide several benefits, including:\n\n* Flexibility: The tool can be easily configured to load commands from various sources, such as HTTP URLs, GitHub repositories, or local files.\n* Scalability: The system can handle a large number of commands and repositories, making it suitable for large-scale deployments.\n* Maintainability: The system will make it easier to manage and update commands, reducing the risk of errors and inconsistencies.\n\n### Consequences\n\n* The tool will require a new configuration file to specify the Command Repository and other settings.\n* The tool will need to be updated to use the Command Loader, Command Manager, and Command Resolver modules.\n* The system will require additional testing to ensure that it works correctly and efficiently.\n\n### Alternatives Considered\n\nWe considered alternative approaches, such as:\n\n* Hardcoding commands into the tool's codebase.\n* Using a static configuration file to store commands.\n* Implementing a custom command loading mechanism.\n\nHowever, these approaches were deemed less suitable due to their limitations and inflexibility. The Command Repository System provides a more scalable and maintainable solution that meets the tool's requirements.","GenTime":"2024-08-10 09:43:49"}
{"File Name":"macondo\/0007-automatic-aligning-of-user-host-user-for-better-interoperability.md","Context":"## Context\\nA well constructed macondo command should be easy to run directly (without\\nmacondo) by just executing it. This is hard to achieve unless the user in the\\ncontainer resembles as much as possible the host running it.\\nOne case where this is important is modifying files in the host. This is\\nachieved by mounting a volume into the running container. However, the files\\nwritten from the docker container are owned by the docker user, i.e. the user id\\nand group id of the file is that of whatever docker user happened to write it.\\nSo if the container runs with the root user, which is unfortunately common, then\\nthe files written to the host also are owned by root, making them innacessible\\nto the host user.\\nPotential ideas:\\n- A flag that enables user alignment and mounts HOME into docker's HOME.\\n- Customize home when running in OSX to be \/Users\/bla instead of \/home\/ble\\n- Even if mounting the whole home, it should be easy to mount current PWD into\\nsomething else \/mnt\/blablabla and use tha as working directory\\n","Decision":"Add a mechanism to align the user\/group of the host system with that of the docker container.\\n","tokens":237,"id":2193,"Prediction":"## Decision\n\nTo address the issue of running macondo commands directly without macondo and ensuring that files written from the container are accessible to the host user, we will implement a feature to align the user and group IDs of the container with those of the host. This will be achieved by setting the `USER` and `GROUP` environment variables in the container to match the host's user and group IDs.\n\nWe will also provide an option to mount the host's home directory into the container's home directory, allowing the user to access files written from the container as if they were written on the host. Additionally, we will allow the user to specify a custom working directory in the container by mounting a volume at a specific path.\n\n## Context\n\nThe context for this decision is the need to run macondo commands directly without macondo and ensure that files written from the container are accessible to the host user. This is particularly important when modifying files on the host, as the files written from the container are currently owned by the root user, making them inaccessible to the host user.\n\n## Rationale\n\nThe proposed solution addresses the issue by aligning the user and group IDs of the container with those of the host, ensuring that files written from the container are owned by the correct user and group. This will make it possible to run macondo commands directly without macondo and access files written from the container as if they were written on the host.\n\nThe option to mount the host's home directory into the container's home directory will provide additional flexibility and ease of use, allowing the user to access files written from the container as if they were written on the host.\n\n## Consequences\n\nThe implementation of this feature will require changes to the macondo command and the Docker container configuration. The user will need to specify the host's user and group IDs when running the macondo command, and the container will need to be configured to use the specified user and group IDs.\n\nThe user will also need to specify the custom working directory in the container by mounting a volume at a specific path. This will require additional configuration and setup, but will provide the flexibility and ease of use needed to run macondo commands directly without macondo.\n\nOverall, the proposed solution will provide a more user-friendly and flexible way to run macondo commands directly without macondo, while also ensuring that files written from the container are accessible to the host user.","GenTime":"2024-08-10 09:44:39"}
{"File Name":"connecting-to-services\/0015-add-info-page.md","Context":"## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n","Decision":"The application will provide an info page.\\n","tokens":55,"id":2198,"Prediction":"## Decision\n\nTo address the need to ensure the deployed version of the application and to provide accurate UTC timestamps for debugging purposes, we will implement a feature to display the application version and current UTC time in a centralized location.\n\n## Context\n\nAs mentioned in the context, it is essential to have a clear understanding of the deployed version of the application when debugging. Additionally, when dealing with date and time-related issues, such as openness, it is crucial to have accurate UTC timestamps from the application's perspective.\n\n## Solution\n\nTo achieve this, we will introduce a new feature that displays the application version and current UTC time in a prominent location, such as the application's dashboard or a dedicated debug page. This information will be retrieved from a reliable source, such as a configuration file or a database, to ensure accuracy and consistency.\n\n## Rationale\n\nBy displaying the application version and current UTC time, we can:\n\n1. Ensure that developers and operators have a clear understanding of the deployed version of the application, which is essential for debugging and troubleshooting.\n2. Provide accurate UTC timestamps for date and time-related issues, such as openness, which is critical for debugging and resolving these types of problems.\n3. Simplify the debugging process by providing a single location where developers can quickly access this information, reducing the need to search for this information across multiple sources.\n\n## Consequences\n\nBy implementing this feature, we will:\n\n1. Increase the overall debugging efficiency and effectiveness by providing easy access to critical information.\n2. Reduce the time spent searching for information and troubleshooting issues.\n3. Improve the overall quality and reliability of the application by ensuring that developers and operators have a clear understanding of the deployed version and current UTC time.\n\n## Alternatives\n\n1. Displaying the application version and current UTC time in a separate log file or debug console.\n2. Implementing a custom solution using a third-party library or framework.\n\nHowever, these alternatives may not provide a centralized location for this information, which could lead to confusion and make it more difficult to access. The proposed solution provides a clear and concise way to display this information, making it easier to debug and troubleshoot the application.","GenTime":"2024-08-10 09:46:24"}
{"File Name":"connecting-to-services\/0003-use-postcodes-io-for-postcode-lookup.md","Context":"## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat\/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat\/lon that can be used to query against.\\n","Decision":"[Postcodes.io](https:\/\/postcodes.io\/) provides both an out-code and a postcode\\nlookup. The results of which return latitude and longitude. The service is\\nfree, [supported](https:\/\/postcodes.io\/about),\\n[monitored](https:\/\/status.ideal-postcodes.co.uk\/) and contains the full set of\\nactive postcodes for the UK. Supplied by the\\n[ONS](https:\/\/data.gov.uk\/dataset\/national-statistics-postcode-lookup-uk).\\n","tokens":68,"id":2204,"Prediction":"## Decision\n\n**Decision:** Use the OpenCage Geocoder API to resolve postcodes to lat\/lon coordinates.\n\n## Context\n\nThe application is about finding services closest to the search point. All services have a coordinate in lat\/lon format. Currently, the application requests either an out-code or a postcode as the search point. The submitted postcode needs to be resolved to a lat\/lon that can be used to query against.\n\n## Problem\n\nThe application currently relies on a simple postcode to lat\/lon conversion, which is prone to errors and may not provide accurate results. A more reliable and accurate solution is needed to resolve postcodes to lat\/lon coordinates.\n\n## Trade-off\n\nWe considered the following options:\n\n1. Implementing a custom postcode to lat\/lon conversion algorithm: This would require significant development effort and may not provide the level of accuracy and reliability required.\n2. Using an open-source library: While open-source libraries exist, they may not be as reliable or up-to-date as a commercial API.\n3. Using a commercial API: OpenCage Geocoder API is a reliable and accurate solution that provides a simple and easy-to-use API.\n\n## Decision Rationale\n\nWe chose to use the OpenCage Geocoder API because it provides a reliable and accurate solution for resolving postcodes to lat\/lon coordinates. The API is well-maintained and updated regularly, ensuring that our application receives the most accurate results. Additionally, the API is easy to use and integrates seamlessly with our application.\n\n## Consequences\n\n* Improved accuracy and reliability of postcode to lat\/lon conversions\n* Reduced development effort and complexity\n* Enhanced user experience with more accurate search results\n\n## Status\n\nThis decision is finalized and has been implemented in the application.","GenTime":"2024-08-10 09:48:19"}
{"File Name":"connecting-to-services\/0008-add-basic-authentication-to-test-environments.md","Context":"## Context\\nEvery environment the application is available in, be that development, review,\\nstaging or public are openly available to anybody. There is no access control\\nat all.  This isn't suitable for a site that could be misinterpreted as the\\n'real' version either by people or by search engines and other bots. There\\nneeds to be some form of hurdle to overcome in order to prompt people to\\nconsider whether this is the correct site. There should also be a barrier to\\nprevent access by bots.\\n","Decision":"We have decided to use basic authentication on the service in all environments\\napart from the public facing one. This will only be applicable to the\\nenvironments hosted within the Rancher environment. The username and password\\nwill not be secret and will be included within the `README` of the application.\\n","tokens":107,"id":2207,"Prediction":"## Decision\n\nTo address the concern of having an openly available application without access control, we will implement a CAPTCHA system to add a hurdle for users to overcome before accessing the application. This will prompt users to think twice before accessing the site and ensure that they are intentional about visiting the site.\n\nAdditionally, we will implement a rate limiting system to prevent bots from accessing the site. This will help prevent automated scripts from crawling and indexing the site, which could lead to misinterpretation as the \"real\" version.\n\n## Rationale\n\nThe lack of access control in the current application poses a significant risk of misinterpretation and potential abuse. By implementing a CAPTCHA system, we can ensure that users are prompted to think twice before accessing the site, which will help reduce the likelihood of accidental or malicious access.\n\nThe rate limiting system will also help prevent bots from accessing the site, which will reduce the risk of the site being crawled and indexed by search engines and other bots. This will help maintain the integrity of the site and prevent it from being mistaken for the \"real\" version.\n\n## Consequences\n\nImplementing a CAPTCHA system and rate limiting system will require additional development effort and may impact the user experience. However, the benefits of added security and reduced risk of misinterpretation outweigh the costs.\n\n## Alternatives\n\nAlternative solutions considered included:\n\n* Implementing a login system with authentication and authorization\n* Implementing a geolocation-based access control system\n* Implementing a token-based access control system\n\nHowever, these alternatives were deemed too complex or invasive for the current application, and the CAPTCHA and rate limiting systems were chosen as the most suitable solutions.","GenTime":"2024-08-10 09:49:23"}
{"File Name":"fare-platform\/0002-permission-crud-operations-on-records-config.md","Context":"## Context\\nInitially a record could be made by anyone, but the problem is that that record was only some metadata without file attached on it.\\n","Decision":"To avoid the situation where a record is created without file, we put `deny_all` permission in fare\/records\/config.py that denies to all the possibilities to create, update and delete this kind of records.\\n","tokens":30,"id":2215,"Prediction":"## Decision\n\n**Decision:** Restrict the creation of new records to only authorized users, and require a file attachment to be added along with the metadata.\n\n**Context:** Initially, a record could be made by anyone, but the problem is that that record was only some metadata without a file attached to it.\n\n**Problem:** Allowing anyone to create a record without a file attachment leads to incomplete and useless records, which can cause confusion and waste of time.\n\n**Rationale:** To ensure that records are complete and useful, we need to restrict the creation of new records to only authorized users who have a legitimate need to create records. Additionally, requiring a file attachment ensures that the record has a tangible and meaningful content.\n\n**Consequences:**\n\n* Only authorized users can create new records, reducing the risk of incomplete or useless records.\n* Users are more likely to provide accurate and complete metadata, as they are also required to attach a file.\n* The quality of records improves, making it easier for others to find and use them.\n\n**Alternatives Considered:**\n\n* Allowing anyone to create a record, but requiring a file attachment: This would not solve the problem of incomplete records, as users could still create records without a file attachment.\n* Not restricting the creation of new records at all: This would lead to a large number of incomplete records, which would be difficult to manage and maintain.\n\n**Status:** Implemented\n\n**Implementors:** [List the names of the implementors, if applicable]\n\n**References:** [List any relevant references, such as design documents or code snippets]","GenTime":"2024-08-10 09:51:56"}
{"File Name":"decodeweb\/0003-store-configuration-into-local-storage.md","Context":"## Context\\nTo provide the required functionality for the pilot we need to store some\\nconfiguration in a persistent way while just operating within a browser\\nenvironment. This state could be written to a backend database, or we could\\njust store into localstorage.\\n","Decision":"We will store the user's configuration into localstorage protected by a PIN\\ncode.\\n### Proposed storage structure\\n```json\\n{\\n\"1234\": {\\n\"keypair\": {},\\n\"blindSignature\": {},\\n\"devices\": {\\n\"abc123\": {\\n\"longitude\": 12.2,\\n\"latitude\": 52.4,\\n\"exposure\": \"INDOOR\",\\n\"communities\": {\\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\\n\"credential\": {},\\n\"blindproofCredential\": {}\\n}\\n}\\n}\\n}\\n}\\n}\\n```\\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\\n","tokens":52,"id":2221,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to store the configuration in local storage instead of writing it to a backend database.\n\n## Context\n\nAs mentioned, we need to store some configuration in a persistent way while operating within a browser environment. We could write this state to a backend database, but this would require additional infrastructure and complexity. On the other hand, local storage provides a simple and straightforward way to store data locally on the user's device.\n\n## Problem\n\nThe main problem with writing to a backend database is the added complexity and overhead it would introduce. We would need to set up a database, implement data storage and retrieval logic, and handle potential errors and exceptions. This would add significant development time and resources, which could be better spent on other aspects of the pilot.\n\n## Decision\n\nStoring the configuration in local storage addresses the problem of persistent storage in a browser environment without introducing unnecessary complexity. Local storage is a built-in feature of web browsers that allows us to store data locally on the user's device. This approach is simple, lightweight, and easy to implement.\n\n## Consequences\n\nBy choosing to store the configuration in local storage, we will:\n\n* Reduce development time and resources\n* Simplify the overall architecture and implementation\n* Provide a more seamless user experience\n* Minimize the risk of errors and exceptions\n\nHowever, we should also consider the potential limitations of local storage, such as:\n\n* Data is stored locally on the user's device, which means it may not be accessible from other devices\n* Data is not encrypted by default, which may raise security concerns\n* Local storage is not suitable for storing large amounts of data\n\nTo mitigate these risks, we will implement appropriate security measures, such as encrypting sensitive data, and ensure that the user is aware of the limitations of local storage.\n\n## Rationale\n\nOur decision to store the configuration in local storage is based on the trade-offs between complexity, simplicity, and user experience. By choosing this approach, we can provide a more seamless user experience while minimizing the risk of errors and exceptions.","GenTime":"2024-08-10 09:53:58"}
{"File Name":"cf-k8s-networking\/0007-maintain-generated-istio.md","Context":"## Context \ud83e\udd14\\nCf-k8s-networking was designed to be integrated with\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/). The Istio\\ninstallation used to be maintained by\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/), but the networking\\nteam needed to be able to easily make changes to [Istio](https:\/\/istio.io\/)\\nconfiguration to enable more networking features for [Cloud\\nFoundry](https:\/\/www.cloudfoundry.org\/).\\n","Decision":"We decided to move the scripts to build Istio configuration, and maintain a\\ngenerated Istio configuration within the cf-k8s-networking repository.\\nThe build scripts and `ytt` overlays for Istio live in this repo (links removed\\nas they are no longer relevant or accurate). **UPDATE** This configuration has\\nmoved as a result of [ADR\\n017](.\/0017-moving-istio-configuration-out-of-this-repo.md).\\n","tokens":133,"id":2236,"Prediction":"## Decision\n\n**Decision:** The Istio installation will be managed by the networking team, and the Istio configuration will be decoupled from the cf-for-k8s project.\n\n**Context:** As mentioned in the context, the Istio installation used to be maintained by cf-for-k8s, but the networking team needs to be able to easily make changes to Istio configuration to enable more networking features for Cloud Foundry.\n\n**Problem:** The current setup where Istio installation is managed by cf-for-k8s limits the flexibility and autonomy of the networking team to make changes to Istio configuration.\n\n**Reasoning:** By decoupling the Istio configuration from cf-for-k8s, the networking team will have more control and flexibility to make changes to Istio configuration, enabling them to implement new networking features for Cloud Foundry. This will also reduce the dependency on the cf-for-k8s project, making it easier to maintain and update Istio configuration independently.\n\n**Consequences:**\n\n* The networking team will have more autonomy to make changes to Istio configuration.\n* The Istio installation will be managed by the networking team, reducing the dependency on cf-for-k8s.\n* This change will require coordination with the cf-for-k8s team to ensure a smooth transition and minimize impact on the overall system.\n\n**Alternatives Considered:**\n\n* Keeping the Istio installation managed by cf-for-k8s and finding alternative ways for the networking team to make changes to Istio configuration.\n* Creating a new project to manage Istio configuration, which would introduce additional complexity and overhead.\n\n**Status:** Implemented","GenTime":"2024-08-10 09:58:26"}
{"File Name":"cf-k8s-networking\/0016-job-for-upgrading-istio-sidecars-on-workloads.md","Context":"## Context\\nIstio's service mesh capabilites are facilitated via sidecars injected into\\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\\nto the version of Istio that injects them.\\nTypically when new versions of Istio are released, new versions of the sidecars\\nare released as well. Istio has been good so far about supporting older versions\\nof sidecars that were deployed before Istio was upgraded, but it is still\\n[documented best practice](https:\/\/istio.io\/latest\/docs\/setup\/upgrade\/) to roll\\nall the pods after an Istio upgrade.\\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\\nother bash scripts. This limits our options considerably. See this [Slack\\nthread](https:\/\/cloudfoundry.slack.com\/archives\/CH9LF6V1P\/p1592521879117400) on\\nthat constraint.\\n","Decision":"We will use the kubernetes\\n[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resource\\nto run the kubectl command needed to roll workload pods, after waiting for the\\nnew Istio control plane to be up and healthy.\\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\\nneeded to list resources in the `istio-system` namespace, and restart resources\\nin the configured workload namespace. We will also build and maintain a\\ncontainer image that contains the Job's logic.\\nAll istio components will be tagged with their Istio version so that the job can\\npositively determine that the correct version of control plane components are\\nalive and healthy. We will also name the job according to it's Istio version, so\\nthat we can take advantage of `Jobs` inherent immutability in cases where a\\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\\nagain will not cause it to rerun, preventing workloads from rolling\\nunnecessarily). Subsequent jobs will clean up previous ones.\\n","tokens":224,"id":2240,"Prediction":"## Decision\n\nTo ensure seamless Istio upgrades with minimal downtime and no post-install hooks, we will adopt a \"rolling upgrade\" strategy for Istio sidecars in cf-for-k8s clusters. This means that we will upgrade the Istio sidecars in a rolling manner, one pod at a time, to ensure that all pods are running with the new version of Istio before moving on to the next one.\n\nThis decision is driven by the need to align with Istio's best practices, which recommend rolling all pods after an Istio upgrade to ensure compatibility and avoid potential issues. Additionally, the operators of cf-for-k8s clusters require a single `kapp deploy` command with no post-install hooks or other bash scripts, which makes a rolling upgrade the most feasible option.\n\nBy adopting this strategy, we can minimize downtime and ensure a smooth upgrade process for Istio sidecars in cf-for-k8s clusters.","GenTime":"2024-08-10 09:59:30"}
{"File Name":"cf-k8s-networking\/0006-rewrite-http-liveness-readiness-probes-for-healthchecks.md","Context":"## Context\\nWith Istio auto mTLS enabled in `STRICT` mode, [http liveness and readiness\\nprobes](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/configure-liveness-readiness-startup-probes\/) no longer work because the `kubelet`, which makes the http requests, does not have Istio issued certificates.\\nIstio [supports rewriting `http` probes](https:\/\/istio.io\/docs\/ops\/configuration\/mesh\/app-health-check\/#enable-globally-via-install-option) during the sidecar injection process.\\n#### Figure 1\\n_Liveness probe flow when Istio mTLS is disabled or `PERMISSIVE`. Probe `GET` request regularly travels through the Envoy sidecar to the app._\\n![No mTLS\/PERMISSIVE mTLS mode liveness probe diagram](..\/assets\/liveness-probe-adr-1.png)\\n#### Figure 2\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe is not rewritten. Probe `GET` request fails at the Envoy sidecar because it does not include the correct certificates._\\n![STRICT mTLS liveness probe diagram with no probe rewrite](..\/assets\/liveness-probe-adr-2.png)\\n#### Figure 3\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe **is rewritten by Istio**. Probe `GET` request bypasses the sidecar and goes through the Istio `pilot-agent` instead. The `pilot-agent` is configured to direct the request to the app._\\n![STRICT mTLS liveness probe diagram with probe rewrite](..\/assets\/liveness-probe-adr-3.png)\\n","Decision":"We have decided to install Istio with\\n`--set values.sidecarInjectorWebhook.rewriteAppHTTPProbe=true`\\nThis will rewrite the liveness and readiness probes on any app pods when injecting a sidecar into the app pod. Any namespaces that have the label `istio-injection=enabled` will have their liveness and readiness probes rewritten, as shown in Figure 3.\\n","tokens":360,"id":2243,"Prediction":"## Decision\n\nTo resolve the issue with Istio auto mTLS enabled in `STRICT` mode, we will enable rewriting of `http` probes during the sidecar injection process in Istio.\n\n## Context\n\nAs mentioned in the context, with Istio auto mTLS enabled in `STRICT` mode, http liveness and readiness probes no longer work because the `kubelet`, which makes the http requests, does not have Istio issued certificates. Istio supports rewriting `http` probes during the sidecar injection process, which would allow the probes to bypass the Envoy sidecar and go through the Istio `pilot-agent` instead.\n\n## Decision Rationale\n\nEnabling rewriting of `http` probes during the sidecar injection process in Istio will allow the probes to work correctly even when Istio auto mTLS is enabled in `STRICT` mode. This will ensure that the liveness and readiness probes continue to function as expected, providing a reliable way to monitor the health of our applications.\n\n## Decision Consequences\n\n* The liveness and readiness probes will continue to function correctly even when Istio auto mTLS is enabled in `STRICT` mode.\n* The probes will bypass the Envoy sidecar and go through the Istio `pilot-agent` instead, which will ensure that the correct certificates are included in the requests.\n* This decision will require configuration changes in Istio to enable rewriting of `http` probes during the sidecar injection process.\n\n## Decision Status\n\nThis decision is approved and will be implemented as described.","GenTime":"2024-08-10 10:00:28"}
{"File Name":"cf-k8s-networking\/0008-implement-workarounds-for-capi-and-log-cache-to-unblock-global-strict-mtls.md","Context":"## Context\\nWe need to turn on STRICT mTLS for all components on the mesh. However, some\\ncomponents are currently incompatible with this mode.\\nCAPI is incompatible because it uses an init container to run migrations. This\\ninit container comes up before the sidecar, so it is unable to establish an mTLS\\nconnection with the capi database. This causes the init container to fail and\\nprevents capi from coming up. See [this\\nissue](https:\/\/github.com\/cloudfoundry\/capi-k8s-release\/issues\/12) in capi.\\nLog-cache is incompatible because it is configured to establish its own tls\\nconnection, which is incompatible with the mTLS the sidecars are attempting to\\nestablish.\\n","Decision":"We have provided configuration workarounds in the form of Policies, that were\\nplaced in the cf-for-k8s repo to be owned by the respective teams that manage\\nthe troublesome components.\\n[Pull Request](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/pull\/35)\\n","tokens":153,"id":2249,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to implement a hybrid mTLS strategy for the mesh. This strategy will allow us to enable STRICT mTLS for most components, while providing a temporary workaround for the incompatible components (CAPI and Log-cache).\n\n## Context and Problem\n\nAs mentioned in the context, CAPI is incompatible with STRICT mTLS because its init container runs before the sidecar, preventing it from establishing an mTLS connection with the CAPI database. Similarly, Log-cache is incompatible because it is configured to establish its own TLS connection, which conflicts with the mTLS established by the sidecars.\n\n## Decision\n\nTo address these incompatibilities, we will implement the following hybrid mTLS strategy:\n\n1. For components that are compatible with STRICT mTLS, we will enable it and ensure that they establish mTLS connections with each other.\n2. For CAPI, we will use a custom init container that establishes a TLS connection with the CAPI database before the sidecar starts. This will allow CAPI to function with mTLS enabled.\n3. For Log-cache, we will configure it to use the mTLS connection established by the sidecar, rather than establishing its own TLS connection. This will ensure that Log-cache is able to function with mTLS enabled.\n\n## Rationale\n\nBy implementing a hybrid mTLS strategy, we can enable STRICT mTLS for most components while providing a temporary workaround for the incompatible components. This will allow us to gradually phase out the incompatible components and ensure that the entire mesh is secure.\n\n## Consequences\n\n* The hybrid mTLS strategy will require additional configuration and testing to ensure that it is working correctly.\n* The custom init container for CAPI will add complexity to the deployment process.\n* The configuration changes for Log-cache will require careful testing to ensure that it is functioning correctly with the mTLS connection established by the sidecar.\n\n## Pros and Cons\n\nPros:\n\n* Enables STRICT mTLS for most components, improving the overall security of the mesh.\n* Provides a temporary workaround for incompatible components, allowing us to gradually phase them out.\n\nCons:\n\n* Adds complexity to the deployment process with the custom init container for CAPI.\n* Requires additional testing and configuration to ensure that the hybrid mTLS strategy is working correctly.\n\n## Monitoring and Feedback\n\nWe will monitor the performance and security of the mesh with the hybrid mTLS strategy and gather feedback from the development team and other stakeholders. We will use this feedback to refine the strategy and make adjustments as needed to ensure that it is working effectively.","GenTime":"2024-08-10 10:02:57"}
{"File Name":"cf-k8s-networking\/0013-rename-master-branch.md","Context":"## Context\\nA [proposal](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/topic\/75070528#9059) was\\nmade on the cf-dev mailing list to rename our `master` branch to `main` to make\\nthe CF community a more welcoming and inclusive environment.\\n","Decision":"We are going to rename our `master` branch to `develop`. `develop` was chosen\\ninstead of `main` by team consensus because it better describes the use of the\\nbranch.\\n","tokens":63,"id":2251,"Prediction":"## Decision\n\n**Decision:** Renaming the `master` branch to `main` in our software project.\n\n**Context:** A proposal was made on the cf-dev mailing list to rename our `master` branch to `main` to make the CF community a more welcoming and inclusive environment.\n\n**Problem:** The current use of the term \"master\" in our branch naming convention may be perceived as offensive or insensitive to some individuals, particularly those from underrepresented groups in technology.\n\n**Reasoning:** Renaming the branch to `main` aligns with the broader effort to create a more inclusive and welcoming environment in the CF community. This change will help to reduce potential barriers to entry and ensure that all individuals feel comfortable contributing to our project.\n\n**Consequences:**\n\n* The rename will require updating all existing references to the `master` branch in our codebase, documentation, and other project materials.\n* The change may cause temporary disruption to automated workflows and CI\/CD pipelines that rely on the `master` branch.\n* The rename will need to be communicated to all team members, contributors, and stakeholders to ensure a smooth transition.\n\n**Decision Drivers:**\n\n* The proposal made on the cf-dev mailing list\n* The need to create a more inclusive and welcoming environment in the CF community\n* The potential impact on team morale and diversity\n\n**Alternatives Considered:**\n\n* Not renaming the branch, which would maintain the status quo but potentially alienate individuals who may be offended by the term \"master\".\n* Renaming the branch to a different name, which may not effectively address the issue and could create confusion.\n\n**Status:** This decision has been made, and the rename process is underway.","GenTime":"2024-08-10 10:03:31"}
{"File Name":"princeton_ansible\/0002-group-vars.md","Context":"## Context\\nThere are a lot of different ways to set variables in Ansible and we should have\\na consistent way to do it in all playbooks for each group.\\nWe want to make sure that there's a smaller vault.yml file per group, instead of\\na gigantic vault.yml file for everything. This is because one huge vault.yml\\nfile causes merge conflicts in multiple Github Pull Requests.\\n","Decision":"Unique group variables will be placed in `\/group_vars\/<groupname>`\\nEncrypted group variables will be placed in `\/group_vars\/<groupname>\/vault.yml`\\nShared group variables will be placed in `\/group_vars\/all\/`\\n","tokens":82,"id":2255,"Prediction":"## Decision\n\nTo address the issue of inconsistent variable settings in Ansible playbooks and the growing size of the `vault.yml` file, we will implement a centralized variable management system for each group of playbooks.\n\n## Context\n\nAs mentioned in the context, there are various ways to set variables in Ansible, and having a consistent approach will ensure maintainability and scalability of our playbooks. Additionally, the current practice of having a single large `vault.yml` file is causing merge conflicts in multiple pull requests, making it difficult to manage.\n\n## Decision Rationale\n\nTo achieve consistency and scalability, we will create a separate `vault.yml` file for each group of playbooks. This approach will allow us to:\n\n* Keep related variables together, making it easier to manage and maintain\n* Reduce the size of individual `vault.yml` files, making them less prone to merge conflicts\n* Ensure consistency in variable naming and formatting across playbooks\n\n## Decision Description\n\nWe will create a new directory structure to store the separate `vault.yml` files for each group of playbooks. Each group will have its own `vault.yml` file, which will contain the specific variables required for that group.\n\nTo ensure consistency, we will establish a set of naming conventions and formatting guidelines for variable names and values. This will help to reduce errors and make it easier to maintain the `vault.yml` files.\n\n## Decision Consequences\n\nThis decision will:\n\n* Improve the maintainability and scalability of our Ansible playbooks\n* Reduce the likelihood of merge conflicts in pull requests\n* Ensure consistency in variable naming and formatting across playbooks\n* Require additional effort to create and manage the separate `vault.yml` files for each group\n\n## Decision Status\n\nThis decision is approved and will be implemented in the next release.","GenTime":"2024-08-10 10:04:46"}
{"File Name":"princeton_ansible\/0004-key-scope","Context":"## Context\\nIn many cases we can generate keys or tokens as-needed. We may also be able to create keys with varying permissions.\\nWe'd like to reduce our vulnerability in the case of one of these keys becoming compromised, either via a compromised box or via a software exploit to application code.\\n","Decision":"Keys should be minted on a per-feature, per-application basis.\\n","tokens":61,"id":2256,"Prediction":"## Decision\n\n**Decision:** Implement a Key Rotation and Revocation mechanism for generated keys and tokens.\n\n## Context\n\nAs mentioned, we can generate keys or tokens as-needed, and we may also be able to create keys with varying permissions. However, this also increases the risk of vulnerability in the case of one of these keys becoming compromised, either due to a compromised box or a software exploit to application code.\n\n## Problem\n\nWithout a mechanism to rotate and revoke keys, a single compromised key can lead to a significant security breach, allowing unauthorized access to sensitive data and systems. This can have severe consequences, including data theft, financial losses, and reputational damage.\n\n## Solution\n\nTo mitigate this risk, we will implement a Key Rotation and Revocation mechanism. This will allow us to:\n\n1. Rotate keys and tokens regularly to reduce the impact of a single key compromise.\n2. Revoke compromised keys and tokens to prevent further unauthorized access.\n3. Implement a centralized key management system to track and manage key rotation and revocation.\n\n## Consequences\n\nBy implementing a Key Rotation and Revocation mechanism, we will:\n\n1. Reduce the risk of a single key compromise leading to a security breach.\n2. Improve the overall security posture of our application and data.\n3. Enhance our ability to respond to security incidents and minimize their impact.\n\n## Rationale\n\nThe benefits of implementing a Key Rotation and Revocation mechanism outweigh the costs. The increased security and reduced risk of a single key compromise justifying the additional complexity and overhead.\n\n## Alternatives Considered\n\n1. Not implementing a Key Rotation and Revocation mechanism, relying solely on manual key management and revocation.\n2. Implementing a more complex and costly solution, such as a Hardware Security Module (HSM).\n\n## Decision Drivers\n\n1. Security risk reduction\n2. Compliance with industry security standards\n3. Improved incident response and recovery capabilities","GenTime":"2024-08-10 10:05:07"}
{"File Name":"sre-adrs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made for our team.\\n","Decision":"We will use Architecture Decision Records, as discussed in our original proposal: https:\/\/docs.google.com\/document\/d\/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU\/edit#\\n","tokens":16,"id":2261,"Prediction":"Here is an Architectural Decision Record (ADR) for your team:\n\n**Decision:** Introduce a Microservice Architecture for the New Project\n\n**Context:**\n\nWe are embarking on a new project that requires scalability, flexibility, and maintainability. The project involves multiple features and functionalities that need to be developed independently and deployed separately. Our current monolithic architecture is not suitable for this project, as it would lead to a complex and rigid system that is difficult to maintain and evolve.\n\n**Problem:**\n\nOur current monolithic architecture is not scalable, making it difficult to handle increased traffic and user load. It also makes it challenging to develop and deploy new features independently, which would lead to a slower time-to-market.\n\n**Decision:**\n\nTo address the scalability and maintainability issues, we will adopt a microservice architecture for the new project. This will allow us to break down the system into smaller, independent services that can be developed, tested, and deployed separately. Each microservice will be responsible for a specific business capability, and they will communicate with each other using APIs.\n\n**Consequences:**\n\n* Improved scalability: With a microservice architecture, we can scale individual services independently, which will improve the overall system's scalability and responsiveness.\n* Increased flexibility: With separate services, we can develop and deploy new features independently, which will reduce the risk of introducing new features and improve the overall system's maintainability.\n* Easier maintenance: With a microservice architecture, we can maintain and update individual services without affecting the entire system, which will reduce the risk of introducing bugs and improve the overall system's reliability.\n\n**Rationale:**\n\nWe chose a microservice architecture because it aligns with our project requirements and goals. It will allow us to develop and deploy new features independently, which will improve the overall system's scalability, flexibility, and maintainability. Additionally, a microservice architecture will enable us to use different programming languages, frameworks, and databases for each service, which will provide more flexibility and opportunities for innovation.\n\n**Status:**\n\nThis decision is currently in the \"proposed\" status and will be reviewed and approved by the team before implementation.\n\n**References:**\n\n* [1] \"Microservices: A Critical Perspective\" by Sam Newman\n* [2] \"Designing Distributed Systems\" by Brendan Burns","GenTime":"2024-08-10 10:06:55"}
{"File Name":"wordpress-template\/0003-theme-namespace-naming-convention.md","Context":"## Context\\nWe need to pick a namespace for themes, and renaming can be tedious.\\nOnly one theme is ever active at a time, so there's no risk in naming collisions except when dealing with child themes.\\n","Decision":"We'll use the `\\Theme` namespace for every theme we make except for child themes which will have the namespace `\\ChildTheme`.\\n","tokens":45,"id":2262,"Prediction":"## Decision\n\n**Theme Namespace**\n\n### Context\n\nAs discussed, we need to pick a namespace for themes, and renaming can be tedious. Only one theme is ever active at a time, so there's no risk in naming collisions except when dealing with child themes.\n\n### Decision\n\nWe will use the `theme` namespace for all theme-related resources, including theme files, configuration files, and other theme-specific assets. This will help to keep our code organized and make it easier to identify theme-related components.\n\n### Rationale\n\nUsing a consistent namespace for themes will help to:\n\n* Keep our code organized and easy to maintain\n* Reduce the risk of naming collisions when dealing with child themes\n* Make it easier for developers to identify and locate theme-related resources\n\n### Consequences\n\n* We will need to update our existing code to use the `theme` namespace for theme-related resources\n* We will need to ensure that our theme files and configuration files are properly named and organized within the `theme` namespace\n\n### Alternatives Considered\n\n* Using a different namespace, such as `app\/themes` or `resources\/themes`\n* Not using a namespace at all, and relying on other mechanisms to keep our code organized\n\n### Status\n\nThis decision is approved and will be implemented in our codebase.","GenTime":"2024-08-10 10:07:09"}
{"File Name":"spin-archetype-base\/0002-unified-terminology.md","Context":"## Context\\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\\n","Decision":"A `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\\n","tokens":34,"id":2270,"Prediction":"## Decision\n\n**Use a centralized naming convention and a consistent vocabulary for technical terms**\n\n## Context\n\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\n\n## Problem\n\nUsing inconsistent or ambiguous naming conventions can lead to confusion, errors, and misunderstandings among team members and future maintainers of the codebase. This can result in a decrease in code quality, increased debugging time, and a higher likelihood of bugs and defects.\n\n## Decision\n\nTo mitigate this problem, we will adopt a centralized naming convention and a consistent vocabulary for technical terms. This will ensure that all team members use the same terminology and naming conventions, making it easier to understand and maintain the codebase.\n\n## Rationale\n\n1. Consistency: A centralized naming convention and consistent vocabulary will ensure that all team members use the same terminology and naming conventions, making it easier to understand and maintain the codebase.\n2. Clarity: Using consistent and clear naming conventions will make the code easier to read and understand, reducing the likelihood of errors and misunderstandings.\n3. Maintainability: A centralized naming convention and consistent vocabulary will make it easier to maintain and update the codebase over time, as new team members can quickly understand the terminology and naming conventions used.\n\n## Consequences\n\n1. Initial investment: Implementing a centralized naming convention and consistent vocabulary will require an initial investment of time and effort to define and document the conventions.\n2. Changes to existing code: Existing code may need to be updated to conform to the new naming conventions, which may require additional time and effort.\n3. Improved collaboration: The use of a centralized naming convention and consistent vocabulary will improve collaboration among team members, as everyone will be using the same terminology and naming conventions.\n\n## Status\n\nThis decision is currently being implemented, and we will monitor its effectiveness and make adjustments as needed.\n\n## Links\n\n* [Naming Conventions](https:\/\/example.com\/naming-conventions)\n* [Technical Terms](https:\/\/example.com\/technical-terms)","GenTime":"2024-08-10 10:09:49"}
{"File Name":"where-away\/0004-test-with-jest.md","Context":"## Context\\nI am familiar with Jest and am consistently happy with it. Also it's pretty\\npopular and widely used so there's good support.\\n","Decision":"Use Jest for testing.\\n","tokens":31,"id":2273,"Prediction":"## Decision\n\nAs the testing framework for our software, we will use Jest. Given the user's familiarity with Jest and its widespread adoption, it is a suitable choice for our project. Jest's popularity ensures that there is a large community of developers who can provide support and contribute to its development, which can be beneficial for our project.\n\n## Rationale\n\n* The user's familiarity with Jest will allow for a smoother onboarding process and faster development time.\n* The large community of developers using Jest means that there are many resources available for troubleshooting and learning.\n* Jest's popularity also means that it is likely to be well-maintained and receive updates and bug fixes regularly.\n\n## Consequences\n\n* We will need to ensure that our project's testing infrastructure is set up to work with Jest.\n* We will need to consider the specific configuration options and settings required by Jest.\n* We will need to document our testing setup and configuration for future reference.\n\n## Status\n\nThis decision is currently in the \"Done\" state, as it has been made and implemented.","GenTime":"2024-08-10 10:10:39"}
{"File Name":"where-away\/0009-use-xml-for-bookmarks-format.md","Context":"## Context\\nOptions:\\n- custom format, custom parser\\n- that's a lot of work\\n- json\\n- awkward format for depicting a tree\\n- yaml\\n- significant whitespace makes this more error-prone, and the bookmarks file\\nis likely to be edited frequently\\n- xml\\n- classic\\n- shows trees well\\n- less error prone to edit\\n- not supported natively in node\\n- neither parsing nor schema validation\\n- schema is simple enough that I could hand-roll a validator\\n- see discussion on parsing -- it needs a library: https:\/\/stackoverflow.com\/questions\/11398419\/trying-to-use-the-domparser-with-node-js\\n- top two promising libraries:\\n- jsdom, 32 dependencies https:\/\/www.npmjs.com\/package\/jsdom\\n- xmldom, 0 dependencies https:\/\/www.npmjs.com\/package\/xmldom\\n","Decision":"Use XML for the bookmarks format. Use\\n[xmldom](https:\/\/www.npmjs.com\/package\/xmldom) to parse, and hand-roll the\\nvalidation.\\n","tokens":180,"id":2274,"Prediction":"## Decision\n\nGiven the context of needing a format to store and parse tree-like data in a bookmarks file, and considering the options of custom format, JSON, YAML, and XML, we have decided to use XML as the format for the bookmarks file.\n\n## Rationale\n\nWe chose XML over the other options for several reasons:\n\n* XML is a classic and widely supported format for representing hierarchical data, making it well-suited for depicting trees.\n* XML is less error-prone to edit compared to YAML, which relies heavily on whitespace.\n* XML has a more straightforward syntax, making it easier to read and write.\n* While JSON is a popular choice, it is not as well-suited for representing hierarchical data as XML.\n* We can use the xmldom library, which has zero dependencies, to parse and validate the XML format.\n\n## Trade-offs\n\n* We will need to use a library to parse and validate the XML format, which adds a small amount of complexity to the codebase.\n* XML can be verbose, but in this case, the simplicity of the schema and the need for a human-readable format make it a suitable choice.\n\n## Conclusion\n\nBy choosing XML as the format for the bookmarks file, we can take advantage of its strengths in representing hierarchical data and its ease of use, while minimizing the complexity and potential errors associated with other formats.","GenTime":"2024-08-10 10:10:54"}
{"File Name":"where-away\/0011-render-buttons-instead-of-anchor-elements-for-link-nodes.md","Context":"## Context\\n- semantically, an html element that takes you to a new web page when you click\\nit is an anchor -- `<a>`.\\n- I'd like tab navigation and the Enter key to work for navigating between\\nbookmarks.\\n- buttons can be accessed via Tab and activated via Enter\\n- not so with an anchor tag. See\\nhttps:\/\/stackoverflow.com\/questions\/41476292\/how-to-give-focus-to-anchor-tag-without-href,\\netc.\\n","Decision":"Model the links as `<button>` elements to make the interaction more normal.\\n","tokens":100,"id":2280,"Prediction":"## Decision\n\n### Decision: Use a custom component instead of an anchor tag for bookmarks\n\n### Context\n\nAs discussed, we want to create a bookmark feature that allows users to navigate between bookmarks using the Tab key and Enter key. However, the anchor tag (`<a>`) does not provide this functionality out of the box. While we could use JavaScript to achieve this, it would add complexity and potential accessibility issues.\n\n### Problem\n\nThe anchor tag is not suitable for our bookmark feature because it does not provide a way to focus on the element without an `href` attribute, which would cause issues with keyboard navigation.\n\n### Decision Criteria\n\n* We need a component that can be focused using the Tab key and activated using the Enter key.\n* We want to avoid using JavaScript to achieve this functionality, as it would add complexity and potential accessibility issues.\n* We want to maintain semantic meaning in our HTML structure.\n\n### Decision\n\nWe will create a custom component that mimics the behavior of an anchor tag but provides the necessary functionality for keyboard navigation. This component will be a wrapper around a `button` element, which can be focused and activated using the Enter key. This approach will also allow us to maintain semantic meaning in our HTML structure.\n\n### Consequences\n\n* We will need to create a custom component that wraps a `button` element.\n* We will need to handle the focus and activation of the component using the Tab key and Enter key.\n* We will need to ensure that the component is accessible and follows accessibility guidelines.\n\n### Rationale\n\nUsing a custom component instead of an anchor tag will provide the necessary functionality for keyboard navigation and maintain semantic meaning in our HTML structure. This approach will also allow us to avoid using JavaScript to achieve this functionality, which would add complexity and potential accessibility issues.","GenTime":"2024-08-10 10:12:43"}
{"File Name":"stac-ml-aoi-extension\/0004-multiple-label-items.md","Context":"## Context\\nShould each `ml-aoi` Item be able to bring in multiple labels?\\nThis would be a useful feature for training multi-class classifiers.\\nOne can imagine having a label STAC item for buildings and separate STAC item for fields.\\nSTAC Items Links object is an array, so many label items could be linked to from a single `ml-aoi` STAC Item.\\n#### Limiting to single label link\\nLimiting to single label link however is appealing because the label item metadata could be copied over to `ml-aoi` Item.\\nThis would remove the need to follow the link for the label item during processing.\\nIn practice this would make each `ml-aoi` Item also a `label` Item, allowing for its re-use by tooling that understands `label`.\\nIf multi-class label dataset would be required there would have to be a mechanical pre-processing step of combining\\nexisting labels into a single STAC `label` item. This could mean either union of GeoJSON FeatureCollections per item or\\na configuration of a more complex STAC `label` Item that links to multiple label assets.\\n#### Allowing multiple labels\\nThe main appeal of consuming multi-label `ml-aoi` items is that it would allow referencing multiple label sources,\\nsome which could be external, without the need for pre-processing and thus minimizing data duplication.\\nIf multiple labels were to be allowed the `ml-aoi` the pre-processing step above would be pushed into `ml-aoi` consumer.\\nThe consumer would need appropriate metadata in order to decipher how the label structure.\\nThis would require either crawling the full catalog or some kind of meta-label structure that combines the metadata\\nfrom all the included labels into a single structure that could be interpreted by the consumer.\\n","Decision":"`ml-aoi` Items should be limited to linking to only a single label item.\\nRequiring the consumer to interpret multiple label items pushed unreasonable complexity on the user.\\nAdditionally combining labels likely requires series of processing and validation steps.\\nEach one of those would likely require judgment calls and exceptions.\\nFor instance when combining building and fields label datasets the user should check that no building and field polygons overlap.\\nIt is not realistic to expect all possible requirements of that process to be expressed by a simple metadata structure.\\nTherefore it is better to explicitly require the label combination as a separate process done by the user.\\nThe resulting label catalog can capture that design and iteration required for that process anyway.\\n","tokens":375,"id":2286,"Prediction":"## Decision\n\nAfter careful consideration of the trade-offs, we have decided to allow each `ml-aoi` Item to bring in multiple labels. This decision is based on the following reasons:\n\n* The ability to reference multiple label sources without the need for pre-processing and data duplication is a significant advantage.\n* Allowing multiple labels enables the use of `ml-aoi` Items as a hub for referencing multiple label sources, which can be useful for training multi-class classifiers.\n* The complexity of implementing a mechanical pre-processing step to combine existing labels into a single STAC `label` item is avoided.\n\nHowever, this decision also introduces some challenges:\n\n* The `ml-aoi` consumer will need to be designed to handle multiple labels and decipher the label structure.\n* The consumer will need to have access to metadata that describes the label structure, which could be achieved through a meta-label structure or crawling the full catalog.\n\nTo mitigate these challenges, we will implement a meta-label structure that combines the metadata from all the included labels into a single structure that can be interpreted by the consumer. This will enable the consumer to easily understand the label structure and make decisions based on the multiple labels.\n\nOverall, allowing multiple labels on `ml-aoi` Items provides more flexibility and functionality, but requires careful consideration of the implementation details to ensure that the consumer can effectively handle the multiple labels.","GenTime":"2024-08-10 10:14:37"}
{"File Name":"texas-holdem-code-challenge\/0008-use-typescript.md","Context":"## Context\\nThe approach I have chosen (see docs\/model.md and docs\/algorithm.md) leads to\\nmany small unit-testable components and a main function that ties them together.\\nThe options I can see for the main function are:\\n1. Write it with unit tests, heavily mocking the dependencies\\n2. Use a type system to check that the plumbing matches up, don't unit test the\\nmain function\\n3. Don't test the main function, don't use types. My guess is I'll make some\\nmistake along the way and have to debug it.\\nIn all cases I'll run integration tests, but without (1) or (2) I won't know\\nwhere bugs are when the integration test fails.\\nMy guess is that (2) will be more efficient than (1), and either of them will be\\nmore efficient than (3).\\nFor JavaScript static typing, I'm familiar with TypeScript.\\n","Decision":"Use TypeScript.\\n","tokens":193,"id":2290,"Prediction":"## Decision\n\n**Option 2: Use a type system to check that the plumbing matches up, don't unit test the main function**\n\n**Rationale:**\n\n* The approach of breaking down the system into many small, unit-testable components is a good one, as it allows for easier maintenance and testing of individual components.\n* Writing unit tests for the main function with heavy mocking of dependencies (Option 1) would add unnecessary complexity and overhead, and might not provide much value in terms of testing the main function's logic.\n* Using a type system, such as TypeScript, to check that the plumbing matches up (Option 2) is a more efficient and effective way to ensure that the main function is correctly wired together. TypeScript's type checking can catch errors at compile-time, rather than at runtime, which can save time and effort in the long run.\n* Option 3, not testing the main function at all, is not a viable option, as it would leave the system vulnerable to bugs and errors.\n* Running integration tests is still necessary to ensure that the system as a whole works correctly, but with Option 2, the type system will provide additional confidence that the main function is correctly configured.\n\n**Consequences:**\n\n* The main function will not have explicit unit tests, but its correctness will be ensured by the type system.\n* The system will require a good understanding of TypeScript's type system and how to use it effectively.\n* The development process may require more attention to type annotations and interfaces, but this will pay off in the long run by reducing errors and improving code quality.","GenTime":"2024-08-10 10:15:35"}
{"File Name":"texas-holdem-code-challenge\/0002-use-node-js.md","Context":"## Context\\n- The problem instructions state \"C++ or node.js are preferred\".\\n- I am most familiar with node.js out of those two.\\n- I am completing this problem in my spare time and have a limited amount of\\ntime to spend on it: it's convenient to work faster if possible.\\n","Decision":"Use Node.js to complete this problem.\\n","tokens":67,"id":2294,"Prediction":"## Decision\n\nGiven the context, I decide to use Node.js for this project. The problem instructions explicitly state that C++ or Node.js are preferred, and since you are most familiar with Node.js, it will be a more efficient use of your time. Additionally, considering you have a limited amount of time to spend on this project, using a technology you are already familiar with will allow you to work faster and complete the project more quickly. This decision will enable you to leverage your existing knowledge and skills, making the development process more efficient and reducing the overall time spent on the project.","GenTime":"2024-08-10 10:16:18"}
{"File Name":"texas-holdem-code-challenge\/0009-use-typescript-3-4-5.md","Context":"## Context\\nThe TypeScript plugin for ESLint requires TypeScript below 3.5.0. The next earlier\\nversion is 3.4.5 (from ~ 1 month ago)\\nWarning from ESLint:\\n```\\n=============\\nWARNING: You are currently running a version of TypeScript which is not officially supported by typescript-estree.\\nYou may find that it works just fine, or you may not.\\nSUPPORTED TYPESCRIPT VERSIONS: >=3.2.1 <3.5.0\\nYOUR TYPESCRIPT VERSION: 3.5.1\\nPlease only submit bug reports when using the officially supported version.\\n=============\\n```\\nFurther, there are no cutting edge TypeScript features I need for this.\\n","Decision":"Use TypeScript 3.4.5\\n","tokens":158,"id":2296,"Prediction":"## Decision\n\nDowngrade the TypeScript version to 3.4.5 to ensure compatibility with the TypeScript plugin for ESLint, which only supports versions below 3.5.0.\n\n## Rationale\n\nThe TypeScript plugin for ESLint requires a specific version of TypeScript to function correctly, and downgrading to 3.4.5 will ensure compatibility. Since the user does not require cutting-edge TypeScript features, this downgrade will not impact the functionality of the software.\n\n## Consequences\n\n* The software will be compatible with the TypeScript plugin for ESLint.\n* The user will not be able to use newer TypeScript features.\n* The software will be running an older version of TypeScript, which may have security vulnerabilities or other issues that are fixed in later versions.\n\n## Alternatives\n\n* Upgrade the TypeScript plugin for ESLint to support newer versions of TypeScript, which would require significant changes to the plugin and may not be feasible.\n* Use a different TypeScript plugin that supports newer versions of TypeScript, which may require additional configuration and may not provide the same level of functionality as the current plugin.\n* Continue using the current version of TypeScript, which may result in issues with the TypeScript plugin for ESLint and may not be compatible with future versions of the plugin.","GenTime":"2024-08-10 10:16:53"}
{"File Name":"skunkworks\/state-management.md","Context":"## Context\\nTypes of State:\\n- Local State\\n- e.g. React `useState` Hook\\n- Shared State\\n- e.g. React Context, Redux\\n- Remote State (i.e. API)\\n- e.g. React Context, Redux\\nShared State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked`\\nRemote State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked` \/ `@tanstack\/react-query`\\n","Decision":"- Local State: React `useState` Hook\\n- Shared State: `recoil`\\n- Remote State: `@tanstack\/react-query`\\n","tokens":208,"id":2298,"Prediction":"Here is the Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\nWe will use `@tanstack\/react-query` for managing Remote State in our application.\n\n**Context**\n\nAs we have multiple types of state in our application, including Local State, Shared State, and Remote State, we need to decide on a suitable solution for managing Remote State. We have considered several options, including `redux` + `redux-observable` + `rxjs` + `react-redux`, React Context, and `constate` \/ `recoil` \/ `react-tracked`. However, after evaluating the pros and cons of each option, we have decided to use `@tanstack\/react-query` for managing Remote State.\n\n**Rationale**\n\nWe chose `@tanstack\/react-query` for several reasons:\n\n* **Easy to use**: `@tanstack\/react-query` provides a simple and intuitive API for managing Remote State, making it easy to integrate into our application.\n* **Automatic caching**: `@tanstack\/react-query` provides automatic caching of Remote State, which reduces the number of requests to the API and improves performance.\n* **Error handling**: `@tanstack\/react-query` provides built-in error handling for Remote State, which helps to handle errors and edge cases more effectively.\n* **Less boilerplate**: `@tanstack\/react-query` requires less boilerplate code compared to other options, making it easier to maintain and update our application.\n\n**Consequences**\n\nBy using `@tanstack\/react-query` for managing Remote State, we will:\n\n* **Improve performance**: By caching Remote State, we will reduce the number of requests to the API, which will improve performance and reduce the load on our servers.\n* **Simplify code**: By using a simple and intuitive API, we will reduce the complexity of our code and make it easier to maintain and update.\n* **Improve error handling**: By using built-in error handling, we will be able to handle errors and edge cases more effectively, which will improve the overall reliability of our application.\n\n**Status**\n\nThis decision is approved and will be implemented in our application.","GenTime":"2024-08-10 10:17:40"}
{"File Name":"lbh-adrs\/Validation.md","Context":"## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n","Decision":"**Fluent Validation**\\nFluent Valdation is widely used, offers a lot of flexibility and allows for a clean, customisable and testable approach to validation.\\n","tokens":276,"id":2307,"Prediction":"## **Decision**\n\n**Title:** Validation Strategy for API\n\n**Status:** Accepted\n\n**Context:** Validation is a crucial function within APIs to ensure that data submitted via API calls meets the business requirements.\n\n**Problem:** The current validation approach using Data Annotations has several issues, including scattered validation throughout the codebase, difficulty in testing, and inflexibility in error messaging and conditional validation.\n\n**Decision:** We will use Fluent Validation as our validation strategy for API.\n\n**Rationale:**\n\n* Fluent Validation addresses the issues with Data Annotations, providing a more centralized and flexible approach to validation.\n* It allows for easy configuration, testing, and customization of validation rules.\n* The ability to externalize error messaging using dependency injection provides greater flexibility and maintainability.\n* Fluent Validation's built-in validation features, such as conditional validation, make it a more powerful and efficient solution.\n\n**Consequences:**\n\n* We will need to refactor our existing validation logic to use Fluent Validation.\n* We will need to update our testing strategy to accommodate the new validation approach.\n* We will need to ensure that our error messaging is properly configured and tested.\n\n**Alternatives Considered:**\n\n* Continuing to use Data Annotations, despite its limitations.\n* Implementing a custom validation solution, which would require significant development effort and maintenance.\n\n**Implementation:**\n\n* We will start by configuring Fluent Validation in our `Startup.cs` file.\n* We will refactor our existing validation logic to use Fluent Validation.\n* We will update our testing strategy to accommodate the new validation approach.\n* We will ensure that our error messaging is properly configured and tested.\n\n**Monitoring and Feedback:**\n\n* We will monitor the performance and effectiveness of Fluent Validation in our API.\n* We will gather feedback from developers and stakeholders on the new validation approach.\n* We will make adjustments to our validation strategy as needed to ensure it meets the business requirements and is maintainable.","GenTime":"2024-08-10 10:20:43"}
{"File Name":"lbh-adrs\/Event-Driven-Architecture.md","Context":"## **Context**\\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\\nIn AWS, two options can be considered:\\n1. SNS (Simple Notification Service)\\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\\n","Decision":"**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\\n","tokens":209,"id":2309,"Prediction":"## **Decision**\n\nWe will use Amazon SNS (Simple Notification Service) + SQS (Simple Queue Service) as our message broker for implementing an event-driven architecture in our microservices.\n\n## **Rationale**\n\nWe chose this approach because it provides the benefits of both pub\/sub messaging and message queuing. Using SNS allows us to publish messages to multiple subscribers, which enables loose coupling between our microservices. Additionally, using SQS provides a buffer between sending and receiving components, allowing us to decouple the timing of message processing and ensuring that messages are not lost in case of failures.\n\nThis approach also provides flexibility and scalability, as we can use SNS to send messages to multiple subscribers and SQS to store messages for later processing. This allows us to handle different use cases and requirements for our microservices, such as real-time notifications and batch processing.\n\n## **Consequences**\n\n* We will need to implement message handlers for each microservice that subscribes to the SNS topic.\n* We will need to configure the SNS topic and SQS queue to ensure that messages are properly routed and processed.\n* We will need to monitor and manage the SNS topic and SQS queue to ensure that messages are being processed correctly and efficiently.\n* We will need to consider the trade-offs between using SNS and SQS, such as the cost of using SQS versus the benefits of message persistence.\n\nOverall, using Amazon SNS + SQS provides a scalable and flexible solution for implementing an event-driven architecture in our microservices.","GenTime":"2024-08-10 10:21:16"}
{"File Name":"lbh-adrs\/Event-Driven-Architecture-Message-Types.md","Context":"## **Context**\\nAlongside the decision to adopt an event driven architecture, there is a need to define what an event will look like. There are several options for events:\\n- **Thin Events**\\nA thin event consists of the minimum amount of data that is required that will allow a subscriber to retrieve everything it needs. This normally consists of an ID with which to make an API call back to the source publisher to gather the data it needs.\\nThe benefits of thin events are:\\n- The payload is small in size\\n- Data is always up to date as it is retrieved at the point of consumption\\n- If calls to APIs fail due to unavailability of APIs, the message can be replayed\\n- Very little need for event versioning\\nThe downsides are:\\n- Consumers need to make API calls to gather the data they need\\n- **Fat Events**\\nA fat event contains all the data necessary for any subscriber to be able to perform its job.\\nThe benefits of fat events are:\\n- all the data needed for consumer processing is present in the event\\n- no need to make any API calls to retrieve data\\nThe downsides are:\\n- Event payload could grow to be quite big\\n- Data present in the payload may no longer be required by any consumer\\n- It is difficult to version events easily (and multiple versions of the same event may need to be sent for backwards compatibility)\\n**Hybrid Approach**\\nIdeally, we should use thin events wherever possible,as this reduces the complexity around sharing events, updating consumers with new versions of events, etc. However, there are some instances where a thin event might not be possible - notably when updating an activity audit log with details of what has changed. Therefore, the best solution would be to have a thin event that contains an optional message body to hold a specific payload.\\n","Decision":"**Hybrid Approach**\\nThe easiest solution is to use a hybrid approach, with consumers gathering the data they need using API calls. This has the benefit of reducing the need for event versioning, and event payloads that grow in size over time.\\nThe event payload will be:\\n```\\n{\\n\"id\": \"8e648f3d-9556-4896-8400-211cb1c5451b\",\\n\"eventType\": \"personCreated\",\\n\"sourceDomain\": \"person\",\\n\"sourceSystem\": \"personAPI\",\\n\"version\": \"v1\",\\n\"correlationId\": \"f4d541d0-7c07-4524-8296-2d0d50cb58f4\",\\n\"dateTime\": \"2021-05-17T11:59:57.25Z\",\\n\"user\": {\\n\"id\": \"ac703d87-c100-40ec-90a0-dabf183e7377\",\\n\"name\": \"Joe Bloggs\",\\n\"email\": \"joe.bloggs@hackney.gov.uk\"\\n},\\n\"entityId\": \"45c76564-2e38-48f3-bb31-6bab2fef8623\",\\n\"eventBody\": {\\n\"oldData\": {\\n\"optionalProperty1\": \"Property value\",\\n\"optionalProperty2\": \"Property value\",\\n\"optionalProperty3\": \"Property value\"\\n}\\n}\\n}\\n```\\n","tokens":384,"id":2316,"Prediction":"## **Decision**\n\nWe will adopt a hybrid approach to designing events, where we use thin events as the default, but also allow for optional message bodies in certain situations where a fat event is necessary. This approach balances the benefits of thin events (small payload, up-to-date data, and easy replayability) with the need for a fat event in specific cases where the data is complex or changing frequently.\n\n## **Rationale**\n\nThe hybrid approach allows us to take advantage of the benefits of thin events while still providing a flexible solution for cases where a fat event is necessary. By using thin events as the default, we can reduce the complexity around sharing events and updating consumers with new versions of events. However, by allowing for optional message bodies in certain situations, we can provide a fat event-like experience for specific use cases where the data is complex or changing frequently.\n\n## **Consequences**\n\n* The event payload will be smaller in size, reducing the overhead of event processing and storage.\n* Consumers will need to make API calls to gather data in certain situations, but this will be minimized by using thin events as the default.\n* The complexity around sharing events and updating consumers with new versions of events will be reduced.\n* The approach will provide a flexible solution for cases where a fat event is necessary, allowing for a more tailored experience for specific use cases.\n\n## **Alternatives Considered**\n\n* Using only thin events, which would reduce the complexity around event processing and storage, but may not provide a suitable solution for cases where a fat event is necessary.\n* Using only fat events, which would provide a more comprehensive solution for certain use cases, but may result in larger event payloads and increased complexity around event versioning.\n\n## **Monitoring and Feedback**\n\nWe will monitor the performance and scalability of the event-driven architecture and gather feedback from consumers and stakeholders to ensure that the hybrid approach is effective and efficient. We will also regularly review and refine the approach as needed to ensure that it continues to meet the evolving needs of the system.","GenTime":"2024-08-10 10:23:30"}
{"File Name":"lbh-adrs\/Microfrontend-Composition.md","Context":"## **Context**\\nAccording to the [Microfrontend-Architecture ADR](https:\/\/github.com\/LBHackney-IT\/lbh-adrs\/blob\/feature\/create-ADRs\/Platform\/Accepted\/Microfrontend-Architecture.md), we are going to define micro-frontends by using a vertical splitting.\\nIn this way each micro-frontends has got its repository and each team manages the development and the deployment of it.\\nAbout the client-side micro-frontends compositions, there are two options to implement that:\\n1. Writing our own boilerplate code\\nEach micro frontend is included in the html page using a \\<script> tag and each of those applications exposes a global function as its entry-point. The container application determines which frontend should be mounted and calls the relevant function to tell a micro frontend when and where to render itself.\\n2. Using a framework as Single SPA (https:\/\/single-spa.js.org\/ )\\nSingle SPA is a Javascript framework for frontend microservices. In practice, it applies a lifecycle to every application. Each app can respond to url routing events and must know how to bootstrap, mount and unmount itself from the DOM.\\n","Decision":"**Using Single SPA framework**\\nThis framework adopts the same principle as we would implement our own boilerplate code, but the advantage is that we don\u2019t have to build and document our own logic but instead we use this light framework supported by an open source community.\\nThe other advantage is that the framework allows to compose also applications written in Angualar and Vue.js\\n","tokens":242,"id":2317,"Prediction":"## **Decision**\n\nAfter careful consideration of the options, we have decided to use the Single SPA framework to implement the client-side micro-frontends compositions. This decision is based on the following reasons:\n\n* **Reusability**: Single SPA provides a reusable solution for managing multiple micro-frontends, which aligns with our goal of creating a scalable and maintainable architecture.\n* **Simplified Development**: Using Single SPA reduces the need to write boilerplate code for each micro-frontend, allowing our teams to focus on developing their respective applications.\n* **Easier Maintenance**: Single SPA's lifecycle management and event-driven architecture make it easier to maintain and debug our micro-frontends.\n* **Improved Collaboration**: By using a standardized framework, our teams can collaborate more effectively and share knowledge and best practices.\n\nWe will use Single SPA to manage the composition of our micro-frontends, allowing each application to respond to URL routing events and manage its own lifecycle. This will enable us to take advantage of the benefits of micro-frontends while minimizing the complexity and overhead of managing multiple independent applications.\n\n## **Status**\n\nAccepted\n\n## **Implementors**\n\n* [Name 1]\n* [Name 2]\n\n## **References**\n\n* [Microfrontend-Architecture ADR](https:\/\/github.com\/LBHackney-IT\/lbh-adrs\/blob\/feature\/create-ADRs\/Platform\/Accepted\/Microfrontend-Architecture.md)\n* [Single SPA](https:\/\/single-spa.js.org\/)","GenTime":"2024-08-10 10:23:46"}
{"File Name":"easyvista_toolkit\/0001_Using_globalVariable.md","Context":"## Context\\nWe want to be able to have autocompletion in the cmdlet to make them more user friendly.\\nBecause the easyvista rest API relies on GUID for some parameters we'll need to be able to query their friendly name within in the cmdlet parameters.\\nWe tried using environment variables but they don't seem to work with hashtable (we did not check extensively).\\n","Decision":"We will use global variables (named *$Global:EZVvariablename*) set by a dedicated cmdlet (*set-EZVcontext*). That cmdlet will define an execution context for all other cmdlet in the project.\\n","tokens":79,"id":2329,"Prediction":"## Decision\n\nTo enable autocompletion in the cmdlet and to query the friendly name of GUID-based parameters, we will use a combination of the `Get-Command` cmdlet and the `PSObject` properties to populate a custom autocomplete dropdown.\n\n## Context\n\nAs mentioned in the context, we want to provide an autocompletion feature in the cmdlet to make it more user-friendly. Since the EasyVista REST API relies on GUIDs for some parameters, we need to be able to query their friendly names within the cmdlet parameters. We tried using environment variables, but they don't seem to work with hashtables.\n\n## Decision Rationale\n\nUsing the `Get-Command` cmdlet and `PSObject` properties will allow us to dynamically generate a list of available parameters and their friendly names, which can then be used to populate the autocomplete dropdown. This approach is more flexible and scalable than using environment variables, which are limited to a fixed set of values.\n\n## Consequences\n\n* We will need to implement a custom autocomplete dropdown using the `Get-Command` cmdlet and `PSObject` properties.\n* We will need to populate the dropdown with the available parameters and their friendly names.\n* The autocomplete feature will be available for all cmdlet parameters that have a corresponding friendly name in the EasyVista REST API.\n\n## Alternatives Considered\n\n* Using environment variables, but as mentioned earlier, this approach has limitations.\n* Implementing a custom autocomplete dropdown using a separate data source, such as a database or a file, but this would require additional infrastructure and maintenance.\n\n## Status\n\nThis decision is approved and will be implemented in the next release of the cmdlet.","GenTime":"2024-08-10 10:26:24"}
{"File Name":"generator-latex-template\/0005-custom-example-environment.md","Context":"## Context and Problem Statement\\nWe want to present LaTeX examples.\\nThey should be shown both as LaTeX source and as rendered output.\\n","Decision":"* Write once - have two outputs (source and rendered)\\n* Good interaction with all pacakges\\n* Should support floats\\n* Should support compilation without special configuration (such as `--shell-escape`)\\nChosen option: \"custom solution\", because resolves all forces.\\n### Positive Consequences\\n* Works with floats and umlauts\\n### Negative Consequences\\n* A little bit more than minimal code is presented to the user\\n","tokens":28,"id":2337,"Prediction":"## Decision\n\n**Decision:** We will use a combination of LaTeX rendering libraries and Markdown syntax to present LaTeX examples in the application.\n\n**Context:** We want to present LaTeX examples, showing both the LaTeX source code and the rendered output. This requires a solution that can render LaTeX code and display the output alongside the source.\n\n**Rationale:**\n\n* We need a library that can render LaTeX code to produce the output, which can be achieved using a LaTeX rendering library such as MathJax or KaTeX.\n* We also need a way to display the LaTeX source code alongside the rendered output, which can be achieved using Markdown syntax to format the code blocks.\n\n**Consequences:**\n\n* The application will require the installation of a LaTeX rendering library, which may add some complexity to the setup process.\n* The rendered output may not be identical to the output produced by a standalone LaTeX compiler, as the rendering libraries may use different rendering engines or have different settings.\n* The use of Markdown syntax for code blocks will allow for easy formatting of the code and output, making it easier for users to read and understand the examples.\n\n**Alternatives Considered:**\n\n* Using a single library that can render LaTeX code and display the output, such as a LaTeX-aware Markdown processor. However, this may limit our flexibility in terms of customization and configuration.\n* Using a separate LaTeX compiler to render the code and then displaying the output, which may add complexity to the application and require additional dependencies.\n\n**Status:** This decision has been implemented and is currently in use.","GenTime":"2024-08-10 10:29:05"}
{"File Name":"interlok\/0003-deprecate-dynamicservicelocator.md","Context":"## Context and Problem Statement\\nWith the standard document trading network, there is a move towards centralised processing such that companies need only define their \"mapping specification\" and key information, and everything is handled centrally.\\nTraditionally, this was done via DynamicServiceLocator with files located on the fileystem named in the form : `SRC-DST-MSG_TYPE.xml`; recently, this has been done in a dedicated application which is now deprecated.\\nThe way that DynamicServiceLocator works contains a lot of extraneous configuration that only have a single implementation; it was designed for extensibility, but it's over complicated in terms of XML and coupling. It needs to be simplified so that it's more understandable in the UI.\\n","Decision":"Deprecate DynamicServiceLocator, leave it available but marked as deprecated. Improve DynamicServiceExcutor so that it can be used instead.\\n### Enhancing DynamicServiceExecutor.\\nWhat this means is to implement additional ServiceExtractor implementations that allow the user to extract the _`_service-to-execute_ from an external location; currently the two supplied implementations simply use the _AdaptrisMessage_ object to extract the services.\\nThe additions required are :\\n* Extract from a URL such that you can configure `http:\/\/my.server.com\/%message{source}\/%message{destination}\/%message{messageType}.xml`. This would replace the existing DynamicServiceLocator functionality from _RemoteServiceStore_ and _LocalServiceStore_.\\n* Extract from Database -> such that you can configure something like `SELECT dynamicService FROM services WHERE src='%message{source}' AND dest='%message{destination}' AND msgType='%message{messageType}'`\\n* The above would be quite \"open\" to SQL injection style attacks; so people should probably use _JdbcDataQueryService_ and then handle it from metadata.\\n* Extract from a Cache -> similar to doing a RetrieveFromCache\\n* Extract it from metadata.\\n","tokens":146,"id":2342,"Prediction":"## Decision\n\n**Title:** Simplification of DynamicServiceLocator Configuration\n\n**Context:** With the shift towards centralized processing, companies need to define their \"mapping specification\" and key information, and everything is handled centrally. The traditional approach using DynamicServiceLocator with XML files is complex and needs to be simplified.\n\n**Problem Statement:** The current DynamicServiceLocator configuration is overcomplicated, with excessive XML and coupling, making it difficult to understand and maintain.\n\n**Decision:** We will replace the DynamicServiceLocator configuration with a more straightforward and intuitive approach using a dedicated configuration file (e.g., JSON or YAML) that is easier to read and maintain. This will reduce the complexity of the configuration and improve the overall usability of the system.\n\n**Rationale:**\n\n1. Simplify the configuration process: By using a dedicated configuration file, we can eliminate the need for complex XML files and reduce the amount of extraneous configuration.\n2. Improve maintainability: A simpler configuration will be easier to understand and maintain, reducing the risk of errors and making it easier to make changes.\n3. Enhance usability: The new configuration approach will provide a more intuitive and user-friendly experience, making it easier for users to define their mapping specifications and key information.\n\n**Consequences:**\n\n1. Reduced complexity: The new configuration approach will be simpler and easier to understand.\n2. Improved maintainability: The configuration will be easier to maintain and update.\n3. Enhanced usability: Users will have a more intuitive and user-friendly experience when defining their mapping specifications and key information.\n\n**Implementation:** We will create a new configuration file format (e.g., JSON or YAML) that is easier to read and maintain. We will then migrate the existing DynamicServiceLocator configuration to the new format and update the system to use the new configuration approach.","GenTime":"2024-08-10 10:30:48"}
{"File Name":"interlok\/0006-workflow-callback.md","Context":"## Context and Problem Statement\\nWhen you enable [Dead Letter Queues](https:\/\/docs.aws.amazon.com\/AWSSimpleQueueService\/latest\/SQSDeveloperGuide\/sqs-dead-letter-queues.html) in SQS; the contract is that messages read from a source queue are failed over to the dead letter queue once the redrive policy is fired (i.e. after a max number of attempts to deliver the message). From testing, this is predicated on the fact that the message is not deleted. Of course, our sqs-polling-consumer deletes the message after submitting it to the workflow.\\nSince we can't rely on the JMS semantics either (since onMessage() traditionally doesn't throw exceptions, and RuntimeExceptions aren't propagated by Interlok) we then have to think about some way of having an asynchronous callback.\\n","Decision":"Chosen option: Modify AdaptrisMessageListener to have callbacks.\\n","tokens":171,"id":2343,"Prediction":"## Decision\n\nTo address the issue of deleted messages in SQS Dead Letter Queues and the lack of reliable asynchronous callback mechanisms in JMS, we will implement a custom retry mechanism using a separate retry queue and a custom retry handler.\n\n## Context\n\nAs described in the problem statement, enabling Dead Letter Queues in SQS requires that messages are not deleted after being read from the source queue. However, our sqs-polling-consumer deletes the message after submitting it to the workflow. Additionally, JMS semantics do not provide a reliable way to propagate exceptions or implement asynchronous callbacks.\n\n## Rationale\n\nThe custom retry mechanism will provide a way to handle failed messages in a more robust and reliable manner. By using a separate retry queue, we can store failed messages and attempt to retry them a specified number of times before moving them to the dead letter queue. This approach will ensure that messages are not lost and will provide a more robust way to handle failures.\n\n## Decision Drivers\n\n* The need to ensure that messages are not lost in the event of failures\n* The lack of reliable asynchronous callback mechanisms in JMS\n* The requirement to handle failed messages in a more robust and reliable manner\n\n## Consequences\n\n* Additional complexity in the sqs-polling-consumer codebase\n* Increased overhead in terms of message storage and processing\n* Improved reliability and fault tolerance in the event of failures\n\n## Alternatives Considered\n\n* Implementing a custom JMS provider with asynchronous callback support\n* Using a different messaging system that provides built-in support for asynchronous callbacks and dead letter queues\n* Ignoring the issue and relying on the JMS semantics, which may lead to message loss in the event of failures\n\n## Implementation\n\nThe custom retry mechanism will be implemented using a separate retry queue and a custom retry handler. The retry handler will be responsible for checking the message status, attempting to retry the message if necessary, and moving it to the dead letter queue if all retries have failed. The sqs-polling-consumer will be modified to store failed messages in the retry queue and to use the custom retry handler to handle retries.","GenTime":"2024-08-10 10:31:11"}
{"File Name":"agentframework\/0008-minify-build.md","Context":"## Context\\nAgentFramework is also use for Browser\\n","Decision":"We need minify build because package.json don't have a property called `minMain`\\n","tokens":11,"id":2359,"Prediction":"## Decision\n\n**Decision:** To use a separate JavaScript library for handling browser-specific features in the AgentFramework, instead of relying on the same framework for both browser and server-side development.\n\n**Context:** As the AgentFramework is also being used for browser-based applications, we need to consider the differences in how the framework is used in a browser versus a server. This decision aims to address the challenges and limitations that come with using the same framework for both environments.\n\n**Rationale:**\n\n1. **Browser-specific features**: The browser has its own set of features and APIs that are not available on the server-side. By using a separate JavaScript library, we can take advantage of these features, such as WebSockets, WebRTC, and browser-specific storage mechanisms, without compromising the server-side functionality.\n2. **Separation of concerns**: Using a separate library for browser-specific features helps to keep the AgentFramework focused on its core responsibilities, such as handling agent logic and communication. This separation of concerns makes it easier to maintain and update the framework.\n3. **Reusability**: A separate JavaScript library for browser-specific features can be reused across multiple browser-based applications, reducing the need for duplicated code and increasing maintainability.\n4. **Improved performance**: By using a library optimized for browser-specific features, we can improve the performance and responsiveness of our browser-based applications.\n\n**Consequences:**\n\n1. **Increased complexity**: Using a separate library for browser-specific features adds to the overall complexity of the AgentFramework, requiring additional development, testing, and maintenance efforts.\n2. **Additional dependencies**: The AgentFramework will have an additional dependency on the browser-specific library, which may introduce new versioning and compatibility issues.\n\n**Implementation:**\n\n1. **Choose a suitable library**: Select a JavaScript library that provides the necessary browser-specific features and is well-maintained and widely used.\n2. **Integrate the library**: Integrate the chosen library into the AgentFramework, ensuring seamless communication between the two.\n3. **Test and iterate**: Thoroughly test the AgentFramework with the new library and iterate on the design and implementation as needed.\n\nBy making this decision, we can effectively handle browser-specific features and improve the overall performance and maintainability of the AgentFramework.","GenTime":"2024-08-10 10:36:31"}
{"File Name":"agentframework\/0002-use-es6-proxy-to-provide-transparent-interceptor-layer.md","Context":"## Context\\nIn ES5, implement interceptor will modify existing class and this operation is not reversible.\\nIt may cause compatibility issue with other frameworks.\\n","Decision":"We will add ES6 Proxy for a transparent layer which not modifying user's class\/object.\\n","tokens":31,"id":2360,"Prediction":"## Decision\n\nIn order to avoid potential compatibility issues with other frameworks and to ensure reversibility, we will not implement interceptors by modifying existing classes in ES5. Instead, we will use a decorator pattern to wrap the original class and achieve the desired functionality.\n\n## Context\n\nAs you mentioned, in ES5, modifying an existing class using an interceptor is not reversible and may cause compatibility issues with other frameworks.\n\n## Decision Rationale\n\nThe decorator pattern provides a more flexible and maintainable solution for adding additional functionality to an existing class without modifying its original implementation. This approach allows us to decouple the interceptor logic from the original class, making it easier to test, debug, and maintain.\n\n## Consequences\n\n* The interceptor logic will be decoupled from the original class, reducing the risk of compatibility issues with other frameworks.\n* The original class will remain unchanged, making it easier to maintain and test.\n* The decorator pattern will provide a more flexible and maintainable solution for adding additional functionality to the class.\n\n## Pros and Cons\n\nPros:\n\n* Decouples interceptor logic from the original class, reducing the risk of compatibility issues.\n* Allows for easier testing, debugging, and maintenance of the interceptor logic.\n* Provides a more flexible and maintainable solution for adding additional functionality to the class.\n\nCons:\n\n* May require additional boilerplate code to implement the decorator pattern.\n* May require additional testing to ensure that the decorator pattern is working correctly.\n\n## Implementation\n\nWe will implement the decorator pattern using a simple wrapper class that wraps the original class and adds the desired functionality. For example:\n```javascript\nfunction interceptorDecorator(originalClass) {\n  return class extends originalClass {\n    \/\/ Add interceptor logic here\n  };\n}\n```\nWe will then use this decorator function to wrap the original class and add the desired functionality. For example:\n```javascript\nconst originalClass = class {\n  constructor() {\n    console.log('Original class constructor');\n  }\n};\n\nconst decoratedClass = interceptorDecorator(originalClass);\n\nconst instance = new decoratedClass();\ninstance.constructor(); \/\/ Output: \"Interceptor decorator constructor\"\n```\nBy using the decorator pattern, we can add additional functionality to the original class without modifying its original implementation, ensuring that the class remains compatible with other frameworks and easy to maintain.","GenTime":"2024-08-10 10:36:55"}
{"File Name":"zendesk_app_scaffold_react_redux\/replace-migration-scaffold-with-clean-scaffold-in-master-branch.md","Context":"## Context\\nAt the moment, the master branch has the scaffold specifically built to help developers migrate v1 apps to v2. From monitor, the majority usage comes from [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator) and [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) which use the master branch as the download source.\\nIt is more helpful to show developers coming directly to this repo a clean (from scratch) scaffold, one usable as a starting point to build new apps respecting our current conventions out of the box.\\n","Decision":"### New app scaffold\\nA new *clean* app scaffold has been built on branch [offapps-migration](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/offapps-migration) based on branch [from-scratch](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/from-scratch)\\n### Move migration scaffold into App Migrator repository\\nMigration scaffold in master branch will be moved into [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator), used and maintained as a *built-in* template resource\\n### Move new app scaffold to master branch\\n### Add support in [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) to create new app with the new scaffold\\n* New command option `zat new --scaffold` will create a v2 app using the new app scaffold.\\n","tokens":124,"id":2378,"Prediction":"## Decision\n\n**Decision:** Move the \"clean\" scaffold to a new branch, `main`, and remove the migration-specific code from the master branch.\n\n**Context:** As stated, the current master branch has a scaffold specifically built to help developers migrate v1 apps to v2. However, the majority of usage comes from [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator) and [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) which use the master branch as the download source. This means that developers who come directly to this repo are currently seeing the migration-specific code, which may be confusing or overwhelming.\n\n**Problem:** The current master branch is serving two purposes: providing a migration scaffold and serving as a starting point for new apps. This is causing confusion and may lead to developers getting lost in the migration code.\n\n**Solution:** Create a new branch, `main`, which will contain a clean, \"from scratch\" scaffold that can be used as a starting point for building new apps. This will allow developers to easily start building new apps that respect the current conventions out of the box. The migration-specific code will be removed from the master branch, and the master branch will be updated to point to the new `main` branch.\n\n**Rationale:** This decision will provide a clear and clean starting point for developers who want to build new apps, while also keeping the migration-specific code separate and organized. This will make it easier for developers to find what they need and will reduce confusion.\n\n**Consequences:**\n\n* Developers who want to build new apps will have a clear and clean starting point.\n* The migration-specific code will be organized and separate from the main codebase.\n* The master branch will no longer contain migration-specific code, making it easier for developers to find what they need.\n* [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator) and [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) will need to be updated to point to the new `main` branch.\n\n**Monitoring:** The impact of this decision will be monitored by tracking the usage of the new `main` branch and the feedback from developers. If there are any issues or concerns, the decision can be revisited and adjusted accordingly.","GenTime":"2024-08-10 10:42:35"}
{"File Name":"beis-report-official-development-assistance\/0002-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":2383,"Prediction":"## Decision\n\n**Decision:** Implement a query logging mechanism to track and analyze database queries in the application.\n\n## Context\n\nAs you mentioned, it can be easy to miss inefficient database queries during code review, which can lead to performance issues and negatively impact the user experience. To address this, we will implement a query logging mechanism that tracks and analyzes database queries in the application.\n\n## Problem\n\nInefficient database queries can lead to:\n\n* Poor performance and slow response times\n* Increased load on the database and infrastructure\n* Decreased user satisfaction and experience\n\n## Rationale\n\nImplementing a query logging mechanism will allow us to:\n\n* Identify and track database queries in real-time\n* Analyze query performance and identify bottlenecks\n* Optimize queries to improve performance and efficiency\n* Monitor query changes and detect potential issues\n\n## Decision Criteria\n\nWe considered the following criteria when making this decision:\n\n* Ease of implementation and maintenance\n* Effectiveness in identifying and tracking database queries\n* Ability to analyze query performance and optimize queries\n* Compatibility with existing database and infrastructure\n\n## Consequences\n\nImplementing a query logging mechanism will:\n\n* Provide a centralized location for tracking and analyzing database queries\n* Allow for easier identification and optimization of inefficient queries\n* Improve overall application performance and user experience\n* Increase transparency and visibility into database query activity\n\n## Status\n\nThis decision is currently in the \"proposed\" state. We will implement the query logging mechanism in the next sprint and monitor its effectiveness.\n\n## Affected Components\n\n* Database queries\n* Application performance\n* User experience\n\n## References\n\n* [Query Logging Best Practices](https:\/\/www.percona.com\/blog\/2018\/06\/12\/query-logging-best-practices\/)\n* [Database Query Optimization](https:\/\/www.tutorialspoint.com\/dbms\/dbms_query_optimization.htm)","GenTime":"2024-08-10 10:44:32"}
{"File Name":"beis-report-official-development-assistance\/0011-use-govuk-design-system-form-builder.md","Context":"## Context\\nBuilding forms in Rails that are compliant with the GOVUK Design System involve\\nmanually declaring the correct HTML structure, class names and ARIA attributes,\\nwhich is time-consuming and hard to get right.\\nAdditionally, our validation errors currently use Rails' default pattern, rather\\nthan the one recommended for use as part of the design system, which is designed\\nwith accessibility in mind.\\n","Decision":"We will use DfE's `govuk_design_system_formbuilder` to simplify the creation of\\nGOV.UK Design System-compliant forms.\\nAs we are currently using Simple Form rather than Rails' default form builder\\nfor our other forms, the two form builders can co-exist for the time being,\\nwhilst we transition the forms over.\\n","tokens":83,"id":2386,"Prediction":"## Decision\n\nTo simplify and improve the building of forms in Rails that are compliant with the GOVUK Design System, we will create a custom Rails gem that provides a set of form helpers that automatically generate the correct HTML structure, class names, and ARIA attributes. This will reduce the time and effort required to build compliant forms and ensure that they are accessible.\n\n## Context\n\nBuilding forms in Rails that are compliant with the GOVUK Design System involve manually declaring the correct HTML structure, class names, and ARIA attributes, which is time-consuming and hard to get right. Additionally, our validation errors currently use Rails' default pattern, rather than the one recommended for use as part of the design system, which is designed with accessibility in mind.\n\n## Problem\n\nThe current process of building forms in Rails that are compliant with the GOVUK Design System is manual and error-prone. This can lead to forms that are not accessible or do not meet the design system's guidelines. The use of Rails' default validation error pattern also means that our forms are not following best practices for accessibility.\n\n## Decision Criteria\n\n* Ease of use: The form helpers should be easy to use and integrate into existing Rails applications.\n* Accessibility: The form helpers should generate HTML that is accessible and follows the guidelines of the GOVUK Design System.\n* Customizability: The form helpers should allow for customization of the generated HTML to accommodate specific requirements.\n\n## Rationale\n\nCreating a custom Rails gem that provides form helpers will simplify the process of building forms in Rails that are compliant with the GOVUK Design System. This will reduce the time and effort required to build compliant forms and ensure that they are accessible. The gem will provide a set of form helpers that automatically generate the correct HTML structure, class names, and ARIA attributes, making it easier to build forms that meet the design system's guidelines.\n\n## Consequences\n\n* The gem will provide a set of form helpers that are easy to use and integrate into existing Rails applications.\n* The gem will generate HTML that is accessible and follows the guidelines of the GOVUK Design System.\n* The gem will allow for customization of the generated HTML to accommodate specific requirements.\n* The gem will reduce the time and effort required to build compliant forms and ensure that they are accessible.\n\n## Status\n\nThis decision has been made and the custom Rails gem is currently being developed. The gem will be released as an open-source project and will be maintained by the development team.","GenTime":"2024-08-10 10:45:36"}
{"File Name":"beis-report-official-development-assistance\/0004-add-rollbar-for-application-monitoring.md","Context":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n","Decision":"Use Rollbar to collect and manage our application errors.\\n","tokens":187,"id":2387,"Prediction":"## Decision\n\n**Error Management Mechanism: Rollbar vs. Other Solutions**\n\n## Context\n\nWe need a mechanism to capture and manage application errors, as our current debugging options are limited and inefficient. We have been using Rollbar for several years, but have not reviewed this decision since its implementation. With 14 applications currently using Rollbar, we need to consider whether this solution remains the best choice or if we should explore other options.\n\n## Problem\n\nOur current error management mechanism using Rollbar has some limitations. We have experienced issues with environment names not matching between Rollbar and our Rails application, which can make it difficult to manage and troubleshoot errors. Additionally, we have projects using different error management tools, such as Sentry, which can lead to inconsistencies and added complexity.\n\n## Decision\n\nAfter careful consideration, we have decided to continue using Rollbar as our primary error management mechanism for the following reasons:\n\n1. Familiarity: We have already invested significant time and resources into using Rollbar, and switching to a new solution would require a significant learning curve and additional effort.\n2. Scalability: Rollbar has proven to be a scalable solution for our growing application portfolio, and we have confidence in its ability to handle the increasing volume of errors and events.\n3. Integration: Rollbar provides a range of integrations with other tools and services, which can help us to streamline our error management workflow and improve our overall debugging and troubleshooting capabilities.\n\nHowever, we will also consider using other error management solutions, such as Sentry, on a project-by-project basis, where they may be better suited to the specific needs of that project. This will allow us to take advantage of the benefits of each solution and provide a more tailored error management approach for each application.\n\n## Solution\n\nTo address the issue of environment names not matching between Rollbar and our Rails application, we will make explicit use of the `ROLLBAR_ENV` variable to manage this configuration. This will help to ensure consistency and clarity in our error management configuration.\n\n## Rationale\n\nOur decision to continue using Rollbar as our primary error management mechanism is based on its scalability, familiarity, and integration capabilities. By making explicit use of the `ROLLBAR_ENV` variable, we can ensure consistency and clarity in our error management configuration and address the issue of environment names not matching between Rollbar and our Rails application.","GenTime":"2024-08-10 10:46:02"}
{"File Name":"beis-report-official-development-assistance\/0005-use-travis-for-ci.md","Context":"## Context\\nTravis is actively used for many other projects within the BEIS GitHub account.\\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\\n","Decision":"Use Travis to build and test the application.\\nUse Travis to automatically deploy the applications to live.\\n","tokens":45,"id":2393,"Prediction":"## Decision\n\nTo ensure consistency with other projects within the BEIS GitHub account and to accommodate the use of containers, we will use Travis CI as our Continuous Integration\/Continuous Deployment (CI\/CD) tool. Specifically, we will utilize Travis CI's Docker support to run our application in a containerized environment.\n\n## Rationale\n\n* Consistency: By using Travis CI, we can leverage the existing knowledge and expertise within the BEIS GitHub account, reducing the learning curve and minimizing the risk of errors.\n* Container support: Travis CI's Docker support allows us to run our application in a containerized environment, which is essential for our project's requirements.\n* Flexibility: Travis CI provides a wide range of features and integrations, making it a versatile tool for our CI\/CD pipeline.\n* Community support: Travis CI has a large and active community, ensuring that we can find help and resources when needed.\n\n## Consequences\n\n* We will need to configure our Travis CI file to support Docker and our application's specific requirements.\n* We will need to ensure that our application is containerized and can be run in a Docker environment.\n* We will need to test and validate our CI\/CD pipeline to ensure it is working correctly and efficiently.\n\n## Alternatives Considered\n\n* We considered using other CI\/CD tools, such as CircleCI or Jenkins, but ultimately chose Travis CI due to its widespread adoption within the BEIS GitHub account and its support for Docker.\n* We also considered using a different containerization platform, such as Kubernetes, but decided to stick with Docker due to its ease of use and wide adoption in the industry.","GenTime":"2024-08-10 10:48:18"}
{"File Name":"beis-report-official-development-assistance\/0030-run-data-migrations-manually.md","Context":"## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n","Decision":"With this in mind, we've decided to drop the use of the Data Migrate gem,\\nand instead run any data migrations manually. We have easy access to the\\nconsole via GOV.UK PaaS, so this is relatively painless.\\nWe have added a generator which creates an empty Ruby file with a timestamp and the name of the migration as the filename. Once the code is\\ndeployed, we run the migration on the production server.\\nAs the code is plain old Ruby, it's also easier for us to write tests, if\\nwe think it's worth doing (for example, the migrations code is suitably\\ncomplex).\\n","tokens":69,"id":2396,"Prediction":"## Decision\n\n**Decision:** To replace the Data Migrate gem with a custom solution for database migrations.\n\n**Context:** The Data Migrate gem has caused issues in the past, including running silently during deploys and resulting in unexpected errors. Additionally, the gem itself has had bugs that have caused problems during deployment.\n\n**Problem:** The Data Migrate gem's behavior is unpredictable and has caused issues with our deployment process. Its bugs have also caused unexpected errors, which can be difficult to debug.\n\n**Reasoning:** To mitigate these issues, we will replace the Data Migrate gem with a custom solution for database migrations. This will give us more control over the migration process and allow us to better debug any issues that arise.\n\n**Consequences:**\n\n* We will need to write custom code to handle database migrations, which will require additional development time.\n* We will have more control over the migration process and can better debug any issues that arise.\n* We will no longer be reliant on a third-party gem that has caused issues in the past.\n\n**Alternatives Considered:** We considered alternative gems for database migrations, but decided that a custom solution would give us more control and flexibility.\n\n**Status:** This decision is approved and will be implemented in the next deployment cycle.","GenTime":"2024-08-10 10:49:16"}
{"File Name":"beis-report-official-development-assistance\/0009-use-pundit-to-manage-permissions.md","Context":"## Context\\nThe service will be used by a variety of different types of users, and we need\\nto ensure that those users are only able to access the parts of the service that\\nwe want them to.\\nOur current chosen authentication provider, Auth0, has support for assigning\\nroles to users, but this couples the service tightly to their service, so we\\nshould avoid this.\\n","Decision":"We will use the 'Pundit' gem to manage users' access to the service. The\\npermissions will be grouped into roles that can then be assigned to users\\nrequiring a particular level of access.\\n","tokens":81,"id":2397,"Prediction":"## Decision\n\nWe will implement a role-based access control (RBAC) system within our service, using a custom solution that is decoupled from any specific authentication provider. This will allow us to manage access to different parts of the service based on user roles, without being tied to a specific authentication provider.\n\n## Context\n\nAs mentioned in the context, our service will be used by a variety of different types of users, and we need to ensure that those users are only able to access the parts of the service that we want them to. Our current chosen authentication provider, Auth0, has support for assigning roles to users, but this couples the service tightly to their service, so we should avoid this.\n\n## Problem\n\nWe need to implement a solution that allows us to manage access to different parts of the service based on user roles, without being tied to a specific authentication provider. This will ensure that our service remains flexible and scalable, and that we can easily switch to a different authentication provider if needed.\n\n## Decision\n\nWe will implement a role-based access control (RBAC) system within our service, using a custom solution that is decoupled from any specific authentication provider. This will allow us to manage access to different parts of the service based on user roles, without being tied to a specific authentication provider.\n\n## Consequences\n\n* We will need to implement a custom RBAC system within our service, which will require additional development and testing effort.\n* We will need to manage user roles and permissions separately from the authentication provider, which will require additional complexity and maintenance effort.\n* We will have more flexibility and scalability, as we will not be tied to a specific authentication provider.\n\n## Rationale\n\nImplementing a custom RBAC system within our service will allow us to manage access to different parts of the service based on user roles, without being tied to a specific authentication provider. This will ensure that our service remains flexible and scalable, and that we can easily switch to a different authentication provider if needed.","GenTime":"2024-08-10 10:49:38"}
{"File Name":"beis-report-official-development-assistance\/0031-replace-publicactivity-with-our-own-hand-rolled-way-of-recording-changes.md","Context":"## Context\\nWe adopted the `PublicActivity` gem (as documented in [0019](https:\/\/github.com\/UKGovernmentBEIS\/beis-report-official-development-assistance\/blob\/731b91f5773c4306f919433af81976d29c5feef1\/doc\/architecture\/decisions\/0019-use-public-activity-gem.md)), but the information we record has never been used, or displayed to the user.\\nNow we're looking at surfacing information about changes to end users it's becoming increasingly clear that is `PublicActivity` is not fit for our needs, and would be easier to build our own approach to logging what changed were made by what user.\\n","Decision":"We have decided to introduce a `HistoricalEvent` model to record changes to models, starting with `Activity`s.\\nOnce we have applied `HistoricalEvents` to all the models we need to track, we can remove all the calls to `PublicActivity` and remove the gem entirely.\\n","tokens":145,"id":2403,"Prediction":"## Decision\n\n**Decision:** Replace the `PublicActivity` gem with a custom logging solution for tracking changes made by users.\n\n**Context:** We initially adopted the `PublicActivity` gem to record information about changes made to our system, but it has never been used or displayed to the user. As we now need to surface information about changes to end users, it has become clear that `PublicActivity` is not suitable for our needs and would be easier to build a custom approach to logging changes.\n\n**Reasoning:** The `PublicActivity` gem was designed to provide a simple way to record and display activity streams, but it lacks the flexibility and customization options we need to effectively track changes made by users. By building a custom solution, we can tailor the logging functionality to our specific requirements and provide a more intuitive and user-friendly experience for end users.\n\n**Consequences:**\n\n* We will need to invest time and resources in developing a custom logging solution.\n* We will have more control over the functionality and customization options of our logging solution.\n* We will be able to better integrate the logging solution with our existing system and provide a more seamless user experience.\n\n**Alternatives Considered:**\n\n* Continuing to use the `PublicActivity` gem, despite its limitations.\n* Modifying the `PublicActivity` gem to meet our specific needs, which could be time-consuming and may not be feasible.\n\n**Decision Owner:** [Name of decision owner]\n\n**Status:** Approved\n\n**Links:** [Link to related issues or documentation]","GenTime":"2024-08-10 10:51:30"}
{"File Name":"beis-report-official-development-assistance\/0006-use-auth0-for-authentication.md","Context":"## Context\\nWe need to allow a number of users to sign in to the service in order to use it.\\nIn order to implement this quickly, we'll use Auth0 to manage this.\\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\\nto another service, if BEIS have a preference for something else.\\nAuth0 provides views for authentication that we can use in our user journeys.\\nThere are two versions of these views 'Classic' and 'New'.\\nClassic uses JavaScript and is not progressively enhanced. New uses language\\nthat results in a poor user experience.\\n","Decision":"We will use the free tier and 'Classic' views of Auth0 for the private beta.\\n","tokens":127,"id":2405,"Prediction":"## Decision\n\nWe will use the \"New\" version of Auth0's authentication views in our user journeys, despite the potential for a poor user experience, in order to future-proof our implementation and ensure compatibility with a variety of devices and browsers.\n\n## Rationale\n\nWhile the \"Classic\" version of Auth0's authentication views may provide a more traditional and familiar user experience, it is not progressively enhanced and may not work well on all devices and browsers. The \"New\" version, on the other hand, uses modern language and techniques that will ensure our implementation is compatible with a wide range of devices and browsers.\n\nAdditionally, by choosing the \"New\" version, we are future-proofing our implementation and ensuring that it will be easy to maintain and update in the future. This is particularly important given the potential for BEIS to switch to a different authentication service in the future.\n\nWhile the \"New\" version may provide a less-than-ideal user experience, we believe that the benefits of using modern language and techniques outweigh the potential drawbacks. We will work with the design team to identify opportunities to improve the user experience and provide a better overall experience for our users.\n\n## Consequences\n\n* We will use the \"New\" version of Auth0's authentication views in our user journeys.\n* We will work with the design team to identify opportunities to improve the user experience and provide a better overall experience for our users.\n* We will ensure that our implementation is compatible with a wide range of devices and browsers.\n* We will future-proof our implementation to make it easy to maintain and update in the future.","GenTime":"2024-08-10 10:52:11"}
{"File Name":"beis-report-official-development-assistance\/0025-use-terraform-to-set-up-and-deploy-to-the-paas.md","Context":"## Context\\nWe like to control our infrastructure with code. On a previous project we used shell scripts to set up the spaces and services within a PaaS organisation and to also do deploys from Travis.\\nWhile shell scripts are fine there is now a useful [terraform provider](https:\/\/github.com\/cloudfoundry-community\/terraform-provider-cf) for cloudfoundry.\\nThe cloudfoundry provider also deploys the app.\\nTerraform is the default choice for provisioning infrastructure these days.\\n","Decision":"- spaces and services should be created with Terraform\\n- the application should be deployed with Terraform\\n- Terraform should be run from Travis to deploy the application\\n","tokens":107,"id":2409,"Prediction":"## Decision\n\nAs we aim to control our infrastructure with code, we will adopt Terraform as our infrastructure as code (IaC) tool of choice for managing our Cloud Foundry PaaS organization, including setting up spaces and services, and deploying applications. This decision is driven by the availability of the Terraform provider for Cloud Foundry, which allows us to leverage Terraform's robust features and ecosystem for infrastructure provisioning and management.\n\n## Rationale\n\n* Terraform is widely adopted and considered the de facto standard for IaC, providing a consistent and reproducible way to manage infrastructure.\n* The Cloud Foundry provider for Terraform offers a more robust and scalable solution compared to shell scripts, allowing us to manage complex infrastructure configurations and deployments.\n* By using Terraform, we can ensure that our infrastructure is consistently configured and deployed across different environments and teams, reducing errors and improving collaboration.\n* The Terraform provider for Cloud Foundry also deploys applications, making it a one-stop solution for our infrastructure and application deployment needs.\n\n## Consequences\n\n* We will need to learn and adopt Terraform's syntax and best practices for infrastructure provisioning and management.\n* We will need to refactor our existing shell scripts to use Terraform, which may require significant effort and resources.\n* We will need to ensure that our Terraform configuration is properly version-controlled and reviewed to maintain consistency and quality.\n* We will need to integrate Terraform with our existing continuous integration and continuous deployment (CI\/CD) pipeline to automate the deployment of our applications and infrastructure.\n\nBy adopting Terraform as our IaC tool of choice, we can improve the efficiency, consistency, and scalability of our infrastructure management and deployment processes, ultimately leading to better collaboration, reduced errors, and improved overall quality of our software systems.","GenTime":"2024-08-10 10:53:27"}
{"File Name":"beis-report-official-development-assistance\/0022-use-govuk-paas-for-hosting.md","Context":"## Context\\nAs a government entity, BEIS want to host their service on [GOVUK\\nPaaS](https:\/\/www.cloud.service.gov.uk\/). This was always the service teams\\nintention.\\n","Decision":"Host all environments excpet local development on BEIS own GOVUK PaaS account.\\n","tokens":44,"id":2410,"Prediction":"## Decision\n\n**Decision:** To host the BEIS service on GOV.UK PaaS (Platform as a Service) as the primary deployment environment.\n\n## Context\n\nAs a government entity, BEIS wants to host their service on GOV.UK PaaS, which was always the service team's intention. This decision aligns with the government's cloud-first strategy and ensures compliance with the necessary security and governance requirements.\n\n## Problem\n\nThe service team needs to ensure that the service is hosted in a secure, scalable, and managed environment that meets the government's requirements. GOV.UK PaaS provides a pre-configured and managed platform that meets these requirements, allowing the service team to focus on developing and maintaining the service.\n\n## Decision Drivers\n\n* Compliance with government security and governance requirements\n* Scalability and reliability of the service\n* Reduced operational overhead for the service team\n* Alignment with the government's cloud-first strategy\n* Integration with existing GOV.UK infrastructure and services\n\n## Trade-Offs\n\n* Potential limitations on customisation and configuration of the PaaS environment\n* Dependence on GOV.UK PaaS for service availability and performance\n\n## Consequences\n\n* The service will be hosted on a managed platform that meets government security and governance requirements\n* The service team will have reduced operational overhead and can focus on developing and maintaining the service\n* The service will be scalable and reliable, with built-in redundancy and failover capabilities\n* The service will align with the government's cloud-first strategy and integrate with existing GOV.UK infrastructure and services\n\n## Rationale\n\nBy hosting the service on GOV.UK PaaS, the service team can ensure that the service meets the necessary security and governance requirements, while also reducing operational overhead and improving scalability and reliability. This decision aligns with the government's cloud-first strategy and ensures that the service is integrated with existing GOV.UK infrastructure and services.","GenTime":"2024-08-10 10:53:48"}
{"File Name":"arch\/0019-server-request-and-upgrade-capacity-evaluation.md","Context":"## Context\\n1. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\uff0c\u4f7f\u7528\u7387\u5747\u5728 90% \u5de6\u53f3\uff0c\u53e6\u4e00\u4e9b\u673a\u5668\u5404\u9879\u6307\u6807\u4f7f\u7528\u7387\u5728 1% \u5de6\u53f3\uff1b\\n2. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\u642d\u914d\u4e0d\u5408\u7406\uff0cCPU \u4f7f\u7528\u7387 1%\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u7387\u5728 90% \u5de6\u53f3\uff1b\\n3. \u4e00\u4e9b\u5bf9\u78c1\u76d8\u8bfb\u5199\u8981\u6c42\u9ad8\u7684\u670d\u52a1\uff0c\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0c\u6bd4\u5982\uff0c\u6570\u636e\u5e93\uff0cSVN\u7b49\uff1b\\n4. \u7533\u8bf7\u673a\u5668\u65f6\uff0c\u65e0\u6cd5\u63d0\u51fa\u914d\u7f6e\u8981\u6c42\uff0c\u57fa\u672c\u9760\u62cd\u8111\u95e8\u51b3\u5b9a\uff1b\\n5. \u5bf9\u670d\u52a1\u7684\u53d1\u5c55\u6ca1\u6709\u601d\u8003\uff0c\u914d\u7f6e\u8981\u4e86 12 \u4e2a\u6708\u540e\u624d\u80fd\u4f7f\u7528\u5230\u7684\u914d\u7f6e\u3002\\n","Decision":"1. \u538b\u529b\u6d4b\u8bd5\uff1b\\n2. \u5206\u6790\u4e1a\u52a1\u5404\u4e2a\u6307\u6807\u7684\u4f7f\u7528\u60c5\u51b5\uff0cCPU \u5bc6\u96c6\u578b\uff0c\u5185\u5b58\u5bc6\u96c6\u578b\u8fd8\u662f\u6709\u5176\u4ed6\u7684\u7279\u70b9\uff1b\\n3. \u9274\u4e8e Aliyun ECS \u968f\u65f6\u53ef\u4ee5\u6269\u5c55\uff0c\u53ef\u4ee5\u5148\u7528\u4f4e\u914d\u673a\u5668\uff0c\u6839\u636e\u4f7f\u7528\u60c5\u51b5\uff0c\u8fdb\u884c\u5355\u6307\u6807\u5782\u76f4\u5347\u7ea7\uff1b\\n4. \u6c34\u5e73\u6269\u5c55\uff0c\u5373\u63d0\u5347\u4e86\u670d\u52a1\u7684\u5904\u7406\u80fd\u529b\u53c8\u505a\u4e86\u9ad8\u53ef\u7528\uff1b\\n5. \u5bf9\u4e8e\u4e0d\u5408\u7406\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u8981\u5206\u6790\u81ea\u5df1\u7a0b\u5e8f\u4e2d\u662f\u5426\u6709\u5185\u5b58\u6cc4\u6f0f\u6216\u5927\u6570\u636e\u52a0\u8f7d\u3002\\n","tokens":208,"id":2440,"Prediction":"## Decision\n\n**Decision:** Implement a more sophisticated resource allocation and provisioning system to better match the needs of our services, taking into account the varying usage patterns and requirements of our machines.\n\n**Context:** As described in the context, we have a situation where some machines have CPU, memory, and disk usage rates that are either very high (90% or above) or very low (1% or below). Additionally, some machines have mismatched resource configurations, where CPU usage is low but memory usage is high. Furthermore, we have services that require high disk I\/O, such as databases and SVN, which are currently using standard cloud storage. We also lack a systematic approach to provisioning machines, relying on intuition and guessing the required configurations. Finally, our current approach does not account for future growth and scalability needs.\n\n**Problem:** Our current approach to resource allocation and provisioning is inefficient, leading to wasted resources, poor performance, and difficulties in scaling our services. The lack of a systematic approach also makes it challenging to predict and plan for future growth and changes in our services.\n\n**Reasoning:** By implementing a more sophisticated resource allocation and provisioning system, we can better match the needs of our services, reduce waste, and improve overall performance. This system should take into account the varying usage patterns and requirements of our machines, as well as the needs of our services. This will enable us to make more informed decisions about resource allocation and provisioning, and to better plan for future growth and changes.\n\n**Consequences:**\n\n* Improved resource utilization and reduced waste\n* Better performance and scalability for our services\n* Increased confidence in our ability to plan and predict future growth and changes\n* Improved decision-making and reduced uncertainty in resource allocation and provisioning\n\n**Alternative Considerations:** We considered alternative approaches, such as:\n\n* Continuing with our current approach and hoping for the best\n* Implementing a simple and inflexible resource allocation system\n* Outsourcing resource allocation and provisioning to a third-party provider\n\nHowever, these alternatives were deemed less effective and more risky than implementing a more sophisticated resource allocation and provisioning system.","GenTime":"2024-08-10 11:04:32"}
{"File Name":"arch\/0045-reuse-python-custom-libs.md","Context":"## Context\\n\u6211\u4eec\u6709\u591a\u4e2a\u9879\u76ee\u4f7f\u7528 Python \u5f00\u53d1\uff0c\u968f\u7740\u9879\u76ee\u7684\u53d1\u5c55\uff0c\u5927\u5bb6\u4e5f\u5199\u4e86\u8bb8\u591a\u7684\u5e93\uff0c\u6bd4\u5982\uff0cprice\u3001sms\u3001mail \u7b49\u3002\u800c\u5176\u4ed6\u9879\u76ee\u4e5f\u6709\u8fd9\u6837\u7684\u9700\u6c42\uff0c\u5f53\u524d\u9879\u76ee\u4e4b\u95f4\u662f\u901a\u8fc7\u62f7\u8d1d\u7684\u65b9\u5f0f\u8fdb\u884c\u590d\u7528\uff0c\u4e0d\u662f\u5e93\u8fd8\u5b58\u5728\u9879\u76ee\u5185\u72ec\u81ea\u81ea\u884c\u66f4\u65b0\u3002\u8fd9\u5c31\u5bfc\u81f4\u9879\u76ee\u4e4b\u95f4\u6240\u4f7f\u7528\u7684\u5e93\u4ea7\u751f\u4e0d\u4e00\u81f4\uff0c\u5e76\u91cd\u590d\u9020\u4e86\u5f88\u591a\u7684\u8f6e\u5b50\u3002\\n","Decision":"1. \u6784\u5efa\u81ea\u5df1\u7684 pypi \u670d\u52a1\u5668\uff1b\\n* \u4e0d\u53ea\u53ef\u4ee5\u89e3\u51b3\u81ea\u5efa\u5e93\u7684\u590d\u7528\u95ee\u9898\uff1b\\n* \u4e5f\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u5e38\u7528\u5e93\u7f13\u5b58\uff0c\u52a0\u901f pip \u7684\u5b89\u88c5\uff1b\\n* \u6709\u7ef4\u62a4\u6210\u672c\u3002\\n2. \u4f7f\u7528 git \u4f5c\u4e3a pip \u5b89\u88c5\u5305\\n* \u5355 repo \u5355 package\uff1a\u6240\u6709 libs \u805a\u5408\u5728\u4e00\u4e2a package \u91cc\uff0c\u5bf9\u4e8e\u5927\u91cf\u7b80\u5355 lib \u53ef\u884c\uff1b\\n* \u5355 repo \u591a package\uff1a\u5206\u76ee\u5f55\u6784\u5efa package\uff0c\u9002\u5408\u7a0d\u5fae\u5927\u4e9b\u7684 libs \u4e4b\u95f4\u505a\u9694\u79bb\uff1b\\n* \u591a repo \u591a package\uff1a\u6bcf\u4e2a repo \u662f\u4e00\u4e2a package\uff0c\u9002\u5408\u5927\u7684 lib \u5e93\uff0clib \u7c92\u5ea6\u8fc7\u7ec6\u4f1a\u6709\u5e93\u7ba1\u7406\u95ee\u9898\u3002\\n3. \u4f7f\u7528 Artifactory\uff0cPro+ \u7248\u672c\u624d\u63d0\u4f9b pypi \u670d\u52a1\u3002\\n\u9274\u4e8e\u5f53\u524d lib \u5e76\u4e0d\u591a\uff0c\u800c\u4e14\u6bcf\u4e2a lib \u5f88\u7b80\u5355\uff0c\u6545\u9009\u62e9\u5355 repo \u5355 package \u65b9\u6848\u3002\\n","tokens":119,"id":2446,"Prediction":"## Decision\n\nTo address the issue of duplicated code and inconsistent libraries across multiple Python projects, we will adopt a library management strategy using a centralized library repository.\n\n## Context\n\nAs mentioned, we have multiple Python projects with similar library requirements, such as price, sms, and mail. Currently, we are copying code from one project to another, which leads to duplicated code and inconsistent libraries. This approach not only increases maintenance costs but also reduces the overall quality and reliability of our projects.\n\n## Problem\n\n1. Duplicated code: Repeating the same code in multiple projects leads to increased development time, higher maintenance costs, and a higher risk of errors.\n2. Inconsistent libraries: Different projects using different versions of the same library can lead to compatibility issues and make it difficult to maintain a consistent development environment.\n3. Lack of scalability: As the number of projects grows, managing duplicated code and inconsistent libraries becomes increasingly challenging.\n\n## Decision\n\nTo address these problems, we will create a centralized library repository that contains a set of reusable libraries for our Python projects. This repository will be maintained by a dedicated team and will provide a single source of truth for our library dependencies.\n\nKey features of the centralized library repository:\n\n1. Version control: Each library will have a version number, and changes will be tracked using version control systems.\n2. Dependency management: The repository will manage dependencies between libraries, ensuring that each project uses the correct version of a library.\n3. Documentation: The repository will provide documentation for each library, including usage guidelines and API documentation.\n4. Testing: The repository will include automated tests for each library, ensuring that changes do not break existing functionality.\n\n## Consequences\n\n1. Reduced duplicated code: By using a centralized library repository, we can avoid duplicating code and reduce the maintenance burden.\n2. Improved consistency: The repository will ensure that all projects use the same version of a library, reducing the risk of compatibility issues.\n3. Increased scalability: The repository will make it easier to manage and maintain our library dependencies as the number of projects grows.\n4. Improved collaboration: The centralized repository will facilitate collaboration between developers by providing a single source of truth for our library dependencies.\n\n## Rationale\n\nThe centralized library repository is the best approach to address the issues mentioned above. It provides a scalable and maintainable solution for managing our library dependencies, reduces duplicated code, and improves consistency across our projects.","GenTime":"2024-08-10 11:07:18"}
{"File Name":"arch\/0036-ha-for-mysql.md","Context":"## Context\\n1. \u6570\u636e\u5e93\u7248\u672c 5.1\uff0c\u592a\u65e7\uff0c\u6027\u80fd\uff0c\u5b89\u5168\uff0c\u4e3b\u4ece\u590d\u5236\u90fd\u5b58\u5728\u95ee\u9898\uff1b\\n2. \u6570\u636e\u5e93\u90e8\u7f72\u5728 ECS \u4e0a\uff0c\u4f46\u78c1\u76d8\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0cIOPS \u5df2\u5230\u9608\u503c\uff08\u4f18\u5148\u7ea7\u6700\u9ad8\uff09\uff1b\\n3. \u6570\u636e\u5e93\u4e00\u4e3b\u4e24\u4ece\uff0c\u4f46\u65e0\u9ad8\u53ef\u7528\uff1b\\n4. \u4e1a\u52a1\u7aef\u4f7f\u7528 IP \u8fde\u63a5\u4e3b\u6570\u636e\u5e93\u3002\\n","Decision":"1. \u63d0\u4ea4 Aliyun \u5de5\u5355\uff0c\u5c1d\u8bd5\u662f\u5426\u80fd\u7533\u8bf7\u4e0b 5.1 \u7248\u672c\u7684 MySQL\uff0c\u8fc1\u79fb\u6570\u636e\u81f3 RDS\uff0c\u89e3\u51b3 2\uff0c3\uff0c4 \u95ee\u9898\uff08\u6c9f\u901a\u540e\uff0c5.1 \u7248\u672c\u5df2\u4e0d\u518d\u63d0\u4f9b\uff0cPASS\uff09\uff1b\\n2. \u5c06\u90e8\u5206\u6570\u636e\u5e93\u8fc1\u79fb\u51fa\uff0c\u7f13\u89e3\u5f53\u524d MySQL \u670d\u52a1\u5668\u538b\u529b\uff0c\u7ef4\u62a4\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\uff08\u5e76\u672a\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0cPASS\uff0c\u5f53\u524d\u538b\u529b\u6700\u7ec8\u786e\u8ba4\u662f\u6162\u67e5\u8be2\u539f\u56e0\uff09\uff1b\\n3. ECS \u4e0a\u81ea\u5efa HA\uff0c\u5e76\u542f\u7528\u65b0\u7684\u5b9e\u4f8b\u78c1\u76d8\u4e3a SSD\uff0c\u5207\u6362\u65b0\u5b9e\u4f8b\u4e3a Master\uff0c\u505c\u6389\u65e7\u5b9e\u4f8b\uff08\u6839\u672c\u95ee\u9898\u672a\u89e3\u51b3\uff0c\u6280\u672f\u503a\u4e00\u76f4\u5b58\u5728\uff0c\u81ea\u884c\u7ef4\u62a4\u4ecd\u7136\u5b58\u5728\u98ce\u9669\u70b9\uff09\uff1b\\n4. \u8c03\u7814 5.5 \u548c 5.1 \u7684\u5dee\u5f02\uff0c\u76f4\u63a5\u8fc1\u79fb\u81ea\u5efa\u6570\u636e\u5e93\u81f3 Aliyun RDS MySQL 5.5\u3002\\n\u9274\u4e8e\u67e5\u770b\u6587\u6863\u540e\uff0c 5.1 \u5230 5.5 \u7684\u5dee\u5f02\u6027\u5f71\u54cd\u4e0d\u5927\uff0cAliyun \u5b98\u65b9\u4e5f\u652f\u6301\u76f4\u63a5 5.1 \u5230 5.5 \u7684\u8fc1\u79fb\uff0c\u6240\u4ee5\u8ba1\u5212\u76f4\u63a5\u8fc1\u79fb\u81f3 RDS \u7684 5.5 \u7248\u672c\u3002\\n\u4e3a\u4e86\u675c\u7edd\u98ce\u9669\uff1a\\n1. \u6309\u4e1a\u52a1\u5206\u6570\u636e\u5e93\u5206\u522b\u8fc1\u79fb\uff1b\\n2. \u6240\u6709\u8fc1\u79fb\u5148\u8d70\u6d4b\u8bd5\u6570\u636e\u5e93\uff0c\u7531 QA \u505a\u5b8c\u6574\u7684\u6d4b\u8bd5\u3002\\nECS self built MySQL 5.1 to RDS 5.5 with DTS \u8fc1\u79fb\u6d41\u7a0b\uff1a\\n1. \u5728 RDS \u4e2d\u521b\u5efa\u539f MySQL \u6570\u636e\u5e93\u5bf9\u5e94\u7684\u8d26\u53f7(\u5404\u4e2a\u9879\u76ee\u8d26\u53f7\u72ec\u7acb)\uff1b\\n2. \u66f4\u65b0\u767d\u540d\u5355\uff1a\u6dfb\u52a0\u9879\u76ee\u6240\u90e8\u7f72\u7684\u670d\u52a1\u5668\uff1b\\n3. \u660e\u786e\u6570\u636e\u89c4\u6a21\uff0c\u5bf9\u540c\u6b65\u65f6\u95f4\u505a\u4e2a\u9884\u671f\uff1b\\n4. \u540c\u6b65\uff08\u5168\u91cf or \u589e\u91cf\uff09\uff0c\u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff1b\\n5. \u66f4\u65b0\u6570\u636e\u5e93\u8fde\u63a5\u914d\u7f6e\u6587\u4ef6\uff1b\\n6. \u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff0c\u505c\u670d\uff1b\\n7. \u786e\u5b9a\u6570\u636e\u91cf\u4e00\u81f4\uff08\u7531\u9884\u5148\u5199\u597d\u7684\u811a\u672c\u5224\u65ad\uff09(1min)\uff1b\\n8. \u5173\u95ed\u8fc1\u79fb\u670d\u52a1(10s)\uff1b\\n9. \u91cd\u542f\u670d\u52a1\u5668\uff0810s\uff09\u3002\\n6 \u81f3 9 \u6b65\u51b3\u5b9a\u6211\u4eec\u7684\u505c\u670d\u65f6\u95f4\u3002\\n\u9274\u4e8e\u6211\u4eec\u4f7f\u7528\u4ece\u5e93\u4f5c\u4e3a\u8fc1\u79fb\u7684\u6570\u636e\u6e90\uff0c\u9700\u66f4\u65b0\u5982\u4e0b\u914d\u7f6e\uff1a\\n* log-slave-updates=1\\n* binlog-format=row\\n","tokens":112,"id":2459,"Prediction":"## Decision\n\n**Upgrade Database Version and Storage**\n\n## Context\n\n1. The current database version (5.1) is outdated, leading to performance, security, and replication issues.\n2. The database is deployed on ECS with a standard cloud disk, which has reached its IOPS threshold, causing high priority concerns.\n3. The database has a single master and two replicas, but lacks high availability.\n4. The business end uses IP connections to connect to the master database.\n\n## Problem\n\nThe current database setup is causing performance issues, security concerns, and replication problems, which may lead to data loss and downtime. The outdated version and standard cloud disk are the main contributors to these issues.\n\n## Decision\n\nWe will upgrade the database version to a newer version that supports better performance, security, and high availability. We will also migrate the database to a high-performance storage solution, such as a solid-state drive (SSD) or a high-IOPS cloud disk. Additionally, we will implement a high-availability architecture with multiple masters and replicas to ensure data redundancy and minimize downtime.\n\n## Rationale\n\nUpgrading the database version will provide better performance, security, and support for new features. Migrating to a high-performance storage solution will improve IOPS and reduce latency. Implementing a high-availability architecture will ensure that the database is always available and can withstand hardware failures.\n\n## Consequences\n\n* Upgrade process may require downtime, which may impact business operations.\n* Additional costs for high-performance storage and high-availability architecture.\n* Requires careful planning and testing to ensure a smooth upgrade process.\n\n## Alternatives Considered\n\n* Downgrade the database version to a lower version that is compatible with the current storage solution.\n* Implement a caching layer to improve performance, but this may not address the underlying issues.\n* Do nothing and continue to operate with the current setup, which may lead to further performance and security issues.\n\n## Implementation Status\n\nThe upgrade process will be implemented in the following phases:\n\n1. Database version upgrade: Upgrade the database version to a newer version that supports better performance, security, and high availability.\n2. Storage migration: Migrate the database to a high-performance storage solution.\n3. High-availability architecture implementation: Implement a high-availability architecture with multiple masters and replicas.\n\nThe implementation will be monitored and tested to ensure a smooth transition and minimize downtime.","GenTime":"2024-08-10 11:11:43"}
{"File Name":"google-cloud-cpp\/2019-03-26-binary-distributions.md","Context":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n","Decision":"will not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n","tokens":272,"id":2462,"Prediction":"Here is the Architectural Decision Record (ADR) for the given context and decision:\n\n**Decision:** No Binary Packages\n\n**Context:** Compiling google-cloud-cpp from source is not always possible nor desired by all users. Some users may want to install a binary package that contains our public headers and pre-compiled libraries so that they can link against our library. There is no single standard binary package manager in C++, instead there are a variety of others that users might want to use (e.g., dpkg, rpm, vcpkg).\n\n**Decision:** We will not directly support nor provide any binary packages. We will not test binary distributions of our code. And we will not host configs for binary distributions, since that would involve hosting files which we do not test. However, we will do our best to make our code easily packageable by others in a wide variety of formats. For more context, see [#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\n\n**Consequences:**\n\n* This decision will shield us from endorsing any particular binary package management system.\n* Other individuals who want to build and maintain a binary distribution of our code may do so without our permission or knowledge.\n* If they need small and reasonable tweaks from us to support their binary distributions, they may file an issue and we'll do our best to accommodate them within reason.\n\n**Rationale:** By not providing binary packages, we are avoiding the complexity and maintenance burden that comes with supporting multiple binary package formats. Additionally, this decision allows users to package our code in a way that best suits their needs, without relying on us to provide a specific binary package.","GenTime":"2024-08-10 11:12:42"}
{"File Name":"google-cloud-cpp\/2018-06-13-storage-always-retries.md","Context":"**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","Decision":"non-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","tokens":188,"id":2466,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Default Retry Policy for Operations with Indeterminate Status Codes\n\n**Context:** Operations that change state in the server may complete with an indeterminate status code, requiring a retry mechanism to ensure reliable communication.\n\n**Decision:** By default, the library will retry all requests, including non-idempotent ones. The library will also allow application developers to override the definition of what operations are automatically retried.\n\n**Rationale:** The default retry policy simplifies the usage of the library for application developers, as they do not need to worry about handling indeterminate status codes. However, this policy also introduces the risk of double uploads or unintended side effects in rare cases.\n\n**Consequences:**\n\n* Most operations become easier to use for application developers.\n* In very rare cases, the operation may result in double uploads or a new generation of the object or metadata being created.\n* In even more rare cases, the operation may fail, for example, an operation to create an object with `IfGenerationMatch(0)` would fail on the second attempt.\n\n**Status:** [Approved]\n\n**Implementors:** [List the teams or individuals responsible for implementing this decision]\n\n**Links:** [List any relevant links, such as design documents or code changes]","GenTime":"2024-08-10 11:13:23"}
{"File Name":"google-cloud-cpp\/2019-01-04-error-reporting-with-statusor.md","Context":"**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","Decision":"Instead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","tokens":436,"id":2467,"Prediction":"Here is the Architectural Decision Record (ADR) for the given context:\n\n**Decision**: No Exceptions in API\n\n**Context**: We know there will be users of these C++ libraries who want to use C++ exceptions as well as those who are not able to. Our C++ libraries must work for all of our users, regardless of their ability to use exceptions.\n\n**Problem**: Our C++ libraries must cater to users who are not able to use exceptions, while also providing a consistent vocabulary for all users.\n\n**Decision**: None of our APIs will throw exceptions to indicate errors. Instead, our APIs will typically report errors to callers by returning a `Status` or `StatusOr<T>` object, unless the library we're using has another non-throwing way to report errors (e.g., [badbit][badbit-link] in the standard I\/O library).\n\n**Consequences**:\n\n* This decision will result in a single set of APIs and a consistent vocabulary for all users, whether or not they choose to compile with exceptions.\n* This decision does not prevent callers from using exceptions in their own code.\n* A downside of this decision is that our APIs will not be natural or idiomatic for the [50+%][survey-link] of users who might prefer exceptions for error reporting.\n* Changing existing APIs from throwing exceptions to returning `StatusOr<T>` is a breaking change.\n\n**Timeline**: We plan to immediately change the API for Google Cloud Storage. We have no immediate plans to change the API for Cloud Bigtable. We will communicate a timeline to change this API in a separate document.\n\n**References**:\n\n* [badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\n* [bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\n* [gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\n* [survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02","GenTime":"2024-08-10 11:13:46"}
{"File Name":"openfido-app-service\/0002-project-structure.md","Context":"## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n","Decision":"Create a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n","tokens":49,"id":2470,"Prediction":"## Decision\n\nTo ensure secure access to the GridLabD jobs and resources, we will implement an authentication and authorization mechanism that verifies the user's permission to access the resources via an organization.\n\n## Context\n\nThe OpenFIDO project requires a secure way to mediate access to a workflow service that runs GridLabD jobs. The frontend React application needs to access these resources, but only if the user has the necessary permission to access via an organization.\n\n## Problem\n\nThe React application needs to access the GridLabD jobs and resources, but we need to ensure that only authorized users with the correct organization can access these resources.\n\n## Decision\n\nTo solve this problem, we will implement an authentication and authorization mechanism that verifies the user's permission to access the resources via an organization. This will be achieved by:\n\n1. Implementing an authentication mechanism that verifies the user's identity.\n2. Implementing an authorization mechanism that checks the user's permission to access the resources via an organization.\n3. Using an access control list (ACL) to store the permissions and organizations.\n\n## Rationale\n\nThis decision ensures that only authorized users with the correct organization can access the GridLabD jobs and resources. This provides an additional layer of security and ensures that the resources are protected from unauthorized access.\n\n## Consequences\n\nThis decision has the following consequences:\n\n1. The React application will need to implement the authentication and authorization mechanism.\n2. The access control list (ACL) will need to be maintained and updated regularly.\n3. The authentication and authorization mechanism will need to be tested and validated to ensure that it works correctly.\n\n## Alternatives\n\nThe alternative to this decision would be to not implement an authentication and authorization mechanism, which would leave the resources unprotected and vulnerable to unauthorized access.","GenTime":"2024-08-10 11:14:53"}
{"File Name":"openfido-app-service\/0003-deployment.md","Context":"## Context\\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n","Decision":"Use CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- DEV_AWS_ACCESS_KEY_ID\\n- DEV_AWS_SECRET_ACCESS_KEY\\n- PROD_AWS_ACCESS_KEY_ID\\n- PROD_AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n","tokens":24,"id":2471,"Prediction":"## Decision\n\n**Decision:** We will use AWS CodeBuild for automated testing and deployment of our application to AWS ECS, and AWS CodePipeline for continuous integration and continuous deployment (CI\/CD) pipeline.\n\n**Context:** We plan to use AWS as a development\/staging environment and as a production environment, on ECS.\n\n**Problem:** We need a solution to automate the testing and deployment of our application to AWS ECS, ensuring that our application is built, tested, and deployed consistently across environments.\n\n**Reasoning:** AWS CodeBuild provides a fully managed service that automates the compilation and testing of our application, allowing us to focus on writing code rather than managing build processes. AWS CodePipeline provides a visual representation of our CI\/CD pipeline, enabling us to model the entire process from source code to production deployment. By using these services, we can ensure that our application is built, tested, and deployed consistently across environments, reducing the risk of errors and increasing the speed of delivery.\n\n**Consequences:**\n\n* We will need to configure AWS CodeBuild and AWS CodePipeline to integrate with our application and ECS clusters.\n* We will need to write scripts and configuration files to automate the testing and deployment process.\n* We will need to monitor and troubleshoot the CI\/CD pipeline to ensure that it is functioning correctly.\n\n**Alternatives:**\n\n* We could use Jenkins or other CI\/CD tools to automate testing and deployment, but this would require more manual configuration and maintenance.\n* We could use AWS Lambda to automate testing and deployment, but this would require more complex scripting and may not provide the same level of visibility and control as AWS CodeBuild and CodePipeline.\n\n**Status:** This decision is approved and will be implemented in the next sprint.","GenTime":"2024-08-10 11:15:12"}
{"File Name":"openfido-app-service\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2472,"Prediction":"Here is an Architectural Decision Record (ADR) for your software project:\n\n## Decision: Use of Microservices Architecture\n\n### Context\n\nWe need to record the architectural decisions made on this project.\n\n### Decision\n\nAfter careful consideration, we have decided to implement a microservices architecture for our software project. This decision was made to improve the scalability, maintainability, and flexibility of the system.\n\n### Problem\n\nOur previous monolithic architecture has become increasingly difficult to maintain and scale. With the growing complexity of the system, it has become challenging to identify and isolate issues, leading to decreased performance and reliability.\n\n### Reasoning\n\nMicroservices architecture allows us to break down the system into smaller, independent services that can be developed, tested, and deployed independently. This approach enables us to:\n\n* Improve scalability by allowing each service to scale independently\n* Enhance maintainability by allowing each service to be updated and maintained separately\n* Increase flexibility by allowing us to use different programming languages and frameworks for each service\n* Improve fault tolerance by allowing each service to fail without affecting the entire system\n\n### Consequences\n\n* Increased complexity in communication between services\n* Additional overhead in managing multiple services\n* Potential for increased latency due to service-to-service communication\n\n### Status\n\nThis decision is currently in progress. We are in the process of designing and implementing the microservices architecture.\n\n### Affected Components\n\n* All components of the system will be affected by this decision, including the front-end, back-end, and database layers.\n\n### References\n\n* [1] Microservices Architecture by Martin Fowler\n* [2] Designing Distributed Systems by Brendan Burns\n\nI hope this helps! Let me know if you have any questions or need further clarification.","GenTime":"2024-08-10 11:15:30"}
{"File Name":"CrossyToad\/adr-0007-SUPERSEDED-mtl-style-for-affects.md","Context":"## Context\\nIn Haskell we need a way to manage side-effects, particularly for games. There are a _bunch_ of\\noptions including:\\n- MTL Style\\n- Using `IO` for everything\\n- Freer Monads\\nI haven't used mtl style in anger yet yet so I'm keen to give it a go.\\nI'm also steaing a lot of ideas from [dino-rush](https:\/\/github.com\/jxv\/dino-rush\/) and it\\nseems to be using MTL style.\\n","Decision":"We're using MTL style.\\n","tokens":114,"id":2473,"Prediction":"## Decision\n\n**Title:** Choose MTL (Monad Transformer Library) Style for Managing Side-Effects in Haskell Game Development\n\n**Context:** As discussed, we need a way to manage side-effects in our Haskell game development project. After considering various options, including MTL Style, using `IO` for everything, and Freer Monads, we have decided to adopt MTL Style for managing side-effects.\n\n**Decision:** We will use MTL Style to manage side-effects in our Haskell game development project, leveraging its modular and composable design to handle various types of side-effects.\n\n**Rationale:**\n\n1. **Modularity**: MTL Style provides a modular approach to managing side-effects, allowing us to break down complex systems into smaller, more manageable pieces. This aligns with our project's goal of creating a maintainable and scalable game engine.\n2. **Composability**: MTL Style's monad transformer design enables us to compose multiple monads, making it easy to combine different types of side-effects (e.g., input, output, and game state updates) in a flexible and efficient manner.\n3. **Reusability**: By using MTL Style, we can reuse existing monads and transformers, reducing the amount of code we need to write and maintain.\n4. **Influence from dino-rush**: As mentioned, dino-rush, a Haskell game engine, uses MTL Style, which has inspired our decision. We can learn from their experience and adapt their approaches to our project.\n\n**Consequences:**\n\n1. **Learning curve**: As we are new to MTL Style, there will be a learning curve for our team members to understand and master its concepts and implementation.\n2. **Additional complexity**: MTL Style can introduce additional complexity to our codebase, particularly when dealing with multiple monads and transformers.\n3. **Increased flexibility**: However, the benefits of MTL Style's modularity and composability will provide increased flexibility and maintainability in the long run.\n\n**Status:** This decision is approved and will be implemented in our Haskell game development project. We will monitor its effectiveness and adjust our approach as needed.","GenTime":"2024-08-10 11:15:54"}
{"File Name":"CrossyToad\/adr-0008-use-lenses.md","Context":"## Context\\nBecause we're writing a game we have lots of nested state that we need to update, traverse and\\notherwise manipulate. Querying data is relatively straightforward in vanilla haskell but\\nupdating nested records is a pain.\\nThis is the problem [lens](https:\/\/github.com\/ekmett\/lens) is supposed to help us with.\\nAdditionally by using lenses we have the option to use the \"classy\" lenses style to let\\nindividual subsystems of our game target a subset of our State monad instead of requiring the\\nfull state.\\n","Decision":"It's Lens time!\\n","tokens":118,"id":2480,"Prediction":"## Decision\n\n**Decision:** We will use the Lens library to manipulate nested state in our game, utilizing its features to simplify state updates and querying.\n\n**Context:** As we are building a game with complex nested state, querying data is relatively straightforward in vanilla Haskell. However, updating nested records is a challenging task, which is where the Lens library comes into play. By using lenses, we can simplify state updates and querying, and also take advantage of the \"classy\" lenses style to allow individual subsystems to target a subset of our State monad instead of requiring the full state.\n\n**Rationale:**\n\n* The Lens library provides a powerful and expressive way to manipulate nested state, making it easier to update and query complex data structures.\n* By using lenses, we can avoid the need to manually traverse and update nested records, reducing the risk of errors and making our code more maintainable.\n* The \"classy\" lenses style allows us to decouple individual subsystems from the full state, making it easier to reason about and test each subsystem in isolation.\n* The Lens library is widely used and well-maintained, ensuring that we can take advantage of its features and community support.\n\n**Consequences:**\n\n* We will need to learn and understand the Lens library and its syntax, which may require additional time and effort.\n* We will need to refactor our existing code to use lenses, which may require significant changes to our data structures and update logic.\n* We will need to ensure that we are using lenses correctly and efficiently, to avoid performance issues and maintainability problems.\n\n**Alternatives Considered:**\n\n* We considered using other libraries or approaches to manipulate nested state, such as manual recursion or explicit data structure traversal. However, the Lens library provides a more elegant and efficient solution to this problem.\n* We considered not using lenses at all, and instead relying on manual updates and querying. However, this approach would likely lead to more complex and error-prone code, and would not provide the same level of expressiveness and maintainability as lenses.\n\n**Status:** This decision is finalized, and we will begin using the Lens library to manipulate nested state in our game.","GenTime":"2024-08-10 11:18:05"}
{"File Name":"CrossyToad\/adr-0010-support-linear-and-discrete-input.md","Context":"## Context\\nWe need to support both discrete and continous input.\\nDiscrete input is the detection of an individual keypress. We need discrete input to support\\nmovement in game menus, state transitions and other \"one keypress one action\" scenarios.\\nContinous input is the detection of the state of an input on an ongoing basis. We need continous\\nimport to support linear movement in the game, notably the movement of the toad.\\nCurrently we are using SDL to retrieve discrete input and we have no mechanism for continous input.\\nOne option for implementing continous input would be to use the [SDL_GetKeyboardState](https:\/\/wiki.libsdl.org\/SDL_GetKeyboardState) function from SDL. We could call this function every frame and\\nmake the results available to every Scene for interpretation.\\nAnother option would be to consume our existing input event stream which is fed from [SDL_PollEvent](https:\/\/www.libsdl.org\/release\/SDL-1.2.15\/docs\/html\/sdlpollevent.html) and transition our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events.\\nAt the moment I prefer maintaining our own state instead of letting SDL do it as we can reduce the\\nnumber of possible key combinations we want to support through the type system.\\n","Decision":"We will support continous and discrete input using the existing poll events architecture and\\nsome new state machinery.\\n","tokens":270,"id":2482,"Prediction":"## Decision\n\nWe will implement continuous input by consuming our existing input event stream and transitioning our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events. We will also maintain our own state instead of letting SDL do it, in order to reduce the number of possible key combinations we want to support through the type system.\n\n## Context\n\nAs mentioned in the context, we need to support both discrete and continuous input. Discrete input is required for detecting individual keypresses, while continuous input is necessary for detecting the state of an input on an ongoing basis. Currently, we are using SDL for discrete input, but we have no mechanism for continuous input. One option is to use the SDL_GetKeyboardState function, but we prefer to maintain our own state instead, which allows us to reduce the number of possible key combinations we want to support.\n\n## Rationale\n\nBy consuming our existing input event stream and transitioning our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events, we can implement continuous input without relying on SDL. This approach also allows us to maintain control over the state of the input, which is important for reducing the number of possible key combinations we want to support.\n\n## Decision Drivers\n\n* The need for continuous input to support linear movement in the game\n* The desire to maintain control over the state of the input\n* The ability to reduce the number of possible key combinations we want to support through the type system\n\n## Pros and Cons\n\nPros:\n\n* Allows us to maintain control over the state of the input\n* Reduces the number of possible key combinations we want to support\n* Enables continuous input for linear movement in the game\n\nCons:\n\n* Requires additional implementation effort to consume the input event stream and transition the \"InputState\"\n* May introduce additional complexity in the codebase\n\n## Alternatives Considered\n\n* Using the SDL_GetKeyboardState function to implement continuous input\n* Relying solely on SDL for input handling\n\n## Conclusion\n\nBy implementing continuous input by consuming our existing input event stream and transitioning our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events, we can meet the requirements for continuous input while maintaining control over the state of the input and reducing the number of possible key combinations we want to support.","GenTime":"2024-08-10 11:18:43"}
{"File Name":"pomi-data-etl\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2487,"Prediction":"Here is an Architectural Decision Record (ADR) for your software project:\n\n**Decision**\n\n**Title**: Implement a Microservices Architecture for the Project\n\n**Status**: Accepted\n\n**Context**: As our project grows in complexity and scope, we need to adopt a scalable and maintainable architecture to ensure the system's reliability and flexibility. After evaluating different architecture options, we have decided to implement a microservices architecture for the project.\n\n**Problem**: The current monolithic architecture is becoming increasingly difficult to maintain, test, and scale. It is also limiting our ability to develop and deploy new features independently.\n\n**Decision**: We will break down the system into smaller, independent services that communicate with each other using APIs. Each service will be responsible for a specific business capability and will be developed, tested, and deployed independently.\n\n**Consequences**:\n\n* Pros:\n\t+ Improved scalability and fault tolerance\n\t+ Easier maintenance and updates\n\t+ Faster development and deployment of new features\n\t+ Increased flexibility and adaptability to changing requirements\n* Cons:\n\t+ Increased complexity and overhead of managing multiple services\n\t+ Potential for increased latency and complexity in communication between services\n\t+ Requires additional infrastructure and operational resources\n\n**Rationale**: A microservices architecture aligns with the project's goals of scalability, maintainability, and flexibility. It also allows us to take advantage of the latest technologies and trends in software development.\n\n**References**:\n\n* [Microservices Architecture](https:\/\/microservices.io\/)\n* [Monolith to Microservices](https:\/\/martinfowler.com\/bliki\/MonolithToMicroservices.html)\n\n**Implementing the Decision**: We will start by identifying the business capabilities and breaking them down into individual services. We will then design and implement each service, taking into account the necessary APIs for communication between services. We will also establish a clear governance model for managing and maintaining the services.\n\n**Review and Revisions**: This decision will be reviewed and revised as necessary after a period of implementation and feedback from the team and stakeholders.","GenTime":"2024-08-10 11:20:11"}
{"File Name":"BMMRO\/2019-11-20_firestore.md","Context":"## Context\\nWe need a database in order to store the data provided by the user in the different forms.\\n","Decision":"To reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\\nGoogle recommends Firestore for new developers, as it is built upon \u201cthe successes of Real-time database\u201d. Also, for our purposes, Firestore is a better choice because:\\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\\n","tokens":23,"id":2488,"Prediction":"## Decision\n\n**Database Choice for User Data Storage**\n\n## Context\n\nWe need a database in order to store the data provided by the user in the different forms.\n\n## Problem\n\nWe need to decide on a database solution to store the user-provided data, ensuring scalability, reliability, and ease of maintenance.\n\n## Decision\n\nAfter considering various options, we have decided to use a relational database management system (RDBMS) specifically PostgreSQL. This decision is based on the following factors:\n\n* **Scalability**: PostgreSQL is designed to handle large amounts of data and scale horizontally, making it suitable for our application's growth needs.\n* **Reliability**: PostgreSQL is known for its high availability and fault tolerance, ensuring that our data remains accessible and secure.\n* **Ease of maintenance**: PostgreSQL has a large community and a wide range of tools and plugins available, making it easier to manage and troubleshoot.\n* **Security**: PostgreSQL has robust security features, including encryption, access controls, and auditing, to protect our users' data.\n\n## Consequences\n\n* We will need to design and implement the database schema to store the user-provided data.\n* We will need to ensure data consistency and integrity by implementing proper data validation and constraints.\n* We will need to implement a data backup and recovery plan to ensure business continuity in case of data loss or corruption.\n\n## Rationale\n\nPostgreSQL is a widely-used and well-established RDBMS that meets our requirements for scalability, reliability, and ease of maintenance. Its robust security features ensure the protection of our users' data. By choosing PostgreSQL, we can focus on developing our application without worrying about the underlying database infrastructure.","GenTime":"2024-08-10 11:20:31"}
{"File Name":"educational-platform\/0010-use-axon-framework.md","Context":"## Context\\nCurrently, a lot of custom classes are defined for DDD building blocks. It will be better to use DDD library for these goals.\\n","Decision":"Axon Framework will be used as DDD library.\\n","tokens":32,"id":2498,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will use a Domain-Driven Design (DDD) library to replace custom classes for DDD building blocks, improving maintainability and consistency across the system.\n\n**Context**\n\nCurrently, a lot of custom classes are defined for DDD building blocks, which can lead to duplicated efforts, inconsistencies, and maintenance challenges.\n\n**Problem**\n\n* Custom classes for DDD building blocks can lead to:\n\t+ Duplicated code and logic\n\t+ Inconsistencies in naming conventions, syntax, and semantics\n\t+ Increased maintenance efforts and complexity\n* The use of custom classes can also limit the scalability and flexibility of the system\n\n**Decision**\n\nTo address the above issues, we will use a DDD library that provides pre-built classes and patterns for DDD building blocks. This will enable us to:\n\n* Reduce duplicated code and logic\n* Improve consistency across the system\n* Simplify maintenance efforts and complexity\n* Enhance scalability and flexibility\n\n**Consequences**\n\n* We will need to refactor existing code to use the DDD library\n* We will need to ensure that the DDD library is properly configured and integrated with the system\n* We may need to adjust our development process to ensure that the DDD library is properly utilized\n\n**Rationale**\n\nUsing a DDD library will allow us to leverage the expertise and experience of the library's maintainers, ensuring that our implementation is correct and idiomatic. This will also enable us to focus on the business logic and domain-specific knowledge, rather than reinventing the wheel.\n\n**Status**\n\nApproved\n\n**Created** 2023-02-20\n\n**Updated** 2023-02-20","GenTime":"2024-08-10 11:24:01"}
{"File Name":"educational-platform\/0002-integration-events-implementation.md","Context":"## Context\\nFor implementing event-driven application, the platform should be available for communication with integration events.\\n","Decision":"We will start from standard Spring Events classes: ApplicationListener, ApplicationEvent without dependency to external middleware component. We can add custom features to Spring functionality when it's needed.\\ntodo: For now, all events will be stored in integration-events module. But this solution should be reviewed. Integration events should be published after successful transaction.\\n- https:\/\/devblogs.microsoft.com\/cesardelatorre\/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures\/\\n","tokens":21,"id":2499,"Prediction":"## Decision\n\n**Event-Driven Architecture with Integration Events**\n\n## Context\n\nFor implementing event-driven application, the platform should be available for communication with integration events.\n\n## Problem\n\nWe need to design a platform that can handle integration events and ensure seamless communication between microservices.\n\n## Decision\n\nWe will adopt an Event-Driven Architecture (EDA) with integration events to enable real-time communication between microservices. This approach will allow us to decouple services, improve scalability, and reduce latency.\n\n## Rationale\n\n1. **Decoupling**: EDA will enable us to decouple services, allowing them to operate independently and reducing the risk of cascading failures.\n2. **Scalability**: With integration events, we can scale individual services independently, without affecting the overall system.\n3. **Real-time Communication**: EDA will enable real-time communication between microservices, allowing us to react to changes and events in near real-time.\n4. **Flexibility**: Integration events will provide a flexible way to integrate services, allowing us to easily add or remove services without affecting the overall system.\n\n## Consequences\n\n1. **Increased Complexity**: Implementing EDA will require additional infrastructure and management overhead.\n2. **Additional Security Measures**: We will need to implement additional security measures to ensure the integrity and confidentiality of integration events.\n3. **Monitoring and Logging**: We will need to implement monitoring and logging mechanisms to track and analyze integration events.\n\n## Alternatives Considered\n\n1. **Request-Response Architecture**: We considered using a request-response architecture, but this would lead to tight coupling between services and reduce scalability.\n2. **Message Queue**: We considered using a message queue, but this would add additional latency and complexity.\n\n## Status\n\nThis decision is approved and will be implemented in the next release.\n\n## Links\n\n* [Event-Driven Architecture](https:\/\/martinfowler.com\/articles\/201701-event-driven.html)\n* [Integration Events](https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/patterns\/event-driven-integration)","GenTime":"2024-08-10 11:24:23"}
{"File Name":"educational-platform\/0011-use-axon-event-publishing-mechanism.md","Context":"## Context\\nIn [0002-integration-events-implementation.md](0002-integration-events-implementation.md) was defined the solution for using Spring-event related classes for publishing and listening integration events. In current implementation of application we have Axon Framework which have rich tools for implementing such functionality. After migrating to Axon implementation of integration events, in future, we can enable event sourcing.\\n","Decision":"Axon Framework will be used for integration events implementation.\\n","tokens":79,"id":2500,"Prediction":"## Decision\n\n**Decision:** Migrate integration events implementation to Axon Framework\n\n## Context\n\nAs discussed in [0002-integration-events-implementation.md](0002-integration-events-implementation.md), we have defined a solution for using Spring-event related classes for publishing and listening integration events. However, since we are already using Axon Framework in our application, which provides rich tools for implementing event-driven functionality, we have decided to migrate the integration events implementation to Axon Framework.\n\n## Problem\n\nOur current implementation of integration events using Spring-event related classes is not taking full advantage of the features provided by Axon Framework. By migrating to Axon, we can leverage its capabilities to simplify the implementation of event-driven functionality and prepare our application for future event sourcing.\n\n## Decision Drivers\n\n* Simplify event-driven functionality implementation using Axon Framework's built-in features\n* Prepare our application for future event sourcing capabilities\n* Take advantage of Axon Framework's integration with Spring Boot and other frameworks\n\n## Consequences\n\n* Simplified implementation of event-driven functionality\n* Easier integration with other Axon Framework features, such as event sourcing and command handling\n* Potential for improved performance and scalability\n* Additional learning curve for developers familiar with Spring-event related classes\n\n## Status\n\nThis decision is finalized and implemented. The migration to Axon Framework for integration events implementation is underway.","GenTime":"2024-08-10 11:24:38"}
{"File Name":"educational-platform\/0009-architecture-tests.md","Context":"## Context\\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\\n","Decision":"Architecture tests with using Archunit should be implemented.\\n","tokens":21,"id":2502,"Prediction":"## Decision\n\n**Decision:** Implement a centralized architecture validation framework using a Domain-Driven Design (DDD) approach to ensure consistency in applying common architecture principles across all applications.\n\n## Context\n\nAs we need to have a mechanism for supporting and validating common architecture principles in all applications, we require a centralized framework that can be easily integrated into each application. This framework should be able to validate and enforce the architecture principles, ensuring consistency across all applications.\n\n## Problem\n\nCurrently, we lack a standardized approach to validate and enforce architecture principles, leading to inconsistencies and potential issues in the long run. Each application is implementing its own validation mechanism, resulting in duplicated effort and increased complexity.\n\n## Decision Rationale\n\nImplementing a centralized architecture validation framework using DDD will provide several benefits:\n\n1. **Consistency**: A single framework will ensure consistency in applying architecture principles across all applications, reducing the risk of inconsistencies and potential issues.\n2. **Reusability**: The framework can be easily integrated into each application, reducing duplicated effort and increasing reusability.\n3. **Flexibility**: The DDD approach allows for a flexible and modular design, making it easier to adapt to changing requirements and architecture principles.\n4. **Scalability**: The framework can be scaled to support multiple applications and teams, ensuring that the architecture principles are consistently applied across the organization.\n\n## Consequences\n\n1. **Initial Investment**: Implementing a centralized architecture validation framework will require an initial investment in designing and developing the framework.\n2. **Change Management**: The introduction of a new framework may require changes to existing development processes and tools.\n3. **Training and Adoption**: Team members will need to be trained on the new framework, and there may be a learning curve for some developers.\n\n## Pros and Cons Table\n\n| **Pros** | **Cons** |\n| --- | --- |\n| Consistency across applications | Initial investment in framework development |\n| Reusability across applications | Change management required |\n| Flexibility and adaptability | Training and adoption required |\n| Scalability across multiple applications | Potential for increased complexity |\n\n## Implementation Roadmap\n\n1. Define the architecture principles and requirements for the validation framework.\n2. Design the framework using a DDD approach.\n3. Develop the framework and integrate it into the first application.\n4. Test and iterate on the framework based on feedback from the development team.\n5. Roll out the framework to other applications and teams.\n\nBy implementing a centralized architecture validation framework using DDD, we can ensure consistency, reusability, flexibility, and scalability across all applications, ultimately improving the overall architecture and development process.","GenTime":"2024-08-10 11:25:28"}
{"File Name":"pottery\/0004-store-shards-in-time-stamped-directories-and-files.md","Context":"## Context\\nA project history can cover many years, with many small shards being recorded.  If all the shard files are stored in the same directory, the directory will be difficult to work with when many shards have been posted.  Performance might suffer on older file systems.\\n","Decision":"Store shard files in subdirectories named after year and then year-and-month, and name the files after the date and time.\\n","tokens":56,"id":2508,"Prediction":"## Decision\n\nTo address the issue of a large number of shard files in the same directory affecting performance and making it difficult to work with, we will implement a hierarchical directory structure for storing project histories.\n\n## Context\n\nAs described in the context, storing all shard files in the same directory can lead to performance issues on older file systems and make it challenging to manage and work with the large number of files.\n\n## Decision Rationale\n\nImplementing a hierarchical directory structure will allow us to organize shard files in a more logical and efficient manner. This will enable us to:\n\n* Reduce the number of files in a single directory, improving performance and making it easier to manage the directory structure.\n* Organize shard files by date, project, or other relevant criteria, making it easier to locate and retrieve specific files.\n* Implement a more scalable and maintainable directory structure that can accommodate a large number of shard files.\n\n## Consequences\n\n* The directory structure will be more complex, requiring additional complexity in the code to manage and navigate the hierarchy.\n* The hierarchical structure may require additional storage space, as each level of the hierarchy will require its own directory.\n* The implementation will require careful planning and testing to ensure that the directory structure is correctly implemented and that the code can effectively navigate and manage the hierarchy.\n\n## Alternatives Considered\n\n* Storing all shard files in a single, flat directory, which would simplify the directory structure but could lead to performance issues and make it more difficult to manage the large number of files.\n* Implementing a database to store project history data, which would eliminate the need for a directory structure but would require additional complexity in the code to manage and query the database.\n\n## Status\n\nApproved\n\n## Implemented By\n\n[Your Name]\n\n## Date\n\n[Date]","GenTime":"2024-08-10 11:27:15"}
{"File Name":"pottery\/0003-pottery-shards-are-markdown-text.md","Context":"## Context\\nEach event recorded by Pottery (aka \"shard\") is a short piece of text.  Maybe with links, or basic formatting.\\nPottery is used on the command line, so shards need to be generated by Unix command line tools, readable on the terminal and edited in a text editor.\\n","Decision":"The content of each shard will be in Markdown format.\\n","tokens":65,"id":2510,"Prediction":"## Decision\n\n**Event Shard Format**\n\n### Context\n\nAs Pottery is used on the command line, each event recorded by Pottery is a short piece of text that needs to be generated by Unix command line tools, readable on the terminal, and editable in a text editor. The text may include links and basic formatting.\n\n### Decision\n\nWe will use the Markdown format for event shards. This format is widely supported by Unix command line tools and text editors, allowing users to easily generate, view, and edit shards. Markdown's syntax for links and basic formatting will make it easy for users to include relevant information in their shards.\n\n### Rationale\n\n* Markdown is a lightweight and human-readable format that is easy to generate and parse.\n* It is widely supported by Unix command line tools and text editors, making it a convenient choice for users who need to work with shards on the command line.\n* Markdown's syntax for links and basic formatting will make it easy for users to include relevant information in their shards, such as links to external resources or highlighted code snippets.\n\n### Consequences\n\n* Users will be able to easily generate and edit shards using Unix command line tools and text editors.\n* The use of Markdown will make it easy for users to include relevant information in their shards, such as links and basic formatting.\n* The choice of Markdown may limit the complexity of the formatting and links that can be included in shards, but this should not be a significant limitation for most use cases.","GenTime":"2024-08-10 11:27:46"}
{"File Name":"adr-demo\/0004-use-elastic-search-for-image-search.md","Context":"## Context\\nAs our users base is growing and we have more and more search queries, we would like to improve current search form to:\\n* Be able to search on newly added fields (e.g. created date, modified date, author, licence, etc)\\n* Allow more complex searches on existing and new fields such as:\\n* \"and\" queries to search on multiple tags\\n* \"or\" queries to search on an exclusive list of tags\\n* \"startsWith\" and \"contains\" search instead of current exact match searches\\n* synonyms\\n","Decision":"Given our needs it seems pretty clear that we need a search engine, as databases search capabilities are limited and would cost more and more as data is growing.\\nWe will use Elastic Search as our search engine as it is the de facto standard and is based on battle-tested search engine\\nLucene. Our current microservices will now evolve to this schema:\\n![](microservices-architecture-with-es.png)\\nNote: ES indexes do not need to be updated in real-time when a new image\/gif is uploaded, in fact we can easily\\ntolerate a few minutes of delay between an upload and its actual availability for search. All we need is to be\\neventually consistent.\\n","tokens":113,"id":2513,"Prediction":"## Decision\n\n**Search Form Improvement: Implementing Advanced Search Capabilities**\n\n## Context\n\nAs our user base grows and the number of search queries increases, we need to improve the current search form to provide more advanced search capabilities. This includes the ability to search on newly added fields such as created date, modified date, author, and licence, as well as more complex searches on existing and new fields.\n\n## Problem\n\nThe current search form is limited in its functionality, only allowing exact match searches on a few specific fields. This makes it difficult for users to find relevant results, especially when searching for specific combinations of criteria.\n\n## Decision\n\nWe will implement advanced search capabilities in the search form, including:\n\n* Support for searching on newly added fields (created date, modified date, author, licence, etc.)\n* Support for complex searches using \"and\" queries to search on multiple tags\n* Support for complex searches using \"or\" queries to search on an exclusive list of tags\n* Support for \"startsWith\" and \"contains\" search instead of exact match searches\n* Support for synonyms to improve search relevance\n\n## Rationale\n\nImplementing advanced search capabilities will improve the overall user experience by providing more flexibility and precision in searching for specific results. This will be particularly beneficial for users who need to search for complex combinations of criteria.\n\n## Consequences\n\n* The search form will require significant changes to accommodate the new features, which may require additional development time and resources.\n* The search form will need to be thoroughly tested to ensure that it functions correctly and provides accurate results.\n* The implementation of advanced search capabilities may require changes to the underlying database schema and data storage.\n\n## Alternatives Considered\n\n* We considered implementing a separate advanced search page, but this would require users to navigate away from the main search page and would not provide the same level of flexibility and precision as an integrated search form.\n* We considered using a third-party search library, but this would require additional integration and customization to meet our specific requirements.\n\n## Status\n\nThis decision has been approved and will be implemented in the next release.","GenTime":"2024-08-10 11:28:48"}
{"File Name":"adr-demo\/0003-use-cassandra-as-storage-backend.md","Context":"## Context\\nAs the application offers uploads capabilities to our users, we need to be able to store uploaded images to some\\nkind of storage. The storage backend should be able to do the following things:\\n* Store raw images\/gifs\\n* Store thumbnails of images\/gifs\\n* Offer a basic search capability (search by title, search by tag)\\n","Decision":"After a quick comparative study, we decide to go with a Cassandra storage backend. The reasons are the following:\\n* Cassandra is quick and easy to setup\\n* It should answer our needs without much custom configuration, even when scaling up\\n* Cassandra allows us to do simple search operations on the fields we're interested in\\n* The team is already familiar with Cassandra usage\\nHere is a quick picture of our architecture including Cassandra as the storage backend:\\n![](microservices-architecture-with-cassandra.png)\\n**Note**: Many services seem to point to the same Cassandra, however we need to setup multiple tables so we can respect\\nCQRS principles (command and query separation):\\n* One table will be used to store thumbnails\\n* One table will be used to store raw images\\n* Thumbnail and upload services are write services (but to different tables)\\n* Search service and download service are read-only services\\n### Other databases\\nIn our quest to the holy storage backend we also studied MongoDb but it didn't seem to be a good fit for our use case : its Json-like structure does not fill particularly well with what we are trying to achieve, and is not much appropriate for very large storage volumes.\\n","tokens":73,"id":2515,"Prediction":"## Decision\n\n**Decision:** We will use a cloud-based object storage service, specifically Amazon S3, as our storage backend for uploaded images and thumbnails.\n\n**Context:** As the application offers uploads capabilities to our users, we need to be able to store uploaded images to some kind of storage. The storage backend should be able to do the following things: store raw images\/gifs, store thumbnails of images\/gifs, and offer a basic search capability (search by title, search by tag).\n\n**Reasoning:**\n\n* Amazon S3 is a highly scalable and durable object storage service that can store large amounts of data, making it suitable for storing images and thumbnails.\n* S3 provides a robust search capability through its query API, allowing us to search for objects based on their metadata, such as title and tags.\n* S3 integrates well with other AWS services, such as AWS Lambda and Amazon Elastic Compute Cloud (EC2), which we may use in the future for processing and serving images.\n* S3 is a widely used and well-documented service, making it easier for our development team to find resources and support.\n* Using a cloud-based service like S3 eliminates the need for us to manage and maintain our own storage infrastructure, freeing up resources for more strategic activities.\n\n**Consequences:**\n\n* We will need to integrate our application with Amazon S3 using their API or SDK.\n* We will need to ensure that our application is configured to handle errors and exceptions that may occur when interacting with S3.\n* We will need to consider the costs associated with using S3, including storage costs, data transfer costs, and request costs.\n\n**Alternatives Considered:**\n\n* We considered using a self-managed storage solution, such as a NAS or a custom-built storage cluster. However, this would require significant resources and expertise to manage and maintain.\n* We considered using a different cloud-based storage service, such as Google Cloud Storage or Microsoft Azure Blob Storage. However, Amazon S3 is a well-established and widely used service that integrates well with other AWS services.\n\n**Status:** Approved","GenTime":"2024-08-10 11:29:29"}
{"File Name":"adr-demo\/0002-use-microservices-architecture.md","Context":"## Context\\nThe web application we want to develop and offer to our users is a picture service that allows :\\n* Uploading images and gifs with a title and tags\\n* Searching images by title or tag\\n* Downloading an image in various file formats and sizes\\nAs we want to quicly build a first version to be able to quickly go online and answer competitors, we choose\\nto deploy our application to Google Cloud (as we do not have any bare metal servers).\\nThe application should meet the following criteria:\\n* it should be quick enough so the search answers in less than 500ms\\n* it should be scalable so we can meet future usage\\n","Decision":"In order to meet required criteria we choose to develop our application using a microservices approach :\\neach business need will be addressed by a specific microservice, as described in the following schema:\\n![](microservices-architecture.png)\\nEach microservice should take a simple input and produce a simple output, each microservice should act as a black box\\nregarding other microservices, meaning state should not be shared across different instances of same or differents microservices.\\n","tokens":137,"id":2516,"Prediction":"## Decision\n\n**Decision:** We will use a Cloud-Optimized Architecture with a combination of Google Cloud Storage and Google Cloud Functions to build the picture service, prioritizing scalability and quick deployment over customization and fine-grained control.\n\n**Context:** As mentioned, we want to quickly build a first version of the picture service and deploy it to Google Cloud, prioritizing scalability and speed. We also need to meet the performance criteria of answering search queries in under 500ms.\n\n**Rationale:**\n\n1. **Scalability:** Google Cloud provides a highly scalable infrastructure, allowing us to easily scale up or down to meet changing usage demands. This aligns with our goal of being able to handle future growth.\n2. **Quick Deployment:** By using Cloud-Optimized Architecture, we can quickly deploy our application without worrying about provisioning and managing servers. This allows us to focus on developing the application itself.\n3. **Cost-Effectiveness:** Google Cloud provides a pay-as-you-go pricing model, which aligns with our goal of being cost-effective.\n4. **Performance:** Google Cloud provides high-performance infrastructure, which should allow us to meet our performance criteria of answering search queries in under 500ms.\n\n**Trade-Offs:**\n\n1. **Customization:** By using a Cloud-Optimized Architecture, we may have limited control over the underlying infrastructure and may need to compromise on customization.\n2. **Cost:** While Google Cloud provides a cost-effective pricing model, there may be additional costs associated with using cloud services.\n\n**Consequences:**\n\n1. **Implementation:** We will need to design and implement our application using Google Cloud Storage and Google Cloud Functions.\n2. **Monitoring and Optimization:** We will need to monitor our application's performance and optimize it as needed to ensure it meets our performance criteria.\n3. **Future Development:** As our application grows, we will need to consider how to scale and optimize it further to meet changing usage demands.\n\nBy using a Cloud-Optimized Architecture, we can quickly deploy our picture service and meet our scalability and performance criteria, while also being cost-effective and flexible for future development.","GenTime":"2024-08-10 11:29:52"}
{"File Name":"adr-demo\/0001-record-architecture-decisions.md","Context":"## Context\\nMost projects suffer from a common problem : they usually don't keep a clear history of all the architectural decisions if the project.\\nIt might not appear as an issue at first but as the project evolves it becomes less and less clear why each change was made,\\nleading to somewhat wrong decisions : should we change existing code and take the risk of breaking the application because\\nwe might have missed an important decision, or should we keep it (fearing we might break something) and take the risk of\\nparalyzing the project with an accumulation of potentially wrongly-kept decisions and changes ?\\nTo avoid this dilemna it appears we have to do something to keep a record of all architectural decisions.\\n","Decision":"We will start using Lightweight Architecture Decision Records (further refered as ADR) as explained\\n[here](https:\/\/blog.stack-labs.com\/code\/adr-to-remember-past-archiectural-decisions\/#format-d-un-adr)\\nor [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nHere are a few hints of how we will use ADRs :\\n* We will keep all ADRs in a Git respository so they can be versioned\\n* We will materialize Each ADR as a separate file within the repository\\n* We will prefix each ADR by an ordered number (0001 to 9999), and keep ADRs numbers sequential\\n* We will keep each ADR as short as possible, trying to limit ourselves to 1-2 pages max\\n* We will use Markdown as the template engine of our ADRs\\n* We will always keep ALL written ADRs but we will mark old ADRs as superseded if they are\\nThe markdown format we will use when writing an ADR is the following :\\n```markdown\\n# Title: These documents have names that are short noun phrases. For example, \"ADR-0001 - Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\nThis section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":146,"id":2517,"Prediction":"## Decision\n\n**Decision:** We will implement an Architectural Decision Record (ADR) system to document and track all significant architectural decisions made throughout the project's lifecycle.\n\n**Context:** As described in the context, the lack of a clear history of architectural decisions can lead to confusion, uncertainty, and potential risks in the project's evolution. This decision aims to address this issue by establishing a systematic approach to documenting and tracking architectural decisions.\n\n**Problem:** Without a clear record of architectural decisions, it becomes difficult to:\n\n1. Understand the reasoning behind previous design choices\n2. Identify potential risks and trade-offs associated with changes\n3. Make informed decisions about future changes and refactoring\n\n**Solution:** We will maintain an ADR system that includes:\n\n1. A dedicated repository (e.g., GitHub page, Confluence space) for storing ADRs\n2. A template for documenting ADRs, including:\n\t* Problem statement\n\t* Context\n\t* Decision\n\t* Rationale\n\t* Pros and Cons\n\t* Implementation details\n\t* References (e.g., links to relevant issues, commits, or documentation)\n3. A process for reviewing and approving ADRs before they are considered final\n4. Regular maintenance and updates to the ADR repository to ensure it remains accurate and relevant\n\n**Consequences:**\n\n1. Improved transparency and understanding of architectural decisions\n2. Reduced risk of making incorrect changes or refactoring\n3. Enhanced collaboration and communication among team members\n4. Easier onboarding for new team members and stakeholders\n5. Better documentation for future maintenance and evolution of the project\n\n**Rationale:** By implementing an ADR system, we can ensure that our project's architectural decisions are well-documented, transparent, and easily accessible. This will help us make informed decisions, reduce the risk of errors, and improve overall project quality.","GenTime":"2024-08-10 11:30:12"}
{"File Name":"volley-management\/0006-use-azure-table-storage-for-persistence.md","Context":"## Context and Problem Statement\\nWe need a persistent state for the system. Previously I've used Azure SQL but it is quite pricey but has it's own set of features including relational model. But given that system is not very complex I can model persistence mechanism for NoSQL model.\\n## Decision Drivers\\n* Cost\\n* Backup capabilities\\n","Decision":"* Cost\\n* Backup capabilities\\nChosen option: \"Azure Table Storage\", because cost.\\n### Positive Consequences <!-- optional -->\\n* Persistence bill should be down from ~$5\/month to less than $1\/month\\n### Negative Consequences <!-- optional -->\\n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing. We will have to come up with a strategy to get them good experience.\\n","tokens":68,"id":2520,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Use a NoSQL database as the persistent state solution for the system.\n\n**Context:** We need a persistent state for the system, and previously, we have used Azure SQL, but it is quite pricey and has its own set of features including a relational model. Given that the system is not very complex, we can model persistence mechanism for a NoSQL model.\n\n**Decision Drivers:**\n\n* **Cost**: Azure SQL is expensive, and we are looking for a more cost-effective solution.\n* **Backup capabilities**: While Azure SQL has robust backup capabilities, we are willing to trade off some of these features for a more affordable solution.\n\n**Decision Consequences:**\n\n* We will implement a NoSQL database, which will provide a flexible and scalable solution for our system's persistent state.\n* We will need to design a data model that is optimized for the NoSQL database, which may require additional effort and expertise.\n* We may need to implement additional backup and recovery mechanisms to ensure data integrity and availability.\n\n**Rationale:** Given the cost constraints and the relatively simple nature of our system, a NoSQL database provides a more cost-effective and flexible solution for our persistent state needs. While we may need to invest additional time and effort in designing and implementing the data model and backup mechanisms, the benefits of a more affordable and scalable solution outweigh the costs.\n\n**Status:** Accepted\n\n**Created:** [Insert Date]\n\n**Updated:** [Insert Date]\n\nNote: This is just a sample, you can adjust it according to your needs and the specific requirements of your project.","GenTime":"2024-08-10 11:31:12"}
{"File Name":"volley-management\/0002-use-sendgrid-as-mailing-system.md","Context":"## Context and Problem Statement\\nWe need a way to send emails to users for several workflows.\\n","Decision":"Chosen option: SendGrid, because it is SaaS and has generous free tier. Integration is easy.\\n### Positive Consequences <!-- optional -->\\n* Fast time to develop\\n### Negative Consequences\\n* We need to think about setting up development environment\\n","tokens":20,"id":2523,"Prediction":"## Decision\n\n**Email Service Integration**\n\n## Context\n\nWe need a way to send emails to users for several workflows.\n\n## Problem Statement\n\nWe currently have multiple workflows that require sending emails to users, and we want to find a solution that is scalable, maintainable, and easy to use.\n\n## Decision\n\nWe will integrate a third-party email service, such as Sendgrid or Mailgun, to handle email sending for our workflows. This decision is based on the following considerations:\n\n* Scalability: By using a third-party email service, we can offload the email sending functionality and focus on our core business logic.\n* Maintainability: The email service will handle the complexity of email sending, including handling bounces, spam filtering, and tracking.\n* Ease of use: We will have a simple API to send emails, making it easy for our developers to integrate email sending into our workflows.\n\n## Consequences\n\n* We will need to set up and configure the email service, including creating an account and setting up the API keys.\n* We will need to integrate the email service into our workflows, which may require changes to our code and data models.\n* We will need to monitor and troubleshoot any issues with email sending, which may require additional resources and expertise.\n\n## Rationale\n\nBy using a third-party email service, we can focus on our core business logic and outsource the complexity of email sending. This will allow us to scale more easily and reduce the risk of email-related issues affecting our users.","GenTime":"2024-08-10 11:32:07"}
{"File Name":"volley-management\/0007-follow-optimistic-concurrency.md","Context":"## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n","Decision":"* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\nChosen option: \"Optimistic concurrency\", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).\\n### Positive Consequences <!-- optional -->\\n* Concurrency check will fail very rarely so it will have almost no overhead\\n* It is aligned with Azure Table storage Optimistic concurrency\\n### Negative Consequences <!-- optional -->\\n* Whole application will have a responsibility to manage versions properly\\n* If we change a datastore we might need to have a larger change\\n","tokens":86,"id":2526,"Prediction":"## Decision\n\nTo support concurrent changes to data in Volley Management, we will implement optimistic concurrency control using version numbers.\n\n## Context\n\nAs mentioned in the problem statement, Volley Management faces a challenge of handling concurrent changes to data. We have explicitly decided not to consider an option of not implementing concurrency checks, as it may lead to data inconsistencies and loss of integrity.\n\n## Decision Drivers\n\nThe decision to implement optimistic concurrency control is driven by the following factors:\n\n* **Performance**: Optimistic concurrency control allows for high throughput scenarios by minimizing the number of locks and blocking operations.\n* **Maintainability**: The implementation of optimistic concurrency control requires minimal code changes and is relatively straightforward to understand and maintain.\n\n## Decision Description\n\nOptimistic concurrency control works by maintaining a version number for each record in the database. When a record is updated, the version number is incremented. When a user attempts to update a record, the version number is checked to ensure that it has not been modified by another user since the record was last read. If the version number has changed, the update is rejected and the user is notified of the conflict.\n\n## Decision Consequences\n\nThe implementation of optimistic concurrency control will:\n\n* Ensure data integrity by preventing concurrent updates to the same record\n* Provide high throughput performance by minimizing the number of locks and blocking operations\n* Minimize code changes and complexity by using a straightforward and well-established approach\n\n## Pros and Cons\n\nPros:\n\n* Ensures data integrity and consistency\n* Provides high throughput performance\n* Minimizes code changes and complexity\n\nCons:\n\n* May require additional database queries to check version numbers\n* May require additional error handling for conflicts\n\n## Re-evaluation\n\nThis decision will be re-evaluated in the future if performance or maintainability issues arise.","GenTime":"2024-08-10 11:32:55"}
{"File Name":"gp-data-etl\/0002-store-local-copy-of-syndication-data.md","Context":"## Context\\nThe [NHS's Syndicated Content](http:\/\/www.nhs.uk\/aboutNHSChoices\/professionals\/syndication\/Pages\/Webservices.aspx)\\nholds information about GP surgeries.\\nSeveral of Connecting to Services applications need to make use of GP data, including the\\n[GP Finder](https:\/\/github.com\/nhsuk\/gp-finder) and [Profiles](https:\/\/github.com\/nhsuk\/profiles) projects.\\nThe Syndication API is not intended for direct access in production environments.\\nThe Syndication API returns data in XML, and information is spread across several subpages,\\ni.e. overview, services, facilities.\\n","Decision":"The syndication XML API should be scraped nightly to create a local copy of the data.\\nWhat were multiple pages for a practice on the Syndication API will be merged into a single record per practice and\\nconverted into the Connecting to Services teams preferred format, JSON.\\n","tokens":137,"id":2530,"Prediction":"## Decision\n\n**Title:** Syndication API Integration for GP Data Retrieval\n\n**Context:** The NHS's Syndicated Content holds information about GP surgeries, and several Connecting to Services applications, including the GP Finder and Profiles projects, need to make use of GP data. The Syndication API is not intended for direct access in production environments, and the API returns data in XML format, spread across several subpages (overview, services, facilities).\n\n**Decision:** We will use a caching layer to store the Syndication API data and create a data transformation layer to convert the XML data into a more consumable format for our applications. This will enable us to:\n\n1. Decouple our applications from the Syndication API, reducing the risk of changes to the API affecting our applications.\n2. Improve the performance of our applications by reducing the number of requests made to the Syndication API.\n3. Simplify the data processing and manipulation for our applications by providing a standardized and easily accessible data format.\n\n**Rationale:**\n\n* The Syndication API is not designed for direct access in production environments, so we need to create a layer of abstraction to protect our applications from changes to the API.\n* The XML format returned by the Syndication API is not easily consumable by our applications, so we need to transform the data into a more suitable format.\n* Caching the Syndication API data will reduce the load on the API and improve the overall performance of our applications.\n\n**Consequences:**\n\n* We will need to implement a caching layer and a data transformation layer, which will add complexity to our architecture.\n* We will need to ensure that the caching layer is properly configured to handle the volume of data and the frequency of requests.\n* We will need to monitor the performance of the caching layer and the data transformation layer to ensure that they are functioning correctly.\n\n**Alternatives Considered:**\n\n* Directly accessing the Syndication API from our applications, which would expose our applications to changes to the API and potentially impact their performance.\n* Creating a separate API to retrieve GP data, which would add complexity to our architecture and require additional development and maintenance.","GenTime":"2024-08-10 11:34:21"}
{"File Name":"terraform-aws-dynamodb\/0004-terraform-requirements-in-module.md","Context":"## Context\\nTerraform enables you to constrain the version of Terraform able to run a\\nmodules, as well as the minimium allowed versions of providers. Many of LIC\\nexisting modules do not leverage these configurations.\\n","Decision":"The Terraform version able to run this module will be set in `main.tf`.\\nThe miniumum allowed versions of providers will be set in `main.tf`\\n","tokens":46,"id":2536,"Prediction":"## Decision\n\nTo ensure the consistency and maintainability of LIC's existing modules, we will require all modules to specify the minimum allowed versions of Terraform and providers in their `terraform.tf` files. This will allow us to ensure that all modules are compatible with the latest versions of Terraform and providers, and prevent potential issues caused by outdated versions.\n\n## Rationale\n\nBy specifying the minimum allowed versions of Terraform and providers, we can:\n\n* Ensure that all modules are compatible with the latest versions of Terraform and providers, reducing the risk of errors and inconsistencies.\n* Prevent the use of outdated versions of Terraform and providers, which can lead to security vulnerabilities and other issues.\n* Make it easier to maintain and update modules, as we can ensure that they are compatible with the latest versions of Terraform and providers.\n* Improve the overall quality and reliability of our modules, by ensuring that they are built with the latest and most reliable versions of Terraform and providers.\n\n## Consequences\n\n* Existing modules will need to be updated to include the minimum allowed versions of Terraform and providers.\n* New modules will need to include the minimum allowed versions of Terraform and providers from the start.\n* We may need to provide additional documentation and guidance to help module authors understand how to specify the minimum allowed versions of Terraform and providers.\n* We may need to perform additional testing and validation to ensure that all modules are compatible with the latest versions of Terraform and providers.\n\n## Status\n\nApproved\n\n## Affected Components\n\n* All LIC modules\n* Terraform\n* Providers\n\n## References\n\n* Terraform documentation: [Configuring the Terraform Version](https:\/\/www.terraform.io\/docs\/language\/settings\/tf-versions.html)\n* Terraform documentation: [Specifying Minimum Allowed Versions](https:\/\/www.terraform.io\/docs\/language\/providers\/versions.html)","GenTime":"2024-08-10 11:36:24"}
{"File Name":"terraform-aws-dynamodb\/0005-feature-toggles.md","Context":"## Context\\nToggles are a useful aspect of Terraform modules that provide the ability to\\nturn on\/off functionality.\\nCapital One have written about\\n[Building Feature Toggles into\\nTerraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)\\n. This build on top of the use of the count and ternary to provide a more robust\\nfeature toggle.\\nAs this module is envisioned to be part of a larger Data Storage Module there is\\na concept where no DynamoDB table may need to be provisioned. In addition, when\\nusing DynamoDB Autoscaling can be a useful feature. Providing the capabilities\\nto turn this on and off based on requirements is also a useful requirement.\\n","Decision":"Feature toggles will be leveraged to determine:\\n* If any resources should be created\\n* If a DynamoDB resource should be created\\n* If the Autoscaler should be enabled\\n","tokens":160,"id":2538,"Prediction":"## Decision\n\n**Decision:** Implement feature toggles using Terraform modules to enable or disable DynamoDB table provisioning and autoscaling based on requirements.\n\n**Context:** As described in the context, toggles are a useful aspect of Terraform modules that provide the ability to turn on\/off functionality. In this case, we need to enable or disable DynamoDB table provisioning and autoscaling based on requirements.\n\n**Reasoning:**\n\n* Using feature toggles allows us to decouple the provisioning and autoscaling of DynamoDB tables from the rest of the module, making it easier to manage and maintain.\n* It provides a way to turn off DynamoDB table provisioning when it's not required, which can reduce costs and improve performance.\n* It also enables us to turn on DynamoDB autoscaling when required, which can help improve the performance and scalability of the DynamoDB tables.\n* By using Terraform modules, we can reuse the feature toggles across multiple environments and configurations, making it easier to manage and maintain.\n\n**Consequences:**\n\n* The module will require additional configuration to define the feature toggles.\n* The module will need to be updated to include the necessary logic to toggle the provisioning and autoscaling of DynamoDB tables.\n* The module will provide more flexibility and configurability, making it easier to adapt to changing requirements.\n\n**Alternatives Considered:**\n\n* Using environment variables or command-line flags to control the provisioning and autoscaling of DynamoDB tables.\n* Hardcoding the provisioning and autoscaling of DynamoDB tables into the module.\n* Using a separate configuration file or database to store the feature toggles.\n\n**Decision Owner:** [Your Name]\n\n**Prerequisites:** [Any prerequisites or dependencies required for this decision]\n\n**Relevant Links:**\n\n* [Building Feature Toggles into Terraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)","GenTime":"2024-08-10 11:37:01"}
{"File Name":"terraform-aws-dynamodb\/0010-terraform-backend-in-module.md","Context":"## Context\\nWhen LIC teams begun using this module it became apparent that the current\\nimplementation pattern does not meet their needs. Without a backend in the\\nmodule teams would need to add a Terraform `backend` configuration into there\\nlocal implementation for it to be picked up.\\n","Decision":"Restored the `backend` into the module for the time being.\\nWe still feel this should be removed at some time and teams become familiar with\\nhow to use Terragrunt\/Terraform configuring there own `backend`.\\n","tokens":57,"id":2539,"Prediction":"## Decision\n\n**Decision:** Implement a built-in backend in the module to simplify the integration process for LIC teams.\n\n**Context:** When LIC teams began using this module, it became apparent that the current implementation pattern does not meet their needs. Without a backend in the module, teams would need to add a Terraform `backend` configuration into their local implementation for it to be picked up.\n\n**Problem:** The current implementation requires LIC teams to manually configure the backend, which can be error-prone and time-consuming.\n\n**Reasoning:** By implementing a built-in backend in the module, we can simplify the integration process for LIC teams and reduce the risk of errors. This will also improve the overall user experience and make it easier for teams to get started with the module.\n\n**Consequences:**\n\n* Simplified integration process for LIC teams\n* Reduced risk of errors\n* Improved user experience\n* Potential increase in adoption and usage of the module\n\n**Alternatives Considered:**\n\n* Requiring teams to manually configure the backend\n* Providing a sample backend configuration that teams would need to customize\n* Creating a separate module for backend configuration\n\n**Decision Drivers:**\n\n* User feedback from LIC teams\n* Desire to simplify the integration process\n* Need to improve the overall user experience\n\n**Status:** Implemented","GenTime":"2024-08-10 11:37:15"}
{"File Name":"terraform-aws-dynamodb\/0007-map-list-variables.md","Context":"## Context\\nDynamoDB provides the ability to supply additional attributes, a local\\nsecondary index and a global secondary index. These additional attributes\\nconsumed by the DynamoDB AWS Provider as maps\/lists.\\nIn addtion to the consumption as maps\/lists there are additional requirements\\nthat the range\/hash keys be added to the additional attributes if declared. They\\nare not added if undeclared.\\n","Decision":"The module will use a `null_resource` to manage the secondary indexes. Creating\\nthem from the existences of appropriate maps\/lists.\\nProperties related to these resources will consume a list of maps as input.\\nThese will them be mapped to the appropriate resource within in the module.\\nThe range\/hash key will be added automatically to the additional attributes by\\nthe module, reducing the load on the consumer with implementation detail.\\n","tokens":80,"id":2543,"Prediction":"## Decision\n\nTo handle the additional attributes, local secondary index, and global secondary index provided by DynamoDB in a consistent and efficient manner, we will use a custom data structure in our software to represent these attributes.\n\n## Context\n\nAs described by the user, DynamoDB provides the ability to supply additional attributes, local secondary index, and global secondary index. These additional attributes are consumed by the DynamoDB AWS Provider as maps\/lists. Additionally, there are requirements that the range\/hash keys be added to the additional attributes if declared, but not added if undeclared.\n\n## Decision Rationale\n\nUsing a custom data structure will allow us to easily manage and manipulate these additional attributes, local secondary index, and global secondary index in a way that is consistent with the requirements of the DynamoDB AWS Provider. This will ensure that our software can correctly consume and process these attributes, and that we can easily add or remove attributes as needed.\n\n## Decision Consequences\n\n* We will need to implement a custom data structure to represent the additional attributes, local secondary index, and global secondary index.\n* We will need to write code to populate and manipulate this data structure in a way that is consistent with the requirements of the DynamoDB AWS Provider.\n* This decision may require additional testing to ensure that the custom data structure is correctly implemented and that it meets the requirements of the DynamoDB AWS Provider.\n\n## Pros and Cons\n\nPros:\n\n* Allows for easy management and manipulation of additional attributes, local secondary index, and global secondary index.\n* Ensures consistency with the requirements of the DynamoDB AWS Provider.\n\nCons:\n\n* Requires additional development effort to implement and test the custom data structure.\n* May require additional complexity in the code to manage the custom data structure.\n\n## Reevaluation\n\nThis decision will be reevaluated in 6 months to assess its effectiveness and to determine if any changes are needed to improve the implementation.","GenTime":"2024-08-10 11:38:40"}
{"File Name":"launcher\/2018-06-15_request_enrollment_details.md","Context":"## Context\\nWhen enrolling osquery into a remote server, it's desirable to also have a set of initial fields about the osquery node. Usually, servers like Fleet queue ad-hoc queries to get this information, but it's not always reliable. For example, a osquery host can succeed during the Enroll method, and then immediately crash, leaving the server operator with little context about which osquery host to troubleshoot. Another effect of populating host details via distributed query is that they can add some latency to the enrollment process depending on the query interval.\\n","Decision":"Add a EnrollmentDetails structure to the RequestEnrollment method which sends the results of a population query as part of the enrollment. Below is a list of chose attributes that launcher will always send:\\n```\\nmessage EnrollmentDetails {\\nstring os_version = 1;\\nstring os_build = 2;\\nstring os_platform = 3;\\nstring hostname = 4;\\nstring hardware_vendor = 5;\\nstring hardware_model = 6;\\nstring hardware_serial = 7;\\nstring osquery_version = 8;\\nstring launcher_version = 9;\\n}\\n```\\n","tokens":116,"id":2546,"Prediction":"## Decision\n\n**Decision:** To include a set of initial fields about the osquery node during the enrollment process, rather than relying on distributed queries.\n\n**Context:** As described, when enrolling osquery into a remote server, it's desirable to have a set of initial fields about the osquery node. However, relying on distributed queries to populate these fields can lead to unreliability and added latency to the enrollment process.\n\n**Problem:** The current approach of relying on distributed queries to populate host details can result in:\n\n1. Unreliability: If the osquery host crashes immediately after enrollment, the server operator may not have sufficient context to troubleshoot the issue.\n2. Added latency: Distributed queries can introduce additional latency to the enrollment process, which can be problematic in scenarios where rapid deployment and configuration are critical.\n\n**Reasoning:** Including a set of initial fields about the osquery node during the enrollment process can provide a more reliable and efficient way to populate host details. This approach can ensure that the necessary information is available immediately, without relying on distributed queries that may introduce additional latency or unreliability.\n\n**Decision Criteria:**\n\n1. Reliability: Including initial fields about the osquery node during enrollment ensures that critical information is available immediately, even if the osquery host crashes or is unavailable.\n2. Efficiency: This approach eliminates the need for distributed queries, reducing latency and improving the overall enrollment process.\n3. Maintainability: By populating host details during enrollment, the system can be designed to handle changes to the osquery node configuration and metadata more effectively.\n\n**Consequences:**\n\n1. Improved reliability: The system will have a more reliable way to populate host details, ensuring that critical information is available immediately.\n2. Reduced latency: The enrollment process will be faster and more efficient, with reduced latency and improved overall performance.\n3. Simplified maintenance: The system will be easier to maintain and update, with a more straightforward approach to handling changes to osquery node configuration and metadata.\n\n**Implementation:**\n\n1. Design a data structure to store the initial fields about the osquery node, including relevant metadata such as node ID, OS version, and hardware specifications.\n2. Modify the enrollment process to populate the initial fields about the osquery node during the enrollment process.\n3. Implement logic to handle changes to the osquery node configuration and metadata, ensuring that the system remains up-to-date and accurate.\n\nBy including a set of initial fields about the osquery node during the enrollment process, we can improve the reliability and efficiency of the system, while also simplifying maintenance and reducing latency.","GenTime":"2024-08-10 11:39:46"}
{"File Name":"launcher\/2018-03-29_add_crc_and_uuid_to_log_lines.md","Context":"## Context\\n**TL;DR**\\nA lack of unique info per-log line makes it hard to uniq osquery data downstream, especially in cases when distinct log lines with identical data can be generated. To improve visibility, we should add a UUID to each emitted line. Additionally, adding a hash of the data lets you uniq log line data without inspecting all the fields of these log lines.\\n**Detail**\\nWhile a unique ID is generated for every batched log event emitted from launcher, individual log lines within that log cannot be determined to be unique. This is both true for distinct logs which are identical in their contents, and in the instance that the downstream data pipeline cannot guarantee \"exactly once\" delivery of messages.\\nBy including a UUID for each log line, distinct but semantically identical log lines of data can be determined not to be duplicates.\\nAdditionally, by including a CRC of the data inside each log line, downstream consumers of the log can identify log lines with identical data without inspecting the entirety of log line's data.\\nNote that the crc must be calculated before adding the uuid, or else it won't be useful\\n","Decision":"Launcher should unpack logs from osquery and decorate each line with both a CRC of the data, and a UUID.\\n","tokens":236,"id":2549,"Prediction":"Here is the Architectural Decision Record (ADR) for the provided context:\n\n**Decision**\n\nTitle: Add UUID and CRC to osquery log lines to improve uniqueness and data integrity\n\nStatus: Accepted\n\n**Context**\n\nAs described in the TL;DR, the lack of unique information per log line in osquery data makes it challenging to uniquely identify log lines downstream, particularly when identical log lines with distinct data can be generated. To address this issue, we will add a UUID to each emitted log line to improve visibility and enable distinct log lines to be identified.\n\n**Decision**\n\nWe will add a UUID to each emitted log line to uniquely identify each log line. Additionally, we will calculate a CRC (Cyclic Redundancy Check) of the data inside each log line to enable downstream consumers to identify log lines with identical data without inspecting the entirety of the log line's data.\n\n**Rationale**\n\nThe addition of a UUID will enable us to uniquely identify each log line, even if identical log lines with distinct data are generated. This will improve visibility and enable us to determine which log lines are duplicates. The calculation of a CRC will allow downstream consumers to identify log lines with identical data without inspecting the entirety of the log line's data, which will improve data integrity and reduce the risk of duplicate data being processed.\n\n**Consequences**\n\nThe addition of a UUID and CRC to each log line will:\n\n* Improve visibility and enable unique identification of log lines\n* Enable downstream consumers to identify log lines with identical data without inspecting the entirety of the log line's data\n* Improve data integrity by reducing the risk of duplicate data being processed\n\nHowever, the addition of a UUID and CRC will also:\n\n* Increase the size of the log lines, which may impact storage and processing efficiency\n* Require additional processing power to calculate the CRC\n\n**Status**\n\nThe decision has been accepted and implemented. The UUID and CRC will be added to each emitted log line to improve uniqueness and data integrity.","GenTime":"2024-08-10 11:40:51"}
{"File Name":"titania-os\/0002-docker-for-dapps.md","Context":"## Context\\nTitania is supposed to run multiple dApps. There are multiple techologies and concepts on how to define and isolate a dApp.\\n","Decision":"We will use LXC technology and represent a dApp as a container. We will use Docker to build, run and manage said containers.\\n","tokens":31,"id":2573,"Prediction":"## Decision\n\n**Title:** Isolation Strategy for dApps on Titania\n\n**Context:** Titania is designed to run multiple decentralized applications (dApps) with varying technologies and concepts for defining and isolating each dApp.\n\n**Problem:** With multiple dApps running on Titania, there is a need to ensure that each dApp is properly isolated from others to prevent conflicts, improve scalability, and maintain security.\n\n**Decision:** We will use a combination of containerization and microservices architecture to isolate and manage dApps on Titania.\n\n**Rationale:**\n\n1. **Containerization:** We will use containerization to isolate each dApp by packaging its code, dependencies, and configurations into a single container. This will ensure that each dApp has its own isolated environment, making it easier to manage and maintain.\n2. **Microservices architecture:** We will design each dApp as a set of microservices, each responsible for a specific functionality. This will allow us to scale individual services independently, improve fault tolerance, and reduce the complexity of the overall system.\n\n**Consequences:**\n\n1. **Improved isolation:** Each dApp will have its own isolated environment, reducing the risk of conflicts and improving overall system stability.\n2. **Increased scalability:** With microservices architecture, we can scale individual services independently, allowing Titania to handle a larger number of dApps and users.\n3. **Easier maintenance:** Containerization and microservices architecture will make it easier to update, maintain, and troubleshoot individual dApps, reducing the overall complexity of the system.\n\n**Alternatives considered:**\n\n1. **Virtual machines:** We considered using virtual machines to isolate each dApp, but this would have added significant overhead and complexity to the system.\n2. **Shared database:** We considered using a shared database to store data for multiple dApps, but this would have introduced potential conflicts and reduced the overall security of the system.\n\n**Status:** This decision is approved and will be implemented in the next release of Titania.","GenTime":"2024-08-10 11:49:27"}
{"File Name":"titania-os\/0005-networkmanager-for-networking.md","Context":"## Context\\nSystemD provides native tools for managing the network connections. NetworkManager is an alternative service that provides more features and can be controlled and queried with a DBus interface. Wicd is another alternative, but is rarely used in embedded applications. There are several other options, but they are not feature full compared to NetworkManager.\\n","Decision":"We will disable systemd network capability and use NetworkManager for managing the networks (duh!).\\n","tokens":69,"id":2574,"Prediction":"## Decision\n\n**Decision:** Integrate NetworkManager instead of SystemD for managing network connections in our embedded application.\n\n**Context:** As per the provided context, SystemD provides native tools for managing network connections, but NetworkManager is an alternative service that provides more features and can be controlled and queried with a DBus interface. Additionally, NetworkManager is a more feature-rich option compared to other alternatives like Wicd.\n\n**Rationale:**\n\n* NetworkManager provides more features and flexibility for managing network connections, which is essential for our embedded application.\n* The DBus interface allows for programmatic control and querying of network connections, making it easier to integrate with our application.\n* SystemD's native tools may not provide the same level of customization and control as NetworkManager.\n* Considering the trade-offs, NetworkManager is a more suitable choice for our embedded application.\n\n**Consequences:**\n\n* Our application will need to be configured to use NetworkManager instead of SystemD.\n* We will need to integrate with the DBus interface to control and query network connections.\n* We may need to handle potential compatibility issues with other services that rely on SystemD.\n\n**Alternatives Considered:**\n\n* Using SystemD's native tools, which may not provide the same level of features and customization.\n* Using Wicd, which is rarely used in embedded applications and may not provide the necessary features.\n* Implementing a custom network management solution, which would require significant development effort and resources.\n\n**Status:** This decision is approved and will be implemented in our embedded application.","GenTime":"2024-08-10 11:49:44"}
{"File Name":"slingshot\/0003-use-one-geoserver-instance-with-multiple-workspaces.md","Context":"## Context\\nThere are a few problems that need to be solved around handling authentication with GeoServer:\\n1. We have some layers which can only be accessed by authenticated users and some that are publicly accessible.\\n2. The user never interacts directly with GeoServer. Requests to GeoServer for data come from the user's browser either through `img` tags or through AJAX requests from Leaflet. This means the user needs to be preauthenticated with GeoServer in some way.\\n3. GeoServer's authentication is incredibly difficult to work with.\\nIn the past, we solved these problems by leveraging the SSO capabilities of Shibboleth and simply running two GeoServer instances--one that was publicly accessible and one that was behind Shibboleth. Touchstone authentication happened for the user through the Rails application. Given the decision to move away from Shibboleth this is no longer an option. Running two GeoServer instances also adds additional burdens to deployment, management and the data publication process.\\n","Decision":"Run a single GeoServer instance. GeoServer supports what it calls workspaces, which is just a way of partitioning layers within the system. The authentication for each workspace can be configured separately. Configure one workspace to be readable by anyone and one workspace to require Basic Auth using a predefined username and password.\\nAccess to this GeoServer instance will need to be proxied. The proxy (our Geoblacklight instance) will handle SAML authentication and augment the proxy request to GeoServer with the Basic Auth login if the user has authenticated with Geoblacklight.\\n","tokens":200,"id":2578,"Prediction":"## Decision\n\nTo handle authentication with GeoServer, we will implement a custom authentication mechanism that uses a Rails application as a proxy to authenticate users with GeoServer. This approach will allow us to:\n\n1. Control access to GeoServer layers by authenticated users and publicly accessible layers.\n2. Pre-authenticate users with GeoServer through the Rails application, eliminating the need for direct user interaction with GeoServer.\n3. Simplify the authentication process, reducing the complexity and burden of working with GeoServer's authentication mechanisms.\n\n## Rationale\n\nThe decision to move away from Shibboleth and running two GeoServer instances is driven by the need for a more scalable, manageable, and maintainable solution. By implementing a custom authentication mechanism, we can:\n\n* Decouple the authentication process from the GeoServer instances, allowing for easier deployment and management.\n* Reduce the complexity of the authentication process, making it easier to troubleshoot and maintain.\n* Improve the overall user experience by eliminating the need for multiple GeoServer instances and reducing the number of requests and redirects.\n\n## Consequences\n\n* The Rails application will need to be modified to handle authentication with GeoServer, which may require additional development and testing.\n* The GeoServer instances will need to be configured to use the custom authentication mechanism, which may require additional configuration and testing.\n* The data publication process may need to be modified to accommodate the new authentication mechanism.\n\nOverall, this decision will provide a more scalable, manageable, and maintainable solution for handling authentication with GeoServer, while also improving the overall user experience.","GenTime":"2024-08-10 11:51:07"}
{"File Name":"thing\/0002-authentication-via-touchstone-saml.md","Context":"## Context\\nA mechanism for providing end-user authentication of MIT users is required for\\ncore aspects of this application.\\nMIT officially only supports Shibboleth \/ Touchstone which traditionally is enabled via an Apache httpd module.\\nThe [MIT OpenID Pilot](https:\/\/mitlibraries.github.io\/oauth.html) was determined\\nunacceptable for this project.\\nIn order to use Shibboleth we intended to containerize the application. However,\\nwhile that process ended in a successful proof-of-concept, it used SAML and not\\nmod_shib. Once we went down the path of not using the officially MIT supported\\nmod_shib solution, we felt using SAML in-app should be explored.\\n","Decision":"We will use an in-app Touchstone SP using ruby-saml as documented in our\\n[Developer Documentation](https:\/\/mitlibraries.github.io\/touchstone_saml.html).\\n","tokens":144,"id":2582,"Prediction":"## Decision\n\n**Authentication Mechanism for MIT Users**\n\n### Context\n\nAs per the requirement, a mechanism is needed to provide end-user authentication for MIT users in core aspects of the application. MIT officially supports Shibboleth\/Touchstone, which is typically enabled via an Apache httpd module. However, due to the limitations of the MIT OpenID Pilot and the need to containerize the application, we have decided to explore using SAML in-app instead of mod_shib.\n\n### Decision\n\nWe will implement SAML-based authentication in the application, utilizing the SAML protocol to authenticate MIT users. This decision is made to ensure compliance with MIT's official authentication standards while also accommodating the need for containerization.\n\n### Rationale\n\nThe use of SAML in-app allows us to decouple the authentication mechanism from the Apache httpd module, providing greater flexibility and control over the authentication process. Additionally, SAML is a widely supported standard, making it easier to integrate with other systems and services.\n\n### Consequences\n\n* The application will require SAML configuration and setup.\n* The authentication process will be handled in-app, rather than relying on mod_shib.\n* The use of SAML will provide a more flexible and scalable authentication mechanism.\n\n### Status\n\nThis decision is considered stable, and the implementation of SAML-based authentication will be carried out as part of the next development phase.","GenTime":"2024-08-10 11:52:20"}
{"File Name":"thing\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2583,"Prediction":"## Decision\n\n**Decision:** Implement a Microservices Architecture for the Project\n\n**Context:** We need to record the architectural decisions made on this project.\n\n**Problem:** The current monolithic architecture of the project is causing scalability issues and making it difficult to maintain and update individual components.\n\n**Reasoning:** A microservices architecture will allow us to break down the project into smaller, independent services that can be developed, tested, and deployed independently. This will improve scalability, flexibility, and maintainability.\n\n**Decision Criteria:**\n\n* Scalability: A microservices architecture will allow us to scale individual services independently, improving overall system scalability.\n* Flexibility: With a microservices architecture, we can develop and deploy services using different programming languages and frameworks, improving overall system flexibility.\n* Maintainability: A microservices architecture will make it easier to maintain and update individual services without affecting the entire system.\n\n**Consequences:**\n\n* Increased complexity: A microservices architecture will introduce additional complexity, requiring more planning and coordination to ensure service communication and data consistency.\n* Higher operational costs: A microservices architecture will require additional infrastructure and operational resources to manage and maintain individual services.\n* Improved scalability and flexibility: A microservices architecture will provide improved scalability and flexibility, allowing us to respond more quickly to changing business requirements.\n\n**Status:** Accepted\n\n**Implementors:** [List of team members or individuals responsible for implementing the decision]\n\n**References:** [List of relevant documentation or resources]\n\n**Revisions:** [List of revisions to the decision, including dates and changes]\n\nThis decision record provides a clear and concise summary of the decision to implement a microservices architecture for the project. It outlines the problem, reasoning, decision criteria, consequences, and implementors, and provides a reference to relevant documentation or resources.","GenTime":"2024-08-10 11:52:39"}
{"File Name":"Corvus.Extensions.Newtonsoft.Json\/0001-json-framework-independence.md","Context":"## Context\\nWe frequently need to deal with information that will be serialized as JSON, either because it needs to be used in a web API, or because it is serialized in some JSON-based storage system. For many years, Json.NET has been the de facto API for working with JSON in .NET applications. However, now that .NET Core has `System.Text.Json` built in, there are good reasons to want to move to that. This means that libraries that depend on Json.NET become problematic.\\nSo we need ways for our libraries to work with JSON data without forcing the decision of whether to use Json.NET or `System.Text.Json`.\\n","Decision":"We have moved types previously in `Corvus.Extensions.Newtonsoft.Json` into `Corvus.Json.Abstractions`, having modified them to remove any direct dependency on Json.NET.\\nCurrently, only the property bag type has been moved. (And it is now an `IPropertyBag` interface.) A great deal of what's in `Corvus.Extensions.Newtonsoft.Json` is already in the right place, because it deals directly with Json.NET-specific concerns (e.g., custom type converters).\\n","tokens":132,"id":2585,"Prediction":"## Decision\n\nTo decouple our libraries from specific JSON serialization libraries and allow for flexibility in choosing the JSON serialization library, we will implement a JSON serialization abstraction layer.\n\n## Context\n\nAs described in the context, we frequently work with JSON data in our.NET applications, and we have a choice between using Json.NET and `System.Text.Json`. However, using libraries that depend on Json.NET can be problematic when we want to switch to `System.Text.Json`. To avoid this issue, we need a way to abstract away the specific JSON serialization library and allow our libraries to work with JSON data without making a specific choice.\n\n## Problem\n\nOur libraries are tightly coupled to Json.NET, which makes it difficult to switch to `System.Text.Json` or any other JSON serialization library. This tight coupling can lead to maintenance issues and limit our flexibility in choosing the best JSON serialization library for our needs.\n\n## Decision\n\nTo solve this problem, we will implement a JSON serialization abstraction layer that allows our libraries to work with JSON data without specifying a specific JSON serialization library. This abstraction layer will provide a common interface for working with JSON data, and our libraries will be decoupled from specific JSON serialization libraries.\n\n## Consequences\n\nBy implementing a JSON serialization abstraction layer, we will:\n\n* Decouple our libraries from specific JSON serialization libraries, making it easier to switch to a different library if needed.\n* Increase flexibility in choosing the best JSON serialization library for our needs.\n* Reduce maintenance issues by avoiding tight coupling to a specific library.\n* Improve the scalability of our libraries by allowing them to work with different JSON serialization libraries.\n\n## Rationale\n\nThe JSON serialization abstraction layer will provide a common interface for working with JSON data, allowing our libraries to work with different JSON serialization libraries without making a specific choice. This will enable us to switch to a different JSON serialization library if needed, without affecting our libraries. Additionally, the abstraction layer will reduce maintenance issues and improve the scalability of our libraries.","GenTime":"2024-08-10 11:53:23"}
{"File Name":"jfluentvalidation\/0001-primitive-array-constraints.md","Context":"## Context and Problem Statement\\nMy first pass at building out array constraint was to use a generic parameter `A` with `java.lang.reflect.Array` to obtain\\nthe length of the property representing `A`.\\nI was curious what the cost of using `java.lang.reflect.Array` compared to grabbing the `length` property from a known type was.\\nAnything with a name like `reflect*` gives me nightmares about terrible performance.\\nI decided to write a JMH benchmark to determine the performance impact `java.lang.reflect.Array` to assist in determining\\nwhich implementation to use.\\n## Decision Drivers\\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\n","Decision":"1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\nI decided to choose option 1 as I prioritized performance above the overhead to maintain additional classes and having\\nduplicate logic.\\nWhile it might be premature optimization and such a small impact (1 - 2 ns) the benchmark results below still convinced me.\\nI'm sure someone can convince me that the overhead is insignificant or that I simply messed up the bencharmark at which point\\nit should be easier refactor to option 2.\\nI've included a rough [implementation of option 2](#option-2-implementation) just in case.\\n```java\\npackage jfluentvalidation.constraints.array;\\nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;\\nimport jfluentvalidation.rules.PropertyRule;\\nimport jfluentvalidation.validators.RuleContext;\\nimport jfluentvalidation.validators.ValidationContext;\\nimport org.openjdk.jmh.annotations.*;\\nimport org.openjdk.jmh.runner.Runner;\\nimport org.openjdk.jmh.runner.options.OptionsBuilder;\\nimport java.util.concurrent.TimeUnit;\\n@BenchmarkMode(Mode.AverageTime)\\n@OutputTimeUnit(TimeUnit.NANOSECONDS)\\n@State(Scope.Benchmark)\\npublic class LengthBenchmark {\\npublic static class Foo {\\nprivate boolean[] bar;\\npublic Foo(boolean[] bar) {\\nthis.bar = bar;\\n}\\n}\\nRuleContext<Foo, boolean[]> ruleContext;\\nBooleanArrayExactLengthConstraintAlternative booleanArrayExactLengthConstraintAlternative;\\nArrayExactLengthConstraint arrayExactLengthConstraint;\\nBooleanArrayExactLengthConstraint booleanArrayExactLengthConstraint;\\n@Setup\\npublic void prepare() {\\nFoo f = new Foo(new boolean[5]);\\nPropertyRule propertyRule = new PropertyRule(foo -> f.bar, \"bar\");\\nruleContext = new RuleContext<>(new ValidationContext(f), propertyRule);\\nbooleanArrayExactLengthConstraintAlternative = new BooleanArrayExactLengthConstraintAlternative(5);\\narrayExactLengthConstraint = new ArrayExactLengthConstraint(5);\\n}\\n@Benchmark\\npublic void booleanArrayExactLengthConstraintAlternative() {\\nbooleanArrayExactLengthConstraintAlternative.isValid(ruleContext);\\n}\\n@Benchmark\\npublic void arrayExactLengthConstraint() {\\narrayExactLengthConstraint.isValid(ruleContext);\\n}\\npublic static void main(String[] args) throws Exception {\\nnew Runner(new OptionsBuilder()\\n.include(LengthBenchmark.class.getSimpleName())\\n.forks(1)\\n.warmupIterations(2)\\n.measurementIterations(5)\\n.build())\\n.run();\\n}\\n}\\n```\\nRun 1\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.504 | \u00b1 0.143 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.099 | \u00b1 0.022 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 2\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.436 | \u00b1 0.049 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.041 | \u00b1 0.013 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 3\\n| Benchmark                                                     | Mode  | Cnt | Score     | Error       | Units  |\\n|---------------------------------------------------------------|-------|-----|-----------|-------------|--------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | thrpt | 25  | 0.424     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | thrpt | 25  | 0.626     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt  | 25  | 2.362     | \u00b1 0.022     | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt  | 25  | 1.599     | \u00b1 0.007     | ns\/op  |\\n| LengthBenchmark.arrayExactLengthConstraint                    | ss    | 5   | 48261.200 | \u00b1 4493.186  | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | ss    | 5   | 18690.000 | \u00b1 23530.434 | ns\/op  |\\nA ~20% performance impact on throughput and even larger on average time is a bit too much for me to ignore.\\n### Positive Consequences\\n- Gain roughly a 20% performance instead in average time and throughput.\\n### Negative Consequences\\n- We need 36 classes (8 primitive types + 1 object type and 4 constraint classes for each type) compared to 4 constraint classes\\n(ExactLength, BetweenLength, MinimumLength, MaximumLength) to implement array length constraints for primitive arrays.\\n- Duplicate logic in constraint classes across types.\\n","tokens":164,"id":2591,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**: We will use a generic parameter `A` with `java.lang.reflect.Array` to obtain the length of the property representing `A` in the array constraint, despite potential performance concerns, in order to avoid adding additional classes and duplicating logic across constraints.\n\n**Context**: The context is the implementation of an array constraint in a software system, where we need to determine the length of the property representing the array type `A`. The initial approach was to use a generic parameter `A` with `java.lang.reflect.Array` to obtain the length of the property, but this raised concerns about performance due to the use of reflection.\n\n**Decision Drivers**:\n\n1. **Performance**: We want to keep performance in mind and attempt to be as performant as possible across all constraints.\n2. **Code simplicity and maintainability**: We want to avoid adding additional classes and limit duplicating logic across constraints when possible.\n\n**Rationale**: While using `java.lang.reflect.Array` may have performance implications, it allows us to avoid adding additional classes and duplicating logic across constraints. This approach is also more concise and easier to maintain. The performance concerns can be mitigated by profiling and optimizing the code as needed.\n\n**Consequences**:\n\n* Potential performance impact due to the use of reflection\n* Simplified code and reduced duplication\n* Easier maintenance and fewer additional classes\n\n**Monitoring and Review**: We will monitor the performance of the system and review the code regularly to ensure that the use of `java.lang.reflect.Array` is not causing significant performance issues. If necessary, we will consider alternative approaches or optimizations to improve performance.","GenTime":"2024-08-10 11:54:36"}
{"File Name":"cljdoc\/0008-use-circleci-as-analysis-sandbox.md","Context":"## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https:\/\/github.com\/martinklepsch\/cljdoc\/blob\/72da65055ab94942f33fb63b29b732e81b559508\/doc\/isolation.md)\\n","Decision":"For a first version of cljdoc we will use CircleCI to run analysis for us. The result of\\nthis analysis will be made available as a build artifact which can then be laoded in\\na trusted environment to import data into Grimoire and build HTML (or other) documentation\\nfrontends.\\n","tokens":160,"id":2601,"Prediction":"## Decision\n\nTo mitigate the risks associated with analyzing untrusted Clojure code, we will utilize a cloud-based continuous integration service to run and monitor job execution in a sandboxed environment. Specifically, we will leverage GitHub Actions, which provides a secure and scalable way to automate the build, test, and deployment of our code.\n\n## Context\n\nAs described in the provided notes on Isolation, running untrusted Clojure code requires a sandboxed environment to prevent malicious code from executing and causing harm. Docker provides a solution for this, but it still requires us to run and monitor job execution, which can be vulnerable to compute-stealing attacks. AWS Lambda is another option, but it may not provide sufficient isolation. Additionally, \"hacking\" a continuous integration service to do the job for us is not a feasible or secure solution.\n\n## Solution\n\nGitHub Actions provides a secure and scalable way to automate the build, test, and deployment of our code. By using GitHub Actions, we can create a custom workflow that runs our code in a sandboxed environment, without the need to run and monitor job execution ourselves. This approach provides several benefits, including:\n\n* Improved security: GitHub Actions provides a secure environment for running our code, with features such as code review and approval, and automated testing and validation.\n* Scalability: GitHub Actions can handle large volumes of code and job execution, without requiring us to manage and maintain our own infrastructure.\n* Flexibility: GitHub Actions provides a wide range of features and integrations, allowing us to customize our workflow to meet our specific needs.\n\n## Rationale\n\nBy using GitHub Actions, we can ensure that our code is run in a secure and sandboxed environment, without the need to manage and maintain our own infrastructure. This approach provides a scalable and flexible solution for analyzing untrusted Clojure code, and minimizes the risk of compute-stealing attacks.\n\n## Consequences\n\n* Improved security: By using GitHub Actions, we can ensure that our code is run in a secure and sandboxed environment, reducing the risk of compute-stealing attacks and other security threats.\n* Increased scalability: GitHub Actions can handle large volumes of code and job execution, allowing us to scale our workflow as needed.\n* Reduced maintenance: By using GitHub Actions, we can reduce the need for manual intervention and maintenance, freeing up resources for more important tasks.\n\nOverall, using GitHub Actions to run and monitor job execution in a sandboxed environment provides a secure, scalable, and flexible solution for analyzing untrusted Clojure code.","GenTime":"2024-08-10 11:58:06"}
{"File Name":"cljdoc\/0014-add-support-for-examples.md","Context":"## Context\\nExamples are an often suggested feature for cljdoc that could enable library authors and the community to further guide library users.\\n","Decision":"Add support for examples to cljdoc. Allow libary users to provide examples through their Git repository but also run a community-maintained examples repository where examples can be maintained outside of a project's own Git repository.\\n","tokens":28,"id":2616,"Prediction":"## Decision\n\n**Title:** Implementing Examples in Cljdoc\n\n**Context:** Examples are an often suggested feature for cljdoc that could enable library authors and the community to further guide library users.\n\n**Problem:** Cljdoc, a documentation generator for Clojure libraries, currently lacks a feature to include examples in the generated documentation. This makes it difficult for library authors to provide clear and concise usage examples, which can lead to confusion and frustration for users.\n\n**Decision:** We will implement examples in cljdoc, allowing library authors to include example code snippets in their documentation. This will enable users to better understand how to use the library and its functions.\n\n**Rationale:**\n\n* Examples are a crucial aspect of effective documentation, as they provide concrete examples of how to use a library or function.\n* Including examples in cljdoc will make it easier for library authors to provide clear and concise usage examples, which can improve the overall user experience.\n* Examples can also help to reduce the number of support requests and issues reported by users, as they will have a better understanding of how to use the library.\n\n**Consequences:**\n\n* We will need to modify the cljdoc parser to support example code snippets in the documentation.\n* We will need to provide a way for library authors to specify which code snippets are examples and which are not.\n* We will need to ensure that the examples are properly formatted and displayed in the generated documentation.\n\n**Status:** Accepted\n\n**Implementation:** The implementation of examples in cljdoc will be done in the following steps:\n\n1. Modify the cljdoc parser to support example code snippets.\n2. Add a new syntax for specifying examples in the documentation.\n3. Implement the logic for formatting and displaying examples in the generated documentation.\n4. Test the new feature to ensure it works correctly.\n\n**Review:** This decision will be reviewed in 3 months to assess its impact and effectiveness.","GenTime":"2024-08-10 12:03:39"}
{"File Name":"containers\/2-.md","Context":"## Context\\n======\\nFedora has minimal base image too, and it will typically have more current versions of any software we need compared to the Ubuntu images.\\n","Decision":"======\\nChange all Dockerfiles to use the latest `fedora-minimal` image as the base and update any necessary commands to work with Fedora system tools like `dnf`.\\n","tokens":34,"id":2619,"Prediction":"## Decision\n\n**Use Fedora as the base image for our application**\n\n## Context\n\nFedora has minimal base image too, and it will typically have more current versions of any software we need compared to the Ubuntu images.\n\n## Problem\n\nWe need to choose a base image for our application that provides a stable and reliable foundation for our software. We have considered using Ubuntu images, but they may not have the latest versions of the software we need.\n\n## Decision\n\nAfter careful consideration, we have decided to use Fedora as the base image for our application. Fedora's minimal base image and frequent updates to its software packages make it a more suitable choice for our needs.\n\n## Rationale\n\n* Fedora's minimal base image reduces the attack surface and minimizes the risk of introducing unnecessary dependencies or vulnerabilities.\n* Fedora's frequent updates ensure that we have access to the latest versions of the software we need, which is critical for our application's functionality and security.\n* Fedora's community-driven development process and open-source nature allow us to contribute to and customize the base image to meet our specific needs.\n\n## Consequences\n\n* We will need to ensure that our application is compatible with Fedora's package management system and dependencies.\n* We will need to test our application thoroughly to ensure that it functions correctly on the Fedora base image.\n* We may need to invest time and resources in customizing the Fedora base image to meet our specific requirements.\n\n## Status\n\nThis decision is approved and will be implemented in the next release of our application.","GenTime":"2024-08-10 12:04:36"}
{"File Name":"ftd-scratch3-offline\/0009-use-jackson-to-read-json-files.md","Context":"## Context\\nScratch save files are zip files that contain the actual code in a `project.json` file.\\nTo read this file we need a deserialization library.\\nJackson, GSON and org.json are common libraries.\\nJackson seems to offer the most features and the authors already have used it.\\nGSON offers many features Jackson also features.\\norg.json seems to be only a json parser and the use has to create the resulting object themselves, while the other libraries also feature object mapping.\\n","Decision":"We will use [Jackson](https:\/\/github.com\/FasterXML\/jackson-databind\/).\\n","tokens":104,"id":2625,"Prediction":"## Decision\n\nWe will use Jackson as the deserialization library to read the `project.json` file in scratch save files.\n\n## Context\n\nAs described, scratch save files are zip files that contain the actual code in a `project.json` file. To read this file, we need a deserialization library. Jackson, GSON, and org.json are common libraries that can be used for this purpose.\n\n## Problem\n\nWe need to choose a suitable deserialization library that can efficiently and accurately read the `project.json` file in scratch save files.\n\n## Reasoning\n\nAfter evaluating the features and capabilities of each library, we decided to use Jackson as the deserialization library. Jackson offers the most features among the three options, including advanced serialization and deserialization capabilities, support for complex data structures, and a large community of users and contributors. Additionally, the authors of the project have already used Jackson in the past, which makes it a more familiar and comfortable choice.\n\nGSON is another popular option that offers many features similar to Jackson. However, since Jackson is already being used in the project, it makes sense to stick with a library that we are already familiar with and have experience using.\n\norg.json is a simple JSON parser that does not offer object mapping capabilities, which would require us to manually create the resulting objects, making it less efficient and more error-prone.\n\n## Consequences\n\nBy using Jackson as the deserialization library, we can efficiently and accurately read the `project.json` file in scratch save files. This will enable us to quickly and easily deserialize the JSON data into a usable format.","GenTime":"2024-08-10 12:06:31"}
{"File Name":"ftd-scratch3-offline\/0004-use-multiple-projects.md","Context":"## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n","Decision":"We will try to modularize the software and will use multiple projects to achieve this goal.\\n","tokens":67,"id":2626,"Prediction":"## Decision\n\n**Decision:** Split the software into multiple Gradle projects, each with its own defined API boundaries.\n\n**Context:** As discussed, the software could be developed in one big Gradle project, making integration easier. However, this would also make it harder to reuse the code outside of this project, and could lead to worse code quality due to the lack of defined API boundaries.\n\n**Problem:** The software's complexity and scalability requirements make it necessary to have separate projects for different components, allowing for better maintainability and reusability.\n\n**Reasoning:** By splitting the software into multiple Gradle projects, we can achieve the following benefits:\n\n* Improved maintainability: With separate projects, each component can be maintained and updated independently, reducing the risk of conflicts and making it easier to identify and fix issues.\n* Better code quality: Defined API boundaries will encourage developers to write more modular, reusable, and testable code, which will lead to better overall code quality.\n* Easier reusability: With separate projects, it will be easier to reuse components in other projects or applications, reducing duplication of effort and increasing the overall value of the software.\n\n**Consequences:**\n\n* Integration will require more effort, as we will need to handle communication between projects.\n* Code reuse will be easier, as each project will have its own defined API boundaries.\n\n**Alternatives Considered:** We considered developing the software in one big Gradle project, but ultimately decided against it due to the potential drawbacks mentioned earlier.\n\n**Status:** This decision is approved and will be implemented as part of the software's development process.","GenTime":"2024-08-10 12:06:49"}
{"File Name":"ftd-scratch3-offline\/0011-implement-complex-scratch-functions-in-a-helper-function.md","Context":"## Context\\nSome scratch\/ftduino functions take complex arguments that need to be verified.\\nSome scratch\/ftduino functions are complex to implement.\\nThe code for these functions could be directly generated in the java files for the specific function.\\nThe code for these functions could also be written inside a helper function, such that only a single line - that calls the helper function - is generated for such a function.\\nWriting these functions in a helper function also makes changes and code reuse easier.\\n","Decision":"Complex scratch\/ftduino functions will be implemented in helper functions and those will be called by the generated code.\\n","tokens":99,"id":2629,"Prediction":"## Decision\n\n**Decision:** Implement complex scratch\/ftduino functions as helper functions in Java, allowing for code reuse, easier changes, and improved maintainability.\n\n## Context\n\nAs described in the context, some scratch\/ftduino functions take complex arguments that need to be verified, and some are complex to implement. Additionally, the code for these functions could be directly generated in the Java files for the specific function or written inside a helper function.\n\n## Problem\n\nThe current implementation of scratch\/ftduino functions is complex and difficult to maintain. The functions take complex arguments that need to be verified, and the code is scattered throughout the Java files. This makes it challenging to make changes and reuse code.\n\n## Solution\n\nTo address the problem, we will implement the complex scratch\/ftduino functions as helper functions in Java. This will allow us to:\n\n* Simplify the code by breaking down complex functions into smaller, more manageable pieces\n* Improve code reuse by allowing us to call the helper functions from multiple locations\n* Make changes easier by having a single point of maintenance for the complex logic\n* Improve maintainability by having a clear and concise code structure\n\n## Rationale\n\nImplementing the complex scratch\/ftduino functions as helper functions will improve the overall maintainability and scalability of the codebase. By breaking down complex functions into smaller pieces, we can make the code easier to understand and modify. Additionally, having a single point of maintenance for the complex logic will reduce the risk of introducing bugs or inconsistencies.\n\n## Consequences\n\n* Improved maintainability and scalability of the codebase\n* Easier changes and code reuse\n* Reduced risk of introducing bugs or inconsistencies\n* Potential for improved performance due to reduced complexity\n\n## Decision Drivers\n\n* Improve maintainability and scalability of the codebase\n* Simplify complex functions and improve code reuse\n* Reduce risk of introducing bugs or inconsistencies\n\n## Alternatives Considered\n\n* Implementing complex functions directly in the Java files for the specific function\n* Using a code generation tool to generate the code for the complex functions\n\nWe considered implementing the complex functions directly in the Java files for the specific function, but this would lead to complex and difficult-to-maintain code. We also considered using a code generation tool to generate the code for the complex functions, but this would require additional infrastructure and may not provide the same level of customization and control as implementing the functions as helper functions.","GenTime":"2024-08-10 12:07:59"}
{"File Name":"ftd-scratch3-offline\/0015-use-floats-for-storing-numbers-not-doubles-as-scratch-does.md","Context":"## Context\\nScratch uses Javascript's numbers type to store all numbers it uses. The Javascript number type is effectively a 64 bit double.\\nThe ftduino only supports 32-bit floats.\\n64-bit double emulation might be possible but slow and likely exhaust the available memory and program space.\\n","Decision":"We will use 32-bit floats to store any numbers.\\n","tokens":60,"id":2631,"Prediction":"## Decision\n\nGiven the context that Scratch uses JavaScript's number type to store all numbers, which is effectively a 64-bit double, and the ftduino only supports 32-bit floats, we will implement a custom number type in Scratch that is compatible with the ftduino's 32-bit float capabilities.\n\n## Rationale\n\nSince the ftduino only supports 32-bit floats, using the JavaScript number type directly would result in a loss of precision and potentially incorrect calculations. Additionally, emulating 64-bit doubles on the ftduino would likely be slow and could exhaust the available memory and program space.\n\nBy implementing a custom number type in Scratch that is specifically designed for the ftduino's 32-bit float capabilities, we can ensure that calculations are accurate and efficient. This custom number type can be designed to handle the specific requirements of the ftduino, such as limited precision and range.\n\n## Consequences\n\n* The custom number type will require additional development and testing to ensure it meets the requirements of the ftduino and Scratch.\n* The custom number type may have limitations compared to the JavaScript number type, such as reduced precision or range.\n* The implementation of the custom number type may require changes to existing code and algorithms in Scratch to accommodate the new number type.\n\nOverall, implementing a custom number type in Scratch that is compatible with the ftduino's 32-bit float capabilities is the best solution to ensure accurate and efficient calculations on the ftduino.","GenTime":"2024-08-10 12:08:33"}
{"File Name":"ftd-scratch3-offline\/0013-communicate-with-the-scratch-app-via-a-local-web-server.md","Context":"## Context\\nThe scratch app has to invoke the arduino-cli program to compile the converted programs.\\nThe scratch app has to invoke the scratch-to-c program to convert the scratch blocks to a runnable arduino program.\\nIn both cases the scratch app would have to invoke a native program.\\nThis is not possible in current web browsers.\\nThis would be possible using e.g. Electron, but the authors don't know Electron and Electron would add around 100 MB size overhead and also mean that without Electron the app can't be used.\\nUsing a local web-server all that is needed is any browser, the scratch ftduino app and the local web-server.\\n","Decision":"We will use a local web-server that will be used by the scratch app to invoke certain native programs.\\nThe local web-server may also be used to host the scratch app itself for example when there is no Internet connectivity available.\\n","tokens":135,"id":2634,"Prediction":"Here is the Architectural Decision Record:\n\n## Decision\n\nTo use a local web-server to run the scratch app, which will allow the app to invoke native programs such as `arduino-cli` and `scratch-to-c` without relying on Electron.\n\n## Context\n\nThe scratch app needs to invoke native programs to compile and convert Scratch blocks to a runnable Arduino program. However, this is not possible in current web browsers due to security restrictions. Using Electron would allow us to bypass these restrictions, but it would add a significant size overhead and limit the app's portability.\n\n## Problem\n\nThe scratch app needs to be able to invoke native programs, but current web browsers do not allow this.\n\n## Decision Drivers\n\n* The need to compile and convert Scratch blocks to a runnable Arduino program requires the ability to invoke native programs.\n* Electron would add a significant size overhead to the app, making it less portable and more difficult to maintain.\n* Using a local web-server allows the app to be run on any browser, without the need for Electron.\n\n## Decision Outcome\n\nWe will use a local web-server to run the scratch app, which will allow us to invoke native programs such as `arduino-cli` and `scratch-to-c` without relying on Electron.\n\n## Rationale\n\nUsing a local web-server allows us to bypass the security restrictions of web browsers, while still providing a lightweight and portable solution. This approach also allows us to keep the app's size overhead to a minimum, making it easier to maintain and distribute.\n\n## Pros and Cons\n\nPros:\n\n* Allows the app to invoke native programs without relying on Electron.\n* Keeps the app's size overhead to a minimum.\n* Provides a lightweight and portable solution.\n\nCons:\n\n* Requires the user to have a local web-server installed and running.\n* May require additional configuration to set up the web-server.\n\n## Adoption Strategy\n\nWe will implement the local web-server solution and test it thoroughly to ensure it works as expected. We will also provide clear instructions for users on how to set up and use the local web-server.","GenTime":"2024-08-10 12:09:27"}
{"File Name":"ftd-scratch3-offline\/0017-don-t-support-scratch-images-sprites-and-image-sprite-related-blocks.md","Context":"## Context\\nScratch supports images\/sprites.\\nThe ftduino has no way to show images\/sprites.\\n","Decision":"Scratch images\/sprites and image\/sprite related blocks are not supported.\\n","tokens":23,"id":2637,"Prediction":"## Decision\n\n**Decision:** To implement a text-based representation of images\/sprites in ftduino, using ASCII art or a similar approach, to provide a similar user experience to Scratch.\n\n**Context:** Scratch supports images\/sprites, which are an essential part of its visual programming language. However, ftduino, being a text-based platform, does not have a direct way to display images\/sprites.\n\n**Problem:** The lack of image\/sprite support in ftduino may lead to a disjointed user experience, making it difficult for users to transition from Scratch to ftduino.\n\n**Rationale:** Implementing a text-based representation of images\/sprites in ftduino will allow users to create a similar visual programming experience to Scratch, making it easier for them to adapt to the new platform. ASCII art or a similar approach can be used to represent images\/sprites, providing a clear and concise way to visualize them.\n\n**Consequences:**\n\n* Users will be able to create a similar visual programming experience to Scratch, making it easier to transition to ftduino.\n* The implementation will require additional development effort to create the text-based representation of images\/sprites.\n* The representation may not be as visually appealing as the original images\/sprites, but it will provide a functional equivalent.\n\n**Alternatives Considered:**\n\n* Not implementing image\/sprite support at all, which would result in a disjointed user experience.\n* Implementing a limited image\/sprite support, which would not provide a similar user experience to Scratch.\n\n**Status:** Accepted\n\n**Implementor:** [Name of the implementor]\n\n**Date:** [Date of the decision]\n\n**References:** [Any relevant references or links]","GenTime":"2024-08-10 12:10:17"}
{"File Name":"dotcom-rendering\/009-static-types.md","Context":"## Context\\nAdding static types to a JavaScript project can be incredibly useful to reduce the likelihood of introducing runtime type errors.\\nWe have considered two methods of adding static types to dotcom-rendering.\\n### Flow\\nFlow makes it easy to retrofit a useful level of implicit type checking to a legacy project.\\nHowever, it produces error messages that are difficult to understand.\\nFlow requires developers to check in types for 3rd party libraries. These can be automatically generated based on the `package.json`, but developers must remember to keep these up to date.\\nIts configuration is often too coarse-grained, meaning code gets littered with opt-outs and `any` types. It doesn't feel like a natural type system, as it is only annotations on top of a dynamic language.\\nFlow's types are not exact by default.\\nThe ecosystem is still immature.\\n### TypeScript\\nTypeScript produces more understandable error messages.\\nIt allows developers to install versioned types for third party libraries.\\nTypeScript is a language in its own right, and feels more natural. It can improve JavaScript language features to make them more type-friendly. Its configuration is more fine-grained.\\nTypes are exact by default.\\nThe ecosystem is more mature.\\n","Decision":"Use TypeScript for static typing.\\n","tokens":252,"id":2652,"Prediction":"## Decision\n\nWe will use TypeScript instead of Flow to add static types to our JavaScript project, dotcom-rendering.\n\n## Context\n\nAs discussed, we considered two methods of adding static types to our project: Flow and TypeScript. Both have their pros and cons, but after careful evaluation, we have decided to go with TypeScript.\n\n## Rationale\n\nThe main reasons for choosing TypeScript over Flow are:\n\n* More understandable error messages: TypeScript produces more informative and actionable error messages, which will help our developers to quickly identify and fix type-related issues.\n* Better support for third-party libraries: TypeScript allows us to install versioned types for third-party libraries, which will reduce the maintenance burden and ensure that our types stay up-to-date.\n* More natural type system: TypeScript is a language in its own right, and its type system feels more natural and integrated with the JavaScript language. This will make it easier for our developers to use and maintain.\n* More mature ecosystem: The TypeScript ecosystem is more mature and widely adopted, which means that there are more resources available, and it will be easier to find and integrate with other tools and libraries.\n\n## Consequences\n\nBy choosing TypeScript, we will need to invest time in learning and adopting the new technology. However, we believe that the benefits will outweigh the costs, and that TypeScript will help us to improve the quality and maintainability of our codebase. We will also need to update our build process and tooling to support TypeScript, but this will be a one-time effort.\n\nOverall, we believe that TypeScript is the better choice for our project, and we are excited to start using it to improve the quality and reliability of our code.","GenTime":"2024-08-10 12:15:12"}
{"File Name":"dotcom-rendering\/004-emotion.md","Context":"## Context\\nUsing a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.\\nStyletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.\\nStyled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).\\n","Decision":"We will use Emotion as our CSS-in-JS library.\\n","tokens":138,"id":2655,"Prediction":"## Decision\n\nAfter considering the pros and cons of each CSS-in-JS library, we decide to use Emotion as our CSS-in-JS library for this project. The main reasons for this decision are:\n\n* Emotion supports more complex features such as nesting and sibling selectors, which are not supported by Styletron.\n* Emotion has a similar API to Styled Components, making it easier for developers to adapt to the library.\n* Emotion is smaller in size compared to Styled Components, which is an important consideration for our project's performance and bundle size.\n* Emotion supports adding styles to the `css` prop on React components, which provides more flexibility in terms of styling our components.\n\n## Rationale\n\nWe considered using Styletron for its ability to generate atomic CSS, which could lead to smaller CSS output. However, its limitations in terms of supported language features make it less suitable for our project's requirements. On the other hand, Emotion's support for complex features, similar API to Styled Components, and smaller size make it a more attractive option.\n\n## Consequences\n\n* We will need to adapt our styling code to use Emotion's API and syntax.\n* We will need to ensure that our styling code does not rely on features that are not supported by Emotion.\n* We will need to monitor the performance and bundle size of our application to ensure that Emotion's smaller size does not negatively impact our application's performance.","GenTime":"2024-08-10 12:16:12"}
{"File Name":"dotcom-rendering\/018-react-context-api.md","Context":"# React Context API\\n## Context\\nWe don't use any state management in dotcom at the moment and this means props have to be\\nexplicitly passed around; this can lead to 'prop drilling'.\\n[This PR](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801) was a spike to demonstrate using\\nthe react context api to extract the `edition` property to prevent this.\\n","Decision":"-   Our component tree is shallow so we shouldn't implement any 'magic' that\\nsteps away from having explicit props showing where and what data a component is using.\\n-   This decision should be revisited as the codebase grows.\\n","tokens":88,"id":2667,"Prediction":"Here is an Architectural Decision Record (ADR) based on the provided context:\n\n**Decision**: Use the React Context API for managing state and props in the dotcom application.\n\n**Context**: The current implementation of the dotcom application does not utilize a state management system, leading to prop drilling issues. The React Context API was explored as a potential solution to alleviate this problem.\n\n**Problem**: Prop drilling is a common issue in React applications where props need to be passed down multiple levels of components, making the code harder to maintain and understand. In the dotcom application, the lack of a state management system exacerbates this problem.\n\n**Decision Rationale**: The React Context API was chosen because it provides a simple and efficient way to manage state and props in a React application. By using the Context API, we can avoid prop drilling and make the code more scalable and maintainable.\n\n**Consequences**: Implementing the React Context API will require changes to the existing component hierarchy and the way props are passed between components. This may require additional testing and debugging to ensure that the application functions correctly.\n\n**Pros**: The React Context API will allow us to:\n\n* Avoid prop drilling and make the code more maintainable\n* Simplify the component hierarchy and reduce the complexity of prop passing\n* Improve the scalability of the application by reducing the amount of prop data that needs to be passed\n\n**Cons**: The React Context API may introduce additional complexity to the application, requiring additional testing and debugging to ensure that it functions correctly.\n\n**Status**: This decision has been approved and will be implemented in the dotcom application.\n\n**References**:\n\n* [React Context API documentation](https:\/\/reactjs.org\/docs\/context.html)\n* [The PR that demonstrated using the React Context API in the dotcom application](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801)","GenTime":"2024-08-10 12:19:39"}
{"File Name":"archcolider\/008 At least once delivery for ready to pay order.md","Context":"## Context\\n\"Ready to pay\" orders should be handled with special care. We'd like to guarantee \"at least once delivery\" for each order. Order payment processing is a business-critical scenario because selling meals is the whole point. In this case, there should be a guarantee, that the order store and payment processor can pick the order for execution.\\nAt the same time, it's very important to avoid double payments, because of concurrency issues. When an order with a \"Ready to pay\" state arrives, in theory, it can be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.\\n","Decision":"Delivery of \"ready to pay\" orders performed by a MessageQueue software with a message acknowledgment option. Additionally, we expect that the order comes with a unique id from client devices at the time of processing. During order processing, the existence of the order with the same id can be checked and the version number should be used for staleness validation. In this case, the event with the same version will be discarded by the processing service.\\n","tokens":139,"id":2672,"Prediction":"## Decision\n\nTo ensure \"at least once delivery\" for each \"Ready to pay\" order and prevent double payments, we will implement a combination of idempotent payment processing and a distributed lock mechanism.\n\n## Context\n\nAs mentioned in the context, \"Ready to pay\" orders require special care due to the business-critical nature of order payment processing. We need to guarantee that each order is processed at least once and avoid double payments due to concurrency issues.\n\n## Problem\n\nThe main problem is ensuring that each order is processed only once, even in the presence of concurrency issues. If an order is processed multiple times, it can lead to incorrect charges and financial losses.\n\n## Decision\n\nTo solve this problem, we will implement the following solution:\n\n1.  **Idempotent Payment Processing:** We will design our payment processing system to be idempotent, meaning that it can be safely retried multiple times without causing unintended side effects. This will ensure that even if an order is processed multiple times, it will not lead to double payments.\n2.  **Distributed Lock Mechanism:** We will use a distributed lock mechanism to ensure that only one order can be processed at a time. This will prevent multiple orders from being processed concurrently, reducing the risk of double payments.\n\n## Rationale\n\nThe idempotent payment processing approach ensures that even if an order is processed multiple times, it will not lead to double payments. The distributed lock mechanism ensures that only one order can be processed at a time, reducing the risk of concurrency issues.\n\n## Consequences\n\nThis solution ensures that \"at least once delivery\" is guaranteed for each \"Ready to pay\" order, and double payments are avoided. However, it also introduces some additional complexity and requires careful implementation to ensure that the distributed lock mechanism is properly configured and the idempotent payment processing is correctly implemented.\n\n## Alternatives Considered\n\nWe considered the following alternatives:\n\n*   **Using a message broker with a guaranteed delivery:** This approach would ensure that each order is processed at least once, but it would not prevent double payments.\n*   **Implementing a custom locking mechanism:** This approach would require a significant amount of custom code and would be more prone to errors.\n*   **Not handling concurrency issues:** This approach would lead to double payments and financial losses.\n\nAll of these alternatives have significant drawbacks, and the chosen solution provides the best balance between ensuring \"at least once delivery\" and preventing double payments.","GenTime":"2024-08-10 12:21:40"}
{"File Name":"archcolider\/011 Every meal delivery has pick up pin code.md","Context":"## Context\\nNetwork connection might be lost, when a meal is delivered to a fridge and a user comes to grab it. Then the fridge can't check data of the user online by card swapping, or in-app distance opening.\\nBut the fridge has a pin pad keyboard and still has quite sophisticated software and internal memory to process orders.\\nUpdate 2020-11-24:\\nWe expect that every meal has its own unique id provided by the kitchen because some meals might be customized from the general catalog. Let's say lactose-free lasagna should be addressed to a specific user.\\nThen, at the purchase or production process, we can update the user's device with the meal's unique id and generate an access code based on meal ID.\\n","Decision":"Meals dispatched from a Ghost Kitchen, will have a special 6-8 digit code.\\n","tokens":154,"id":2678,"Prediction":"## Decision\n\n**Decision:** Implement a local offline mode for the fridge's software to process meal orders and access controls, utilizing the pin pad keyboard and internal memory, in case of a network connection loss.\n\n**Context:** As described in the context, network connection loss may occur when a meal is delivered to a fridge and a user comes to grab it, preventing the fridge from checking data online. The fridge has a pin pad keyboard and sophisticated software, making it capable of processing orders and storing data locally.\n\n**Rationale:**\n\n1. Ensuring seamless user experience: By implementing a local offline mode, users can still access and retrieve their meals even in the event of a network connection loss.\n2. Utilizing existing capabilities: The fridge's pin pad keyboard and internal memory can be leveraged to process orders and store data locally, minimizing the need for additional infrastructure.\n3. Enhanced security: The unique meal ID and access code generation can be performed locally, ensuring that sensitive data is not transmitted over the network, reducing the risk of data breaches.\n\n**Consequences:**\n\n1. Increased complexity: Implementing a local offline mode may require additional development and testing to ensure seamless integration with the existing software.\n2. Data consistency: To maintain data consistency, the local offline mode will need to synchronize data with the online system once the network connection is restored.\n\n**Alternatives Considered:**\n\n1. Implementing a network-based solution: This would require the fridge to always maintain a network connection, which may not be feasible in all scenarios.\n2. Using a third-party service: This would introduce additional dependencies and may not provide the desired level of control and customization.\n\n**Status:** Implemented\n\n**Owners:** [Name], [Name]\n\n**History:**\n\n* 2020-11-24: Initial decision made\n* 2020-12-10: Implementation started\n* 2021-01-15: Local offline mode implemented and tested","GenTime":"2024-08-10 12:23:30"}
{"File Name":"archcolider\/010 Feedback System separation.md","Context":"## Context\\nA user would like to read or write a review for a meal that he has ordered. Businesses would like to obtain feedback on its service in order to improve it.\\nTwo different feedback methods:\\n- surveys (occasional questionnaires about general aspects of the app\/service)\\n- feedback (is an opinion about an order or app\/service)\\n","Decision":"We'll create a simple in app feedback system that will allow users to provide feedback about orders and service.\\nWe'll incorporate surveys into feedback in order to take advantage of 3rd parties services which specialize in feedback acquisitions such survey monkey, google surveys, etc..\\n","tokens":74,"id":2681,"Prediction":"## Decision\n\n**Decision:** Implement a separate entity for reviews and feedback, with a many-to-one relationship between orders and reviews\/feedback.\n\n**Context:** A user would like to read or write a review for a meal that he has ordered. Businesses would like to obtain feedback on its service in order to improve it. There are two different feedback methods: surveys (occasional questionnaires about general aspects of the app\/service) and feedback (an opinion about an order or app\/service).\n\n**Rationale:**\n\n* Separating reviews and feedback into separate entities allows for more flexibility in terms of storage and querying. For example, we can store reviews and feedback in separate tables, which makes it easier to query and analyze them separately.\n* A many-to-one relationship between orders and reviews\/feedback allows for a clear association between an order and the reviews\/feedback related to it. This makes it easier to retrieve and display relevant feedback for a specific order.\n* Implementing separate entities for reviews and feedback also allows for more granular control over the data, for example, we can have different fields for reviews and feedback, or different validation rules for each type of feedback.\n* This decision also allows for the possibility of having different types of feedback, for example, we can have a separate entity for \"likes\" or \"dislikes\" for a specific order.\n\n**Consequences:**\n\n* This decision may require additional database tables and relationships, which can increase the complexity of the database schema.\n* It may also require additional code to handle the relationships between orders, reviews, and feedback.\n* However, this decision provides more flexibility and scalability in terms of storing and querying feedback data.\n\n**Alternatives:**\n\n* Implement a single entity for all feedback, with a single table and a single set of fields.\n* Implement a separate entity for each type of feedback (e.g. reviews, feedback, surveys), but with a one-to-one relationship between orders and feedback.\n\n**Status:** Accepted\n\n**Implementer:** [Name]\n\n**Date:** [Date]\n\n**References:**","GenTime":"2024-08-10 12:24:41"}
{"File Name":"functionaut\/0002-use-google-closure-compiler.md","Context":"## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n","Decision":"We acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n","tokens":38,"id":2687,"Prediction":"## Decision\n\nTo achieve the required functionality, we will implement a modern JavaScript transpiler that can convert the latest JavaScript syntax and features into compatible code for older execution environments. This transpiler will also include a linter to automatically check the code against common development mistakes.\n\n## Context\n\nAs per the requirement, we need to allow developers to use new JavaScript syntax and features without excluding older execution environments. This means we need a solution that can take the latest JavaScript code and convert it into a compatible format for older browsers or environments that do not support the latest features.\n\nAdditionally, we need to ensure that the code is optimized for download and execution. This includes minification, compression, and other optimization techniques to reduce the size of the code and improve its performance.\n\n## Decision Rationale\n\nWe have considered several options, including:\n\n* Using a polyfill library to add support for new JavaScript features in older environments. However, this approach would require significant maintenance and updates to keep the polyfills compatible with the latest JavaScript features.\n* Using a JavaScript compiler to compile the code into a lower-level language, such as C++ or machine code. However, this approach would require significant expertise and resources, and may not be feasible for our use case.\n* Implementing a custom solution using a combination of JavaScript and native code. However, this approach would be complex and may not be scalable or maintainable.\n\nAfter careful consideration, we have decided to implement a modern JavaScript transpiler that can convert the latest JavaScript syntax and features into compatible code for older execution environments. This approach will allow us to take advantage of the latest JavaScript features while still supporting older environments.\n\n## Status\n\nThis decision is currently in the \"proposed\" status and will be reviewed and refined as necessary before implementation.\n\n## Consequences\n\nThis decision has the following consequences:\n\n* We will need to invest time and resources in implementing and maintaining the transpiler.\n* We will need to ensure that the transpiler is compatible with a wide range of older execution environments.\n* We will need to balance the need for compatibility with the need for performance and optimization.\n\nOverall, this decision will allow us to support the latest JavaScript features while still maintaining compatibility with older environments. It will also enable us to automatically check the code against common development mistakes and optimize it for download and execution.","GenTime":"2024-08-10 12:26:30"}
{"File Name":"functionaut\/0003-test-distributed-files-only.md","Context":"## Context\\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\\n","Decision":"Testing will be made against the production bundle to catch compilation errors before they reach our users.\\n","tokens":67,"id":2689,"Prediction":"## Decision\n\nTo ensure that the code distributed to the public works as intended and avoid false positives in testing, we will use the `--compilation_level=SIMPLE_OPTIMIZATIONS` option for the Google Closure Compiler in our production build process. This will help to minimize the likelihood of successful tests against development sources failing against the production bundle.\n\n## Context\n\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\n\n## Rationale\n\nUsing the `--compilation_level=SIMPLE_OPTIMIZATIONS` option will help to preserve the original code structure and avoid the radical transformations that occur with the advanced compilation mode. This will reduce the likelihood of errors that may not be caught during testing and ensure that the code works as intended in production.\n\n## Consequences\n\n* The production build process will be slightly slower due to the simpler compilation mode.\n* The code may not be as optimized for production, but this is a trade-off we are willing to make for the increased reliability of our tests.\n\n## Alternatives Considered\n\n* We considered using the advanced compilation mode for production, but this would increase the risk of errors that may not be caught during testing.\n* We considered using a different compiler or build process, but this would require significant changes to our existing infrastructure and may introduce new complexity.\n\n## Status\n\nApproved.","GenTime":"2024-08-10 12:27:08"}
{"File Name":"operational-data-hub\/0017-ddd-defines-ubiquitous-language.md","Context":"## Context\\nDomain-driven design (DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the business domain. For example, if a software processes loan applications, it might have classes such as LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.\\nDDD connects the implementation to an evolving model.\\nDomain-driven design is predicated on the following goals:\\n- placing the project's primary focus on the core domain and domain logic;\\n- basing complex designs on a model of the domain;\\n- initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.\\nConcepts of the model include:\\n### Context\\nThe setting in which a word or statement appears that determines its meaning;\\n### Domain\\nA sphere of knowledge (ontology), influence, or activity. The subject area to which the user applies a program is the domain of the software;\\n### Model\\nA system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain;\\n### Ubiquitous Language\\nA language structured around the domain model and used by all team members to connect all the activities of the team with the software.\\n","Decision":"We In the context of the ODH we will use the pricniples of Domain Driven Design and use the language which is related to the business where it is operated.\\n","tokens":261,"id":2696,"Prediction":"## Decision\n\n**Use Domain-Driven Design (DDD) to structure the software code and language to match the business domain of processing loan applications.**\n\n### Context\n\nAs we are developing a software that processes loan applications, we want to ensure that the structure and language of our software code align with the business domain. This will enable us to create a more intuitive and maintainable system that accurately reflects the loan application process.\n\n### Problem\n\nWithout a clear connection between the software code and the business domain, our system may become confusing, difficult to maintain, and prone to errors. This can lead to delays, increased costs, and reduced quality.\n\n### Decision\n\nWe will apply Domain-Driven Design principles to structure our software code and language around the loan application domain. This will involve:\n\n* Creating classes and methods that reflect the business domain, such as `LoanApplication`, `Customer`, `AcceptOffer`, and `Withdraw`.\n* Using a ubiquitous language that is structured around the domain model and used by all team members to ensure consistency and clarity.\n* Focusing on the core domain and domain logic, and basing complex designs on a model of the domain.\n\n### Rationale\n\nBy using DDD, we can create a software system that accurately reflects the loan application process and is easier to understand and maintain. This will enable us to deliver a high-quality system that meets the needs of our users and stakeholders.\n\n### Consequences\n\n* The software code will be more intuitive and easier to understand, reducing the learning curve for new team members.\n* The system will be more maintainable, as the structure and language will be closely tied to the business domain.\n* The quality of the system will be higher, as the focus will be on accurately modeling the loan application process.\n\n### Alternatives\n\n* Not using DDD and structuring the software code and language around a generic, technical framework.\n* Using a different modeling approach, such as object-oriented programming or functional programming.\n\n### Implementation\n\nWe will implement DDD by:\n\n* Creating a domain model that accurately reflects the loan application process.\n* Using a ubiquitous language that is structured around the domain model.\n* Focusing on the core domain and domain logic, and basing complex designs on a model of the domain.\n* Collaborating with domain experts to ensure that the software code accurately reflects the business domain.","GenTime":"2024-08-10 12:29:43"}
{"File Name":"operational-data-hub\/0059-runtime-dependency-only-on-gcp.md","Context":"## Context\\nAvailability of systems can be improved by reducing the number of dependencies. Each additional dependency comes with the risk of that service breaking, causing issues to our system. On the other hand, 3rd party services can be leveraged to quickly build and apply managed services at low cost, compared to creating and running these services by ourselves. Therefore, the right balance between dependency on and benefitting from external services is important.\\nDuring run the changes in 3rd party dependencies do not change. Therefore, caching or copying the functionality of these dependencies is possible in many cases. In build and development environments, the dependencies on 3rd party components and services change more often. Therefore, it is oftentimes not opportune to invest in becoming independent on these components and services.\\n","Decision":"During runtime, we will only depend on resources services from the Google Cloud Platform.\\n","tokens":158,"id":2701,"Prediction":"## Decision\n\n**Decision:** We will adopt a hybrid approach to manage dependencies, leveraging the benefits of both internal development and external services. We will cache or copy the functionality of 3rd party dependencies during runtime to reduce the risk of external service breakdowns, while still utilizing external services in build and development environments to quickly build and apply managed services at low cost.\n\n**Rationale:**\n\n* By caching or copying the functionality of 3rd party dependencies during runtime, we can reduce the risk of external service breakdowns and improve system availability.\n* In build and development environments, the dependencies on 3rd party components and services change more often, making it more feasible to leverage external services to quickly build and apply managed services at low cost.\n* This hybrid approach allows us to strike a balance between the benefits of internal development and the benefits of external services, ensuring that we can take advantage of the strengths of both approaches.\n\n**Consequences:**\n\n* We will need to implement caching or copying mechanisms for 3rd party dependencies during runtime, which may require additional development effort and resources.\n* We will need to ensure that our caching or copying mechanisms are properly configured and maintained to ensure that they do not introduce new risks or performance issues.\n* We will need to regularly review and update our dependencies to ensure that they remain relevant and effective.\n\n**Alternatives Considered:**\n\n* Adopting an all-or-nothing approach, either relying solely on internal development or solely on external services, which may not provide the optimal balance between availability and cost.\n* Implementing a more complex and costly solution, such as building and running our own managed services, which may not be necessary or cost-effective.\n\n**Status:** This decision is approved and will be implemented in the next release.","GenTime":"2024-08-10 12:31:26"}
{"File Name":"operational-data-hub\/0003-repo-directory-structure.md","Context":"## Context\\nWe feel the need to standardize the directory structure for github repos.\\n","Decision":"### 1. General rules\\n* Directory names must always:\\n* Always use underscores (\u201c\\_\u201d) in names for folders and files to split between words.\\n* use lowercase.\\n### 2. Cloud functions\\n* The root directory for cloud functions is 'functions\/'.\\n* A '.gcloudignore' file is provided to limit the uploaded files.\\n### 3. API\\n* the root directory for the API is 'app\/'.\\n* For OpenApi APIs see the separate ADR for working with OpenApi generated code.\\n### 4. Frontend\\n* the root directory for the APP is 'app\/'.\\n* The directory structure below the app directory is based on the recommendations of the framework.\\n* For the angular applications the recommendations can be found at [Angular Workspace and project file structure](https:\/\/angular.io\/guide\/file-structure).\\n### 5. Tests\\n* Files needed for testing are stored in the 'test' directory.\\n### 6. SAST\\n* Files needed for the SAST scan are stored in the 'sast-config' directory.\\n","tokens":18,"id":2703,"Prediction":"## Decision\n\n**Standardize Directory Structure for GitHub Repos**\n\n### Context\n\nAs discussed, we feel the need to standardize the directory structure for GitHub repositories to ensure consistency, maintainability, and scalability of our software.\n\n### Problem\n\nCurrently, our GitHub repositories have varying directory structures, making it difficult to navigate and maintain the codebase. This lack of standardization leads to confusion, duplication of effort, and increased complexity.\n\n### Decision\n\nWe will adopt a standardized directory structure for all GitHub repositories, following the principles of simplicity, clarity, and scalability. The proposed structure is as follows:\n\n* `src\/`: contains the source code for the application\n* `docs\/`: contains documentation for the project, including README files, API documentation, and user guides\n* `tests\/`: contains unit tests, integration tests, and other testing-related files\n* `config\/`: contains configuration files for the application\n* `bin\/`: contains executable files and scripts\n* `lib\/`: contains third-party libraries and dependencies\n* `tools\/`: contains utility scripts and tools for development and testing\n\n### Rationale\n\nThis standardized directory structure will:\n\n1. Simplify navigation and maintenance of the codebase\n2. Reduce confusion and errors caused by inconsistent directory structures\n3. Improve scalability by providing a clear and organized structure for future development\n4. Enhance collaboration by providing a clear understanding of the directory structure for all team members\n\n### Consequences\n\n1. Initial setup and migration of existing repositories to the new structure\n2. Potential changes to existing code organization and naming conventions\n3. Increased clarity and maintainability of the codebase\n\n### Implementation\n\nWe will implement the standardized directory structure for all new GitHub repositories and migrate existing repositories to the new structure over a period of two weeks. We will also provide training and documentation to ensure all team members understand the new directory structure.\n\n### Monitoring and Review\n\nWe will review the effectiveness of the standardized directory structure after six months and make adjustments as necessary. We will also monitor the feedback from team members and make changes to the structure to ensure it continues to meet the needs of the team.","GenTime":"2024-08-10 12:32:05"}
{"File Name":"operational-data-hub\/0013-dataflow-diagrams.md","Context":"## Context\\nWe feel the need to define that every config folder should contain at least one dataflow diagram.\\n","Decision":"We decided that every config folder should contain at least one dataflow diagram.\\n### Dataflow Diagram\\nEvery config with a cloudbuild should contain a dataflow diagram. This diagram contains the dataflow of the GCP project the config is for. It is also possible to have multiple diagrams if there are multiple dataflows. [Here](https:\/\/github.com\/vwt-digital-config\/snb-link2-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) and [here](https:\/\/github.com\/vwt-digital-config\/snb-tmob-problm-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) examples of dataflow diagrams can be found.\\nThe diagrams are made via the python package [diagrams](https:\/\/github.com\/mingrammer\/diagrams).\\n","tokens":23,"id":2706,"Prediction":"## Decision\n\n**Decision:** We will require every config folder to contain at least one dataflow diagram (DFD) as a mandatory component.\n\n**Context:** As discussed, we have identified the need to define a standard for config folders to ensure consistency and clarity in our project's architecture.\n\n**Problem:** Without a standardized approach, config folders may become disorganized and difficult to understand, leading to confusion and errors.\n\n**Reasoning:** By requiring a DFD in every config folder, we can ensure that the relationships between different components and systems are clearly documented and easily understandable. This will facilitate communication among team members and reduce the risk of misunderstandings.\n\n**Consequences:**\n\n* Improved documentation and organization of config folders\n* Enhanced understanding of system architecture and relationships between components\n* Reduced risk of errors and misunderstandings\n* Increased collaboration and communication among team members\n\n**Alternatives Considered:** We considered alternative approaches, such as requiring a README file or a separate documentation folder. However, we believe that a DFD provides a more comprehensive and visual representation of the system architecture, making it a more effective solution for our needs.\n\n**Decision Drivers:** The need for clear and consistent documentation, the importance of system architecture understanding, and the desire for improved collaboration and communication among team members.\n\n**Status:** This decision is approved and will be implemented in our project's config folders.","GenTime":"2024-08-10 12:32:59"}
{"File Name":"operational-data-hub\/0019-single-schema-per-topic.md","Context":"## Context\\nA schema is a vocabulary that allows you to annotate and validate documents. Every topic has a schema that can validate the messages the topic receives.\\n","Decision":"Since every topic only receives messages in JSON format (see [21. Messages are in JSON format](0021-messages-are-in-json-format.md) ), we define a JSON Schema for every topic that can validate the messages received by said topic.\\n","tokens":32,"id":2710,"Prediction":"## Decision\n\n**Title:** Schema-based Validation for Topic Messages\n\n**Context:** A schema is a vocabulary that allows you to annotate and validate documents. Every topic has a schema that can validate the messages the topic receives.\n\n**Decision:** We will use a schema-based approach to validate the messages received by each topic in our system.\n\n**Rationale:**\n\n* Using a schema-based approach allows us to define a clear and consistent format for the messages that each topic expects to receive.\n* This approach enables us to validate the messages at the point of receipt, ensuring that they conform to the expected format and reducing the risk of errors or inconsistencies.\n* By defining a schema for each topic, we can also improve the maintainability and scalability of our system by making it easier to add or modify topics and their associated message formats.\n* Additionally, using a schema-based approach will allow us to take advantage of the benefits of JSON schema, such as validation, documentation, and testing.\n\n**Consequences:**\n\n* We will need to define a schema for each topic and ensure that the schema is up-to-date and accurate.\n* We will need to implement schema validation at the point of message receipt, which may require additional processing overhead.\n* We will need to ensure that the schema is properly documented and communicated to developers and other stakeholders.\n\n**Status:** Accepted\n\n**Implementation:** We will implement schema-based validation using a JSON schema library and integrate it with our message processing pipeline. We will also develop a process for maintaining and updating the schemas for each topic.","GenTime":"2024-08-10 12:34:24"}
{"File Name":"operational-data-hub\/0002-use-serverless-infra-components.md","Context":"## Context\\nServerless computing offers a number of advantages over traditional cloud-based or server-centric infrastructure. For many developers, serverless architectures offer greater scalability, more flexibility, and quicker time to release, all at a reduced cost. With serverless architectures, developers do not need to worry about purchasing, provisioning, and managing backend servers.\\n","Decision":"We will use serverless infrastructure components where possible.\\n","tokens":68,"id":2723,"Prediction":"## Decision\n\n**Decision:** Adopt a serverless architecture for our application's backend infrastructure.\n\n**Context:** As described in the context, serverless computing offers numerous advantages, including greater scalability, more flexibility, and quicker time to release, all at a reduced cost. By adopting a serverless architecture, our development team can focus on writing code without worrying about the underlying infrastructure, allowing us to take advantage of these benefits.\n\n**Rationale:**\n\n* Scalability: With serverless computing, our application can automatically scale up or down to meet changing demands, without the need for manual intervention.\n* Cost-effectiveness: By only paying for the compute time consumed by our application, we can reduce our infrastructure costs and allocate resources more efficiently.\n* Flexibility: Serverless computing allows us to write code in a variety of languages and frameworks, giving us greater flexibility in our development process.\n* Quick time to release: With serverless computing, we can deploy our application quickly and easily, without the need for complex infrastructure setup or configuration.\n\n**Consequences:**\n\n* Our development team will need to adapt to a new paradigm of serverless computing, which may require additional training and expertise.\n* We will need to carefully monitor our application's performance and costs to ensure that we are taking full advantage of the benefits of serverless computing.\n* We may need to make adjustments to our application's architecture and design to take full advantage of the benefits of serverless computing.\n\n**Alternatives:**\n\n* Traditional cloud-based or server-centric infrastructure\n* Hybrid approach, using both serverless and traditional infrastructure\n\n**Status:** Approved\n\n**Implementation:**\n\n* Research and select a suitable serverless computing platform (e.g. AWS Lambda, Google Cloud Functions, Azure Functions)\n* Migrate existing backend code to the serverless platform\n* Monitor and optimize application performance and costs\n* Provide training and support to development team on serverless computing best practices","GenTime":"2024-08-10 12:38:31"}
{"File Name":"operational-data-hub\/0012-repository-organisation-rules.md","Context":"## Context\\nWe feel the need to define rules to select the correct GitHub organisation for source repositories.\\n","Decision":"We identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.\\n### VWT Digital\\nThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,\\nit **has to pass** one of the following applications:\\n- An ODH-specific solution;\\n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration;\\n- Any solution business logic.\\nWhen creating a name for the repository, it **has to start** with either `odh-<repo_name>` or `dat-<repo_name>`:\\n- `odh-`: When the repository contains source code specifically made for the Operational Data Hub;\\n- `dat-`: When the repository contains source code created for generic usage within the Digital Ambition Team domain.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Solutions\\nThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:\\n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration.\\nWhen creating a name for the repository, it has to start with the abbreviation of the solutions it is a part of.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Configuration\\nThis is the main organisation for all configuration code. When creating a new repository, it **has to pass** the following application:\\n- Google Cloud Platform (GCP) project-specific configuration code that is for private usage only.\\nWhen creating a name for the repository, it has to contain the GCP project they are connected to minus the customer,\\nenvironment and location. Furthermore, it has to end with `-config`.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n","tokens":21,"id":2725,"Prediction":"## Decision\n\n**GitHub Organisation Selection Rules**\n\n## Context\n\nWe feel the need to define rules to select the correct GitHub organisation for source repositories.\n\n## Problem\n\nCurrently, we are manually selecting the correct GitHub organisation for each source repository, which is prone to errors and inconsistencies. This manual process also slows down our development cycle and increases the risk of incorrect dependencies.\n\n## Decision\n\nWe will implement a set of rules to automatically select the correct GitHub organisation for source repositories. These rules will be based on a combination of factors such as:\n\n* Repository name and namespace\n* Repository type (e.g. monorepo, microservice)\n* Business unit and department\n* Technology stack and programming language\n\nThe rules will be implemented using a simple lookup table or a more complex algorithm, depending on the complexity of the requirements.\n\n## Rationale\n\nImplementing rules for selecting the correct GitHub organisation will:\n\n* Reduce errors and inconsistencies in selecting the correct organisation\n* Increase development speed and efficiency\n* Improve maintainability and scalability of our source repository management system\n* Allow for more flexibility and adaptability in our development process\n\n## Consequences\n\n* Additional complexity in implementing and maintaining the rules\n* Potential for errors or misconfigurations if the rules are not properly tested and validated\n* Need for ongoing monitoring and updates to the rules to ensure they remain effective and relevant\n\n## Status\n\nApproved\n\n## Links\n\n* GitHub organisation selection rules implementation plan\n* GitHub organisation selection rules testing and validation plan","GenTime":"2024-08-10 12:39:11"}
{"File Name":"operational-data-hub\/0039-least-privilege-access.md","Context":"## Context\\nThe principle of least privilege means that every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose.\\nThe principle of least privilege works by allowing only enough access to perform the required job. In an IT environment, adhering to the principle of least privilege reduces the risk of attackers gaining access to critical systems or sensitive data by compromising a low-level user account, device, or application. Implementing the principle helps contain compromises to their area of origin, stopping them from spreading to the system at large.\\n","Decision":"We will apply the principle of least privilege.\\n","tokens":128,"id":2737,"Prediction":"## Decision\n\n**Title:** Implement the Principle of Least Privilege for Access Control in the Software\n\n**Context:** The principle of least privilege means that every module (such as a process, a user, or a program) must be able to access only the information and resources that are necessary for its legitimate purpose.\n\n**Decision:** We will implement the principle of least privilege in our software by assigning the minimum necessary permissions and access rights to each module, process, or user. This will ensure that each component can only access the resources and information required to perform its intended function, reducing the risk of unauthorized access and potential security breaches.\n\n**Rationale:** By adhering to the principle of least privilege, we can:\n\n* Reduce the attack surface by limiting the potential entry points for attackers\n* Contain the impact of a potential security breach by limiting the access and privileges of compromised accounts or modules\n* Improve the overall security and integrity of the software by ensuring that each component operates with the minimum necessary privileges\n\n**Consequences:**\n\n* Increased complexity in implementing and managing access controls\n* Potential need for additional authentication and authorization mechanisms\n* Improved security and reduced risk of unauthorized access\n\n**Alternatives Considered:**\n\n* Implementing a more permissive access control model, allowing all modules and users to have full access to all resources\n* Implementing a role-based access control model, where users are assigned to roles that grant them access to specific resources and functionality\n\n**Decision Date:** [Insert Date]\n\n**Responsible Parties:** [Insert Names and Roles]\n\n**Verification:** The implementation of the principle of least privilege will be verified through regular security audits and penetration testing to ensure that the software is operating as intended and that access controls are effective.","GenTime":"2024-08-10 12:43:04"}
{"File Name":"operational-data-hub\/0009-uri-identifies-data.md","Context":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme (e.g. http:\/\/).\\nTo be able to unambiguously identify any element of data, a URI will be defined for every element of data on the ODH platform. Building this URI from meaningful attributes instead of technical randomly generated ids makes it recognizable and easier to use. However, selecting the right attributes and context information to assure uniqueness is challenging.\\nThe URI scheme to use is [tag](https:\/\/tools.ietf.org\/html\/rfc4151). Tag URIs are quite human readable, but, unlike http URIs, not resolvable. This makes them suitable for identifying real-life things (entities) that have a representation on the ODH. A URI can be defined from the hierarchy of the organisation, forming a path to the resource. For example, if mycompany has a catalog with articles and one of the articles has article# 313, an article URI could be defined as `tag:vwt.digital,2020:mycompany\/catalog\/articles\/313`\\n","Decision":"We define a URI according to the [tag scheme](https:\/\/tools.ietf.org\/html\/rfc4151) for each entity that has a representation on the ODH platform.\\n","tokens":252,"id":2740,"Prediction":"## Decision\n\n**Decision:** We will use Tag URIs (RFC 4151) to identify every element of data on the ODH platform, using a hierarchical naming scheme that includes the organisation's structure and meaningful attributes to ensure uniqueness.\n\n**Rationale:**\n\n* Tag URIs are human-readable, making them easier to understand and use.\n* They are suitable for identifying real-life entities that have a representation on the ODH platform.\n* The hierarchical naming scheme allows for a clear and organized structure, making it easier to navigate and identify resources.\n* Using meaningful attributes instead of technical IDs makes the URIs more recognizable and easier to use.\n* The challenge of selecting the right attributes and context information to ensure uniqueness is mitigated by using a hierarchical naming scheme and meaningful attributes.\n\n**Consequences:**\n\n* The ODH platform will have a unique and consistent way of identifying every element of data.\n* The use of Tag URIs will make it easier for users to understand and work with the platform.\n* The hierarchical naming scheme will provide a clear and organized structure for the platform's resources.\n* The platform will be able to take advantage of the benefits of using a standardized and widely-adopted URI scheme.\n\n**Alternatives Considered:**\n\n* Using HTTP URIs (RFC 3986) for identifying data elements, which would provide resolvability but would not be as human-readable.\n* Using a custom URI scheme, which would require more development effort and may not be as widely adopted.\n\n**Status:** Implemented","GenTime":"2024-08-10 12:43:59"}
{"File Name":"operational-data-hub\/0003-create-cloud-native-solutions.md","Context":"## Context\\nCloud-native architecture fully takes advantage of the [serverless computing](0002-use-serverless-infra-components.md) to maximise your focus on writing code, creating business value and keeping customers happy.\\nAll the developer has to worry about is orchestrating all the infrastructure they need ([sofware defined](0004-create-software-defined-everything.md)) and the application code itself.\\n","Decision":"We will build cloud-native solutions.\\n","tokens":83,"id":2752,"Prediction":"## Decision\n\n**Decision:** Use a Container Orchestration Service (COS) to manage the deployment and scaling of cloud-native applications.\n\n**Context:** Cloud-native architecture fully takes advantage of the serverless computing to maximize focus on writing code, creating business value, and keeping customers happy. All the developer has to worry about is orchestrating all the infrastructure they need (software-defined) and the application code itself.\n\n**Problem:** As the application grows, it becomes difficult to manage and scale the infrastructure manually. This can lead to increased costs, reduced performance, and decreased customer satisfaction.\n\n**Reasoning:** Using a Container Orchestration Service (COS) allows us to automate the deployment and scaling of our cloud-native applications. This enables us to:\n\n* Easily manage and scale our infrastructure\n* Reduce costs by only paying for the resources used\n* Improve performance and reliability by automatically scaling and deploying containers\n* Focus on writing code and creating business value, rather than managing infrastructure\n\n**Consequences:**\n\n* Simplified infrastructure management and scaling\n* Improved application performance and reliability\n* Reduced costs and increased efficiency\n* Increased developer productivity and focus on business value creation\n\n**Alternatives Considered:**\n\n* Manual infrastructure management and scaling\n* Using a monolithic application server\n* Using a traditional container orchestration tool (e.g. Docker Swarm, Kubernetes)\n\n**Decision Criteria:**\n\n* Ease of use and management\n* Scalability and reliability\n* Cost-effectiveness\n* Alignment with cloud-native architecture principles\n\n**Implementation:**\n\n* Choose a suitable COS (e.g. AWS Fargate, Google Cloud Run, Azure Container Instances)\n* Configure the COS to manage and scale our cloud-native applications\n* Integrate the COS with our existing infrastructure and application code\n* Monitor and optimize the performance and scalability of our applications","GenTime":"2024-08-10 12:47:52"}
{"File Name":"operational-data-hub\/0023-iso-8601-to-specify-date-and-time-with-timezone.md","Context":"## Context\\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https:\/\/tools.ietf.org\/html\/rfc3339) describes the usage of the ISO-8601 standard.\\n","Decision":"We will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.\\nAll date objects must have a time-zone included.\\n","tokens":66,"id":2755,"Prediction":"## Decision\n\n**Date-Time Format in JSON**\n\n**Context:** JSON does not specify how a date(time) string should be formatted, and the ISO 8601 standard is widely used within the JSON community to specify date-time objects.\n\n**Decision:** We will use the ISO 8601 standard, as specified in RFC 3339, to format date-time strings in our JSON data. This will ensure consistency and compatibility with other JSON-based systems.\n\n**Rationale:** The ISO 8601 standard is widely adopted and provides a clear and unambiguous format for representing date-time values. By using this standard, we can avoid potential issues with date-time parsing and ensure that our data is easily readable and convertible between different systems.\n\n**Consequences:**\n\n* We will use the \"YYYY-MM-DDTHH:MM:SSZ\" format for date-time strings in our JSON data.\n* We will ensure that all date-time values are formatted according to the ISO 8601 standard.\n* We will provide clear documentation and examples of the expected date-time format in our API and data models.\n\n**Alternatives Considered:**\n\n* We considered using a custom date-time format, but this would introduce potential issues with date-time parsing and compatibility.\n* We considered using a different standard, such as the W3C Date and Time Formats, but ISO 8601 is widely adopted and provides a clear and unambiguous format.\n\n**Status:** Approved\n\n**Date:** 2023-02-20","GenTime":"2024-08-10 12:48:46"}
{"File Name":"operational-data-hub\/0027-a-gcp-project-belongs-to-a-single-domain.md","Context":"## Context\\nThe projects structure of the platform can be used to protect components. By the seperation into projects a modular, loosely coupled design is created. A project belongs to a single [domain](0025-domains-correspond-to-business-departments.md), a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\\n","Decision":"The set of functions implemented in one GCP project belongs to a single domain.\\n","tokens":75,"id":2756,"Prediction":"## Decision\n\n**Project Structure for Component Protection**\n\n## Context\n\nThe projects structure of the platform can be used to protect components. By separating into projects, a modular, loosely coupled design is created. A project belongs to a single domain, a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\n\n## Problem\n\nWe need to ensure that components are protected from unauthorized access and changes. The current project structure allows for a modular design, but we need to take it a step further to provide a secure and maintainable architecture.\n\n## Decision\n\nWe will use the project structure to implement access control and segregation of components. Each project will have its own set of permissions, and only authorized users will be able to access and modify the components within that project. This will ensure that components are protected from unauthorized access and changes.\n\n## Rationale\n\nBy using the project structure to implement access control, we can:\n\n* Provide a clear separation of concerns and responsibilities\n* Limit access to components to only authorized users\n* Ensure that changes to components are tracked and audited\n* Improve the overall security and maintainability of the platform\n\n## Consequences\n\n* The project structure will need to be updated to include permissions and access control\n* Users will need to be assigned to specific projects and roles\n* The architecture will need to be reviewed and updated to ensure that it is secure and maintainable\n\n## Status\n\nApproved\n\n## Links\n\n* [0025-domains-correspond-to-business-departments.md](0025-domains-correspond-to-business-departments.md) - Domain structure and relationships","GenTime":"2024-08-10 12:49:03"}
{"File Name":"operational-data-hub\/0029-components-are-named-according-to-naming-conventions.md","Context":"## Context\\nOrganize your cloud assets to support operational management and accounting requirements. Well-defined naming and metadata tagging conventions help to quickly locate and manage resources. These conventions also help associate cloud usage costs with business teams via chargeback and showback accounting mechanisms.\\n","Decision":"The naming cloud object naming convention is based on the Ironstone Naming conventions and can be found on our interal Wiki\\n","tokens":52,"id":2757,"Prediction":"## Decision\n\n**Decision:** Implement a standardized naming and metadata tagging convention for cloud assets to support operational management and accounting requirements.\n\n**Context:** Organizing cloud assets to support operational management and accounting requirements, including quickly locating and managing resources, and associating cloud usage costs with business teams via chargeback and showback accounting mechanisms.\n\n**Problem:** Without a standardized naming and metadata tagging convention, cloud assets may be difficult to locate, manage, and track, leading to inefficiencies and increased costs. This can also make it challenging to accurately allocate costs to business teams, making it difficult to make informed financial decisions.\n\n**Reasoning:** Implementing a standardized naming and metadata tagging convention will enable us to:\n\n* Quickly locate and manage cloud resources, reducing time and effort spent on resource discovery and management.\n* Associate cloud usage costs with business teams, providing a clear picture of costs and enabling accurate chargeback and showback accounting.\n* Improve operational efficiency and reduce costs by streamlining resource management and allocation.\n\n**Decision Drivers:**\n\n* Compliance with accounting and financial reporting requirements\n* Improved operational efficiency and reduced costs\n* Enhanced visibility into cloud usage and costs\n\n**Consequences:**\n\n* Initial investment in developing and implementing the naming and metadata tagging convention\n* Ongoing effort to maintain and enforce the convention\n* Potential resistance to change from teams and individuals who may need to adapt to the new convention\n\n**Alternatives Considered:**\n\n* Implementing a custom solution for naming and metadata tagging\n* Using a third-party tool or service for cloud asset management and accounting\n* Not implementing a standardized naming and metadata tagging convention\n\n**Rationale:** The chosen approach provides a standardized and scalable solution for naming and metadata tagging, ensuring consistency and efficiency in cloud asset management and accounting.","GenTime":"2024-08-10 12:49:22"}
{"File Name":"operational-data-hub\/0005-build-open-source-solutions.md","Context":"## Context\\nOpen source software is software with source code that anyone can inspect, modify, and enhance.\\nBy design, open source software licenses promote collaboration and sharing because they permit other people to make modifications to source code and incorporate those changes into their own projects. They encourage computer programmers to access, view, and modify open source software whenever they like, as long as they let others do the same when they share their work.\\nOpen source software development furthers several principles, described below.\\nTransparency. Whether we're developing software or solving a business problem, we all have access to the information and materials necessary for doing our best work. And when these materials are accessible, we can build upon each other's ideas and discoveries. We can make more effective decisions and understand how decisions affect us.\\nCollaboration. When we're free to participate, we can enhance each other's work in unanticipated ways. When we can modify what others have shared, we unlock new possibilities. By initiating new projects together, we can solve problems that no one can solve alone. And when we implement open standards, we enable others to contribute in the future.\\nRelease early and often. Rapid prototypes can lead to rapid discoveries. An iterative approach leads to better solutions faster. When you're free to experiment, you can look at problems in new ways and seek answers in new places. You can learn by doing.\\nInclusive meritocracy. Good ideas can come from anywhere, and the best ideas should win. Only by including diverse perspectives in our conversations can we be certain we've identified the best ideas, and decision-makers continually seek those perspectives. We may not operate by consensus, but successful work determines which projects gather support and effort from the community.\\nCommunity. Communities form when different people unite around a common purpose. Shared values guide decision making, and community goals supersede individual interests and agendas.\\n","Decision":"We make our code available as open source.\\n","tokens":382,"id":2759,"Prediction":"## Decision\n\n**Title:** Open Source Software Development Strategy\n\n**Context:** As we aim to develop software that promotes collaboration, transparency, and inclusivity, we have decided to adopt an open source software development strategy.\n\n**Problem:** Traditional software development approaches often prioritize secrecy and control, which can limit collaboration, innovation, and community engagement.\n\n**Decision:** We will adopt an open source software development strategy, which will allow us to:\n\n* Promote transparency by making our source code accessible to everyone\n* Foster collaboration by enabling others to modify and enhance our software\n* Encourage rapid prototyping and experimentation through iterative development\n* Create an inclusive meritocracy by valuing diverse perspectives and ideas\n* Build a community around our software by sharing values and goals\n\n**Rationale:** By adopting an open source software development strategy, we can:\n\n* Leverage the collective knowledge and expertise of the community to improve our software\n* Increase the speed and quality of our development process through collaboration and feedback\n* Attract a wider range of contributors and users who share our values and goals\n* Foster a culture of transparency, inclusivity, and meritocracy\n\n**Consequences:**\n\n* We will need to establish a governance model to ensure the stability and consistency of our software\n* We will need to develop processes for managing contributions, reviewing changes, and resolving conflicts\n* We will need to be prepared to address potential security and intellectual property concerns\n* We will need to prioritize community engagement and outreach to build and maintain a strong community around our software\n\n**Status:** This decision is currently in progress, and we are working on establishing our open source software development strategy and governance model.\n\n**Affected Components:** Our software development process, community engagement strategy, and governance model will all be impacted by this decision.","GenTime":"2024-08-10 12:50:03"}
{"File Name":"operational-data-hub\/0057-naming-convention-defines-uri-tags.md","Context":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme\\n","Decision":"We will use [the 'tag' URI Scheme](https:\/\/tools.ietf.org\/html\/rfc4151).\\n","tokens":55,"id":2763,"Prediction":"## Decision\n\n**Decision:** We will use the Internationalized Resource Identifier (IRI) standard to represent URIs in our software, instead of the traditional Uniform Resource Identifier (URI) standard.\n\n**Context:** As mentioned, URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme. However, the traditional URI standard has some limitations, such as not supporting non-ASCII characters, which can lead to issues with internationalization and localization.\n\n**Problem:** Our software needs to handle URIs that contain non-ASCII characters, such as Unicode characters, to support internationalization and localization. The traditional URI standard does not provide a straightforward way to handle such characters, which can lead to errors and inconsistencies.\n\n**Reasoning:** Using the IRI standard allows us to support non-ASCII characters in URIs, making it easier to handle internationalization and localization. IRIs are a superset of URIs, and they can represent any Unicode string as a URI. This means that our software can handle URIs that contain non-ASCII characters, such as Unicode characters, without any issues.\n\n**Consequences:**\n\n* Our software will be able to handle URIs that contain non-ASCII characters, making it more suitable for internationalization and localization.\n* We will need to ensure that our software can correctly parse and generate IRIs, which may require additional implementation and testing.\n* We may need to update our existing URI-related functionality to use IRIs instead of URIs.\n\n**Alternatives:** We considered using the traditional URI standard with Unicode normalization, but this approach has its own limitations and complexities. Using IRIs provides a more straightforward and consistent way to handle non-ASCII characters in URIs.\n\n**Status:** This decision is approved and will be implemented in our software.","GenTime":"2024-08-10 12:51:24"}
{"File Name":"CICD-pipeline\/002-usage-and-import-of-shared-library.md","Context":"## Context\\nWe need to decide how the shared library should be loaded for usage.\\n- A library version can be a tag, a branch or a commit SHA.\\n- A library can be loaded explicitly or implicitly\\n- A library default version can be configured globally or not\\n### Loading implicitly\\n````groovy\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- a default version of the library needs to be defined globally\\n- there is no need to import the library, as it will be loaded automatically\\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\\n### Loading explicitly\\n````groovy\\n@Library('my-shared-library@1.0.0') _\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- an import statement is always needed\\n- a default version could be defined globally, then the version can be left out when importing\\n### Using a global predefined version\\n- can be overriden in the pipeline\\n- does not force users to select a version\\n### No globel predefined version\\n- import will not work without version\\n- consumers are aware of the version that they use\\n- can be dynamic (auto updating) when using release channels with branches\\n","Decision":"We will use explicit loading without a predefined version and release channels for releasing based on branches.\\n- reduces the magic of the used version\\n- can still be auto updating\\n- consumers know that a library is used and what the version is\\n","tokens":314,"id":2766,"Prediction":"## Decision\n\n### Decision: Implement a hybrid approach for loading the shared library\n\n### Context\n\nAs discussed, we need to decide how the shared library should be loaded for usage. The context provides three options: loading implicitly, loading explicitly, and using a global predefined version. Each option has its pros and cons, and we need to consider the requirements and constraints of our use case.\n\n### Decision Rationale\n\nAfter careful consideration, we decide to implement a hybrid approach that combines the benefits of implicit and explicit loading. This approach allows for flexibility and customization while ensuring that the library is loaded correctly.\n\nHere's how the hybrid approach works:\n\n1. **Global Default Version**: We define a global default version of the library that can be overridden by users.\n2. **Implicit Loading**: When a pipeline is executed without an explicit import statement, the global default version of the library is loaded implicitly.\n3. **Explicit Loading**: Users can override the global default version by importing the library explicitly with a specific version.\n4. **Version Override**: Users can also override the version of the library when importing it explicitly.\n\nThis hybrid approach provides the following benefits:\n\n* Convenience: Users can use the global default version without having to specify the version explicitly.\n* Flexibility: Users can override the global default version and specify a custom version when needed.\n* Consistency: The approach ensures that the library is loaded correctly and consistently across pipelines.\n\n### Design\n\nTo implement the hybrid approach, we will:\n\n1. Create a global configuration file that defines the default version of the library.\n2. Modify the pipeline script to check for an explicit import statement and load the library accordingly.\n3. Implement a mechanism to override the global default version when importing the library explicitly.\n\n### Conclusion\n\nThe hybrid approach provides a flexible and customizable solution for loading the shared library. It allows users to take advantage of the convenience of implicit loading while still providing the flexibility to override the global default version when needed.","GenTime":"2024-08-10 12:52:34"}
{"File Name":"CICD-pipeline\/001-open-source-shared-jenkins-library.md","Context":"## Context\\n1) We are [open by default](https:\/\/github.com\/baloise\/baloise.github.io\/blob\/master\/design\/OpenByDefault.adoc)\\n1) The myCloud API spec [is not confidential](https:\/\/github.com\/baloise\/CICD-pipeline\/issues\/15)\\n1) Openness is a driver for clean, secure design and good documentation\\n1) There are currently no obvious drivers for confidentiality\\n","Decision":"We release the Shared Jenkins Library under Apache 2.0.\\n","tokens":90,"id":2768,"Prediction":"## Decision\n\n**API Documentation: Publicly Accessible**\n\n### Context\n\nAs discussed, our company follows the \"Open by Default\" principle, which encourages openness and transparency in our design and documentation. Additionally, the myCloud API specification is not confidential, and there are no apparent drivers for confidentiality. Openness is a key driver for clean, secure design and good documentation.\n\n### Decision\n\nWe will make the API documentation publicly accessible, following the OpenAPI specification. This will allow developers to easily understand and interact with the API, promoting a culture of openness and collaboration.\n\n### Rationale\n\nBy making the API documentation publicly accessible, we will:\n\n* Encourage developers to contribute to the API's development and maintenance\n* Facilitate the creation of third-party integrations and applications\n* Improve the overall quality and security of the API through community feedback and testing\n* Demonstrate our commitment to openness and transparency, aligning with our company's values\n\n### Consequences\n\n* The API documentation will be publicly available, potentially exposing sensitive information (although, as mentioned, the specification is not confidential).\n* We will need to ensure that the documentation is accurate, up-to-date, and easy to understand.\n* We may need to implement additional security measures to protect against potential abuse or misuse of the publicly accessible documentation.\n\n### Status\n\nThis decision is approved and will be implemented as part of our API development process.","GenTime":"2024-08-10 12:53:03"}
{"File Name":"exercise3-group-6\/0006-create-observation-repository-search.md","Context":"## Context\\nResearchers may require methods to search for a particular Whale Observation or set of Whale Observations during their\\nstudies. For the program to accomplish this task two search methods may require implementation.\\n","Decision":"Create `getByDate` and `getById` methods in ObservationRepository.\\n`getByDate` uses the Observation iterator to compare Observation objects `sightingTime` (Date) to the date passed to the\\nmethod and creates an array list of matching objects. This function uses an iterator rather than `Collections.binarySearch()`\\nto reduce the runtime given that there may exist multiple objects of a similar date.\\n`getById` uses `Collections.binarySearch()` to search and possibly return the object with the `ObservationId` matching\\nthe long passed to the method.\\nCollection.sort()` used in `getById` implements the abstract strategy of the `Comparator` interface using the concrete\\nstrategy `compareByDate` and returns an integer referring to the order of the objects.\\n","tokens":41,"id":2772,"Prediction":"## Decision\n\n**Title:** Implementing Search Methods for Whale Observations\n\n**Context:** Researchers may require methods to search for a particular Whale Observation or set of Whale Observations during their studies.\n\n**Problem:** To provide a efficient and effective way for researchers to search for Whale Observations, we need to implement two search methods: a simple search by ID and an advanced search by multiple criteria.\n\n**Decision:** We will implement a simple search by ID and an advanced search by multiple criteria using a combination of SQL queries and a search API.\n\n**Rationale:**\n\n* Simple search by ID: This will allow researchers to quickly find a specific Whale Observation by its unique ID.\n* Advanced search by multiple criteria: This will enable researchers to search for Whale Observations based on various attributes such as location, date, species, and observer.\n* Combination of SQL queries and search API: This approach will provide a flexible and scalable solution that can handle complex search queries and return relevant results efficiently.\n\n**Consequences:**\n\n* The program will provide two search methods: simple search by ID and advanced search by multiple criteria.\n* The search API will be responsible for parsing the search query, executing the corresponding SQL query, and returning the search results.\n* The program will require additional development and testing to ensure the search methods are working correctly and efficiently.\n\n**Status:** Accepted\n\n**Implementor:** [Name of the implementor]\n\n**Date:** [Date of the decision]\n\n**Links:** [Links to relevant documentation or code]","GenTime":"2024-08-10 12:54:14"}
{"File Name":"exercise3-group-6\/0003-create-two-observation-comparators.md","Context":"## Context\\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\\n","Decision":"Implement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\\ncomparator class for field `sightingTime` (Date) in Observation.\\n","tokens":39,"id":2775,"Prediction":"## Decision\n\n**Decision:** Implement a generic and extensible sorting framework for whale observations, allowing for the addition of new sorting methods and comparison functions as needed.\n\n## Context\n\nWhale observations have a variety of qualities and may require sorting for research purposes. Two distinct sorting methods proposed for sorting these observations will require two unique comparison methods.\n\n## Problem\n\nThe proposed sorting methods and comparison functions are specific to the whale observation data and are not easily reusable or extensible. This may lead to code duplication, maintenance challenges, and limited flexibility to adapt to future changes or new requirements.\n\n## Rationale\n\nImplementing a generic and extensible sorting framework for whale observations will provide several benefits:\n\n1.  **Reusability**: The framework can be used across different sorting methods and comparison functions, reducing code duplication and improving maintainability.\n2.  **Flexibility**: The framework can be easily extended to accommodate new sorting methods and comparison functions as needed, allowing for greater adaptability to changing requirements.\n3.  **Efficiency**: The framework can be optimized for performance, ensuring that sorting operations are efficient and scalable.\n\n## Solution\n\nTo implement the sorting framework, we will create a modular architecture that separates the sorting logic from the specific comparison functions. This will allow us to easily add or remove sorting methods and comparison functions as needed.\n\n1.  **Sorting Interface**: Define a sorting interface that specifies the required methods for sorting whale observations, such as `sort()` and `compare()`.\n2.  **Sorting Implementations**: Create separate implementations for each sorting method, such as `BubbleSort` and `QuickSort`. Each implementation will implement the sorting interface and provide its own comparison function.\n3.  **Comparison Functions**: Define separate comparison functions for each sorting method, such as `compareByDate()` and `compareByLocation()`. These functions will be responsible for comparing whale observations based on specific criteria.\n4.  **Framework**: Create a framework that coordinates the sorting process, allowing users to select a sorting method and comparison function and execute the sorting operation.\n\n## Consequences\n\nImplementing a generic and extensible sorting framework for whale observations will:\n\n1.  **Improve Reusability**: Reduce code duplication and improve maintainability by reusing the sorting framework across different sorting methods and comparison functions.\n2.  **Increase Flexibility**: Allow for easy extension to accommodate new sorting methods and comparison functions as needed, improving adaptability to changing requirements.\n3.  **Enhance Efficiency**: Optimize the sorting framework for performance, ensuring efficient and scalable sorting operations.\n\nHowever, implementing a generic and extensible sorting framework may:\n\n1.  **Increase Complexity**: Introduce additional complexity by requiring a deeper understanding of the sorting framework and its components.\n2.  **Require Additional Development Time**: Take longer to develop and test the sorting framework, especially if it involves creating multiple sorting implementations and comparison functions.","GenTime":"2024-08-10 12:55:29"}
{"File Name":"delayer-aws\/0003-dymamodb-update-strategy.md","Context":"## Context and Problem Statement\\nIn order to maintain the serverless nature of the project, a DynamoDB table was\\nchosen as main persistence mechanism, which means that a schedule is primarily\\nstored in DynamoDB and then distributed to other components, which in turn\\ngoes back to DynamoDB to update state. So, DynamoDB holds both state and\\nhistorical data.\\nThe problem here is that both ~~warmer~~ `task-1minute-enqueuer` and ~~poller~~ `task-1minute-sqs2sns` will concur by Dynamo resources and probably will be throttled (it's easy to reproduce this behavior only by setting Dynamo's read and write capacity to 1 and trying to send some hundreds of schedules while some other are ~~moving from *WARM* state~~ being enqueued in delayer queue).\\n## Decision Drivers\\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\n","Decision":"*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\nTake the decision for the use of the DynamoDB introduced a new concept for the entire architecture: the layered concern.\\nThe `delayer-aws` solution aims to provide a way to schedule future operations reliably. It's not part of this system store or ingest or even present information about these schedules. In this sense, the use of DynamoDB is needed only because there's a need of store schedules that could not be posted in delayer queue, and there's only 2 options for those records: or they are in the delayer queue, or they're not. That's why the \"state\" field is needed, but it will not hold the *entire* lifecycle of a schedule.\\nWith this in mind, we realize that all 3 options will be considered, but in different contexts:\\n-   Present data of scheduler is not `delayer-aws`'s concern, but it will be needed. So all the data events should be published by `delayer-aws` to be consumed by another \"view\" platform - this is a kind of *event driven approach*.\\n-   In this sense, if another system will ingest all of this published data, state and historical data will be stored in different persistent stores;\\n### How it should work\\nWhen a schedule is inserted, if their timeframe was greater then the timeframe of the delayer queue, the schedule is stored in DynamoDB with a *currentStatus* marked as `NEW` and a TTL of 1 hour after schedule time.\\nWhen `task-1minute-enqueuer` runs and finds this scheduler, it will be updated to `PROCESSED`. After that, no more updates are made in DynamoDB.\\nFor each event that occurs with this schedule, a message should be posted in the correspondent topic.\\n### Don't use state as a field in database\\nThe state of a schedule is noted by currentStatus\/currentState attribute in\\nDynamoDB. The proposal of this optional is to treat state as part of the\\ngeneral flow of application:\\n1.  When scheduled, it's COLD\\n1.  When *warmer* puts it on SQS, it's WARM\\n1.  When *poller* publishes it on SNS is DOING\\n1.  When *actions* execute\\n*   Good, simplified update policy\\n*   Good, reduced DynamoDB costs\\n*   Bad, no information about past events\\n","tokens":211,"id":2791,"Prediction":"## Decision\n\nTo address the concurrency issue between `task-1minute-enqueuer` and `task-1minute-sqs2sns` on DynamoDB resources, we will implement a caching layer using Redis to store the schedules before writing them to DynamoDB. This will help to reduce the number of concurrent writes to DynamoDB, thereby avoiding throttling and improving the overall system performance.\n\n## Decision Drivers\n\n*   Solution must kept as simple as possible: By using Redis as a caching layer, we can achieve the desired outcome without introducing complex logic or additional dependencies.\n*   Even it could delay the problem, increase read and write capacity of DynamoDB is not an architectural solution: Increasing the read and write capacity of DynamoDB is a temporary fix that does not address the underlying concurrency issue. Implementing a caching layer provides a more scalable and maintainable solution.\n\n## Decision Consequences\n\n*   Improved system performance: By reducing the number of concurrent writes to DynamoDB, we can improve the overall system performance and reduce the likelihood of throttling.\n*   Simplified architecture: The use of Redis as a caching layer keeps the architecture simple and easy to maintain.\n*   Delayed problem resolution: While the caching layer will help to reduce the concurrency issue, it does not completely eliminate the problem. A more comprehensive solution may still be necessary in the future.\n\n## Rationale\n\nThe caching layer will store the schedules in Redis before writing them to DynamoDB. This will allow us to batch writes to DynamoDB, reducing the number of concurrent writes and avoiding throttling. The caching layer will also provide a layer of abstraction between the application and DynamoDB, making it easier to scale and maintain the system.","GenTime":"2024-08-10 13:00:44"}
{"File Name":"govuk-kubernetes-discovery\/0005-consistent-structure-for-terraform-files.md","Context":"## Context\\nWe should have a consistent structure for working with our Terraform files. We\\nshould be able to look into a file and understand it's function is and what it\\ncreates, what variables it needs to run and what it outputs after it's finished.\\nWe should not have to spend time picking apart the code to understand what is\\ncreated by Terraform.\\nWe should not have to work with multiple files to understand a single component.\\n","Decision":"Create style guidelines based upon the following:\\n- Header with title and description\\n- Variables listed\\n- Outputs listed\\n- Full descriptions of what variables and outputs do in the code\\n- `terraform fmt` across each file\\nCreate a style guideline document explaining this structure.\\n","tokens":93,"id":2793,"Prediction":"## Decision\n\nTo ensure a consistent structure for working with Terraform files, we will adopt the \"Modular Terraform\" approach, where each file will have a specific purpose and will be organized in a way that makes it easy to understand its function, dependencies, and output.\n\n### Context\n\nAs described by the user, the current state of our Terraform files is chaotic, making it difficult to understand what each file creates, what variables it needs to run, and what it outputs after execution. This lack of consistency is causing unnecessary complexity and time spent understanding the code.\n\n### Solution\n\nTo address this issue, we will adopt the following structure for our Terraform files:\n\n1. **Module files**: Each Terraform file will be responsible for creating a specific component or resource. For example, we will have separate files for creating a VPC, a subnet, a security group, etc.\n2. **Variable files**: We will create separate files for variables that are used across multiple modules. This will help to decouple the variables from the module files and make it easier to manage and update them.\n3. **Output files**: We will create separate files for outputs that are generated by the Terraform execution. This will help to make it clear what outputs are generated by each module and make it easier to consume and use them.\n4. **Main.tf**: We will create a main.tf file that will import and configure all the modules, variables, and outputs.\n\n### Rationale\n\nBy adopting this structure, we will achieve the following benefits:\n\n* Easier to understand: Each file will have a specific purpose, making it easier to understand what it creates, what variables it needs, and what it outputs.\n* Reduced complexity: The modular approach will reduce the complexity of our Terraform code, making it easier to maintain and update.\n* Improved reusability: The separate files for variables and outputs will make it easier to reuse them across multiple modules and projects.\n* Faster development: The consistent structure will allow developers to quickly understand the code and start working on new features and projects.\n\n### Consequences\n\n* Initial complexity: There may be an initial complexity in setting up the new structure, but it will pay off in the long run.\n* Additional files: We will need to create additional files for variables, outputs, and modules, but this will make the code more organized and easier to maintain.\n\n### Status\n\nThis decision is approved and will be implemented in the next release.","GenTime":"2024-08-10 13:01:35"}
{"File Name":"akvo-product-design\/ADR-004.md","Context":"## Context\\nAs we introduce new and more complex response types, we need to come up with a consistent, homogeneous solution for serializing all the values.\\nThe historical solution for this issue has been to separate multiple values with pipes, and in some scenarios, creating a JSON-encoded response (i.e. geoshapes).\\nWe currently need to add a new `code` property to some responses, namely to cascade and option types, but we also want to avoid introducing new properties as the response content grows.\\n","Decision":"As response values are getting more and more complex, there's an increasing need for finding a common solution for all values, and adding new properties will not scale well.\\nThe decision is to extend the JSON-based schema, and encode complex answers in this packed string.\\nWe will also review the meaning of each *response type*, which will determine the encoding of each value (JSON? string? array? etc).\\nMultiple responses (barcodes, option questions, cascades, etc) will be encoded using a **JSON array**.\\n","tokens":103,"id":2802,"Prediction":"## Decision\n\n**Serialize All Values with JSON**\n\n## Context\n\nAs we introduce new and more complex response types, we need to come up with a consistent, homogeneous solution for serializing all the values. The historical solution for this issue has been to separate multiple values with pipes, and in some scenarios, creating a JSON-encoded response (i.e. geoshapes). We currently need to add a new `code` property to some responses, namely to cascade and option types, but we also want to avoid introducing new properties as the response content grows.\n\n## Problem\n\nThe current solution of using pipes to separate values is not scalable and can lead to issues with parsing and deserialization. Additionally, using JSON-encoded responses in some scenarios creates inconsistencies and makes the code harder to maintain.\n\n## Decision\n\nTo address the issue, we will serialize all values using JSON. This will provide a consistent and homogeneous solution for serializing all response values. We will use a single JSON object to represent all response data, including the new `code` property.\n\n## Rationale\n\nUsing JSON to serialize all response values provides several benefits:\n\n* Consistency: All response values will be serialized in the same format, making it easier to parse and deserialize.\n* Scalability: JSON is a widely supported and scalable format that can handle complex data structures.\n* Maintainability: Using a single format for all response values simplifies the code and reduces the risk of introducing inconsistencies.\n\n## Alternatives Considered\n\nWe considered using other serialization formats, such as XML or Protocol Buffers, but ultimately decided on JSON due to its wide adoption and simplicity.\n\n## Implementation\n\nWe will implement this decision by updating the serialization logic to use JSON for all response values. We will also update the code to handle the new `code` property and ensure that it is serialized correctly.\n\n## Consequences\n\nThis decision will provide a consistent and scalable solution for serializing response values. It will also simplify the code and reduce the risk of introducing inconsistencies. However, it may require some updates to the existing code and may introduce some complexity in terms of handling nested JSON objects.","GenTime":"2024-08-10 13:05:00"}
{"File Name":"bananatabs\/0002-react-testing-library.md","Context":"## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state\/model.\\n","Decision":"We will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\\n","tokens":111,"id":2816,"Prediction":"## Decision\n\n**Avoid testing implementation details in our integration tests and focus on verifying the state\/model instead**\n\n## Context\n\nAs we want to use `react-testing-library` to make it easier to make assertions on the rendered DOM and find\/trigger click events on different UI elements, we need to strike a balance between testing the implementation details and the expected behavior of the application.\n\n## Problem\n\nTesting implementation details can lead to tight coupling between the tests and the implementation, making it harder to change the implementation without breaking the tests. On the other hand, relying solely on `react-testing-library` for asserting the rendered DOM can make the tests too broad and less specific.\n\n## Decision\n\nTo avoid testing implementation details and ensure our integration tests are more robust and maintainable, we will focus on verifying the state\/model instead of the rendered DOM. This means that we will write tests that assert the expected state of the application after performing certain actions, rather than asserting the exact implementation details of the view.\n\n## Rationale\n\nBy verifying the state\/model, we are ensuring that the application behaves as expected, regardless of the implementation details. This approach also allows us to decouple the tests from the implementation, making it easier to change the implementation without breaking the tests.\n\n## Consequences\n\n* Our integration tests will be more focused on verifying the expected behavior of the application, rather than the implementation details.\n* We will need to write fewer tests, as we will be verifying the state\/model rather than the rendered DOM.\n* The tests will be more maintainable and easier to change if the implementation changes.\n\n## Status\n\nApproved\n\n## Links\n\n* [React Testing Library documentation](https:\/\/testing-library.com\/docs\/react-testing-library\/intro)\n* [Why you should not test implementation details](https:\/\/blog.thoughtram.io\/javascript\/2017\/09\/07\/testing-implementation-details.html)","GenTime":"2024-08-10 13:09:36"}
{"File Name":"openchs-adr\/0009-dos-and-donts-for-building-health-module-api-responses.md","Context":"## Context\\nSome rules regarding usage of health-module apis.\\n","Decision":"Input for health-modules can be anything from openchs-models\\nOutput of health-modules is loose right now. There is no reason at present to fix this.\\nIf a method returns an array of decisions, it has to return the same array everytime. For example, if it has [{\"highRisk\": ['overweight']}], even if there are no high risks detected, decisions have to return ['highRisk': []], and not an empty array.\\n","tokens":13,"id":2823,"Prediction":"## Decision\n\n**Health Module API Rate Limiting**\n\n## Context\n\nSome rules regarding usage of health-module APIs.\n\n## Problem\n\nThe health-module APIs are being used extensively across the system, and we have noticed a significant increase in the number of requests being made to these APIs. This has led to performance issues and potential errors in the system. To ensure the stability and reliability of our system, we need to implement rate limiting on these APIs to prevent abuse and excessive usage.\n\n## Decision\n\nWe will implement rate limiting on the health-module APIs to prevent excessive usage and ensure the stability of our system. We will use a token bucket algorithm to enforce rate limiting, which will allow a certain number of requests within a specified time window.\n\n## Rationale\n\nImplementing rate limiting on the health-module APIs will help to prevent abuse and excessive usage, which can lead to performance issues and errors in the system. By limiting the number of requests, we can ensure that the system remains stable and responsive to user requests.\n\n## Consequences\n\n* Implementing rate limiting on the health-module APIs will require additional development and testing to ensure that the rate limiting is correctly implemented and configured.\n* The rate limiting will need to be configured to balance the need to prevent abuse and excessive usage with the need to allow legitimate users to access the APIs.\n\n## Alternatives\n\n* Implementing IP blocking to prevent abuse and excessive usage.\n* Implementing a more complex rate limiting algorithm, such as a sliding window algorithm.\n* Not implementing rate limiting at all, which could lead to performance issues and errors in the system.\n\n## Status\n\nApproved\n\n## Links\n\n* [Rate Limiting Algorithm](https:\/\/en.wikipedia.org\/wiki\/Token_bucket)\n* [Health Module API Documentation](https:\/\/example.com\/health-module-api-documentation)","GenTime":"2024-08-10 13:12:06"}
{"File Name":"openchs-adr\/0005-deploy-openchs-server-on-the-cloud.md","Context":"## Context\\n1. Cost - Most users of OpenCHS are NGOs having 5-10 health workers. A running postgres instance and a tomcat server per installation is hard to support\/maintain.\\n2. Availability of internet - While users might not have internet connectivity in the field, they are usually attached to a head office where there is internet connectivity. They usually come to this common location regularly. Since sync is the only reason for internet connectivity, it can be limited during these visits.\\n3. Future direction - 3g\/4g is getting more common across the country, and cloud-based solutions are also getting more pervasive.\\n","Decision":"Openchs server will have capabilities to be deployed on the cloud\\n","tokens":132,"id":2824,"Prediction":"## Decision\n\nGiven the context provided, I recommend that OpenCHS adopt a cloud-based architecture with a centralized database and serverless computing to reduce costs, improve availability, and prepare for future directions.\n\n## Context\n\nThe context provided highlights the challenges of maintaining a running Postgres instance and Tomcat server per installation, which can be costly and difficult to support. Additionally, the availability of internet connectivity is limited in the field, but users have regular access to a head office with internet connectivity. This suggests that a cloud-based solution can be a viable option.\n\n## Decision Rationale\n\n1. Cost: By moving to a cloud-based architecture, we can reduce the costs associated with maintaining and supporting individual servers and databases. Cloud providers can handle the scalability and maintenance of the infrastructure, reducing the burden on OpenCHS.\n2. Availability of internet: The limited internet connectivity in the field can be addressed by allowing users to sync their data during regular visits to the head office. This can be achieved through a cloud-based solution that allows for offline data collection and syncing with the central database during these visits.\n3. Future direction: The increasing adoption of 3G\/4G and cloud-based solutions suggests that a cloud-based architecture is a future-proof direction for OpenCHS. This will allow the system to take advantage of the scalability and reliability offered by cloud providers.\n\n## Decision Description\n\nOpenCHS will adopt a cloud-based architecture with the following components:\n\n* A centralized Postgres database hosted on a cloud provider (e.g., AWS, Google Cloud, Azure)\n* A serverless computing platform (e.g., AWS Lambda, Google Cloud Functions, Azure Functions) to handle data processing and syncing\n* A cloud-based API gateway to manage data access and syncing\n\nThis architecture will allow OpenCHS to reduce costs, improve availability, and prepare for future directions. It will also enable the system to scale more easily and provide a more reliable and maintainable solution for users.","GenTime":"2024-08-10 13:12:27"}
{"File Name":"openchs-adr\/0006-use-row-level-security-in-postgres-to-implement-multitenancy.md","Context":"## Context\\nMultitenancy is required because we are now deploying openchs-server on the cloud. This can be achieved by\\n1. Multiple databases\\n2. Multiple schemas\\n3. Same database with a discriminator column\\n### Constraints\\n1. An organisation should not be able to view or update another organisation's data\\n2. There is common metadata for forms, concepts that organisations can override\\n### Other concerns\/requirements\\n1. The architecture should prevent mistakes in code to leak data from one organisation to another\\n2. Maintaining multiple copies of metadata (if using multiple dbs\/schemas) is hard\\n3. Data per organisation is low\\n","Decision":"- Use discriminators to achieve multitenancy.\\n- Enforce multitenancy using Postgres RLS policies\\n- Create a hierarchical organisation structure where organisations can read metadata from current or parent organisations, and read\/write data for their own organisation\\n- Use OpenCHS as the grandparent organisation. It will hold the default program metadata\\n","tokens":131,"id":2828,"Prediction":"## Decision\n\nTo achieve multitenancy and ensure the required constraints and other concerns are met, we will implement the same database with a discriminator column approach.\n\n### Rationale\n\nWe have considered the three options for multitenancy: multiple databases, multiple schemas, and same database with a discriminator column. After weighing the pros and cons of each option, we have decided to go with the same database with a discriminator column approach for the following reasons:\n\n* This approach is more scalable and easier to maintain, as we only need to manage a single database instance.\n* It allows for better data consistency and reduces the risk of data inconsistencies between different databases or schemas.\n* It is more suitable for our use case, as the amount of data per organization is low and we need to ensure that each organization's data is isolated from others.\n\n### Decision\n\nWe will use the same database with a discriminator column approach to implement multitenancy in our openchs-server application. This means that we will use a single database instance to store data for all organizations, and we will use a discriminator column to differentiate between data belonging to different organizations.\n\n### Trade-offs\n\nBy choosing this approach, we are trading off the potential complexity of managing multiple databases or schemas for the simplicity and scalability of a single database instance. We are also trading off the potential risk of data inconsistencies between different databases or schemas for the guarantee of data consistency provided by a single database instance.\n\n### Conclusion\n\nThe same database with a discriminator column approach is the best option for our use case, as it provides a scalable, maintainable, and consistent solution for implementing multitenancy in our openchs-server application.","GenTime":"2024-08-10 13:13:41"}
{"File Name":"openchs-adr\/0012-create-a-generic-relationship-framework-to-link-between-mother-and-child.md","Context":"## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n","Decision":"Create an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\\n","tokens":103,"id":2829,"Prediction":"## Decision\n\n**Decision:** Implement a generic \"Relationship\" entity in the system, which can be used to represent both the parent-child relationship in the mother program and the family relationships.\n\n**Context:** As described in the context, we need to create a generic structure to model relationships that can support both the use cases of switching between children during delivery and PNC visits, and linking individuals to the head of the household in the family concept.\n\n**Rationale:**\n\n* A generic \"Relationship\" entity allows us to decouple the specific relationship types from the implementation, making it easier to add new relationship types in the future.\n* This approach also enables us to reuse the same relationship modeling across different parts of the system, reducing code duplication and improving maintainability.\n* By using a single entity for all relationships, we can ensure consistency in the way relationships are represented and managed throughout the system.\n\n**Consequences:**\n\n* The system will require a more complex data model to accommodate the generic relationship entity.\n* The implementation will need to include additional logic to determine the specific type of relationship being represented (e.g., parent-child, family member).\n* The system will need to provide a way to create and manage relationships between entities, which may require additional user interface elements and business logic.\n\n**Alternatives Considered:**\n\n* Implementing separate entities for each specific relationship type (e.g., ParentChildRelationship, FamilyRelationship).\n* Using a different data modeling approach, such as using a separate table for each relationship type.\n\n**Status:** Accepted\n\n**Implementation:** The generic \"Relationship\" entity will be implemented as a separate entity in the system, with a foreign key referencing the entities involved in the relationship. The specific relationship type will be determined by the attributes and associations defined on the relationship entity.","GenTime":"2024-08-10 13:14:00"}
{"File Name":"kafkarator\/0003-kafkarator-api-is-focused-around-dedicated-crds.md","Context":"## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n","Decision":"- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n","tokens":151,"id":2834,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will design the Kafkarator API as a set of multiple, separate Custom Resource Definitions (CRDs), each responsible for a specific concern or configuration. This approach will allow for flexibility, scalability, and maintainability, while also aligning with the existing NAIS deploy mechanism.\n\n**Context**\n\nAs discussed, application developers need an API to interact with Kafkarator. The previous vagueness about the API's design has hindered progress, and we need to make a decision to move forward. Our findings suggest that we should consider multiple CRDs, given the possibility of applying multiple resources during NAIS deploy and the need to separate two distinct concerns.\n\n**Rationale**\n\nBy using multiple CRDs, we can:\n\n1. Keep each concern or configuration separate and maintainable, reducing complexity and increasing reusability.\n2. Allow for flexibility in the API design, enabling us to add or remove CRDs as needed without affecting the overall architecture.\n3. Align with the existing NAIS deploy mechanism, making it easier for developers to interact with Kafkarator.\n\n**Consequences**\n\n1. Increased complexity in the short term, as we need to design and implement multiple CRDs.\n2. Improved maintainability and scalability in the long term, as each CRD can be updated or extended independently.\n3. A more flexible and adaptable API design, allowing for easier integration with other components and systems.\n\n**Decision Drivers**\n\n1. The need for a clear and well-defined API design to facilitate interaction with Kafkarator.\n2. The requirement for flexibility and scalability in the API design.\n3. The alignment with the existing NAIS deploy mechanism.\n\n**Assumptions and Dependencies**\n\n1. The NAIS deploy mechanism will be used to apply resources to the cluster.\n2. The multiple CRDs will be designed to be separate and distinct, with clear boundaries and responsibilities.\n\n**Pros and Cons**\n\nPros:\n\n* Improved maintainability and scalability\n* Increased flexibility and adaptability\n* Alignment with existing NAIS deploy mechanism\n\nCons:\n\n* Increased complexity in the short term\n* Potential for confusion or overlap between CRDs\n\n**Next Steps**\n\n1. Design and implement the multiple CRDs for the Kafkarator API.\n2. Detail the API's behavior and expected interactions.\n3. Develop documentation and guidelines for developers to use the API effectively.","GenTime":"2024-08-10 13:15:19"}
{"File Name":"kafkarator\/0004-only-for-aiven.md","Context":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n","Decision":"Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","tokens":70,"id":2837,"Prediction":"## Decision\n\n**Decision:** To use Aiven's hosted Kafka service as the Kafka broker for Kafkarator, instead of maintaining an on-premise Kafka cluster.\n\n**Context:** As NAV plans to move everything to the cloud and eliminate the need for maintaining an on-premise Kafka cluster, using Aiven's hosted Kafka service will provide a scalable and managed solution for Kafka.\n\n**Problem:** Maintaining an on-premise Kafka cluster requires significant resources and effort, which can be a hindrance to the team's productivity and ability to focus on other important tasks.\n\n**Reasoning:** By using Aiven's hosted Kafka service, Kafkarator will be able to take advantage of a scalable and managed Kafka broker, which will reduce the team's administrative burden and allow them to focus on developing and integrating applications with Kafka. Additionally, Aiven's service provides features such as automatic backups, monitoring, and security, which will provide an additional layer of protection and reliability for the Kafka cluster.\n\n**Consequences:**\n\n* Reduced administrative burden for the team\n* Improved scalability and reliability of the Kafka cluster\n* Increased focus on developing and integrating applications with Kafka\n* Simplified maintenance and troubleshooting of the Kafka cluster\n\n**Alternatives Considered:** The team considered maintaining an on-premise Kafka cluster, but this would require significant resources and effort. They also considered using a different cloud-based Kafka service, but Aiven's service was deemed to be the best fit for their needs.\n\n**Status:** Implemented\n\n**Effective Date:** [Insert date]\n\n**Responsible:** [Insert name]\n\n**Verified By:** [Insert name]\n\n**Review Frequency:** [Insert frequency]","GenTime":"2024-08-10 13:16:13"}
{"File Name":"marsha\/0002-videos-languages.md","Context":"## Context\\nWe want to think Marsha as accessible from the beginning. At least from\\nthe point of view of the videos, which are the main content available.\\nWe can think about a video as a main content, with many auxiliary\\ncontents.\\n### Auxiliary contents\\n#### Audio\\nWe have a main video, with an audio track included. The author could\\npropose many other audio tracks, as audio files, and in the player the\\nviewer can change the one to use.\\n#### Timed texts\\nIn addition to audio tracks, many timed text tracks can be available.\\n#### Sign language\\nSome people with disabilities could want a video with the sign language\\ntranscript. For this it can be a video incorporated in the original one,\\nor an other video displayed on the site.\\nAs sign languages are not the same for every spoken language, there can\\nbe several sign languages videos for a single video.\\n","Decision":"We decided to take all these elements into account right from the\\nbeginning.\\nSo we have a main Django model named `Video`, from an author, with the\\nlink to the main video file, including the default audio track.\\nFor the other audio tracks, we have an `AudioTrack` Django model, with a\\n`ForeignKey` to the `Video` instance, named `video`, and a `language`\\nfield (with only one audio track for each video+language)\\nIt's the same for timed text tracks, we have a `TimedTextTrack` Django model,\\nwith the same `video` and `language` fields, but with an additional `mode`\\nfield to indicate that this timed text track is either a simple subtitle,\\na \"[closed\\ncaptioning](https:\/\/en.wikipedia.org\/wiki\/Closed_captioning)\" ie subtitles\\nfor deaf or hard of hearing viewers, or a transcript. So there can be up to\\n3 timed text tracks for each video+language: one for each mode.\\nAnd finally, for sign-languages videos, it's the same as for audio\\ntracks: a Django model named `SignTrack` with the same `video` and\\n`language` field.\\n","tokens":191,"id":2843,"Prediction":"## Decision\n\n**Decision:** To implement a modular and extensible architecture for handling auxiliary contents in Marsha, specifically for videos, to ensure accessibility from the beginning.\n\n**Context:** As discussed, we want to prioritize accessibility in Marsha, particularly for videos, which are the main content available. This includes handling auxiliary contents such as audio tracks, timed texts, and sign language transcripts.\n\n**Problem:** To provide a seamless experience for users with disabilities, we need to ensure that these auxiliary contents are easily accessible and can be switched between. However, this requires a flexible and modular architecture that can handle various types of auxiliary contents and languages.\n\n**Reasoning:** A modular architecture will allow us to easily add or remove auxiliary contents as needed, without affecting the core video player. This will also enable us to support multiple languages and formats for sign language transcripts. By separating the concerns of each auxiliary content, we can ensure that each one is properly handled and displayed correctly.\n\n**Decision Criteria:**\n\n1. **Accessibility**: By implementing a modular architecture, we can ensure that auxiliary contents are easily accessible and can be switched between, providing a better experience for users with disabilities.\n2. **Flexibility**: A modular architecture will allow us to easily add or remove auxiliary contents as needed, without affecting the core video player.\n3. **Scalability**: This architecture will enable us to support multiple languages and formats for sign language transcripts, making it more scalable for future growth.\n\n**Consequences:**\n\n1. **Increased complexity**: Implementing a modular architecture will require additional development effort and complexity.\n2. **Improved accessibility**: By prioritizing accessibility from the beginning, we can ensure that Marsha provides a better experience for users with disabilities.\n3. **Future-proofing**: This architecture will enable us to easily adapt to new auxiliary content formats and languages, ensuring that Marsha remains accessible and usable for a wider range of users.\n\n**Implementation:** We will implement a modular architecture using a combination of micro-frontends and a centralized API. Each auxiliary content will be handled by a separate micro-frontend, which will communicate with the central API to retrieve and display the content. This will allow us to easily add or remove auxiliary contents as needed, while ensuring that the core video player remains unaffected.","GenTime":"2024-08-10 13:16:56"}
{"File Name":"protagonist\/0001-composite-handler-design.md","Context":"## Context and Problem Statement\\n### Context\\nA design for the implementation and delivery of a component of the DLCS\\nwhich can process \"composite\" artifacts. Such artifacts typically take\\nthe form of a single object containing multiple embedded resources, each\\nof which can be rasterized and treated as an individual resource within\\nthe DLCS.\\n","Decision":"Delivery of a standalone component which consumes a DLCS ingest payload\\ntailored to \"composite\" artifacts, and triggers an ingestion workflow\\nwhich rasterizes each of the constituent parts and invokes the existing\\nDLCS ingestion workflow.\\n![0001-flowchart.png](images\/0001-flowchart.png)\\n","tokens":69,"id":2859,"Prediction":"## Decision\n\n### Decision\n\nWe will implement the composite artifact processing component using a hierarchical approach, where the top-level artifact is treated as a container for its embedded resources.\n\n### Context\n\nAs mentioned in the context, the DLCS requires a component that can process \"composite\" artifacts, which are complex objects containing multiple embedded resources. These resources need to be processed individually, such as rasterized, and treated as separate resources within the DLCS.\n\n### Problem Statement\n\nThe main challenge is to design a component that can efficiently and effectively process these complex artifacts, while also ensuring that the embedded resources are properly extracted, processed, and stored.\n\n### Decision Rationale\n\nAfter considering various alternatives, we have decided to implement the component using a hierarchical approach. This approach allows us to:\n\n1. Treat the top-level artifact as a container, which simplifies the processing and storage of the embedded resources.\n2. Use a recursive algorithm to extract and process the embedded resources, ensuring that each resource is processed individually.\n3. Utilize the hierarchical structure to optimize the processing and storage of the resources, reducing the overall complexity and improving performance.\n\n### Trade-Offs\n\nThe hierarchical approach has some trade-offs, including:\n\n1. Increased complexity in the component's architecture, due to the recursive processing and storage of the embedded resources.\n2. Potential performance overhead, as the component needs to traverse the hierarchical structure to extract and process the resources.\n\n### Consequences\n\nThe consequences of this decision are:\n\n1. The component will be able to efficiently process complex artifacts with multiple embedded resources.\n2. The embedded resources will be properly extracted, processed, and stored, ensuring that they can be treated as individual resources within the DLCS.\n3. The hierarchical approach will allow for optimized processing and storage of the resources, reducing the overall complexity and improving performance.\n\n### Alternatives Considered\n\nWe considered alternative approaches, such as:\n\n1. Flattening the hierarchical structure and processing the embedded resources as a flat list.\n2. Using a separate component for each embedded resource, which would increase the overall complexity and overhead.\n\nHowever, the hierarchical approach was deemed the most suitable solution, as it allows for efficient and effective processing of the complex artifacts while minimizing the complexity and overhead.","GenTime":"2024-08-10 13:22:27"}
{"File Name":"james-project\/0032-distributed-mail-queue-cleanup.md","Context":"## Context\\nRead [Distributed Mail Queue](0031-distributed-mail-queue.md) for full context.\\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\\nideal both from a privacy and space storage costs point of view.\\nNote that **enqueuedMailsV3** and **deletedMailsV2** rely on timeWindowCompactionStrategy.\\n","Decision":"Add a new `contentStart` table referencing the point in time from which a given mailQueue holds data, for each mail queue.\\nThe values contained between `contentStart` and `browseStart` can safely be deleted.\\nWe can perform this cleanup upon `browseStartUpdate`: once finished we can browse then delete content of **enqueuedMailsV3**\\nand **deletedMailsV2** contained between `contentStart` and the new `browseStart` then we can safely set `contentStart`\\nto the new `browseStart`.\\nContent before `browseStart` can safely be considered deletable, and is applicatively no longer exposed. We don't need an\\nadditional grace period mechanism for `contentStart`.\\nFailing cleanup will lead to the content being eventually updated upon next `browseStart` update.\\nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueue\\nmanagement APIs.\\n","tokens":97,"id":2880,"Prediction":"## Decision\n\n**Implement a periodic cleanup job to remove stale and unreferenced blobs from enqueuedMailsV3 and deletedMailsV2**\n\n## Context\n\nAs described in the Distributed Mail Queue document, the enqueuedMailsV3 and deletedMailsV2 blobs are never cleaned up, leading to a buildup of stale and unreferenced data. This is a concern from both a privacy and storage cost perspective.\n\n## Problem\n\nThe current design does not provide a mechanism for periodically cleaning up stale and unreferenced blobs, which can lead to:\n\n* Data accumulation and increased storage costs\n* Potential privacy risks if sensitive information is stored in the blobs\n* Inefficient use of resources, as the timeWindowCompactionStrategy is not being utilized effectively\n\n## Decision\n\nTo address this issue, we will implement a periodic cleanup job that regularly scans the enqueuedMailsV3 and deletedMailsV2 blobs for stale and unreferenced data. This job will:\n\n* Identify blobs that are older than a certain threshold (e.g., 30 days) and have no active references\n* Remove these blobs from storage to free up space and reduce storage costs\n* Update the timeWindowCompactionStrategy to reflect the cleaned-up data\n\n## Rationale\n\nImplementing a periodic cleanup job will help to maintain a healthy and efficient storage system by removing stale and unreferenced data. This decision aligns with the overall goal of the Distributed Mail Queue system, which is to provide a scalable and reliable mail processing system.\n\n## Consequences\n\n* The cleanup job will require additional resources (e.g., compute, storage) to run periodically\n* The job will need to be scheduled and monitored to ensure it runs successfully and without errors\n* The design of the timeWindowCompactionStrategy may need to be adjusted to accommodate the cleaned-up data\n\n## Alternatives Considered\n\n* Implementing a real-time cleanup mechanism, which would require significant changes to the system's architecture and may introduce additional complexity\n* Leaving the data accumulation unchecked, which would lead to increased storage costs and potential privacy risks\n\nBy implementing a periodic cleanup job, we can strike a balance between data retention and storage efficiency, while also ensuring the system remains scalable and reliable.","GenTime":"2024-08-10 13:29:13"}
{"File Name":"james-project\/0009-disable-elasticsearch-dynamic-mapping.md","Context":"## Context\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\nThis causes a serie of functional issues:\\n- Maximum field count can easily be exceeded\\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\n","Decision":"Rely on nested objects to represent mail headers within a mapping\\n","tokens":96,"id":2884,"Prediction":"## Decision\n\n**Dynamic Mapping for Exposing Mail Headers as JSON Map**\n\n## Context\n\nAs described, we currently rely on dynamic mappings to expose our mail headers as a JSON map. This approach allows for adding not yet encountered headers in the mapping. However, this has led to a series of functional issues, including:\n\n* Maximum field count can easily be exceeded\n* Field type 'guess' can be wrong, leading to subsequent headers omissions\n* Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively on performance\n\n## Problem\n\nThe current approach to dynamic mapping is causing issues with the scalability and reliability of our system. The maximum field count being exceeded can lead to errors and inconsistencies in our data processing. The field type 'guess' being wrong can result in important headers being omitted, leading to incomplete data. Additionally, the need to pause document indexation during mapping changes is causing performance issues and impacting the overall responsiveness of our system.\n\n## Decision\n\nTo address these issues, we will switch to using a fixed mapping for exposing mail headers as a JSON map. This will allow us to:\n\n* Define a specific set of fields and their corresponding data types, avoiding the maximum field count issue\n* Ensure accurate field types, eliminating the possibility of headers being omitted\n* Remove the need to pause document indexation during mapping changes, improving performance and responsiveness\n\n## Rationale\n\nBy using a fixed mapping, we can ensure the accuracy and consistency of our data processing. This approach will also improve the scalability and reliability of our system, reducing the likelihood of errors and inconsistencies.\n\n## Consequences\n\n* We will need to review and update our mapping configuration to define the specific fields and data types required for our mail headers\n* We will need to update our code to use the fixed mapping instead of dynamic mapping\n* We may need to perform additional testing to ensure that our system is functioning correctly with the new mapping configuration\n\n## Status\n\nThis decision has been approved and will be implemented in the next release.","GenTime":"2024-08-10 13:30:38"}
{"File Name":"james-project\/0047-jmap-push-over-websockets.md","Context":"## Context\\nJMAP Push notifications allow clients to efficiently update (almost) instantly to stay in sync with data changes on the server.\\nIn order to support this, we need to have the server emit newest state changes to the client over a PUSH channel.\\n","Decision":"We will implement [RFC-8887 - A JSON Meta Application Protocol (JMAP) Subprotocol for WebSocket](https:\/\/tools.ietf.org\/html\/rfc8887)\\nas a transport mechanism for PUSH notifications.\\nWe will [generalize EventBus](0046-generalize-event-bus.md) in order to achieve an out-of-the box Publish-Subscribe\\nsystem for JMAP related events, that does not fit in the mailbox API.\\nWe will implement listeners registered on the JMAP event bus for WebSocket clients in order to carry over state changes\\nto the client.\\n","tokens":53,"id":2886,"Prediction":"## Decision\n\nTo support JMAP Push notifications, we will implement a server-side mechanism to emit newest state changes to the client over a PUSH channel, utilizing WebSockets or Server-Sent Events (SSE) technologies. This decision is based on the need to efficiently update clients in real-time to ensure data consistency and provide a seamless user experience.\n\n## Context\n\nJMAP Push notifications enable clients to receive instant updates on data changes on the server, ensuring that the client's state is always in sync with the server's state. To achieve this, the server must be able to push the newest state changes to the client over a PUSH channel.\n\n## Rationale\n\nWe have evaluated several options for implementing the PUSH channel, including WebSockets, Server-Sent Events (SSE), and long polling. After considering the trade-offs between each option, we have decided to use WebSockets or SSE for the following reasons:\n\n* WebSockets and SSE provide a bi-directional communication channel between the server and client, allowing for efficient and real-time updates.\n* Both technologies are well-supported by modern browsers and can be used to establish a persistent connection between the client and server.\n* WebSockets and SSE provide a low-latency and efficient way to push updates to the client, making them suitable for real-time applications.\n\n## Decision Drivers\n\n* The need for real-time updates to ensure data consistency and provide a seamless user experience.\n* The requirement for efficient and scalable communication between the server and client.\n* The need to support multiple clients and handle a high volume of requests.\n\n## Consequences\n\n* The implementation of a server-side mechanism to emit newest state changes to the client over a PUSH channel.\n* The use of WebSockets or SSE technologies to establish a bi-directional communication channel between the server and client.\n* The potential for increased server load and complexity due to the need to manage multiple client connections and push updates in real-time.\n\n## Alternatives Considered\n\n* Long polling: While long polling can be used to establish a PUSH channel, it is less efficient and can lead to increased server load and slower updates.\n* WebRTC: WebRTC provides a more comprehensive set of APIs for real-time communication, but it is more complex to implement and may not be necessary for our specific use case.\n\n## Implementation\n\nThe implementation will involve the following steps:\n\n1. Set up a WebSocket or SSE server-side endpoint to handle client connections and push updates.\n2. Implement a mechanism to detect and emit state changes on the server-side.\n3. Establish a bi-directional communication channel between the client and server using WebSockets or SSE.\n4. Use the communication channel to push updates to the client in real-time.\n\nBy implementing a server-side mechanism to emit newest state changes to the client over a PUSH channel, we will be able to provide a seamless and efficient user experience, ensuring that the client's state is always in sync with the server's state.","GenTime":"2024-08-10 13:31:31"}
{"File Name":"james-project\/0017-file-mail-queue-deprecation.md","Context":"## Context\\nJames offers several implementation for MailQueue, a component allowing asynchronous mail processing upon smtp mail\\nreception. These includes:\\n- Default embedded ActiveMQ mail queue implementation, leveraging the JMS APIs and using the filesystem.\\n- RabbitMQMailQueue allowing several James instances to share their MailQueue content.\\n- And FileMailQueue directly leveraging the file system.\\nWe introduced a junit5 test contract regarding management features, concurrency issues, and FileMailQueue do not meet this\\ncontract. This results in some tests being disabled and in an unstable test suite.\\nFileMailQueue tries to implement a message queue within James code, which does not really makes sense as some other projects\\nalready provides one.\\n","Decision":"Deprecate FileMailQueue components.\\nDisable FileMailQueue tests.\\nTarget a removal as part of 3.6.0.\\n","tokens":146,"id":2888,"Prediction":"## Decision\n\n**Decision:** We will remove the FileMailQueue implementation from James and instead use the RabbitMQMailQueue implementation as the default implementation for MailQueue.\n\n**Context:** As described in the context, the FileMailQueue implementation does not meet the test contract and is not a good fit for James as it tries to implement a message queue within James code, which is not its primary responsibility.\n\n**Rationale:** The RabbitMQMailQueue implementation has already proven to be a reliable and scalable solution for sharing MailQueue content across multiple James instances. By using this implementation as the default, we can ensure that our tests are stable and reliable, and that our users have a consistent and high-quality experience.\n\n**Consequences:**\n\n* We will remove the FileMailQueue implementation from James and its related tests.\n* We will update the default MailQueue implementation to use RabbitMQMailQueue.\n* We will refactor the existing code to use RabbitMQMailQueue instead of FileMailQueue.\n* We will ensure that the RabbitMQMailQueue implementation meets the test contract and is thoroughly tested.\n\n**Alternatives Considered:**\n\n* We could have tried to fix the FileMailQueue implementation to meet the test contract, but this would have required significant refactoring and might not have provided a reliable solution.\n* We could have introduced a new implementation that tries to improve upon the existing FileMailQueue implementation, but this would have added complexity and uncertainty to the project.\n\n**Status:** This decision has been approved and will be implemented in the next release of James.","GenTime":"2024-08-10 13:32:05"}
{"File Name":"james-project\/0031-distributed-mail-queue.md","Context":"## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n","Decision":"Distributed James should ship a distributed MailQueue composing the following softwares with the following\\nresponsibilities:\\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\\nas well in term of Input\/Output operation per seconds.\\nHere are details of the tables composing Cassandra MailQueue View data-model:\\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\\nthe emails that have ever been in the mailQueue. Its content is never deleted.\\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\\nto filter out deleted\/purged items.\\n- **browseStart** store the latest known point in time from which all previous emails had been deleted\/dequeued. It\\nenables to skip most deleted items upon browsing\/deleting queue content. Its update is probability based and\\nasynchronously piggy backed on dequeue.\\nHere are the main mail operation sequences:\\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\\nis fired on *rabbitMQ*.\\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\\nuntil the first non deleted \/ dequeued email is found. This point becomes the new browse start. BrowseStart can never\\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\\ncurrent browse start.\\n- Upon **delete\/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\\nthe condition are marked as deleted in *enqueuedMailsV3*.\\n- Upon **getSize**, we perform a browse and count the returned elements.\\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\\nand of the mailQueue throughput:\\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\\ndecreasing the bucket count might result in some buckets to be lost.\\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue\/deletes. We recommend choosing\\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\\niterating through slices with all their content deleted. This value can be changed freely.\\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\\n","tokens":321,"id":2892,"Prediction":"## Decision\n\n**Decision:** To use Apache Kafka as the messaging broker for the distributed MailQueue implementation, instead of RabbitMQ, to provide more advanced administrative operations and better support for distributed administration.\n\n## Context\n\nAs mentioned in the context, the current distributed James implementation relies on RabbitMQ for messaging, which is good at holding a queue but lacks support for advanced administrative operations such as `browse`, `getSize`, and `arbitrary mail removal`. Additionally, the use of Cassandra as the metadata database introduces complexity and anti-patterns, such as the use of tombstones for delete operations.\n\n## Problem\n\nThe current implementation of the MailQueue using RabbitMQ and Cassandra does not provide a seamless and efficient way to administer the queue, especially in a distributed setup. The lack of support for advanced administrative operations and the complexity of the metadata database make it difficult for administrators to manage the queue effectively.\n\n## Decision Rationale\n\nApache Kafka is a more suitable choice for the MailQueue implementation due to its ability to provide more advanced administrative operations, such as:\n\n* Browse: Kafka allows administrators to browse the queue and retrieve information about the messages it contains.\n* Size: Kafka provides a way to retrieve the size of the queue, which is useful for monitoring and troubleshooting purposes.\n* Arbitrary mail removal: Kafka allows administrators to remove messages from the queue arbitrarily, which is useful for cleaning up the queue and resolving issues.\n\nKafka is also a more scalable and fault-tolerant solution than RabbitMQ, making it a better choice for a distributed MailQueue implementation. Additionally, Kafka's use of a distributed architecture and its ability to handle high volumes of data make it well-suited for handling the large amounts of email data that the MailQueue will process.\n\n## Trade-offs\n\nThe use of Apache Kafka instead of RabbitMQ will require additional resources and infrastructure to set up and maintain. However, the benefits of improved administrative operations and scalability make it a worthwhile investment.\n\n## Conclusion\n\nThe decision to use Apache Kafka as the messaging broker for the distributed MailQueue implementation is based on the need for improved administrative operations and scalability. Kafka's ability to provide advanced administrative operations, such as browse, size, and arbitrary mail removal, makes it a more suitable choice than RabbitMQ. Additionally, Kafka's scalability and fault-tolerance make it a better choice for a distributed MailQueue implementation.","GenTime":"2024-08-10 13:33:33"}
{"File Name":"james-project\/0025-cassandra-blob-store-cache.md","Context":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2896,"Prediction":"## Decision\n\nTo address the issue of high latency for reading small blobs from the Object Storage, we will use a hybrid approach that combines the benefits of both Object Storage and a caching layer.\n\n## Context\n\nAs James mentioned, the BlobStore API is used to store raw data that varies in size and access patterns. While Object Storage is suitable for storing large blobs, it induces high latencies for reading small blobs, which can impact the performance of certain operations such as fetching mailbox message headers or listing JMAP messages.\n\n## Problem\n\nThe current implementation of the BlobStore API relies solely on Object Storage, which results in high latencies for small blobs. This can lead to slow performance for certain operations, which can negatively impact the user experience.\n\n## Decision\n\nTo address this issue, we will implement a hybrid approach that combines the benefits of both Object Storage and a caching layer. Specifically, we will:\n\n1. Store small blobs (< 1MB) in a caching layer (e.g. Redis or Memcached) to reduce latency.\n2. Store larger blobs (> 1MB) in Object Storage to take advantage of its scalability and durability.\n3. Implement a cache invalidation mechanism to ensure that the caching layer is updated when the underlying data changes.\n\n## Rationale\n\nBy using a caching layer for small blobs, we can reduce the latency associated with accessing these blobs from Object Storage. This will improve the performance of operations that frequently access small blobs, such as fetching mailbox message headers or listing JMAP messages. Additionally, by storing larger blobs in Object Storage, we can take advantage of its scalability and durability.\n\n## Consequences\n\n* We will need to implement a cache invalidation mechanism to ensure that the caching layer is updated when the underlying data changes.\n* We will need to monitor the cache hit ratio and adjust the caching layer configuration as needed to ensure optimal performance.\n* We will need to consider the trade-offs between cache size, cache hit ratio, and storage costs when designing the caching layer.\n\n## Alternatives Considered\n\n* We considered using a single storage solution for all blobs, but this would not address the latency issue for small blobs.\n* We considered using a separate caching layer for each type of blob, but this would add complexity and make it harder to manage the caching layer.\n\nBy implementing a hybrid approach that combines the benefits of both Object Storage and a caching layer, we can improve the performance of the BlobStore API and provide a better user experience.","GenTime":"2024-08-10 13:34:59"}
{"File Name":"james-project\/0041-replace-jcloud-with-s3.md","Context":"## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n","Decision":"* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n","tokens":209,"id":2897,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will refactor the Object Storage blob-store implementation to use the AWS SDK for Java instead of Apache jcloud library. This decision is made to simplify the implementation, improve build time, and align with the Reactive style of the codebase.\n\n**Context**\n\nThe current implementation of Object Storage blob-store using Apache jcloud library is complex, does not fit the Reactive style of the codebase, and negatively impacts the project build time. We have also found that Swift provides a good compatibility layer for S3, allowing us to potentially drop our Swift code without losing support for Swift.\n\n**Problem**\n\nThe current implementation of Object Storage blob-store is complex and does not fit the Reactive style of the codebase, making it difficult to maintain and integrate with the rest of the system. Additionally, it negatively impacts the project build time, which can lead to decreased developer productivity and increased risk of errors.\n\n**Decision**\n\nWe will refactor the Object Storage blob-store implementation to use the AWS SDK for Java, which is a more straightforward and efficient way to interact with S3 and Swift Object Storage. This decision is made to simplify the implementation, improve build time, and align with the Reactive style of the codebase.\n\n**Consequences**\n\n* The implementation will be simplified and easier to maintain.\n* The project build time will be improved.\n* The code will be more aligned with the Reactive style of the codebase.\n* We may potentially drop our Swift code without losing support for Swift.\n\n**Alternatives Considered**\n\n* We considered keeping the current implementation and trying to simplify it, but this would likely be a complex and time-consuming task.\n* We considered using a different library or framework, but the AWS SDK for Java is a well-established and widely-used solution that provides a good balance of simplicity and functionality.\n\n**Status**\n\nApproved","GenTime":"2024-08-10 13:35:19"}
{"File Name":"banking-cqrs-es-go\/0002-use-adr-tools-to-manage-the-adrs.md","Context":"## Context\\nA tool to make creating these ADRs helps reduce friction around creating documentation.\\n`adr-tools` is a simple cli tool that has been used in the past on previous projects to great effect (https:\/\/github.com\/npryce\/adr-tools)\\n","Decision":"Install `adr-tools` locally and use it to help quickly generate consistent ADRs\\n","tokens":56,"id":2914,"Prediction":"## Decision\n\nWe will use `adr-tools` to create and manage Architectural Decision Records (ADRs) for our software.\n\n## Context\n\nAs mentioned, `adr-tools` is a simple CLI tool that has been used in the past on previous projects to great effect. This tool helps reduce friction around creating documentation by providing a structured approach to writing ADRs. It also allows us to easily track and manage our architectural decisions, which is essential for maintaining a clear and consistent understanding of our software's design.\n\n## Problem\n\nCurrently, we do not have a standardized way of creating and managing ADRs, which can lead to inconsistencies and difficulties in understanding the design decisions behind our software.\n\n## Solution\n\nBy using `adr-tools`, we can create and manage ADRs in a structured and consistent manner. This will help us to:\n\n* Clearly document our design decisions\n* Easily track and manage our architectural decisions\n* Reduce friction around creating documentation\n* Improve communication among team members and stakeholders\n\n## Consequences\n\nBy using `adr-tools`, we will need to invest some time in setting up the tool and learning how to use it effectively. However, the benefits of having a standardized and structured approach to creating and managing ADRs will far outweigh the costs.\n\n## Rationale\n\nWe chose `adr-tools` because it is a widely-used and well-established tool in the software development community. It provides a simple and intuitive way to create and manage ADRs, and it is easy to integrate into our existing development workflow.","GenTime":"2024-08-10 13:42:12"}
{"File Name":"SAP-Cloud\/node-package-lock.md","Context":"## Context\\nThe node package manager (npm) creates a so called [\"lock file\"](https:\/\/docs.npmjs.com\/files\/package-locks) when running `npm install`.\\nThe recommended action is to commit this file to version control, as stated by the command itself:\\n```\\n$ npm install\\nnpm notice created a lockfile as package-lock.json. You should commit this file.\\n```\\nSome npm commands, which are part of the pipeline, such as `ci` and `audit` require that a package lock file exists.\\nThus, if a project lacks the package lock file, there are tree options:\\n* Refuse to build the project, which enforces the recommended practice of committing the lock file\\n* Silently create a package lock file as part of the pipeline\\n* Avoid npm commands which require a package lock\\n* Would disable the npm audit stage\\n","Decision":"If a project has a `package.json` file, but no package lock, we create a package lock file as part of the pipeline and warn the user about this.\\nThe created package lock file is not archived as part of the pipeline build artifacts.\\n","tokens":183,"id":2924,"Prediction":"## Decision\n\nTo ensure consistency and adherence to best practices, we will refuse to build the project if a package lock file does not exist. This decision is based on the recommendation from npm to commit the package lock file to version control and the requirement of some npm commands, such as `ci` and `audit`, to have a package lock file present.\n\n## Rationale\n\nCommitting the package lock file to version control ensures that the project's dependencies are accurately recorded and can be easily reproduced in the future. By refusing to build the project if a package lock file does not exist, we are enforcing this best practice and ensuring that the project is properly configured.\n\n## Consequences\n\n* Projects without a package lock file will not be built, which may cause issues for developers who are not aware of the requirement.\n* Developers will need to ensure that they commit the package lock file to version control after running `npm install`.\n* Some npm commands, such as `ci` and `audit`, will not be able to run without a package lock file, which may require additional setup or configuration.\n\n## Alternatives Considered\n\n* Silently creating a package lock file as part of the pipeline: This approach would allow projects to be built without a package lock file, but would not ensure that the file is committed to version control.\n* Avoiding npm commands that require a package lock file: This approach would disable the npm audit stage, which is an important security feature.\n* Creating a package lock file as part of the pipeline and allowing projects to be built without it: This approach would not ensure that the package lock file is committed to version control and would not enforce the recommended practice.","GenTime":"2024-08-10 13:45:21"}
{"File Name":"deeplearning4j\/0007 - Nd4j classifiers.md","Context":"## Context\\nNd4j relies upon the c++ library [libnd4j](..\/libnd4j) for native math execution.\\nIt uses [javacpp](https:\/\/github.com\/bytedeco\/javacpp) to link against\\nlibnd4j. Libnd4j is capable of being compiled a myriad of different ways allowing different trade offs to be made\\nin terms of performance and dependencies. This presents complexity in exposing this flexibility to the end user.\\n","Decision":"In order to allow users to pick which configuration they would like to use, while avoiding adding a lot of different artifact\\nids to the project, the following javacpp platform extensions are used:\\ncompiled type (avx etc or blank if normal) - software linked against (cudnn, onednn, armcompute) - version\\nAn example for the one dnn platform extension could be:\\ndnnl-2.2\\navx256-dnnl-2.2\\nThis presents 2 examples where a special compilation is enabled and one where it's not\\nboth linking against dnnl\/onednn 2.2.\\n","tokens":99,"id":2932,"Prediction":"## Decision\n\nTo simplify the exposure of libnd4j's compilation options to the end user, we will introduce a configuration system that allows users to specify their desired compilation options through a simple and intuitive API. This configuration system will be based on a set of predefined profiles, each corresponding to a specific set of compilation options.\n\n## Context\n\nAs mentioned, libnd4j is capable of being compiled in various ways, allowing for different trade-offs between performance and dependencies. However, this flexibility also presents complexity in exposing this flexibility to the end user. Currently, users need to have a deep understanding of the underlying compilation options and their implications, which can be a barrier to entry for new users.\n\n## Problem\n\nThe current approach of using javacpp to link against libnd4j requires users to manually specify the compilation options, which can be error-prone and difficult to manage. This can lead to confusion and frustration among users, especially those who are new to the project.\n\n## Decision\n\nTo address this issue, we will introduce a configuration system that allows users to specify their desired compilation options through a simple and intuitive API. This configuration system will be based on a set of predefined profiles, each corresponding to a specific set of compilation options. Users will be able to choose from a set of pre-defined profiles, such as \"performance\", \"dependencies\", or \"debug\", or create their own custom profile.\n\n## Consequences\n\n* The configuration system will simplify the exposure of libnd4j's compilation options to the end user, making it easier for users to choose the right compilation options for their use case.\n* The system will reduce the complexity of using libnd4j, making it more accessible to new users.\n* The system will provide a more flexible and customizable way of specifying compilation options, allowing users to tailor their compilation options to their specific needs.\n* The system will require additional development effort to implement the configuration system and the predefined profiles.\n\n## Rationale\n\nThe configuration system will provide a simple and intuitive way for users to specify their desired compilation options, making it easier for them to get started with libnd4j. The predefined profiles will provide a clear and concise way for users to choose the right compilation options for their use case, reducing the complexity of using libnd4j.","GenTime":"2024-08-10 13:47:54"}
{"File Name":"deeplearning4j\/0008 - Nd4j eager  shape computation .md","Context":"## Context\\nNd4j's model import framework often has the need to\\ncompute  shapes as variables are created.\\nThis is  in order to resolve how to properly\\ncreate a graph based on a graph descriptor from another framework\\nsuch as tensorflow or pytorch.\\nThis is often called eager mode. This proposal focuses on just eager shape computation\\nintended for use in model import. The assumption is that we could\\nbuild on this later for fully eager computation.\\n","Decision":"In order to aid building model import easier,\\nthis proposal is focused on implementing just dynamic shape computation\\nfor use in the model import context.\\nThis will be composed of a few parts:\\n1. Each outputVariables() call in SDVariable triggers\\nan Nd4j.getExecutioner().exec(..) call on the relevant operation\\nto extract out op shapes. It then sets the appropriate shapes\\nbased on the result for each SDVariable field.\\n2. This will intentionally include dummy calls for control flow ops\\nsuch as if, enter, and while. Shapes from these don't matter\\nbeyond knowing the number of outputs.\\n3. Each SameDiff instance will have an eager mode boolean\\nthat will determine whether this functionality is invoked.\\nThis eager mode variable will be required for some model import use cases.\\nUsually the model import framework will turn eager on as needed\\nwithout the user needing to be involved.\\n4. Each SameDiff instance will have a separate ArrayHolder\\nthat will be used for looking up ndarrays relevant\\nto the eager computation. This will not use proper sessions\\nbut instead store that will be used once for computing shapes.\\n","tokens":97,"id":2933,"Prediction":"## Decision\n\n**Decision:** Implement a separate eager shape computation framework within Nd4j's model import framework to compute shapes as variables are created, leveraging the existing graph descriptor from other frameworks such as TensorFlow or PyTorch.\n\n**Rationale:**\n\n* The need for eager shape computation is specific to model import, and building it as a separate framework allows for a focused implementation that can be optimized for this use case.\n* By leveraging the existing graph descriptor from other frameworks, we can reduce the complexity of implementing eager shape computation from scratch.\n* This decision aligns with the assumption that the eager shape computation framework can be built upon later for fully eager computation, allowing for a more incremental and flexible approach to implementing eager computation in Nd4j.\n\n**Consequences:**\n\n* The model import framework will require additional infrastructure to support eager shape computation, which may introduce additional complexity and overhead.\n* The separate framework will need to be maintained and updated independently of the main Nd4j codebase, which may require additional resources and effort.\n* The implementation of eager shape computation will need to be thoroughly tested and validated to ensure correctness and performance.\n\n**Alternatives Considered:**\n\n* Implementing eager shape computation as a part of the main Nd4j codebase, which may lead to a more monolithic and complex implementation.\n* Using an existing eager computation framework, such as TensorFlow's eager execution, which may require significant modifications to the Nd4j codebase and may not be optimized for model import.\n\n**Status:** Accepted\n\n**Implementors:** [List the implementors responsible for implementing this decision]\n\n**References:** [List any relevant references or resources that support this decision]","GenTime":"2024-08-10 13:48:12"}
{"File Name":"deeplearning4j\/0003-dealing_with_inconsistencies_in_java_naming.md","Context":"## Context\\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\\n`BernoulliDistribution`.\\nTwo possible solutions where suggested:\\n1. Add an additional property that provides us with the correct class name\\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\\n","Decision":"For now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n","tokens":92,"id":2934,"Prediction":"## Decision\n\n**Decision:** We will rename the op class definitions to ensure consistency with the factory methods.\n\n**Context:** As mentioned in the context, there are slight inconsistencies in naming between existing op class definitions and factory methods. For example, a factory method called `bernoulli` in the `random` namespace with a corresponding op class called `BernoulliDistribution`.\n\n**Problem:** The inconsistencies in naming can lead to confusion and make the code harder to maintain. It is also important to ensure that the naming convention is consistent throughout the codebase.\n\n**Reasoning:** Renaming the op class definitions to ensure consistency with the factory methods is the best solution because it will make the code easier to understand and maintain. It will also ensure that the naming convention is consistent throughout the codebase.\n\n**Alternative:** Adding an additional property that provides us with the correct class name is not a good solution because it will make the code more complex and harder to maintain.\n\n**Solution:** We will rename the op class definitions to ensure consistency with the factory methods. We will also provide backwards compatibility via deprecated subclasses to ensure that the old code continues to work.\n\n**Consequences:**\n\n* The code will be easier to understand and maintain.\n* The naming convention will be consistent throughout the codebase.\n* The old code will continue to work due to the deprecated subclasses.\n\n**Status:** Accepted\n\n**Implementor:** [Name of the implementor]\n\n**Date:** [Date of the decision]\n\n**Links:** [Link to the relevant code changes or documentation]","GenTime":"2024-08-10 13:48:28"}
{"File Name":"deeplearning4j\/0001-SameDiff_File_Format.md","Context":"## Context\\nSameDiff models need to be serializable - i.e., something we can save to disk or send over the network.\\nAdditionally, we need to be able to save and load model files in C++, and have those be readable in other languages (mainly Java).\\nCurrently, we have a FlatBuffers-based format for SameDiff graph serialization, but it has a number of problems, as discussed in this issue: https:\/\/github.com\/eclipse\/deeplearning4j\/issues\/8312\\n","Decision":"We will transition from a pure FlatBuffers to a Zip + FlatBuffers model format.\\nFlatBuffers will be used for the graph structure only. Parameters will be stored separately to the graph structure, also within the zip.\\nWe will introduce the ability to support multiple versions of a graph in the model files.\\nThis will enable the model file to support storing\\n* Multiple data types (for example, a FP32 version and a quantized INT8 version)\\n* Multiple different checkpoints (parameters after 1000 iterations, after 5000, and so on)\\n* Multiple versions of a given model (English vs. Chinese, or cased\/uncased, etc)\\nBy default when loading a graph (unless it is otherwise specified) we will load the most recent model tag.\\nTags must be valid file\/folder identifiers, and are not case sensitive.\\nThe structure of the zip file will be as follows:\\n```\\ntags.txt                         \/\/List of graph tags, one per line, in UTF8 format, no duplicates. Oldest first, newest last\\n<tag_name>\/graph.fb              \/\/The graph structure, in FlatBuffers format\\n<tag_name>\/params.txt            \/\/The mapping between variable names and parameter file names\\n<tag_name>\/params\/*.fb           \/\/The set of NDArrays that are the parameters, in FlatBuffers format\\n<tag_name>\/trainingConfig.fb     \/\/The training configuration - updater, learning rate, etc\\n<tag_name>\/updater.txt           \/\/The mapping between variable names and the updater state file names\\n<tag_name>\/updater\/*.fb          \/\/The set of NDArrays that are the updater state\\n```\\nNote that params.txt will allow for parameter sharing via references to other parameters:\\n```\\nmy_normal_param 0\\nshared_param <other_tag_name>\/7\\n```\\nThis means the parameters values for parameter \"my_normal_param\" are present at `<tag_name>\/params\/0.fb` within the zip file, and the parameter values for \"shared_param\" are available at `<other_tag_name>\/params\/7.fb`\\nNote also that the motivation for using the params.txt file (instead of the raw parameter name as the file name) is that some parameters will have invalid or ambiguous file names - \"my\/param\/name\", \"&MyParam*\" etc\\nIn terms of updater state, they will be stored in a similar format. For example, for the Adam updater with the M and V state arrays (each of same shape as the parameter):\\n```\\nmy_param 0 1\\nother_param 2 3\\n```\\nThat means my_param(M) is `<tag_name>\/updater\/0.fb` and my_param(V) is at `<tag_name>\/updater\/1.fb`\\nThis format also allows for updater state sharing, if we need it.\\n**Graph Structure**\\nThe graph structure will be encoded in FlatBuffers format using a schema with 2 parts:\\n1. A list of variables - each with name, datatype, and (for placeholders, constants and parameters) a shape\\n2. A list of operations - each with a name, op name\/type, input variable names, output variable names, and arguments\\nNote that both legacy and custom ops will be encoded in the same way. For legacy ops, we simply need the operation type, and the operation number.\\nOperation argument encoding will be done using named arguments: essentially, a `Map<String,T>` structure, where T is one of `{long, double, boolean, datatype}`.\\nThis allows for improved backward compatibility (no ambiguity as ops are modified after a graph file was written) and improved interpretability compared to using simple arrays of iargs, bargs, targs and dargs.\\nOne consequence\/downside of this is that we need to define mapping between our named arguments and iargs\/bargs\/targs\/dargs. In Java we have essentially done this manually, though clearly don't want to replicate this work in C++ (or any future languages).\\nTo avoid the need to do a significant amount of work (such as moving the name\/arg mapping to code generation) the following is proposed:\\nThe `Map<String,T>` is split up in the FlatBuffers schema into 4 pairs of fields.\\n* `String[] iArgNames`, `long[] iArgs`\\n* `String[] tArgNames`, `double[] dArgs`\\n* `String[] bArgNames`, `boolean[] bArgs`\\n* `String[] dArgNames`, `DataType[] dArgs`\\nClearly the name and value arrays (for each pair) would each be the same length, and name\/value correspondence is by array index.\\nThis is essentially equivalent to the `Map<String,T>` representation, but has the benefit of not needing us to define the mapping for named args to array-style args any time soon in C++, but also allowing us to add it in the future (mainly before we can write graphs from C++, or have better\/proper backward compatibility after op changes)\\n**Extensibility to Other Types**\\nSuppose in the future we want to store other data for a variable, not just an array?\\nExamples include lists and maps (for example, for NLP applications).\\nWhile we will not implement this right now, there are a number of options for adding this without breaking backward compatibility.\\nFirst: we can enhance the params.txt file format, perhaps using something like the following:\\n```\\nmap_param 0 MAP\\n```\\nSecond: We can add a similar text file for other types. For example, a params_maps.txt, same format as params.txt, with content at `<tag_name>\/params_maps\/*.fb`\\n","tokens":104,"id":2944,"Prediction":"## Decision\n\n**Decision:** We will switch from using FlatBuffers to Protocol Buffers (protobuf) for serializing SameDiff models.\n\n**Context:** As mentioned, our current FlatBuffers-based format for SameDiff graph serialization has several issues, including difficulties in debugging and limitations in terms of extensibility. We need a more robust and flexible solution that can be easily used in both C++ and other languages, such as Java.\n\n**Rationale:** Protocol Buffers (protobuf) is a language-agnostic, extensible, and efficient binary serialization format developed by Google. It has been widely adopted in many industries and is known for its simplicity, performance, and flexibility. Using protobuf for serializing SameDiff models will provide several benefits, including:\n\n* Improved debugging capabilities: Protobuf provides a clear and concise format for debugging, making it easier to understand and troubleshoot model serialization issues.\n* Better extensibility: Protobuf allows for easy extension of the model format, enabling us to add new features and fields without breaking backwards compatibility.\n* Language-agnostic: Protobuf can be used in multiple languages, including C++ and Java, making it an ideal choice for our use case.\n\n**Consequences:**\n\n* We will need to refactor our serialization code to use protobuf instead of FlatBuffers.\n* We will need to create protobuf definitions for our SameDiff model format.\n* We will need to update our C++ and Java code to use the new protobuf-based serialization format.\n\n**Alternatives Considered:**\n\n* We considered using JSON or XML for serialization, but these formats are text-based and may not be as efficient as binary formats like protobuf.\n* We considered using a custom binary format, but this would require significant development effort and may not be as widely adopted as protobuf.\n\n**Implementation:**\n\n* We will create a protobuf definition for our SameDiff model format and generate the necessary C++ and Java code using the protobuf compiler.\n* We will refactor our serialization code to use the new protobuf-based format.\n* We will test the new serialization format thoroughly to ensure it works correctly and efficiently.","GenTime":"2024-08-10 13:50:53"}
{"File Name":"deeplearning4j\/0009 - Import node pre processing.md","Context":"## Context\\nNd4j's model import framework supports different protobuf based frameworks\\nfor importing and executing models. This was introduced in [0003-Import_IR.md](0003-Import_IR.md)\\nOne problem with importing models is compatibility between different versions of frameworks.\\nOften,migrations are needed to handle compatibility between versions. A node pre processor is proposed\\nthat: when combined with the model import framework allows for\\nannotation based automatic upgrades of graphs.\\n","Decision":"In order to handle preprocessing a node to handle things like upgrades.\\nAn end user can specify a pre processor via a combination of 2 interfaces:\\n1. An annotation for specifying a class that implements a relevant rule\\nfor processing. This will automatically be discoverable via annotation scanning\\nsimilar to other frameworks. This annotation looks as follows:\\n```kotlin\\nannotation class NodePreProcessor(val nodeTypes: Array<String>, val frameworkName: String)\\n```\\nThe information include the nodeTypes which are the operation types to scan for when doing upgrades on a graph.\\nThe framework name: relevant if multiple import modules are on the classpath. Filters rules\\nby their intended framework for import.\\n2. The necessary pre processing hook that will handle processing the node\\nand may modify the graph. Graph modification maybe necessary if we need to add new nodes to compensate\\nfor modification of a node such as an attribute moving to being an input.\\n```kotlin\\ninterface NodePreProcessorHook<NODE_TYPE : GeneratedMessageV3,\\nTENSOR_TYPE : GeneratedMessageV3,\\nATTRIBUTE_TYPE : GeneratedMessageV3,\\nATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_TYPE>\\nwhere  DATA_TYPE: ProtocolMessageEnum {\\nfun modifyNode(\\nnode: IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>,\\ngraph: IRGraph<GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, ProtocolMessageEnum>\\n): IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>\\n}\\n```\\n","tokens":94,"id":2945,"Prediction":"## Decision\n\n**Title:** Introduce a Node Pre-Processor for Automatic Model Upgrade\n\n**Context:** Nd4j's model import framework supports different protobuf-based frameworks for importing and executing models. However, one of the challenges with importing models is compatibility between different versions of frameworks, often requiring migrations to handle compatibility between versions.\n\n**Decision:** We will introduce a Node Pre-Processor that, when combined with the model import framework, allows for annotation-based automatic upgrades of graphs.\n\n**Rationale:**\n\n* The current model import framework does not provide a seamless way to handle version compatibility, leading to manual migrations and potential errors.\n* A Node Pre-Processor can automatically inspect the model graph and upgrade nodes to the latest version, reducing the need for manual migrations and improving the overall development experience.\n* Annotation-based automatic upgrades will provide a flexible and extensible way to handle version compatibility, allowing developers to specify custom upgrade rules and logic.\n\n**Consequences:**\n\n* The Node Pre-Processor will add an additional layer of complexity to the model import framework, requiring additional testing and validation.\n* The annotation-based approach will require developers to add additional metadata to their models, which may add overhead to the development process.\n* The automatic upgrade feature will provide a significant improvement in development efficiency and reduce the risk of errors caused by manual migrations.\n\n**Alternatives Considered:**\n\n* Manual migrations: This approach would require developers to manually update their models to the latest version, which can be time-consuming and error-prone.\n* Custom upgrade scripts: This approach would require developers to write custom scripts to handle version compatibility, which can be inflexible and difficult to maintain.\n* Abandoning version compatibility: This approach would require developers to start from scratch with each new version of the framework, which can be impractical and limiting.\n\n**Status:** Approved\n\n**Implementation:** The Node Pre-Processor will be implemented as a separate module within the model import framework, with a clear API for developers to specify custom upgrade rules and logic. The implementation will be tested thoroughly to ensure compatibility with different versions of frameworks and models.","GenTime":"2024-08-10 13:51:16"}
{"File Name":"pcmt\/adr-007.md","Context":"## Context\\nPCMT wants to accomplish a number of goals through the use of versions:\\n1. Convey to stakeholders the initial suitability of the project:\\n* Pre-v1: In development for global stakeholders.\\n* Version 1: Production ready for global stakeholders.\\n* Version 2: Production ready for national stakeholders.\\n1. Leverage semantic versioning to convey to dependents the nature of the\\nrelease.  e.g. is it a patch-release, major new functionality, etc.\\n1. Mark a specific state of the code-base, tied to a released asset.\\n1. Indicate which version of Akeneo that PCMT is derived from.\\n","Decision":"1. We will version PCMT by [Semantic Versioning 2.0][semver].  However since\\nDocker tags do not support the use of the `+` symbol, we'll use `-` in\\nits place.\\n1. We will have a build number (from the commit SHA), which is from a CI\\npipeline, that includes a distinct set of commits.\\n1. We will expire and remove old Build numbers, so they need to be pruned\\nregularly.\\n1. We will not publish assets if a pipeline fails, and conversely if a pipeline\\nis re-run, it consists of the same commits (state of the code), and therefore\\nit should have the same build number.\\n1. We will promote build numbers to a showcase or UAT server, manually.\\n1. We will promote build numbers to a release, manually.\\n1. We will keep the semantic version of the system separate from the build\\nnumber, in a file in the SCM.  When an asset is labeled from\\nthe CI pipeline, it'll use the full form by placing the build number in\\nthe build meta-data of the Semantic Version.  e.g. `1.0.0+48af4a30`\\n1. When promoting to a release we will co-label the published assets such that\\none asset will have the build number, and the released asset will not include\\nthat number. e.g. version 1.0.0 might be `image:v1.0.0+SHA` and\\n`image:v1.0.0`, which are equivalent assets.\\n","tokens":141,"id":2951,"Prediction":"## Decision\n\n**Title:** Versioning Strategy for PCMT\n\n**Context:** As described by the user, PCMT aims to achieve several goals through the use of versions, including conveying project suitability to stakeholders, leveraging semantic versioning to indicate release nature, marking a specific code-base state, and indicating the version of Akeneo used.\n\n**Decision:** PCMT will adopt a versioning strategy that utilizes semantic versioning (MAJOR.MINOR.PATCH) to indicate the nature of the release. The versioning scheme will be as follows:\n\n* **Pre-v1**: Development phase, not production-ready\n* **v1**: Production-ready for global stakeholders, indicating major new functionality or significant changes\n* **v2**: Production-ready for national stakeholders, indicating significant changes or new features\n* **PATCH RELEASES**: Incremental updates with minor bug fixes or small improvements (e.g., v1.1.2)\n* **MINOR RELEASES**: Updates with new features or significant changes (e.g., v1.2.0)\n* **MAJOR RELEASES**: Significant updates with major changes or new functionality (e.g., v2.0.0)\n\n**Rationale:**\n\n* Semantic versioning provides a clear and consistent way to indicate the nature of the release, making it easier for stakeholders and dependents to understand the changes.\n* The versioning scheme allows for clear differentiation between development, production-ready, and released states.\n* The inclusion of Akeneo versioning information enables tracking of dependencies and ensures consistency across the project.\n\n**Consequences:**\n\n* Improved communication with stakeholders and dependents regarding the nature and significance of releases.\n* Enhanced transparency and consistency in versioning and release management.\n* Simplified tracking of dependencies and changes across the project.\n\n**Alternatives Considered:**\n\n* Using a custom versioning scheme, which may lead to confusion and inconsistencies.\n* Not using versioning at all, which would make it difficult to track changes and dependencies.\n\n**Status:** Implemented","GenTime":"2024-08-10 13:53:26"}
{"File Name":"status-react\/0007-masking-sensitive-data.md","Context":"## Context\\nWe have some data that we don't want to appear in the logs (user passwords are\\na good example). Currently, they are passed around as strings, that could be\\nprinted out by mistake in a log entry (see https:\/\/github.com\/status-im\/status-react\/issues\/4053)\\n","Decision":"To minimize the risk of leaking passwords through logs, we should not pass\\npasswords as strings in our codebase. We introduced a new type `MaskedData` in\\n`status-im.utils.security`.\\nWe use `(security\/mask-data <data to hide>` to wrap sensitive data into this\\ntype and then use `(security\/unmask <masked-data>)` to get the plaintext back.\\nIt is important to keep that sensitive data masked as much as possible, until\\nyou need the plaintext to pass to the extenral APIs.\\nExample:\\n```clojure\\n(println (security\/mask-data \"my-plaintext-password\")) ;; Outputs \"******\"\\n(println (security\/unmask (security\/mask-data \"my-plaintext-password\"))) ;; Outputs \"my-plaintext-password\"\\n```\\n","tokens":63,"id":2958,"Prediction":"## Decision\n\n**Decision:** Implement a secure logging mechanism to redact sensitive data, such as user passwords, from log entries.\n\n**Context:** As mentioned, we have sensitive data that should not be printed out in log entries, as it could compromise the security of our system. Currently, this data is passed around as strings, making it vulnerable to accidental exposure in log entries.\n\n**Problem:** The existing logging mechanism does not provide a way to securely handle sensitive data, making it a potential security risk.\n\n**Rationale:** Implementing a secure logging mechanism will ensure that sensitive data is properly redacted and cannot be accessed or printed out in log entries. This will help protect the security and integrity of our system.\n\n**Decision Drivers:**\n\n* Security: Protecting sensitive data from accidental exposure in log entries.\n* Compliance: Adhering to security best practices and regulations.\n* Reliability: Ensuring that our system can operate securely and reliably.\n\n**Trade-offs:**\n\n* Complexity: Implementing a secure logging mechanism may add complexity to our system.\n* Performance: There may be a slight performance impact due to the additional processing required for redacting sensitive data.\n\n**Consequences:**\n\n* Secure logging mechanism will be implemented to redact sensitive data.\n* Sensitive data will be properly protected from accidental exposure in log entries.\n* Our system will be more secure and compliant with security best practices and regulations.\n\n**Next Steps:**\n\n* Research and implement a suitable secure logging mechanism.\n* Test the new logging mechanism to ensure it is functioning correctly.\n* Review and update our logging configuration to use the new secure logging mechanism.","GenTime":"2024-08-10 13:55:26"}
{"File Name":"status-react\/0002-extension-uri.md","Context":"## Context\\nExtensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.\\n","Decision":"URI follows ethereum URI schema as specified in [EIP 831](https:\/\/github.com\/ethereum\/EIPs\/blob\/master\/EIPS\/eip-831.md)\\n```\\nuri                     = \"ethereum:\" path\\npath                    = \"status:extension\" \":\" storage \"@\" id\\nstorage                 = STRING (e.g. IPFS)\\nid                      = STRING (e.g. IPFS hash)\\n```\\nAn example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`\\n","tokens":26,"id":2963,"Prediction":"## Decision\n\n**Decision:** Use a URI-based approach to identify and access EDN files stored in decentralized storage, rather than relying on a centralized registry or hardcoded file paths.\n\n**Context:** Extensions are defined by EDN files accessed from decentralized storage, and an URI schema is used to identify those files.\n\n**Problem:** The current approach of relying on a centralized registry or hardcoded file paths to access EDN files may lead to scalability issues, high latency, and reduced fault tolerance.\n\n**Reasoning:** By using a URI-based approach, we can decouple the extension identification from the storage location, making it easier to manage and update extensions without affecting the core system. This approach also allows for greater flexibility in terms of storage options, as decentralized storage solutions can be easily integrated without modifying the core system.\n\n**Consequences:**\n\n* Improved scalability and fault tolerance\n* Reduced latency and increased performance\n* Greater flexibility in terms of storage options\n* Simplified extension management and updates\n\n**Alternatives Considered:**\n\n* Using a centralized registry to store and manage EDN files\n* Hardcoding file paths to access EDN files\n* Using a proprietary file format that requires a specific storage solution\n\n**Decision Drivers:**\n\n* Scalability and performance requirements\n* Flexibility and adaptability to changing storage options\n* Decentralized nature of the storage solution\n* Need for easy extension management and updates","GenTime":"2024-08-10 13:56:51"}
{"File Name":"status-react\/0011-tweak-pr-process.md","Context":"## Context\\nThere was a generally dissatisfaction with our PR flow process from multiple stakeholders, including devs, QA and design. These largely centered around size, speed of integration and quality of PRs.\\nFor more details, please see pain points in the meeting notes held end of February here: https:\/\/notes.status.im\/C5pj8g7gQOu9Wo8PtDZsMw?edit# as well as the preceeding Discuss thread: https:\/\/discuss.status.im\/t\/better-pull-requests-process\/1044\\nAlso see conversations in Core Dev Call #12 and #13: https:\/\/github.com\/status-im\/pm\/\\n","Decision":"These specific policy changes were agreed upon (a la 'rough consensus' model). These are heuristics, and should be more of a guide than strict rules. Exception to these should be minimal though.\\n**1. Reject PRs that don't have tests**\\nAs a rule of thumb, coverage should increase.\\n**2. Don't use or promote nightly as something for end users.**\\n**3. Reject PRs that don't write what tests the author have done and what platforms you used to test, including screenshots, videos or logs**\\nThis means QA and dev have to spend less time catching obvious problems.\\n**4. Run tests when a PR is at \"Review\" stage on https:\/\/github.com\/status-im\/status-react\/projects\/7 (or add new 'e2e stage' between review and `TO TEST`)**\\n**5. Dev asks for 2-3 reviewers, including a designer if the change affects UI, and these reviewers review within a day.**\\nThis ensures we don't have *diffusion of responsibility* and that PRs are reviewed quickly.\\n","tokens":135,"id":2964,"Prediction":"## Decision\n\n**Title:** Implement a PR Review Bot to Improve PR Flow Process\n\n**Context:** There was a generally dissatisfaction with our PR flow process from multiple stakeholders, including devs, QA, and design. These largely centered around size, speed of integration, and quality of PRs.\n\n**Problem:** The current PR review process was causing frustration among stakeholders, leading to delays, and affecting the overall quality of the codebase.\n\n**Decision:** We will implement a PR review bot to automate the review process, provide feedback, and ensure that PRs meet the required standards before they are merged.\n\n**Rationale:** The PR review bot will help to:\n\n* Reduce the workload of human reviewers by automating routine checks\n* Ensure that PRs meet the required standards by providing feedback on code quality, testing, and documentation\n* Improve the speed of integration by reducing the time spent on reviewing PRs\n* Increase the quality of PRs by providing feedback on code quality, testing, and documentation\n\n**Consequences:**\n\n* The implementation of the PR review bot will require additional infrastructure and maintenance\n* The bot will need to be configured and trained to recognize and provide feedback on code quality, testing, and documentation\n* The bot's feedback will need to be reviewed and validated by human reviewers to ensure accuracy and relevance\n\n**Status:** The implementation of the PR review bot is planned to start immediately, and the project timeline is estimated to be 6 weeks. The bot will be trained and configured to recognize and provide feedback on code quality, testing, and documentation. The feedback will be reviewed and validated by human reviewers to ensure accuracy and relevance.\n\n**Follow-up:** The success of the PR review bot will be monitored and evaluated regularly to ensure that it is meeting its intended goals and to identify areas for improvement.","GenTime":"2024-08-10 13:57:10"}
{"File Name":"manuela\/0003-gitops-approach.md","Context":"##  Context\\nSee [2. Overall context and container platform](0002-overall-context-and-container-platform.md)\\n","Decision":"We use git to address the requirement of a data store:\\n* it is a replicated data store\\n* the hierarchy of deployment targets can be represented as hierarchy of directores in the git repository\\n* can host kubernetes manifests describing application deployments\\n* approval workflows, auditability, etc... is built in\\n* can create replicas with limited content, e.g. a regional DC only replicates what applies to the region with all its factories and line data servers, a factory DC then only replicates what applies to the factory dc and the line data servers, etc.\\n","tokens":25,"id":2968,"Prediction":"## Decision\n\n**Title:** Containerization of the Application\n\n**Context:** As outlined in the overall context and container platform documentation, our application needs to be containerized to ensure scalability, reliability, and maintainability.\n\n**Decision:** We will containerize our application using Docker containers. This decision is based on the following factors:\n\n* **Scalability**: Containerization allows us to easily scale individual components of our application independently, without affecting the entire application.\n* **Reliability**: Containers provide a consistent and isolated environment for our application, which reduces the risk of conflicts and errors.\n* **Maintainability**: Containerization makes it easier to manage and update our application, as we can simply update the container image and redeploy it.\n* **Portability**: Docker containers are platform-agnostic, which means we can deploy our application on any platform that supports Docker, without worrying about compatibility issues.\n\n**Consequences:**\n\n* **Initial Complexity**: Containerization may require additional setup and configuration, which could increase the initial complexity of our application.\n* **Additional Infrastructure**: We will need to set up a container orchestration tool, such as Kubernetes, to manage our containers.\n* **Monitoring and Logging**: We will need to implement monitoring and logging tools to ensure we can effectively troubleshoot and debug our application in a containerized environment.\n\n**Rationale:** Containerization is a widely adopted and well-established technology that provides numerous benefits for our application. While it may require some additional setup and complexity, the benefits of scalability, reliability, and maintainability make it a worthwhile investment.\n\n**Status:** Accepted\n\n**Implementation:** We will implement containerization using Docker and Kubernetes, and will monitor and log our application using tools such as Prometheus and Grafana.","GenTime":"2024-08-10 13:57:49"}
{"File Name":"architectural-decision-log\/0005-convention-based-logging-classes.md","Context":"## Context and Problem Statement\\nHamlet Deploy as a product is made up from several \"components\". They are not written in the same coding\/scripting language and so their outputs - particularly when it comes to log behaviour and error messages - are quite different from each other. When an error or debug message is received, how can the end-user and the Hamlet Deploy developer both gain greater insight from the message that is received?\\n## Decision Drivers <!-- optional -->\\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\n","Decision":"* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\nChosen option:  _\"Use Logging Codes Following a Numerical Convention\"_ with _\"Per-Component Code Range\"_\\nIt was deemed easier to implement and there seemed little benefit and greater complexity to implementing an inheritance-based approach.\\nA code range allows each component to implement the definitions in the way that most makes sense to it.\\n","tokens":122,"id":2975,"Prediction":"Here is a possible Architectural Decision Record:\n\n## Decision\n\nWe will implement a standardized logging format for all components of Hamlet Deploy, using a structured logging format that includes a common set of fields. This will enable end-users and developers to easily parse and analyze log messages, regardless of the language or technology used to generate them.\n\n## Context and Problem Statement\n\nHamlet Deploy is a complex system composed of multiple components, each written in a different language (Bash, Python, and Freemarker). As a result, the log messages generated by these components are not easily comparable or analyzable. This makes it difficult for both end-users and developers to gain insight from log messages, leading to increased debugging time and reduced system reliability.\n\n## Decision Drivers\n\n* The solution must be possible to implement in Bash, Python, and Freemarker (Java).\n* The solution must be simple to maintain and integrate with existing logging mechanisms.\n\n## Rationale\n\nImplementing a standardized logging format will enable us to:\n\n* Provide a consistent and easily analyzable log format across all components.\n* Simplify log analysis and debugging for end-users and developers.\n* Reduce the complexity and overhead of integrating log messages from different components.\n\n## Decision Outcome\n\nWe will implement a structured logging format using the JSON format, which is widely supported and easily parsable. We will define a set of common fields that will be included in all log messages, such as timestamp, log level, component name, and message text. This will enable end-users and developers to easily filter, search, and analyze log messages, regardless of the language or technology used to generate them.\n\n## Pros and Cons\n\nPros:\n\n* Simplifies log analysis and debugging\n* Enables consistent and easily analyzable log format\n* Reduces complexity and overhead of integrating log messages\n\nCons:\n\n* May require additional effort to implement and test\n* May require changes to existing logging mechanisms\n\n## Adoption Strategy\n\nWe will adopt the structured logging format for all new components and gradually migrate existing components to use the new format. We will also provide documentation and support for developers to help them integrate the new logging format into their components.\n\n## Reevaluation\n\nWe will reevaluate the effectiveness of the structured logging format after 6 months and assess whether any further improvements or changes are necessary.","GenTime":"2024-08-10 14:00:03"}
{"File Name":"grout\/adr-2-rename-package.md","Context":"## Context\\nThe name `ashlar` [is already taken on PyPi](https:\/\/pypi.org\/project\/ashlar\/).\\nSince PyPi requires unique names for packages, this means that if we want to\\ndistribute our package on PyPi, we'll have to either:\\n1. Convince the owners of `ashlar` to give it to us\\n2. Name the PyPi package something similar to `ashlar` but slightly different,\\nlike `ashlar-core`\\n3. Come up with a new name for the project\\nOption 1 seems unlikely, given the maturity of the ashlar package on PyPi and\\nhow recent the last release was (April 2018, less than four months ago). Number\\n2 is perfectly functional but frustrating from a branding and distribution perspective,\\nsince it has the potential to introduce some confusion and\/or competition with\\nthe existing `ashlar` package.\\nInstead, I believe that the best course of action is to choose option 3 and rename the project.\\nThis will require us to come up with a new name for Ashlar, a [notoriously\\ndifficult decision](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html).\\nSome options that I considered, all based on the idea of \"flexible\\nconstruction materials\":\\n- [Joist](https:\/\/en.wikipedia.org\/wiki\/Joist)\\n- [Lintel](https:\/\/en.wikipedia.org\/wiki\/Lintel)\\n- [Silicone](https:\/\/en.wikipedia.org\/wiki\/Silicone)\\n- [Grout](https:\/\/en.wikipedia.org\/wiki\/Grout)\\n","Decision":"I propose that we rename the project to **Grout**. Among the options above,\\n\"Grout\" is the name that sounds the best to me, and it's the one that I believe\\noffers the closest allegory for the project.\\nGrout is a construction material widely known for its physical flexibility and its\\npractical versatility: a fluid concrete used to create waterproof seals in\\nmasonry structures.\\nSome advantages of the name \"Grout\" include:\\n- \"Grout\" respects the origins of the project by referencing a masonry material,\\nbut unlike \"Ashlar\", the name \"Grout\" emphasizes the core features of the project\\n-- its flexibility and versatility as a base material\\nthat can scaffold and tie together much more complex projects.\\n- \"Grout\" is one syllable (one fewer than \"Ashlar\") and the easiest word to\\npronounce among the options I considered.\\n- Perhaps most importantly, `grout` is [available on\\nPyPi](https:\/\/pypi.org\/project\/grout).\\n","tokens":339,"id":2980,"Prediction":"## Decision\n\n**Renaming the project to avoid conflict with the existing `ashlar` package on PyPi**\n\n## Context\n\nThe name `ashlar` is already taken on PyPi, and we need to choose between convincing the owners of the package to give it to us, renaming our package to something similar but slightly different, or coming up with a new name for the project. After considering the options, I believe that the best course of action is to choose option 3 and rename the project to avoid potential confusion and competition with the existing `ashlar` package.\n\n## Problem\n\nThe existing `ashlar` package on PyPi has a mature presence and recent releases, making it unlikely that the owners will give up the name. Renaming our package to something similar but slightly different, such as `ashlar-core`, would be functional but may introduce confusion and\/or competition with the existing package.\n\n## Decision Criteria\n\n1. Avoid potential confusion and competition with the existing `ashlar` package.\n2. Ensure a unique name for our package on PyPi.\n3. Minimize the impact on branding and distribution.\n\n## Decision Outcome\n\nWe will rename the project to a new, unique name that reflects the theme of \"flexible construction materials\". Some options considered include:\n\n* Joist\n* Lintel\n* Silicone\n* Grout\n\n## Rationale\n\nRenaming the project will allow us to avoid potential conflicts and competition with the existing `ashlar` package, while also ensuring a unique name for our package on PyPi. By choosing a new name that reflects the theme of \"flexible construction materials\", we can maintain a consistent brand identity and minimize the impact on distribution.\n\n## Status\n\nProposed\n\n## Links\n\n* [PyPi page for `ashlar` package](https:\/\/pypi.org\/project\/ashlar\/)\n* [Martin Fowler's article on \"Two Hard Things\"](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html)\n* [Wikipedia articles on Joist, Lintel, Silicone, and Grout](https:\/\/en.wikipedia.org\/wiki\/Joist), (https:\/\/en.wikipedia.org\/wiki\/Lintel), (https:\/\/en.wikipedia.org\/wiki\/Silicone), and (https:\/\/en.wikipedia.org\/wiki\/Grout)","GenTime":"2024-08-10 14:02:07"}
{"File Name":"sre-challenge\/0004-using-aws-ecs-as-platform.md","Context":"## Context\\nI want to use docker at AWS but I don't want to provision a docker\\ninfrastructure myself.  So I'm going to use AWS ECS managed service and create\\na cluster.\\n","Decision":"Done.\\n","tokens":42,"id":2997,"Prediction":"## Decision\n\nTo use AWS Elastic Container Service (ECS) managed service to create a cluster for running Docker containers at AWS, instead of provisioning a Docker infrastructure ourselves.\n\n## Context\n\nAs you mentioned, you want to use Docker at AWS but don't want to provision a Docker infrastructure yourself. This is a common use case, as provisioning and managing a Docker infrastructure can be complex and time-consuming.\n\n## Problem\n\nProvisioning and managing a Docker infrastructure requires expertise in Docker, container orchestration, and infrastructure provisioning. This can be a barrier for teams that want to use Docker but don't have the necessary expertise or resources.\n\n## Decision\n\nTo use AWS ECS managed service to create a cluster for running Docker containers. AWS ECS is a fully managed container orchestration service that allows you to run containers without worrying about the underlying infrastructure. With ECS, you can focus on developing and deploying your applications, while AWS handles the provisioning and management of the infrastructure.\n\n## Rationale\n\nUsing AWS ECS managed service provides several benefits, including:\n\n* Reduced complexity: AWS ECS handles the provisioning and management of the infrastructure, making it easier to get started with Docker.\n* Scalability: ECS allows you to scale your containers up or down as needed, without worrying about the underlying infrastructure.\n* High availability: ECS provides built-in support for high availability, ensuring that your containers are always available and running.\n* Cost-effective: ECS provides a cost-effective way to run containers, as you only pay for the resources you use.\n\n## Consequences\n\nBy using AWS ECS managed service, we will need to adapt our workflow to use the ECS API and CLI, and will need to design our application and infrastructure accordingly. We will also need to ensure that our application is containerized and ready to run on ECS.\n\nOverall, using AWS ECS managed service is a good decision for our use case, as it provides a scalable, cost-effective, and easy-to-use solution for running Docker containers at AWS.","GenTime":"2024-08-10 14:06:10"}
{"File Name":"sre-challenge\/0008-use-taurus-for-benchmark-tests.md","Context":"## Context\\nTo perform benchmarking tests, I want to set some resources capable to generate\\na high volume of requests against the cluster.\\nTo do this, I could use several tools, like JMeter, Garret, or Taurus.\\nBut I like Blazemeter's Taurus over all because it is very simple to setup and\\nexecute tests.\\nWith Taurus you can create JMeter tests, or use other tools like Selenium, in\\norder to execute and collect results.\\nThen It generates a report that can be easily imported in a JMeter tool or\\nopen it in a data analysis tool like jupyter notebook to draw new diagrams.\\n","Decision":"Create an instance with an easy test (Just request the keys and the metrics).\\n","tokens":137,"id":2998,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\n* **Use Blazemeter's Taurus for benchmarking tests**\n\n**Context**\n\nTo perform benchmarking tests, we need to set up resources capable of generating a high volume of requests against the cluster. We considered using several tools, such as JMeter, Garret, or Taurus, but chose Blazemeter's Taurus due to its simplicity in setup and execution.\n\n**Problem**\n\nWe need a tool that can generate a high volume of requests against the cluster to perform benchmarking tests, and also provide an easy way to create and execute tests, as well as collect and analyze results.\n\n**Decision**\n\nWe will use Blazemeter's Taurus for benchmarking tests because it is easy to set up and execute tests, and can create JMeter tests or use other tools like Selenium to execute and collect results. Taurus also generates a report that can be easily imported into JMeter or opened in a data analysis tool like Jupyter Notebook to draw new diagrams.\n\n**Rationale**\n\nThe simplicity of setup and execution of tests with Taurus was a major factor in our decision. Additionally, Taurus provides a wide range of features and tools to create and execute tests, and its ability to generate reports makes it easy to analyze and visualize results.\n\n**Consequences**\n\nBy using Taurus, we will be able to quickly and easily set up and execute benchmarking tests, and analyze the results. This will allow us to identify performance bottlenecks and optimize our system for better performance. However, we may need to invest time in learning how to use Taurus effectively, and ensure that our tests are properly configured to get accurate results.\n\n**Alternatives**\n\nWe considered using other tools such as JMeter, Garret, or Selenium, but ultimately chose Taurus due to its simplicity and ease of use.","GenTime":"2024-08-10 14:06:30"}
{"File Name":"sre-challenge\/0007-use-a-dashboard-for-etcd.md","Context":"## Context\\nTo visualize the metrics of the etcd cluster, I would like to implement a\\ndashboard using the USE and RED methods.\\n- USE to measure the performance of the system hosting the etcd cluster\\n- RED to measure the performance of the gRPC side of the etcd cluster\\nBut I haven't found anything like this and I have no time to waste.  So\\nI have found a dashboard on the grafana site that has some metrics.\\n","Decision":"As I'm out of time, I'm just going to implement it at the configuration\\nmanagement system, to automatically provision the dashboard and the\\ndatasource.\\n","tokens":97,"id":3004,"Prediction":"## Decision\n\n### Decision: Leverage an existing Grafana dashboard for etcd metrics visualization\n\n### Context\n\nTo visualize the metrics of the etcd cluster, we need to measure the performance of the system hosting the etcd cluster using the USE method and the performance of the gRPC side of the etcd cluster using the RED method. After conducting a thorough search, we were unable to find a suitable solution that meets our requirements. Given the time constraint, we have decided to leverage an existing Grafana dashboard that has some relevant metrics for etcd.\n\n### Rationale\n\n* Time constraint: We do not have the luxury of time to develop a custom dashboard from scratch.\n* Existing solution: The Grafana dashboard we found provides a good starting point for visualizing etcd metrics, and we can customize it to meet our specific needs.\n* USE and RED methods: The dashboard provides some of the metrics we need for USE and RED, which will allow us to measure the performance of the system hosting the etcd cluster and the gRPC side of the etcd cluster.\n\n### Consequences\n\n* We will need to customize the existing dashboard to fit our specific use case.\n* We may need to add additional metrics or modify existing ones to accurately measure the performance of the system hosting the etcd cluster and the gRPC side of the etcd cluster.\n* The dashboard may not provide all the metrics we need, so we will need to monitor the performance of the etcd cluster closely and adjust the dashboard as needed.\n\n### Alternatives\n\n* Develop a custom dashboard from scratch: This would require significant time and resources, which we do not have.\n* Use a different dashboarding tool: While there may be other dashboarding tools available, Grafana is widely used and has a large community of users and developers, which makes it a good choice for this use case.\n\n### Status\n\nThis decision is approved and will be implemented as soon as possible.","GenTime":"2024-08-10 14:08:26"}
{"File Name":"sre-challenge\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3006,"Prediction":"Here is an Architectural Decision Record (ADR) for the provided context:\n\n**Decision**\n\n**ADR-001: Use of Microservices Architecture for the Project**\n\n**Context**\n\nAs we are building a complex software system, we need to decide on an architecture that can scale, be maintainable, and allow for independent development and deployment of individual components. The system will consist of multiple features, each with its own set of requirements and constraints.\n\n**Problem**\n\nThe current monolithic architecture of the system is limiting our ability to scale and maintain the system. The system is becoming increasingly complex, and it is difficult to identify and isolate issues. The monolithic architecture also makes it challenging to deploy individual features independently.\n\n**Decision**\n\nWe will adopt a Microservices Architecture (MSA) for the project. This will allow us to break down the system into smaller, independent services that can be developed, tested, and deployed independently. Each service will be responsible for a specific business capability, and they will communicate with each other using APIs.\n\n**Consequences**\n\n* The system will be more scalable and easier to maintain.\n* The system will be more flexible and allow for independent development and deployment of individual features.\n* The system will be more resilient to failures, as each service can be restarted independently.\n* The system will require more complex communication and integration between services.\n* The system will require more expertise in distributed systems and APIs.\n\n**Rationale**\n\nWe chose MSA because it aligns with the project's goals and requirements. It will allow us to build a system that is scalable, maintainable, and flexible. It will also enable us to take advantage of the latest technologies and trends in software development.\n\n**Status**\n\nThis decision is approved and will be implemented in the next iteration.\n\n**References**\n\n* [1] \"Microservices Architecture\" by Martin Fowler\n* [2] \"Building Microservices\" by Sam Newman","GenTime":"2024-08-10 14:09:12"}
{"File Name":"embvm-core\/0021-eliminate-name-variables-from-core-base-classes.md","Context":"## Context\\nUnder the initial design of the `embvm-core`, the following types stored a name in the base class:\\n- Driver\\n- Processor\\n- Hardware Platform\\n- Platform\\nIn some cases, like drivers, this data was duplicated (through the driver registry) or unnecessary (because you use the default name and provide the system name in the driver registry). We don't need to store two different instances of the same name!\\nIn other cases, like the processor and platforms, these names were never used.\\n","Decision":"These names will no longer be required by the base classes.\\n","tokens":107,"id":3023,"Prediction":"## Decision\n\n**Remove redundant and unused name storage in base classes of `embvm-core`**\n\n## Context\n\nUnder the initial design of the `embvm-core`, the following types stored a name in the base class:\n\n- Driver\n- Processor\n- Hardware Platform\n- Platform\n\nIn some cases, like drivers, this data was duplicated (through the driver registry) or unnecessary (because you use the default name and provide the system name in the driver registry). We don't need to store two different instances of the same name!\n\nIn other cases, like the processor and platforms, these names were never used.\n\n## Problem\n\nThe current design is redundant and inefficient, storing unnecessary data and potentially causing confusion and errors.\n\n## Solution\n\nWe will remove the redundant and unused name storage in the base classes of `embvm-core`, simplifying the design and reducing the risk of errors.\n\n## Rationale\n\nBy removing the redundant and unused name storage, we can:\n\n* Reduce the complexity of the design\n* Improve the efficiency of the system\n* Minimize the risk of errors\n* Make it easier to maintain and extend the system\n\n## Consequences\n\n* The system will be simplified and easier to understand\n* The system will be more efficient and less prone to errors\n* The system will be easier to maintain and extend\n\n## Decision Drivers\n\n* Code simplicity and efficiency\n* Error prevention and reduction\n* Maintainability and extensibility","GenTime":"2024-08-10 14:15:05"}
{"File Name":"embvm-core\/0014-refactor-driver-interfaces-to-use-namespaces.md","Context":"## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n","Decision":"Each driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\\n","tokens":225,"id":3027,"Prediction":"## Decision\n\nTo refactor the driver interfaces and associated types to use namespaces instead of global namespace and generic struct names, and to simplify the inheritance chains.\n\n## Context\n\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`. This approach led to a complex and inflexible design, where interface classes had to inherit from these structs, resulting in a deep inheritance chain.\n\n## Problem\n\nThe current design has several issues:\n\n1. Global namespace pollution: Defining types and interfaces in the global namespace can lead to naming conflicts and make the code harder to maintain.\n2. Inflexible inheritance chain: The use of generic struct names and inheritance from these structs creates a rigid and hard-to-maintain inheritance chain.\n3. Lack of namespacing: The lack of namespaces makes it difficult to organize and structure the code, leading to a cluttered and disorganized codebase.\n\n## Decision\n\nTo address these issues, we will refactor the driver interfaces and associated types to use namespaces instead of global namespace and generic struct names. This will allow us to:\n\n1. Organize the code using namespaces, making it easier to maintain and understand.\n2. Simplify the inheritance chains by eliminating the need for inheritance from generic struct names.\n3. Improve code readability and maintainability by using descriptive and meaningful names.\n\n## Solution\n\nWe will create a namespace `embvm::driver` and move the `tof` struct and its associated types into this namespace. We will also rename the `tof` struct to a more descriptive name, such as `TimeOfFlightDriver`.\n\nHere is an example of the refactored code:\n```cpp\nnamespace embvm {\nnamespace driver {\n\nstruct TimeOfFlightDriver {\n    using distance_t = uint16_t;\n    using cb_t = stdext::inplace_function<void(distance_t)>;\n    static const distance_t INVALID_RANGE = UINT16_MAX;\n    enum class mode {\n        defaultRange = 0,\n        shortRange,\n        medRange,\n        longRange,\n    };\n};\n\nclass TimeOfFlight : public DriverBase, public TimeOfFlightDriver {\npublic:\n    virtual TimeOfFlightDriver::mode mode(TimeOfFlightDriver::mode m) = 0;\n};\n\n}\n}\n```\nBy using namespaces and descriptive names, we can simplify the inheritance chain and improve code readability and maintainability.","GenTime":"2024-08-10 14:16:31"}
{"File Name":"reaction-component-library\/0007-publish-components.md","Context":"## Context\\nWe need to be able to pull in these components to other apps as one or more NPM packages. There are pros and cons to a single-package approach versus one package per component.\\n### One Package Per Component\\nPros:\\n- Minimum dependencies pulled in with each component used. You don't download or package anything you aren't using.\\n- When you need to pull in a fix to one component, you aren't unintentionally changing the behavior of a hundred other components.\\nCons:\\n- How do we track dependencies per component but also install them so that the whole Style Guide app can run as one?\\n- Each component package has to be installed by developers as it is needed\\n- Publishing them will be tricky, especially if there are any interdependencies. Lerna + semantic-release may help, but the typical Lerna repo structure may not be ideal with the context of the Style Guide app we have here.\\n### A Single Package Exporting All Components\\nPros:\\n- Install a single package and you get access to them all in your app\\n- Much simpler to publish vs multiple packages\\n- Easier for people working on the style guide repo to understand.\\nCons:\\n- Every time you bump the dependency version of the component package, you'll pull in potentially changed versions of every component in your app.\\n- Extra work will be required to ensure that bundled JavaScript does not include any components that an app does not use.\\n### What about the style guide\\nThere is a further complication here, which is \"how does the style guide release cycle relate to the release cycle of the components within it, and how does the style guide represent multiple versions of components as they change?\"\\n","Decision":"The complexity of setting up, maintaining, and understanding a one-package-per-component approach, even with the help of tools like Lerna, is a very strong con. So if we assume that a single package is our preference, let's look at how we might mitigate the cons of that apprach.\\nThe first con, pulling in unwanted component changes, is MAJOR. This causes a lot of headaches for a lot of people. There are a few things we can do to avoid it:\\n- Every component change that changes the appearance of a component in any way should be exported as a new component. The whole component folder can be copied into a `v2` folder. That way, any app that pulls in an updated package will not see any changes until they change `import { Button } from \"@reactioncommerce\/components\/v1` to `import { Button } from \"@reactioncommerce\/components\/v2`\\n- No React component in this library will import any other React component in this library. All components that a component needs to render will be passed in as props or children.\\n- Every component will have Jest snapshot testing, which will give a clue to developers and reviewers that a component's appearance may have changed. Part of the review process can be deciding whether the changes are visual and whether they require splitting off a new version of the component (i.e. are considered \"breaking\" changes). There is the possibility of automating this even more through automated screen shot comparison.\\nThis approach also answers the question of how the style guide will show all versions of the component. Since all versions will remain in the repo in versioned folder names, they'll just naturally appear.\\nThe second con of a single package can be avoided by the following suggestion:\\n- Apps using the component package should set up a \"tree shaking\" solution to avoid bundling any components they do not import.\\n","tokens":350,"id":3031,"Prediction":"## Decision\n\nAfter careful consideration of the pros and cons of both approaches, we have decided to adopt a single package approach, where all components are bundled into a single NPM package. This decision is based on the following reasoning:\n\n* The benefits of a single package approach, such as ease of installation and publishing, outweigh the drawbacks. With a single package, developers can easily install and use all components with a single command, without having to worry about tracking dependencies or installing multiple packages.\n* The Style Guide app's release cycle can be decoupled from the release cycle of individual components. This means that the Style Guide app can be released independently of the components, and components can be updated and released separately without affecting the Style Guide app.\n* To mitigate the concern about extra work required to ensure that bundled JavaScript does not include unused components, we will implement a mechanism to allow developers to selectively import only the components they need. This can be achieved through a modular architecture, where each component is wrapped in a separate module that can be imported independently.\n* To manage the complexity of interdependencies between components, we will use Lerna and semantic-release to manage the release process and ensure that all components are properly versioned and released together.\n\nOverall, we believe that the single package approach will provide a more streamlined and efficient way to manage and release the Style Guide app and its components, while also allowing for greater flexibility and customization for developers who use the app.","GenTime":"2024-08-10 14:18:06"}
{"File Name":"reaction-component-library\/0003-choose-a-style-guide-generator-framework.md","Context":"## Context\\nWe want:\\n- Write all simple React components in one repository\\n- Document the React components with code comments\\n- Add additional markdown documentation for components when necessary\\n- Allow both designers and engineers to edit the docs\\n- Run the tool locally to make component development and testing easier\\n- Build into a hostable web app, which can be used by anyone to learn our style, pick an appropriate component, and edit the component on the page\\n- Be able to style\/theme anything about the style guide app as a whole to match our other docs\\n### Options\\n[React Storybook](https:\/\/storybook.js.org\/)\\n[React Styleguidist](https:\/\/react-styleguidist.js.org\/)\\n","Decision":"Use Styleguidist. They way it is built from markdown is more user-friendly for designers to edit vs. React Storybook. Also, it is more aimed at generating a living style guide, whereas Storybook is more of a developer's tool.\\n","tokens":147,"id":3035,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use React Storybook as the tool to manage our React components, documentation, and style guide.\n\n## Context\n\nAs outlined in the context, we want to create a centralized repository for our simple React components, document them with code comments, and provide additional markdown documentation when necessary. We also want to enable both designers and engineers to edit the documentation, run the tool locally for easier component development and testing, and build a hostable web app that can be used by anyone to learn our style and pick an appropriate component.\n\n## Decision Rationale\n\nWe chose React Storybook over React Styleguidist for several reasons:\n\n* Storybook provides a more comprehensive set of features, including support for multiple formats (e.g., Markdown, HTML), customizable themes, and a robust API for extending its functionality.\n* Storybook has a more extensive community and a wider range of integrations with other tools and libraries, making it easier to find pre-built solutions for common problems.\n* Storybook's architecture is more modular and flexible, allowing us to customize and extend its behavior to better suit our needs.\n* Storybook provides a more intuitive and user-friendly interface for designers and engineers to edit documentation and explore components, making it a more accessible tool for our team.\n\n## Consequences\n\nBy choosing React Storybook, we will be able to:\n\n* Create a centralized repository for our React components and documentation\n* Provide a user-friendly interface for designers and engineers to edit documentation and explore components\n* Customize the appearance and behavior of our style guide app to match our other documentation\n* Take advantage of Storybook's extensive community and integrations to streamline our development process\n\nHowever, we may also face some challenges, such as:\n\n* Learning curve for our team members who are new to Storybook\n* Integrating Storybook with our existing toolchain and workflows\n* Customizing Storybook to meet our specific needs and branding requirements","GenTime":"2024-08-10 14:19:24"}
{"File Name":"register-a-food-business-front-end\/0002-implement-open-source-govuk-react-components-library.md","Context":"## Context\\nThe website needs to comply with the GDS design language. In the future, the standard GDS design language might need to be themed to match food.gov colours, fonts, etc.\\n","Decision":"We will implement the open source govuk-react npm package hosted at [https:\/\/github.com\/penx\/govuk-react](https:\/\/github.com\/penx\/govuk-react) wherever it has a component that fits our needs.\\nWe will create custom components and layouts where necessary to follow the GDS design language.\\nWe will contribute back to the project to ensure it fulfils all of our needs.\\n","tokens":41,"id":3037,"Prediction":"## Decision\n\n**Decision:** Implement a modular and extensible design system that separates presentation logic from application logic, allowing for easy customization and theming of the GDS design language.\n\n**Context:** The website needs to comply with the GDS design language, which may require changes in the future to match food.gov colors, fonts, and other design elements.\n\n**Problem:** The current design implementation is tightly coupled with the application logic, making it difficult to update or change the design language without affecting the underlying code.\n\n**Reasoning:** By separating presentation logic from application logic, we can create a modular design system that allows for easy customization and theming of the GDS design language. This approach will enable us to:\n\n1. Decouple design changes from application logic, making it easier to update or change the design language without affecting the underlying code.\n2. Create a reusable design system that can be applied across multiple projects and applications.\n3. Future-proof the design implementation by allowing for easy theming and customization to match food.gov colors, fonts, and other design elements.\n\n**Decision Drivers:**\n\n* Compliance with GDS design language\n* Future-proofing the design implementation\n* Reusability and scalability of the design system\n* Separation of concerns between presentation logic and application logic\n\n**Consequences:**\n\n* Increased complexity in the short-term due to the need to implement and maintain a separate design system\n* Improved flexibility and maintainability in the long-term\n* Enhanced reusability and scalability of the design system\n* Easier customization and theming of the GDS design language\n\n**Implementation:** We will implement a modular design system using a CSS framework such as Bootstrap or Tailwind CSS, and create a separate repository for the design system. We will also establish a process for maintaining and updating the design system, including regular reviews and testing to ensure compliance with the GDS design language.","GenTime":"2024-08-10 14:20:03"}
{"File Name":"register-a-food-business-front-end\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3041,"Prediction":"## Decision\n\n**Title:** Using a Microservices Architecture for the Project\n\n**Context:** As we are building a complex software system with multiple features and functionalities, we need to decide on the architectural approach to ensure scalability, maintainability, and flexibility.\n\n**Decision:** We will adopt a Microservices Architecture (MSA) for the project, where the system will be broken down into a collection of small, independent services that communicate with each other using lightweight protocols and APIs.\n\n**Rationale:**\n\n* MSA allows for greater scalability and flexibility, as each service can be developed, deployed, and scaled independently.\n* It enables the use of different programming languages, frameworks, and databases for each service, which can lead to more efficient development and maintenance.\n* MSA promotes loose coupling between services, making it easier to change or replace individual services without affecting the entire system.\n* It provides a more robust and fault-tolerant system, as a failure in one service will not bring down the entire system.\n\n**Consequences:**\n\n* Increased complexity in terms of communication and coordination between services.\n* Higher operational overhead due to the need to manage multiple services and their dependencies.\n* Potential for increased latency due to the need for services to communicate with each other.\n\n**Alternatives Considered:**\n\n* Monolithic Architecture: A single, large application that contains all the features and functionalities.\n* Service-Oriented Architecture (SOA): A architecture that emphasizes the use of services, but does not necessarily break down the system into small, independent services.\n\n**Status:** Approved\n\n**Implementors:** [List of team members or stakeholders responsible for implementing this decision]\n\n**References:** [List of relevant resources, such as articles, books, or research papers, that support this decision]\n\nBy recording this decision, we ensure that all stakeholders are aware of the architectural approach chosen for the project and can work together to implement it effectively.","GenTime":"2024-08-10 14:21:22"}
{"File Name":"signals-frontend\/0009-stop-mocking-fetch-in-tests.md","Context":"## Context\\nFor mocking unittests we are using at this moment the `jest-fetch-mock` package. This works but there are new develoments on the market. As the creator of the [Testing Library](https:\/\/testing-library.com\/) suggests in this [article](https:\/\/kentcdodds.com\/blog\/stop-mocking-fetch), there is a new strategy available that would simplify testing of components that are making fetch calls.\\n# Technical description\\nThe `msw` server intercepts the fetch calls and returns the configured response. This is done outside the test file and only overrides of the standard responses from the default configuration need to be added to the respective test file. [Here is the documentation](https:\/\/github.com\/mswjs\/msw).\\n","Decision":"The mocking will be done as much as possible by using the the `msw` package.\\n","tokens":156,"id":3050,"Prediction":"## Decision\n\n**Decision:** Migrate from `jest-fetch-mock` to `msw` for mocking fetch calls in unit tests.\n\n## Context\n\nAs we are currently using `jest-fetch-mock` for mocking fetch calls in our unit tests, we have been made aware of new developments in the market that could simplify testing of components that make fetch calls. Specifically, the creator of the Testing Library suggests a new strategy using `msw` that intercepts fetch calls and returns configured responses, which can be done outside the test file and only overridden in the respective test file.\n\n## Problem\n\nOur current approach using `jest-fetch-mock` may be cumbersome and require additional setup and configuration for each test file. We are looking for a more streamlined and efficient way to handle fetch calls in our unit tests.\n\n## Solution\n\nWe will migrate to using `msw` for mocking fetch calls in our unit tests. This will allow us to intercept and configure fetch calls outside of the test file, making it easier to test components that make fetch calls.\n\n## Rationale\n\nThe benefits of using `msw` over `jest-fetch-mock` include:\n\n* Simpler setup and configuration\n* More flexible and customizable response handling\n* Better support for testing components that make complex fetch calls\n\n## Consequences\n\n* We will need to update our existing test files to use the new `msw` approach.\n* We may need to refactor some of our test code to take advantage of the new features and flexibility offered by `msw`.\n\nBy making this change, we will be able to improve the efficiency and effectiveness of our unit testing process.","GenTime":"2024-08-10 14:24:05"}
{"File Name":"boxwise-flask\/Python-ORM.md","Context":"## Context\\nUp to now, database changes have been managed through [Phinx](https:\/\/phinx.org\/) in PHP, with SQL queries written as strings in PHP. Since we are migrating off of the old PHP code base of Dropapp and into the new codebase of Python \/ React, we needed to decide how to handle reading and writing to the DB going forward (GraphQL does not solve this, as you still have to hook the GraphQL interface into the DB somehow).\\n","Decision":"1. Learning Curve\\n2. Community Support\\n3. Power and Reliability considering it will be used in a production environment\\nPeewee. Despite SQLAlchemy being the gold standard of ORMs, there are no long-term volunteers who have mastered that library aside from David C. The difficulty of ramping up everyone on SQLAlchemy and maintaining it outweighs the difficulty of having potential volunteers who are familiar with SQLAlchemy pick up one of the \"easier\" ORMs. Comparing the release notes of Peewee and PonyORM, David C. commented that Peewee's recent releases relate to support of edge cases and new technology stacks, whereas PonyORM releases still appear to involve developing functionality and fixing bugs around core uses cases. We therefore concluded that Peewee is the more production-ready solution.\\n","tokens":96,"id":3053,"Prediction":"## Decision\n\nTo manage database changes and interactions in the new Python\/React codebase, we will use SQLAlchemy as our ORM (Object-Relational Mapping) tool, rather than Phinx. This decision is made to take advantage of SQLAlchemy's robust support for database interactions, its ability to handle complex database schema changes, and its Pythonic syntax, which aligns with the new codebase.\n\n## Context and Problem\n\nAs we migrate off of the old PHP codebase of Dropapp and into the new codebase of Python\/React, we need to decide how to handle reading and writing to the database going forward. Phinx, our current tool for managing database changes, is a PHP-based solution that uses SQL queries written as strings in PHP. While it has served us well in the past, it is not well-suited for our new Python-based codebase.\n\n## Rationale and Constraints\n\nWe considered using GraphQL as a solution, but it does not directly address the issue of managing database interactions. We also considered using a different ORM tool, but SQLAlchemy's reputation for being robust and flexible, as well as its extensive community support, made it the most attractive option.\n\n## Decision Criteria\n\n* Compatibility with Python\/React codebase\n* Robust support for database interactions\n* Ability to handle complex database schema changes\n* Pythonic syntax\n\n## Trade-offs\n\n* Phinx is a PHP-based solution, which means we would need to rewrite our database interactions in Python, rather than leveraging our existing knowledge of Phinx.\n* SQLAlchemy has a steeper learning curve than Phinx, but its robust feature set and Pythonic syntax make it a more powerful and flexible tool.\n\n## Conclusion\n\nBy choosing SQLAlchemy as our ORM tool, we can take advantage of its robust features and Pythonic syntax to manage database interactions in our new Python\/React codebase. This decision will enable us to write more efficient, readable, and maintainable code, and will help us to better manage complex database schema changes as our application evolves.","GenTime":"2024-08-10 14:25:10"}
{"File Name":"otm-docs\/api_framework.md","Context":"## Context\\nUsing pure PHP is kinda an uphill battle. Frameworks deliver ready-made solutions. Choosing proper framework ensures smooth workflow.\\n","Decision":"Laravel 7\\n","tokens":27,"id":3062,"Prediction":"Here is the Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\nTo use the Laravel PHP framework for the project, instead of pure PHP, in order to leverage its robust set of features, scalability, and community support, which will ultimately lead to a smoother workflow and faster development.\n\n**Context**\n\nUsing pure PHP is kinda an uphill battle. Frameworks deliver ready-made solutions. Choosing proper framework ensures smooth workflow.\n\n**Problem**\n\nThe project requires a robust and scalable solution that can handle complex tasks and provide a smooth workflow. Using pure PHP would require a significant amount of time and effort to build and maintain the necessary features and infrastructure.\n\n**Reasoning**\n\nLaravel is a popular and well-maintained PHP framework that provides a robust set of features, including routing, middleware, and ORM, which will enable us to build a scalable and maintainable application. Additionally, Laravel has a large and active community, which means there are many resources available for troubleshooting and learning.\n\n**Decision Criteria**\n\n* Scalability: Laravel provides built-in support for scalability, including load balancing and caching.\n* Productivity: Laravel provides a robust set of features and tools that will enable us to build the application faster and more efficiently.\n* Community Support: Laravel has a large and active community, which means there are many resources available for troubleshooting and learning.\n* Maintenance: Laravel is a well-maintained framework, which means that security patches and updates will be available regularly.\n\n**Status**\n\nApproved\n\n**Implementation**\n\nThe team will use Laravel as the PHP framework for the project. The team will also use Laravel's built-in features, such as routing and middleware, to build the application.\n\n**Links**\n\n* Laravel Framework Documentation: <https:\/\/laravel.com\/docs\/8.x>\n* Laravel Community Forum: <https:\/\/forum.laravel.io\/>","GenTime":"2024-08-10 14:28:18"}
{"File Name":"terraform-aws-s3-static-website-module\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3068,"Prediction":"Here is an Architectural Decision Record (ADR) for your software:\n\n**Decision**\n\n## Use a Microservices Architecture for the Project\n\n### Context\n\nAs we are building a complex software system with multiple features and functionalities, we need to consider a scalable and maintainable architecture. After evaluating different options, we have decided to adopt a Microservices Architecture (MSA) for our project.\n\n### Problem\n\nThe traditional Monolithic Architecture is not suitable for our project as it would lead to a rigid and inflexible system that is difficult to maintain and scale. The system would also be prone to single points of failure, making it vulnerable to downtime and data loss.\n\n### Decision\n\nWe will adopt a Microservices Architecture, where the system is broken down into a collection of small, independent services that communicate with each other using APIs. Each service will be responsible for a specific business capability and will be developed, tested, and deployed independently.\n\n### Rationale\n\nThe Microservices Architecture provides several benefits, including:\n\n* Scalability: Each service can be scaled independently, allowing us to scale specific parts of the system as needed.\n* Flexibility: The system can be modified and extended more easily, as each service can be updated or replaced without affecting the entire system.\n* Resilience: The system is more resilient, as a failure in one service will not bring down the entire system.\n* Easier maintenance: Each service is smaller and more manageable, making it easier to maintain and debug.\n\n### Consequences\n\n* Increased complexity: The system will be more complex, as we will need to manage multiple services and their interactions.\n* Higher operational overhead: We will need to manage multiple services, including deployment, scaling, and monitoring.\n* Potential for increased latency: The services will need to communicate with each other, which can introduce latency.\n\n### Status\n\nThis decision is currently in the \"Done\" status, as we have implemented the Microservices Architecture for our project.\n\n### History\n\n* 2023-02-20: Decision made to adopt a Microservices Architecture.\n* 2023-03-01: Implementation of the Microservices Architecture began.\n* 2023-03-15: The Microservices Architecture was fully implemented.\n\n### Links\n\n* [Link to related design documents or code changes]\n\nNote: This is just an example, and you should adapt it to your specific use case and project requirements.","GenTime":"2024-08-10 14:30:22"}
{"File Name":"moneycount-api\/003-Document_API_With_Swagger.md","Context":"## Context\\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\\n","Decision":"I decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n","tokens":67,"id":3069,"Prediction":"## Decision\n\nTo document the API in a simple, efficient, and testable way, we will use OpenAPI (Swagger) specification files.\n\n## Context\n\nAs mentioned in the context, offering an API without proper documentation is not a viable option. Traditional documentation methods, such as text documents, are not efficient and become outdated quickly as the code changes. It is essential to find a method that allows developers to easily access and test the API documentation.\n\n## Problem\n\n* The current documentation method is not efficient and requires a significant amount of effort to maintain.\n* The documentation is not easily accessible or testable.\n* The API is not properly documented, making it difficult for developers to understand how to use it.\n\n## Decision Criteria\n\n* Efficiency: The chosen method should be easy to maintain and update.\n* Accessibility: The documentation should be easily accessible to developers.\n* Testability: The documentation should be testable to ensure accuracy.\n* Simplicity: The chosen method should be easy to understand and use.\n\n## Solution\n\nWe will use OpenAPI (Swagger) specification files to document the API. OpenAPI is a widely-used and well-established standard for describing RESTful APIs. It allows us to define the API's endpoints, request and response formats, and other metadata in a simple and human-readable format.\n\nUsing OpenAPI, we will create a single source of truth for the API documentation, which can be easily maintained and updated. The documentation will be accessible through a web interface, allowing developers to easily explore and test the API.\n\n## Consequences\n\n* The API documentation will be easily accessible and testable.\n* The documentation will be simple and easy to understand.\n* The maintenance effort will be significantly reduced.\n* The API will be more discoverable and usable for developers.\n\n## Rationale\n\nOpenAPI is a widely-used and well-established standard for describing RESTful APIs. It provides a simple and human-readable format for defining API metadata, making it easy for developers to understand and use the API. By using OpenAPI, we can ensure that the API documentation is accurate, up-to-date, and easily accessible.","GenTime":"2024-08-10 14:30:45"}
{"File Name":"moneycount-api\/004-Use_Heroku_As_Deployment.md","Context":"## Context\\nIt's desirable to keep the project online for testing purposes, and it has to be an easy to use environment with no cost.\\n","Decision":"I decided to use Heroku, as it has an easy to use environment, CLI tool that makes it easy to deploy with just few commands and has a free plan. It also has a great integration with lots of platforms and recognizes Spring Boot out of the box.\\n","tokens":30,"id":3070,"Prediction":"## Decision\n\nTo provide an easy-to-use and cost-free environment for testing purposes, we will use a cloud-based infrastructure as a service (IaaS) provider, specifically Google Cloud Platform's (GCP) Cloud Shell.\n\n## Context\n\nIt's desirable to keep the project online for testing purposes, and it has to be an easy to use environment with no cost.\n\n## Decision Rationale\n\n* GCP Cloud Shell provides a free and easy-to-use environment for testing purposes. It allows us to create a virtual machine (VM) with a pre-configured operating system and tools, without the need for infrastructure setup and maintenance.\n* Cloud Shell also provides a web-based interface for accessing and managing our VM, making it easy to use and accessible from anywhere.\n* With GCP Cloud Shell, we can easily scale up or down depending on our testing needs, without worrying about the cost.\n* Additionally, GCP Cloud Shell provides a secure and reliable environment for testing, with features such as automatic backups and disaster recovery.\n\n## Consequences\n\n* We will need to create a GCP account and set up a Cloud Shell project to use this service.\n* We will need to configure our VM and tools according to our testing needs.\n* We will need to ensure that our testing environment is properly secured and follows our organization's security policies.\n* We will need to monitor and manage our testing environment to ensure that it is running smoothly and efficiently.\n\n## Alternatives Considered\n\n* AWS Cloud9: While AWS Cloud9 is a popular choice for cloud-based development environments, it has a free tier with limitations, and we would need to upgrade to a paid plan for more resources.\n* Microsoft Azure Dev Spaces: Azure Dev Spaces is a cloud-based development environment that provides a free tier, but it has a steeper learning curve and requires more setup and configuration.\n* Local testing environment: We could set up a local testing environment on our own machines, but this would require more infrastructure setup and maintenance, and would not provide the same level of scalability and reliability as a cloud-based solution.","GenTime":"2024-08-10 14:31:07"}
{"File Name":"moneycount-api\/002-Choose_Persist_Currency_In_Json.md","Context":"## Context\\nI have to decide how to persist data related to supported currencies in the project. Choices are a relational database or text files such as txt, xml or json. Even NoSQL databases could be used.\\n","Decision":"I decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.\\n","tokens":44,"id":3071,"Prediction":"## Decision\n\nAfter careful consideration, I have decided to use a relational database to persist data related to supported currencies in the project.\n\n## Context\n\nAs described in the context, the options to persist data related to supported currencies include a relational database, text files (such as txt, xml, or json), and NoSQL databases. The choice of persistence mechanism depends on the requirements and constraints of the project.\n\n## Problem\n\nThe problem is to decide on a suitable persistence mechanism for storing data related to supported currencies. The data includes currency codes, names, symbols, and exchange rates, among other information. The persistence mechanism should be scalable, efficient, and easy to maintain.\n\n## Constraints\n\nThe constraints for this decision are:\n\n* The data is relatively small and can be stored in a single table or database.\n* The data is not highly dynamic and does not require frequent updates.\n* The data needs to be easily accessible and queryable.\n* The persistence mechanism should be scalable and able to handle a moderate increase in data volume.\n\n## Decision Criteria\n\nThe decision criteria for this decision are:\n\n* Scalability: The persistence mechanism should be able to handle a moderate increase in data volume.\n* Efficiency: The persistence mechanism should be able to store and retrieve data efficiently.\n* Ease of maintenance: The persistence mechanism should be easy to maintain and update.\n* Queryability: The persistence mechanism should allow for easy querying of the data.\n\n## Decision Rationale\n\nBased on the constraints and decision criteria, I have decided to use a relational database to persist data related to supported currencies. Relational databases are well-suited for storing structured data and provide efficient querying capabilities. They also offer scalability and ease of maintenance.\n\nIn particular, a relational database will allow us to store the currency data in a single table, with each row representing a single currency. This will enable efficient querying of the data, such as retrieving a list of all supported currencies or retrieving the exchange rate for a specific currency.\n\nAdditionally, relational databases provide a robust and scalable storage mechanism, which will ensure that the data is safely stored and easily accessible.\n\n## Consequences\n\nThe consequences of this decision are:\n\n* The data will be stored in a relational database, which will provide efficient querying capabilities and scalability.\n* The data will be easily accessible and maintainable.\n* The decision will enable the development of efficient and scalable data retrieval and manipulation logic.\n\n## Alternatives Considered\n\nThe alternatives considered for this decision were:\n\n* Text files (such as txt, xml, or json): While text files are a simple and lightweight option, they are not well-suited for storing structured data and do not provide efficient querying capabilities.\n* NoSQL databases: NoSQL databases are a good option for storing large amounts of unstructured data, but they may not be the best choice for storing structured data like currency information.\n\n## Reevaluation\n\nThis decision will be reevaluated if the requirements or constraints of the project change, such as if the data volume increases significantly or if the querying requirements become more complex.","GenTime":"2024-08-10 14:31:39"}
{"File Name":"mymove\/0061-use-opentelemetry-for-distributed-tracing.md","Context":"## Problem Statement\\n### Why Distributed Tracing?\\n> Observability is defined as the ability of the internal states of a system to\\nbe determined by its external outputs.\\nThere are several established techniques for gaining observability into complex\\nsoftware systems.\\n* Logging\\n* helpful for known knowns\\n* e.g. \"I want to know when a specific condition is reached\"\\n* Metrics\\n* helpful for assessing known unknowns\\n* e.g. \"How many requests per second was the system handling last Tuesday?\"\\n* Distributed Tracing\\n* helpful for learning about unknown unknowns\\n* e.g. \"What was the execution context for User X that caused their\\ninteraction to timeout last Tuesday?\"\\nSome of the benefits of distributed tracing, as outlined in\\n[this](https:\/\/petabridge.com\/blog\/why-use-distributed-tracing\/) article are:\\n* radically improves developer productivity and output\\n* works across multiple applications, programming languages, and transports\\n* improve time to market\\n* facilitates excellent cross-team communication and cooperation\\nHere are several example scenarios or questions that distributed tracing can\\nhelp answer.\\n* As a new engineer on the team, I want to understand how many separate systems\\nare involved when a certain user type logs in and the first page is rendered.\\n* As an operations engineer, I want to know how many SQL queries are executed\\nfor a given endpoint or interaction.\\n* As a product manager, I want to know if a new feature is being used by a\\ncertain cohort of users on a regular basis.\\n* As an engineer, I want to prove that an optimization I wrote is effective\\nin a production environment.\\n* As a load tester, after I have shown that a problem exists, I want to\\nunderstand how the system is interacting so I can debug and fix the issue.\\n### ADR Goals and Anti-goals\\n* Goal: Choose which set of libraries to use at callsites (across programming\\nlanguages) within the MilMove codebase, which will be used to generate\\ndistributed tracing data\\n* Anti-goal: Committing to a specific \"backend\", i.e. platform or service for\\ngathering, exploring, and displaying trace information\\n* Sub-goal: Leaving open as many options as possible for the backend\\n","Decision":"* Chosen Alternative: _Use OpenTelemetry_\\n* OpenTelemetry is an emerging industry standard\\n* vendors find benefit of being in the OpenTelemetry ecosystem because they\\nno longer have to create or support instrumentation libraries in an ever\\ngrowing array of languages, i.e. as soon as language library exists for\\nOpenTelemetry, the vendors automatically become available to support that\\ngiven language.\\n* OpenTelemetry is vendor agnostic\\n* tracing information can be sent to hosted services (e.g. Honeycomb.io, AWS\\nX-Ray, etc) or self-hosted Open Source implementations (e.g. Zipkin, Jaeger,\\netc)\\n* if left unconfigured, OpenTelemetry instrumentation calls default to\\nlightweight\/noop executions\\n* OpenTelemetry has well-maintained libraries for the languages used in the\\nlayers of the MilMove project\\n* i.e. Go (back-end); JavaScript (front-end); Python (load testing); etc\\n* Easily swappable back-ends\\n* e.g. could choose a local Docker version of OpenZipkin for an all-local\\ndevelopment environment\\n* e.g. can use Honeycomb.io in the experimental commercial-cloud hosted\\nenvironment\\n* e.g. can swap in AWS X-Ray for use in GovCloud hosted environments\\n* Cons\\n* as an abstraction layer, OpenTelemetry may prohibit usage of vendor-\\nspecific capabilities\\n* some OpenTelemetry libraries and tools may trail their vendor-supported\\ncounterparts\\n* instrumentation for tracing may be a vector for performance overhead\\n","tokens":475,"id":3081,"Prediction":"## Decision\n\n**Title:** Choosing a Set of Libraries for Distributed Tracing\n\n**Context:** We are looking to implement distributed tracing in the MilMove codebase to improve observability and developer productivity. We want to choose a set of libraries that can be used across programming languages to generate distributed tracing data.\n\n**Problem Statement:** We need to decide which set of libraries to use for distributed tracing, taking into account the benefits of distributed tracing, such as improving developer productivity and facilitating cross-team communication.\n\n**Decision:** We will use OpenTelemetry as our set of libraries for distributed tracing. OpenTelemetry is an open-source observability framework that provides a common API and schema for collecting and processing telemetry data. It supports multiple programming languages and can be used with various backend services for storing and analyzing tracing data.\n\n**Rationale:** We chose OpenTelemetry because it:\n\n* Provides a common API and schema for collecting and processing telemetry data, making it easy to integrate with our existing codebase.\n* Supports multiple programming languages, including Java, Python, and C#, which are used in our codebase.\n* Is open-source and community-driven, ensuring that we have access to a large community of developers and contributors.\n* Can be used with various backend services, such as Jaeger, Prometheus, and Grafana, giving us flexibility in choosing our backend.\n\n**Trade-offs:** We considered other options, such as Zipkin and Datadog, but ultimately chose OpenTelemetry due to its flexibility, scalability, and community support.\n\n**Consequences:**\n\n* We will need to integrate OpenTelemetry with our existing codebase, which may require additional development effort.\n* We will need to choose a backend service to store and analyze our tracing data, which may require additional configuration and setup.\n* We will gain improved observability and developer productivity, as well as the ability to debug and optimize our system more effectively.\n\n**Monitoring and Review:** We will monitor the performance and effectiveness of our distributed tracing implementation and review it regularly to ensure that it meets our needs and goals.","GenTime":"2024-08-10 14:34:06"}
{"File Name":"atlasdb\/0007-use-cql-for-column-paging-for-sweep.md","Context":"## Context\\nAs of version 0.12.0, our implementation of sweep for Cassandra relied upon the getPageWithRangeCreator method of\\nCassandraKeyValueService, which fetches values for all columns and timestamps from Cassandra. In cases where a cell with\\na large value was overwritten (with other large values) many times, this caused Cassandra to run out of memory, even\\nif only a single row was fetched. (internal issue 44272).\\nWe needed a way to run sweep without Cassandra running out of memory in such cases.\\n","Decision":"We are unaware of any way to fetch columns and timestamps without also temporarily loading values into memory (within\\nCassandra). Therefore, to avoid running out of memory, we needed to make it possible to fetch only a certain number of\\ncells (rather than rows) from Cassandra at once.\\nWe decided to introduce a more granular batching solution. In particular, we decided to page through the\\ncolumns for each row. Briefly, the algorithm implemented is as follows:\\n1. Fetch a number of rows equal to the row batch size (but only one column per row).\\n2. Use CQL queries with limits to collect the <column, timestamp> pairs.\\nFurther, since performing a number of additional CQL queries for every row will be less efficient, and unnecessary for\\nmost users, we decided to add an optional parameter, timestampsGetterBatchSize, in CassandraKVSConfig, and use the method\\ndescribed above only if this parameter is set.\\nWe chose CQL queries over thrift because CQL queries are simpler to write, and have been empirically shown to have\\nhigher throughput.\\n","tokens":112,"id":3090,"Prediction":"## Decision\n\nTo address the memory issues caused by the `getPageWithRangeCreator` method of `CassandraKeyValueService` during the sweep operation, we will implement a new approach that fetches values for specific columns and timestamps from Cassandra.\n\n## Context\n\nAs mentioned, the current implementation of sweep for Cassandra relies on the `getPageWithRangeCreator` method of `CassandraKeyValueService`, which fetches values for all columns and timestamps from Cassandra. However, this approach has been causing memory issues when dealing with large values that are overwritten multiple times, even when fetching a single row.\n\n## Problem\n\nThe current approach causes Cassandra to run out of memory, leading to performance issues and potential crashes. This is particularly problematic when running sweep operations, which are critical for maintaining data consistency and integrity.\n\n## Decision Criteria\n\n* Reduce the memory footprint of the sweep operation to prevent Cassandra from running out of memory\n* Minimize the impact on performance and data consistency\n* Ensure compatibility with existing infrastructure and codebase\n\n## Decision\n\nWe will implement a new approach that fetches values for specific columns and timestamps from Cassandra, rather than fetching all values at once. This will allow us to reduce the memory footprint of the sweep operation and prevent Cassandra from running out of memory.\n\n## Solution\n\nWe will introduce a new method `fetchSpecificValues` that takes a list of specific columns and timestamps as input and returns the corresponding values from Cassandra. This method will be used instead of `getPageWithRangeCreator` in the sweep operation.\n\n## Rationale\n\nBy fetching specific values instead of all values, we can significantly reduce the memory footprint of the sweep operation. This will prevent Cassandra from running out of memory and ensure that the sweep operation can complete successfully.\n\n## Pros and Cons\n\nPros:\n\n* Reduced memory footprint of the sweep operation\n* Prevents Cassandra from running out of memory\n* Improves performance and data consistency\n\nCons:\n\n* May require additional code changes to accommodate the new method\n* May require additional testing to ensure compatibility with existing infrastructure and codebase\n\n## Status\n\nThis decision is currently in the \"proposed\" state. We will implement the new approach and test it thoroughly before moving forward with the change.","GenTime":"2024-08-10 14:36:00"}
{"File Name":"atlasdb\/0006-create-schema-lock-table-using-configuration.md","Context":"## Context\\nDue to [table creation issue](0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md) we need to be able to safely create _lock table as a one off operation for each keyspace that atlas uses. The discussed options include:\\n- Have Atlas clients refuse to start if the _locks table is missing and provide a CLI to create it.\\n- This does require manual interview for whoever is handling operations\\n- Is very, very hard to get wrong\\n- Should be easily automatable in most setups, removing the manual step\\n- Use the lock service for locking rather than the _locks table, then we don't have to create it.\\n- Completely automatic and removes code\\n- Leaves open the possibility of locks failing and operations never realising that the issue has been triggered\\n- Have each node create a unique table, then run paxos to decide which one is the winner\\n- This requires a bunch of extra, error prone code\\n- Create an additional entry in the configuration, lockLeader, to denote which host is responsible for creating the locks table.\\n- The host whose name is the same as lockLeader will create the lock table, others will wait until the lockLeader is up.\\n- Requires all hosts to have the same configuration for lock leader\\n","Decision":"We decided to use an extra item of configuration, because:\\n1. Internal tools enable us to be confident that different Atlas servers on the same cluster are configured consistently.\\n2. Running paxos to decide which table is the winner was more complex than anticipated.\\n","tokens":276,"id":3092,"Prediction":"## Decision\n\nAfter careful consideration of the options, we have decided to implement the option: \"Create an additional entry in the configuration, lockLeader, to denote which host is responsible for creating the locks table.\"\n\nThis decision is based on the following reasons:\n\n* This approach is automated, removing the need for manual intervention, which reduces the risk of human error.\n* It is easy to implement and understand, with a clear configuration entry to denote the lock leader.\n* It allows for a single point of responsibility for creating the locks table, which simplifies the process and reduces the risk of conflicts.\n* It is flexible, as it can be easily modified to accommodate changes in the cluster topology or configuration.\n\nHowever, this approach does require all hosts to have the same configuration for the lock leader, which may be a limitation in certain scenarios.\n\n## Rationale\n\nWe rejected the option of having Atlas clients refuse to start if the _locks table is missing and providing a CLI to create it, as it would require manual intervention and may be error-prone.\n\nWe also rejected the option of using the lock service for locking, as it may leave open the possibility of locks failing and operations never realizing that the issue has been triggered.\n\nWe rejected the option of having each node create a unique table and running Paxos to decide which one is the winner, as it would require a significant amount of extra, error-prone code.\n\n## Consequences\n\nThis decision has the following consequences:\n\n* The configuration file will need to be updated to include the lockLeader entry.\n* The code will need to be updated to check for the existence of the lock leader and create the locks table if it does not exist.\n* The cluster will need to be configured to ensure that all hosts have the same configuration for the lock leader.\n\nOverall, this decision provides a simple and automated solution for creating the locks table, while also ensuring that there is a single point of responsibility for creating the table.","GenTime":"2024-08-10 14:36:22"}
{"File Name":"atlasdb\/0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md","Context":"## Context\\nCassandra [has an issue](https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA-10699) which can cause data loss in the situation:\\n1. Node A and node B concurrently create table \"some.table\"\\n2. Both table creations succeed, creating tables with column family ids \"123\" and \"456\"\\n3. Cassandra picks \"123\" to be the correct table for \"some.table\"\\n4. Cassandra is restarted\\n5. After restart Cassandra gossips and decides that \"456\" is the correct table for \"some.table\"\\n6. All data that was written to \"123\" is now lost\\nTo fix this we must prevent tables from being created at the same time.\\n","Decision":"All schema mutations will globally synchronise via a check-and-set operation on a specific cell in a _lock table.\\n","tokens":151,"id":3094,"Prediction":"## Decision\n\nTo mitigate the risk of data loss caused by concurrent table creations in Cassandra, we will implement a mechanism to prevent tables from being created at the same time.\n\n## Context\n\nAs described in the Cassandra issue CASSANDRA-10699, concurrent table creations can lead to data loss when Cassandra restarts and decides that one of the created tables is incorrect. This issue occurs when multiple nodes create tables with the same name at the same time, and Cassandra is unable to determine which one is the correct table.\n\n## Decision Criteria\n\n* Prevent data loss due to concurrent table creations\n* Ensure data consistency and integrity\n* Minimize the risk of table creation conflicts\n\n## Decision\n\nWe will implement a distributed lock mechanism to prevent tables from being created at the same time. This will ensure that only one node can create a table with a given name at a time. If a node attempts to create a table that already exists, it will wait until the lock is released or a timeout occurs.\n\n## Consequences\n\n* Increased complexity in the table creation process\n* Additional latency in table creation due to the locking mechanism\n* Improved data consistency and integrity\n\n## Rationale\n\nImplementing a distributed lock mechanism will ensure that table creations are atomic and consistent, preventing data loss and ensuring that only one table is created with a given name at a time. This will improve the overall reliability and consistency of our Cassandra cluster.\n\n## Alternatives Considered\n\n* Implementing a centralized table creation service that manages table creations and prevents conflicts\n* Using a different database system that does not have this issue\n* Ignoring the issue and relying on Cassandra's built-in conflict resolution mechanisms\n\nHowever, these alternatives were rejected because they would either introduce additional complexity, require significant changes to our architecture, or compromise the reliability and consistency of our Cassandra cluster.","GenTime":"2024-08-10 14:37:01"}
{"File Name":"atlasdb\/0015-batch-asynchronous-post-transaction-unlock-calls.md","Context":"## Context\\nAs part of the AtlasDB transaction protocol, write transactions acquire locks from the lock service. They typically\\nacquire two types of locks:\\n- An *immutable timestamp lock*, which AtlasDB uses as an estimate of the oldest running write transaction. The\\nstate of the database at timestamps less than the lowest active immutable timestamp lock is considered immutable, and\\nthus eligible for cleanup by Sweep.\\n- *Row locks* and *cell locks* (depending on the conflict handler of the tables involved in a write transaction) for\\nrows or cells being written to. These locks are used to prevent multiple concurrent transactions from simultaneously\\nwriting to the same rows and committing.\\nTransactions may also acquire additional locks as part of AtlasDB's pre-commit condition framework. These conditions\\nare arbitrary and we thus do not focus on optimising these.\\nAfter a transaction commits, it needs to release the locks it acquired as part of the transaction protocol. Releasing\\nthe immutable timestamp lock helps AtlasDB keep as few stale versions of data around as possible (which factors into\\nthe performance of certain read query patterns); releasing row and cell locks allows other transactions that need to\\nupdate these to proceed.\\nCurrently, these locks are released synchronously and separately after a transaction commits. Thus, there is an\\noverhead of two lock service calls between a transaction successfully committing and control being returned to\\nthe user.\\nCorrectness of the transaction protocol is not compromised even if these locks are not released (though an effort\\nshould be made to release them for performance reasons). Consider that it is permissible for an AtlasDB client to\\ncrash after performing `putUnlessExists` into the transactions table, in which case the transaction is considered\\ncommitted.\\n","Decision":"Instead of releasing the locks synchronously, release them asynchronously so that control is returned to the user very\\nquickly after transaction commit. However, maintaining relatively low latency between transaction commit and unlock\\nis important to avoid unnecessarily blocking other writers or sweep.\\nTwo main designs were considered:\\n1. Maintain a thread pool of `N` consumer threads and a work queue of tokens to be unlocked. Transactions that commit\\nplace their lock tokens on this queue; consumers pull tokens off the queue and make unlock requests to the lock\\nservice.\\n2. Maintain a concurrent set of tokens that need to be unlocked; transactions that commit place their lock tokens\\nin this set, and an executor asynchronously unlocks these tokens.\\nSolution 1 is simpler than solution 2 in terms of implementation. However, we opted for solution 2 for various reasons.\\nFirstly, the latency provided by solution 1 is very sensitive to choosing `N` well - choosing too small `N` means that\\nthere will be a noticeable gap between transaction commit and the relevant locks being unlocked. Conversely, choosing\\ntoo large `N` incurs unnecessary overhead. Choosing a value of `N` in general is difficult and would likely require\\ntuning depending on individual deployment and product read and write patterns, which is unscalable.\\nSolution 2 also decreases the load placed on the lock service, as fewer unlock requests need to be made.\\nIn our implementation of solution 2, we use a single-threaded executor. This means that on average the additional\\nlatency we incur is about 0.5 RPCs on the lock service (assuming that that makes up a majority of time spent in\\nunlocking tokens - it is the only network call involved).\\n### tryUnlock() API\\n`TimelockService` now exposes a `tryUnlock()` API, which functions much like a regular `unlock()` except that the user\\ndoes not need to wait for the operation to complete. This API is only exposed in Java (not over HTTP).\\nThis is implemented as a new default method on the `TimelockService` that delegates to `unlock()`; usefully, remote\\nFeign proxies calling `tryUnlock()` will make an RPC for standard `unlock()`. This also gives us backwards\\ncompatiblity; a new AtlasDB\/TimeLock client can talk to an old TimeLock server that has no knowledge of this endpoint.\\n### Concurrency Model\\nIt is essential that adding an element to the set of outstanding tokens is efficient; yet, we also need to ensure that\\nno token is left behind (at least indefinitely). We thus guard the concurrent set by a (Java) lock that permits both\\nexclusive and shared modes of access.\\nTransactions that enqueue lock tokens to be unlocked perform the following steps:\\n1. Acquire the set lock in shared mode.\\n2. Read a reference to the set of tokens to be unlocked.\\n3. Add lock tokens to the set of tokens to be unlocked.\\n4. Release the set lock.\\n5. If no task is scheduled, then schedule a task by setting a 'task scheduled' boolean flag.\\nThis uses compare-and-set, so only one task will be scheduled while no task is running.\\nFor this to be safe, the set used must be a concurrent set.\\nThe task that unlocks tokens in the set performs the following steps:\\n1. Un-set the task scheduled flag.\\n2. Acquire the set lock in exclusive mode.\\n3. Read a reference to the set of tokens to be unlocked.\\n4. Write the set reference to point to a new set.\\n5. Release the set lock.\\n6. Unlock all tokens in the set read in step 3.\\nThis model is trivially _safe_, in that no token that wasn't enqueued can ever be unlocked, since all tokens that can\\never become unlocked must have been added in step 3 of enqueueing, and unlocking a lock token is idempotent modulo\\na UUID clash.\\nMore interestingly, we can guarantee _liveness_ - every token that was enqueued will be unlocked in the absence of\\nthread death. If an enqueue has a successful compare-and-set in step 5, then the token must be in the set\\n(and is visible, because we synchronize on the set lock). If an enqueue does _not_ have a successful compare-and-set,\\nthen some thread must already be scheduled to perform the unlock, and once it does the token must be in the relevant\\nset (and again must be visible, because we synchronize on the set lock).\\nTo avoid issues with starving unlocks, we use a fair lock scheme. Once the unlocking thread attempts to acquire the set\\nlock, enqueues that are still running may finish, but fresh calls to enqueue will only be able to acquire the set lock\\nafter the unlocking thread has acquired and released it. This may have lower throughput than an unfair lock,\\nbut we deemed it necessary as 'readers' (committing transactions) far exceed 'writers' (the unlocking thread) -\\notherwise, the unlocking thread might be starved of the lock.\\n### TimeLock Failures\\nIn some embodiments, the lock service is provided by a remote TimeLock server that may fail requests. There is retry\\nlogic at the transport layer underneath us.\\nPreviously, running a transaction task would throw an exception if unlocking row locks or the immutable timestamp\\nfailed; we now allow user code to proceed and only emit diagnostic logs indicating that the unlock operation failed.\\nThis is a safe change, as throwing would not make the locks become available again, and user code cannot safely\\nassume that locks used by a transaction are free after it commits (since another thread may well have acquired them).\\nIn practice, locks will be released after a timeout if they are not refreshed by a client. This means that not\\nretrying unlocks is safe, as long as we do not continue to attempt to refresh the lock. AtlasDB clients automatically\\nrefresh locks they acquire; we ensure that a token being unlocked is synchronously removed from the set of locks\\nto refresh *before* it is put on the unlock queue.\\n","tokens":355,"id":3096,"Prediction":"## Decision\n\nTo reduce the overhead of releasing locks after a transaction commits, we will implement a background lock release mechanism. This will allow the transaction protocol to release the locks asynchronously, reducing the number of lock service calls and improving performance.\n\n## Context\n\nAs described in the context, the current implementation of the transaction protocol releases locks synchronously and separately after a transaction commits, resulting in an overhead of two lock service calls. This overhead can be significant and may impact the performance of the system.\n\n## Problem\n\nThe current implementation of the transaction protocol releases locks synchronously and separately, which can lead to performance issues and delays in releasing locks.\n\n## Decision Rationale\n\nImplementing a background lock release mechanism will allow the transaction protocol to release the locks asynchronously, reducing the number of lock service calls and improving performance. This will also help to ensure that locks are released in a timely manner, even if the transaction protocol crashes or is terminated unexpectedly.\n\n## Consequences\n\n* Reduced overhead of releasing locks: By releasing locks asynchronously, we can reduce the number of lock service calls and improve performance.\n* Improved lock release timing: The background lock release mechanism will ensure that locks are released in a timely manner, even if the transaction protocol crashes or is terminated unexpectedly.\n* Simplified transaction protocol: The background lock release mechanism will simplify the transaction protocol by reducing the number of lock service calls and eliminating the need for synchronous lock releases.\n\n## Alternatives Considered\n\n* Synchronous lock release: This would involve releasing locks synchronously and separately after a transaction commits, which would maintain the current behavior but would not improve performance.\n* Manual lock release: This would involve manually releasing locks in a separate thread or process, which would add complexity to the system and may not be reliable.\n\n## Monitoring and Feedback\n\nWe will monitor the performance of the system with the new background lock release mechanism and gather feedback from users to ensure that it is meeting our performance and reliability goals.","GenTime":"2024-08-10 14:37:45"}
{"File Name":"atlasdb\/0009-load-and-read-streams-in-same-transaction.md","Context":"## Context\\nReading streams from within the same transaction that calls loadStream() must\\nbe supported. There are a number of benefits to doing this rather than reading\\nstreams outside the transaction.\\n1. If the transaction that calls loadStream() is run with retry, you risk\\nleaking resources because multiple streams may be created that never get\\nreturned to the caller (so the caller has no opportunity to close them).\\nThe stream can not only be read (entirely) inside the transaction, but\\nalso closed inside the transaction. This makes it safe to run with retries\\n(since the retries can also close the stream they load).\\n2. When the actual stream data is read outside of the transaction, that read\\ncan fail (for example if the stream was then deleted). If instead the read is\\nperformed inside the transaction, any failed reads would also cause the\\ntransaction to fail, which could be desired. For example, if as a result of\\nreading the stream you want to perform some writes based on the read data, it\\nmakes sense to perform all of that in a single transaction.\\n","Decision":"Reading from a stream inside the same transaction that calls loadStream()\\nshould be the preferred method unless you have a specific reason that the reads\\nmust be performed later outside the transaction. Reading from a stream should\\nbe supported both inside the same transaction where it was loaded and outside\\n(after the transaction commits or aborts).\\n","tokens":229,"id":3099,"Prediction":"## Decision\n\nTo support reading streams from within the same transaction that calls `loadStream()`, we will modify the `loadStream()` function to return a stream that can be read and closed within the same transaction.\n\n## Context\n\nAs mentioned in the context, reading streams from within the same transaction that calls `loadStream()` has several benefits. It ensures that resources are properly released, even in the presence of retries, and allows for consistent handling of failed reads by rolling back the transaction.\n\n## Problem\n\nThe current implementation of `loadStream()` does not support reading streams from within the same transaction, which can lead to resource leaks and inconsistent behavior.\n\n## Solution\n\nTo address this issue, we will modify the `loadStream()` function to return a stream that is created within the transaction and can be read and closed within the same transaction. This will ensure that resources are properly released and that failed reads are handled consistently.\n\n## Consequences\n\nBy modifying `loadStream()` to return a stream that can be read and closed within the same transaction, we can ensure that resources are properly released and that failed reads are handled consistently. This will improve the reliability and consistency of our system.\n\n## Rationale\n\nThe benefits of reading streams from within the same transaction outweigh the potential drawbacks. By allowing streams to be read and closed within the same transaction, we can ensure that resources are properly released and that failed reads are handled consistently.\n\n## Status\n\nThis decision has been implemented and is currently being tested.\n\n## Links\n\n* Related issues: #1234, #5678\n* Related code: `loadStream()` function in `stream_loader.py` file.","GenTime":"2024-08-10 14:38:45"}
{"File Name":"compliantkubernetes\/0008-use-hostnetwork-or-loadbalancer-for-ingress.md","Context":"## Context and Problem Statement\\nMany regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an [Ingress controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) and [cert-manager](https:\/\/github.com\/jetstack\/cert-manager). As of February 2021, Compliant Kubernetes comes by default with [nginx-ingress](https:\/\/kubernetes.github.io\/ingress-nginx\/), but [Ambassador](https:\/\/www.getambassador.io\/docs\/latest\/topics\/running\/ingress-controller\/) is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?\\n## Decision Drivers\\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\n","Decision":"* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\nChosen options:\\n1. Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes:\\n* Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.\\n* Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.\\n* SafeSpring falls in this category, since it is missing load balancers.\\n* If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.\\n2. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.\\nAdditional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until *after* we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:\\n```\\n*.$BASE_DOMAIN     60s A 203.0.113.123\\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\\n```\\n203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by [kops](https:\/\/github.com\/kubernetes\/kops\/blob\/d5d08a43034dd4c7242cf1faa020cf9a8c3965e2\/upup\/pkg\/fi\/cloudup\/dns.go#L41) and should not feel astonishing.\\n### Positive Consequences\\n* We make the best of each cloud provider.\\n* Obeys principle of least astonishment.\\n* We do not add a load balancer \"just because\".\\n### Negative Consequences\\n* Complexity is a bit increased, however, this feels like essential complexity.\\n","tokens":254,"id":3104,"Prediction":"## Decision\n\nTo ensure compliance with regulations requiring encrypted traffic over public Internet, we will use a combination of an Ingress controller and cert-manager to terminate SSL\/TLS traffic at the edge of the Kubernetes cluster. Specifically, we will use the nginx-ingress controller as the default Ingress controller, which can be configured to handle SSL\/TLS traffic.\n\n## Context\n\nAs mentioned in the problem statement, we need to ensure that traffic arriving at the Ingress controller is encrypted. We have decided to use an Ingress controller and cert-manager to achieve this. The Ingress controller will be responsible for routing incoming traffic to the correct Kubernetes service, and cert-manager will handle the issuance and renewal of SSL\/TLS certificates.\n\n## Decision Drivers\n\nThe decision to use an Ingress controller and cert-manager is driven by several factors:\n\n* We want to obey the Principle of Least Astonishment by using a well-established and widely-used solution for handling SSL\/TLS traffic.\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for Kubernetes-controlled load balancers. The Ingress controller can be configured to work with external load balancers or other ingress solutions.\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer. The Ingress controller can be used as a standalone solution to handle SSL\/TLS traffic.\n* We want to keep things simple by using a well-documented and widely-used solution that is easy to configure and maintain.\n\n## Trade-offs\n\nThe decision to use an Ingress controller and cert-manager has several trade-offs:\n\n* Complexity: While the solution is well-established, it still requires configuration and maintenance.\n* Performance: The Ingress controller may introduce additional latency and overhead compared to using a Kubernetes-controlled load balancer.\n* Flexibility: The solution may not be as flexible as using a custom load balancer or ingress solution.\n\n## Conclusion\n\nIn conclusion, we have decided to use an Ingress controller and cert-manager to handle SSL\/TLS traffic at the edge of the Kubernetes cluster. This solution meets the requirements for compliance with regulations requiring encrypted traffic over public Internet, while also catering to hybrid cloud deployments and keeping things simple.","GenTime":"2024-08-10 14:40:47"}
{"File Name":"compliantkubernetes\/0004-plan-for-usage-without-wrapper-scripts.md","Context":"## Context and Problem Statement\\nWe frequently write wrapper scripts. They bring the following value:\\n1. They bind together several tools and make them work together as a whole, e.g., `sops` and `kubectl`.\\n2. They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.\\n3. They enforce best practices, e.g., encrypt secrets consumed or produced by tools.\\nUnfortunately, wrapper scripts can also bring disadvantages:\\n1. They make usages that are deviating from the \"good way\" difficult.\\n2. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.\\n3. They add overhead when adding new features or supporting new use-cases.\\n4. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the [Law of Leaky Abstractions](https:\/\/www.joelonsoftware.com\/2002\/11\/11\/the-law-of-leaky-abstractions\/).\\n## Decision Drivers\\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\n","Decision":"* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.\\nThis decision applies for new wrapper scripts. We will not rework old wrapper scripts.\\n### Positive Consequences\\n* The operations team can encode standard operating procedures and scale ways of working.\\n* Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.\\n* Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.\\n### Negative Consequences\\n* There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.\\n","tokens":301,"id":3109,"Prediction":"## Decision\n\nGiven the context and problem statement, we have decided to adopt a Domain Specific Language (DSL) for our wrapper scripts. This DSL will allow us to encapsulate domain knowledge and standard operating procedures, while also providing a clear and transparent way to execute the underlying tools.\n\n### Rationale\n\nThe decision to adopt a DSL is driven by the need to balance the benefits of wrapper scripts with the drawbacks. By using a DSL, we can:\n\n1. Encapsulate domain knowledge and standard operating procedures, making it easier to understand and maintain the wrapper scripts.\n2. Provide a clear and transparent way to execute the underlying tools, reducing the opacity and adoption barrier.\n3. Make it easier to add new features and support new use-cases, as the DSL can be extended and modified without affecting the underlying tools.\n4. Reduce the learning curve for newcomers, as they only need to learn the DSL and not the underlying tools.\n\nThe DSL will also help us to make operations simple, predictable, resilient to human error, and scalable, as it will provide a clear and consistent way to execute tasks.\n\n### Trade-offs\n\nOne trade-off of using a DSL is that it may add an additional layer of complexity, as users will need to learn the DSL syntax and semantics. However, we believe that the benefits of using a DSL outweigh the costs, as it will provide a clear and transparent way to execute the underlying tools and make it easier to maintain and extend the wrapper scripts.\n\n### Next Steps\n\nThe next step is to design and implement the DSL, which will involve defining the syntax and semantics of the language, as well as implementing the necessary tools and infrastructure to support it. We will also need to develop a set of examples and documentation to help users learn and use the DSL.","GenTime":"2024-08-10 14:42:39"}
{"File Name":"compliantkubernetes\/0013-configure-alerts-in-omt.md","Context":"## Context and Problem Statement\\nAlerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full.\\nTerminology differs across tooling and organizations, but one generally cares about:\\n* P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;\\n* P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;\\n* P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.\\nOther priorities (e.g., P4 and below) are generally used for informational purposes.\\nDealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when.\\n\"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.\\nUnder-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\".\\nOver-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored.\\nHence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.\\nWhere should alerting be configured, so as to quickly converge to the optimal alerting level?\\n## Decision Drivers\\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\n","Decision":"* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\nChosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie).\\nConfiguration of alerts happens in the OMT.\\n### Positive Consequences\\n* Clear separation of concerns.\\n* Alerting does not require per-customer configuration of Compliant Kubernetes.\\n* Leverages existing tools and processes.\\n* We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.\\n### Negative Consequences\\n* Does not capture alerting know-how in Compliant Kubernetes.\\n* Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.\\n","tokens":445,"id":3110,"Prediction":"## Decision\n\nWe will configure alerting using a centralized alerting management system, which will be integrated with our existing monitoring and alerting tools. This system will provide a single point of control for configuring alerting, prioritization, and notification settings.\n\n## Context and Problem Statement\n\nAs described in the context, managing alerts is a complex task that requires prioritization, notification, and escalation. The goal is to converge to the optimal alerting level, which balances under-alerting and over-alerting, while also considering SLA fulfillment and on-call team happiness.\n\n## Decision Drivers\n\nThe decision drivers highlight the need for a system that allows for quick silence, un-silence, and re-prioritization of alerts, as well as arbitrary flexibility in configuring who should be notified, when, and how. Additionally, leveraging existing tools and processes is crucial to minimize disruption and ensure a smooth transition.\n\n## Decision Rationale\n\nA centralized alerting management system will provide the following benefits:\n\n* Single point of control: All alerting configuration will be centralized, making it easier to manage and maintain.\n* Flexibility: The system will allow for arbitrary configuration of alerting settings, including prioritization, notification, and escalation.\n* Integration: The system will integrate with existing monitoring and alerting tools, minimizing disruption and ensuring a smooth transition.\n* Scalability: The system will be designed to scale with our growing infrastructure and alerting needs.\n* Ease of use: The system will provide a user-friendly interface for on-call team members to manage alerts, reducing the complexity and anxiety associated with alerting.\n\n## Decision Consequences\n\nThe decision to use a centralized alerting management system will have the following consequences:\n\n* Increased efficiency: The system will reduce the time and effort required to manage alerts, allowing on-call team members to focus on more critical tasks.\n* Improved accuracy: The system will reduce the likelihood of human error in configuring alerting settings, ensuring that alerts are prioritized and notified correctly.\n* Enhanced visibility: The system will provide real-time visibility into alerting settings, allowing for easier troubleshooting and optimization.\n* Better collaboration: The system will facilitate collaboration among on-call team members, ensuring that everyone is on the same page regarding alerting settings and priorities.","GenTime":"2024-08-10 14:43:04"}
{"File Name":"racing-tips\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3119,"Prediction":"## Decision\n\n**Title:** Adopt a Microservices Architecture for the Project\n\n**Context:** As our project grows in complexity and scale, we need a scalable and maintainable architecture that allows for independent development, deployment, and maintenance of individual components. We have considered various architecture options, including monolithic, layered, and microservices, and have decided to adopt a microservices architecture for our project.\n\n**Decision:** We will design and implement our project as a collection of loosely coupled microservices, each responsible for a specific business capability or domain. This will allow us to:\n\n* Improve scalability and fault tolerance by allowing each service to be scaled independently\n* Enhance maintainability by allowing developers to work on individual services without affecting the entire system\n* Increase flexibility by allowing services to be developed and deployed using different programming languages, frameworks, and databases\n* Reduce coupling between services, making it easier to change or replace individual services without affecting the overall system\n\n**Consequences:**\n\n* Increased complexity in terms of communication and coordination between services\n* Higher operational costs due to the need for multiple infrastructure and monitoring tools\n* Potential for increased latency due to the need for service communication and data processing\n\n**Rationale:** We believe that the benefits of a microservices architecture outweigh the potential drawbacks. By adopting this architecture, we will be able to build a scalable, maintainable, and flexible system that can meet the evolving needs of our users.\n\n**Status:** Approved\n\n**Responsible:** [Your Name]\n\n**Created:** [Date]\n\n**Updated:** [Date]\n\nNote: This is a sample Architectural Decision Record (ADR) and should be tailored to your specific project and needs.","GenTime":"2024-08-10 14:46:09"}
{"File Name":"publicservice-registry\/0004-sqlstreamstore.md","Context":"## Context\\nSince we decided to use event sourcing, we need a way to store events in our database.\\nIn `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save\/read events were hand-written.\\n**However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.\\nWhile `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.\\nWe currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.\\nAs an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\\n","Decision":"We will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.\\n","tokens":264,"id":3123,"Prediction":"Here is the Architectural Decision Record (ADR) for the provided context:\n\n## ADR Title: Storing Events with SqlStreamStore\n\n## Context\n\nAs we have decided to use event sourcing, we need a way to store events in our database. In `Wegwijs`, we stored events in `MSSQL`, which allows easy debugging of events. All SQL statements to save\/read events were hand-written. However, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds. While `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`. We currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden. As an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\n\n## Decision\n\nWe will use `SqlStreamStore` to store events in our `MSSQL` database. This decision is based on the following reasons:\n\n* `SqlStreamStore` supports storing events into `MSSQL`, which is our current database of choice.\n* `SqlStreamStore` has support for catch-up subscriptions, which will give our event handlers more autonomy over their own rebuilds.\n* `SqlStreamStore` is an open-source library, which means it is free to use and has an active community that contributes to its development.\n* We will not have to host `SqlStreamStore` ourselves, as it can be easily integrated into our existing `MSSQL` infrastructure.\n\n## Consequences\n\n* We will need to integrate `SqlStreamStore` into our existing event sourcing architecture.\n* We will need to configure `SqlStreamStore` to use our existing `MSSQL` database.\n* We will need to test `SqlStreamStore` thoroughly to ensure it meets our requirements.\n* We will need to monitor `SqlStreamStore` for any issues or bugs that may arise.\n\n## Rationale\n\nThe decision to use `SqlStreamStore` is based on the need for a reliable and scalable event storage solution that supports catch-up subscriptions. `SqlStreamStore` meets these requirements and is easy to integrate into our existing infrastructure. Additionally, `SqlStreamStore` is an open-source library, which means it is free to use and has an active community that contributes to its development. This will ensure that we have access to support and updates for the library, which is important for the long-term maintenance of our event sourcing architecture.","GenTime":"2024-08-10 14:47:48"}
{"File Name":"publicservice-registry\/0009-use-more-value-objects.md","Context":"## Context\\nA value object is a small object that represents a simple entity whose equality is not based on identity: i.e. two value objects are equal when they have the same value, not necessarily being the same object. Examples of value objects are objects representing an amount of money or a date range.\\nIn `Wegwijs`, we experienced great value towards type safety from using VOs. We want to bank in even more on the use of VOs.\\n","Decision":"Use a Value Object wherever possible.\\n","tokens":96,"id":3124,"Prediction":"## Decision\n\nTo increase the use of value objects in the `Wegwijs` software, we will create a ValueObjectFactory that provides a way to create and validate value objects throughout the application.\n\n## Context\n\nAs mentioned, value objects are small objects that represent simple entities whose equality is not based on identity. We have already experienced the benefits of using value objects, such as type safety, and we want to continue to leverage this concept in our application.\n\n## Problem\n\nCurrently, value objects are created and validated in various parts of the application, which can lead to code duplication and inconsistencies. We need a centralized way to create and validate value objects to ensure consistency and maintainability.\n\n## Decision\n\nTo address this problem, we will create a ValueObjectFactory that provides a way to create and validate value objects. This factory will be responsible for:\n\n* Creating instances of value objects\n* Validating the values of the value objects\n* Providing a way to compare value objects for equality\n\nThe ValueObjectFactory will be a singleton, so it can be easily accessed throughout the application. It will also be responsible for registering all the value objects that can be created and validated.\n\n## Consequences\n\nBy using a ValueObjectFactory, we will:\n\n* Reduce code duplication and inconsistencies\n* Improve type safety and maintainability\n* Make it easier to add new value objects to the application\n* Provide a centralized way to create and validate value objects\n\n## Rationale\n\nThe use of a ValueObjectFactory will provide a consistent way to create and validate value objects throughout the application. This will help to reduce code duplication and inconsistencies, and improve type safety and maintainability. Additionally, the factory will make it easier to add new value objects to the application, and provide a centralized way to create and validate value objects.","GenTime":"2024-08-10 14:48:07"}
{"File Name":"publicservice-registry\/0012-use-dutch-resource-names.md","Context":"## Context\\nWhen exposing API endpoints, exposing a UI or exposing resource field names, we have to decide on the language used. Do we cater to the majority of the world by using English, or do we use Dutch because we are creating resources for the Flemish Government?\\n","Decision":"We will use Dutch terms because it is an external requirement dictated by the organisation.\\n","tokens":56,"id":3126,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use English as the primary language for exposing API endpoints, UI, and resource field names. This decision is based on the following reasons:\n\n* **Global accessibility**: English is widely understood and spoken globally, making it an ideal choice for international audiences. By using English, we can ensure that our resources are accessible to a broader audience, including those who may not be fluent in Dutch.\n* **Standardization**: Using English as the primary language aligns with industry standards and best practices. Many APIs and web services use English as their primary language, making it easier for developers and users to understand and interact with our resources.\n* **Flexibility**: English is a flexible language that can be easily translated into other languages, including Dutch. If we were to use Dutch as the primary language, it would limit our ability to cater to a broader audience and make it more difficult to translate our resources into other languages.\n* **Government requirements**: Although we are creating resources for the Flemish Government, we can still use English as the primary language while providing translations in Dutch for specific government-related resources or services.\n\n## Consequences\n\n* **Limited audience**: By using English as the primary language, we may limit our audience to those who are familiar with English. However, this limitation is mitigated by the fact that English is widely understood and spoken globally.\n* **Additional translation efforts**: We will need to provide translations in Dutch for specific government-related resources or services, which may require additional effort and resources.\n* **Cultural sensitivity**: We will need to ensure that our resources are culturally sensitive and respectful of the Flemish culture, even if we are using English as the primary language.\n\n## Rationale\n\nOur decision to use English as the primary language is based on the need to balance global accessibility with cultural sensitivity and government requirements. While using Dutch as the primary language would be more culturally sensitive, it would also limit our audience and make it more difficult to translate our resources into other languages. By using English, we can ensure that our resources are accessible to a broader audience while still providing translations in Dutch for specific government-related resources or services.","GenTime":"2024-08-10 14:48:50"}
{"File Name":"publicservice-registry\/0013-handle-eventual-consistency-with-etags.md","Context":"## Context\\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\\n","Decision":"We will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\\nAll Projections will update their ProjectionState with this number.\\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\\n","tokens":83,"id":3127,"Prediction":"## Decision\n\nTo ensure that the UI always displays the latest data, we will implement a mechanism to allow the client to check the status of its latest update in the async projections. This will prevent the user from seeing stale data and reduce confusion about whether their changes were accepted or not.\n\n## Context\n\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change. In this case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\n\n## Problem\n\nThe current system does not provide a mechanism for the client to check the status of its latest update in the async projections. This can lead to a poor user experience, as the user may see stale data and be unsure if their changes were accepted.\n\n## Decision\n\nWe will implement a mechanism to allow the client to check the status of its latest update in the async projections. This will involve introducing a new endpoint that returns the status of the latest update, and updating the client to make use of this endpoint.\n\n## Consequences\n\n* The user will always see the latest data, reducing confusion and improving the overall user experience.\n* The client will need to make additional requests to check the status of its latest update, which may add latency and complexity to the system.\n* The server will need to maintain additional state to track the status of each update, which may add complexity and overhead to the system.\n\n## Rationale\n\nImplementing a mechanism to check the status of the latest update in the async projections is the best solution to this problem. This will ensure that the user always sees the latest data, and will reduce confusion and uncertainty. The additional complexity and overhead of maintaining additional state on the server is a worthwhile trade-off for the improved user experience.","GenTime":"2024-08-10 14:49:10"}
{"File Name":"publicservice-registry\/0010-do-not-use-clr-type-names-for-event-types.md","Context":"## Context\\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\\n","Decision":"Use a dictionary\/map between your message types and the CLR type you want to deserialize your message into.\\n","tokens":57,"id":3129,"Prediction":"## Decision\n\n**Title:** Use a separate namespace for event types to avoid issues with CLR type name changes\n\n**Context:** When using the SqlStreamStore, we noticed a warning against using the CLR type name as the event type in our event store. This is because event types will outlive the.NET CLR types, and moving events along namespaces will break the application.\n\n**Problem:** If we use the CLR type name as the event type, any changes to the CLR type name will break the event store, causing issues with deserialization and event handling.\n\n**Decision:** To avoid this issue, we will use a separate namespace for event types. This will ensure that the event types are decoupled from the CLR type names, allowing us to make changes to the CLR types without affecting the event store.\n\n**Consequences:**\n\n* We will need to create a separate namespace for event types, which may require some refactoring of existing code.\n* This will add an extra layer of complexity to our event handling system, but it will also provide a more robust and maintainable solution.\n\n**Rationale:** The benefits of using a separate namespace for event types outweigh the costs. By decoupling the event types from the CLR type names, we can ensure that our event store remains stable and maintainable even if the CLR types change. This will also make it easier to move events along namespaces without breaking the application.\n\n**Status:** Approved\n\n**Implementation:** We will create a new namespace for event types and update the event store to use this new namespace. We will also refactor existing code to use the new namespace for event types.","GenTime":"2024-08-10 14:49:46"}
{"File Name":"pul_solr\/0001-document-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3131,"Prediction":"## Decision\n\n**Title:** Use of Microservices Architecture\n\n**Context:** As our project grows in complexity and scale, we need to ensure that it remains maintainable, scalable, and flexible. We have considered various architectural approaches, including monolithic and service-oriented architecture (SOA). However, we believe that a microservices architecture is the best fit for our project.\n\n**Problem:** Our current monolithic architecture has become rigid and difficult to maintain. It's hard to scale individual components independently, and changes to one part of the system can have unintended consequences on other parts.\n\n**Decision:** We will adopt a microservices architecture, where our application will be broken down into a collection of small, independent services that communicate with each other using lightweight protocols. Each service will be responsible for a specific business capability and will be developed, tested, and deployed independently.\n\n**Rationale:**\n\n* Microservices allow for greater flexibility and scalability, as each service can be scaled independently.\n* It enables faster development and deployment of new features, as changes can be made to individual services without affecting the entire system.\n* Microservices promote a culture of autonomy and ownership, as each service is responsible for its own development and maintenance.\n* It provides a more robust and fault-tolerant system, as if one service experiences issues, it will not bring down the entire system.\n\n**Consequences:**\n\n* Increased complexity, as we will need to manage multiple services and their interactions.\n* Greater emphasis on communication and coordination between services.\n* Potential for increased overhead due to the additional complexity of service discovery and communication.\n\n**Status:** This decision has been approved and will be implemented in the next release.\n\n**Links:** [Insert links to relevant documentation, such as design documents or architecture diagrams.]\n\n**Informed By:**\n\n* [Insert relevant research, articles, or books that influenced this decision.]\n\n**Notes:** [Insert any additional notes or comments that may be relevant to this decision.]","GenTime":"2024-08-10 14:50:31"}
{"File Name":"modernisation-platform\/0009-use-secrets-manager-for-secrets.md","Context":"## Context\\nThe Modernisation Platform team and its users need a way to store secrets securely. There are several different methods currently used across the MoJ, including [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/), [Parameter Store](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html), [LastPass](https:\/\/www.lastpass.com\/) and [Git-Crypt](https:\/\/github.com\/AGWA\/git-crypt).\\nThere are also other well known industry solutions such as [HashiCorp Vault](https:\/\/www.vaultproject.io\/). We want to have a consistent solution across the Modernisation Platform.\\n","Decision":"We've decided to use [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) for our secrets storage.\\nParameter store can be used to store non secret parameters if needed for environment specific configuration, but the first choice should be using an app_variables.json like [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform-environments\/blob\/main\/terraform\/environments\/sprinkler\/app_variables.json)\\n","tokens":138,"id":3142,"Prediction":"Here is the Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\nWe will use HashiCorp Vault as the secure secrets storage solution for the Modernisation Platform.\n\n**Context**\n\nThe Modernisation Platform team and its users need a way to store secrets securely. There are several different methods currently used across the MoJ, including Secrets Manager, Parameter Store, LastPass, and Git-Crypt. There are also other well-known industry solutions such as HashiCorp Vault. We want to have a consistent solution across the Modernisation Platform.\n\n**Problem**\n\nThe current methods of storing secrets are not consistent, and it is difficult to manage and audit the use of secrets across the platform. This can lead to security risks and make it harder to maintain the platform.\n\n**Decision**\n\nAfter evaluating the available options, we have decided to use HashiCorp Vault as the secure secrets storage solution for the Modernisation Platform. HashiCorp Vault is a widely-used and well-established solution that provides a secure and scalable way to store and manage secrets. It has a strong focus on security and provides features such as encryption, access control, and auditing.\n\n**Rationale**\n\nWe chose HashiCorp Vault because it:\n\n* Provides a secure and scalable way to store and manage secrets\n* Has a strong focus on security and provides features such as encryption, access control, and auditing\n* Is widely used and well-established in the industry\n* Can be easily integrated with other tools and systems\n* Provides a consistent solution across the Modernisation Platform\n\n**Consequences**\n\n* We will need to implement HashiCorp Vault in our infrastructure and integrate it with our existing systems\n* We will need to migrate our existing secrets to HashiCorp Vault\n* We will need to provide training and support to our users on how to use HashiCorp Vault\n* We will need to monitor and audit the use of HashiCorp Vault to ensure its security and integrity\n\n**Status**\n\nThis decision is approved and will be implemented in the next release of the Modernisation Platform.\n\n**Links**\n\n* HashiCorp Vault: https:\/\/www.vaultproject.io\/\n* Secrets Manager: https:\/\/aws.amazon.com\/secrets-manager\/\n* Parameter Store: https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html\n* LastPass: https:\/\/www.lastpass.com\/\n* Git-Crypt: https:\/\/github.com\/AGWA\/git-crypt","GenTime":"2024-08-10 14:54:34"}
{"File Name":"modernisation-platform\/0010-terraform-module-strategy.md","Context":"## Context\\nThe Modernisation Platform uses [Terraform](https:\/\/www.terraform.io\/) for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code you can use [Terraform Modules](https:\/\/www.terraform.io\/docs\/language\/modules\/). There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.\\n","Decision":"Modules used only by the Modernisation Platform core infrastructure will remain in the [terraform\/modules](https:\/\/github.com\/ministryofjustice\/modernisation-platform\/tree\/main\/terraform\/modules) folder where they are currently located. These modules are mainly single use modules but created to keep the code tidier and easier to maintain. Modules used only by the core which currently have their own repository will remain where they are.\\nModules used by users will have their own repository per module which we link to from the main repo. These modules will be versioned with GitHub releases, and tested with [Terratest](https:\/\/terratest.gruntwork.io\/) against a test AWS account.\\n","tokens":82,"id":3144,"Prediction":"## Decision\n\nThe Modernisation Platform will use Terraform Modules to encapsulate reusable infrastructure components, improve code organization, and reduce duplication. This decision aims to standardize the use of modules across the platform, promoting a more modular and maintainable infrastructure-as-code setup.\n\n## Context\n\nAs mentioned in the context, the Modernisation Platform uses Terraform for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code, we can use Terraform Modules. There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.\n\n## Problem\n\nThe current infrastructure-as-code setup lacks modularity, resulting in duplicated code, increased complexity, and difficulty in maintaining and updating the infrastructure. Without a standardized approach to using Terraform Modules, the platform's infrastructure may become difficult to manage and scale.\n\n## Decision Criteria\n\n1. Reusability: Modules will be designed to be reusable across multiple environments and projects, reducing the need for duplicate code.\n2. Code Organization: Modules will improve code organization by grouping related infrastructure components together, making it easier to understand and maintain the infrastructure.\n3. Maintainability: Modules will reduce the complexity of the infrastructure-as-code setup, making it easier to update and maintain the infrastructure.\n4. Scalability: Modules will enable the platform to scale more efficiently by allowing for the reuse of existing infrastructure components.\n\n## Decision\n\nTo address the problem, we will implement a standardized approach to using Terraform Modules across the platform. This will involve:\n\n1. Creating a set of reusable modules for common infrastructure components, such as networks, databases, and storage.\n2. Designing modules to be flexible and configurable, allowing for easy adaptation to different environments and projects.\n3. Documenting the use of modules and their configuration options to ensure consistency and ease of use.\n4. Regularly reviewing and updating the modules to ensure they remain relevant and effective.\n\n## Consequences\n\n1. Improved reusability and maintainability of the infrastructure-as-code setup.\n2. Reduced complexity and increased scalability of the platform.\n3. Improved code organization and readability.\n4. Increased consistency and standardization across the platform.\n\n## Rationale\n\nThe use of Terraform Modules will enable the platform to achieve a more modular and maintainable infrastructure-as-code setup. By standardizing the use of modules across the platform, we can reduce duplication, improve code organization, and increase the scalability and maintainability of the infrastructure.","GenTime":"2024-08-10 14:55:16"}
{"File Name":"modernisation-platform\/0015-use-aws-image-builder-for-managing-amis.md","Context":"## Context\\nThere is likely to  be a common requirement across Modernisation Platform consumers to utilise the benefits of using customised AMIs, such as managed consistency of configuration and speed of EC2 instance deployment.\\nSome options to tackle AMI generation and management were considered\\n* A custom approach - based on a reference architecture (see <https:\/\/aws.amazon.com\/blogs\/awsmarketplace\/announcing-the-golden-ami-pipeline\/>) that brings together a number of tools and techniques to manage AMIs through their lifecycle. This approach brings with it a fair amount of complexity and arguably extra operational overhead.\\n* [Packer](https:\/\/www.packer.io\/) - a common approach across private and public cloud platforms, using this Hashicorp tool to programmatically build AMIs. Such a solution would also need to manage the distribution of AMIs across accounts and lifecycle management of AMIs\\n* [AWS Image Builder](https:\/\/docs.aws.amazon.com\/imagebuilder\/latest\/userguide\/what-is-image-builder.html) - a managed AWS service incorporating concepts such as pipelines, recipes and components, and even a marketplace of build and test components. Image builder is based on the use of AWS Systems Manager (so no ssh connections and exposed ports). A solution based on this (at the time of writing) would need to also handle the lifecycle management of AMIs (as it the case with Packer)\\n","Decision":"We will create an AMI generation and management capability based on AWS Image Builder.\\n","tokens":287,"id":3145,"Prediction":"## Decision\n\nAfter careful consideration of the options, we have decided to utilize AWS Image Builder to manage AMI generation and management for the Modernisation Platform.\n\n## Context\n\nAs mentioned in the context, there are three options to tackle AMI generation and management: a custom approach, Packer, and AWS Image Builder. After weighing the pros and cons of each option, we have chosen AWS Image Builder due to its managed service nature, which provides a more streamlined and efficient way to manage AMIs.\n\n## Rationale\n\nAWS Image Builder provides a robust and scalable solution for managing AMIs, incorporating concepts such as pipelines, recipes, and components. This service also leverages AWS Systems Manager, eliminating the need for SSH connections and exposed ports. Additionally, Image Builder provides a marketplace of build and test components, making it easier to find and integrate pre-built components into our AMI generation process.\n\nWhile a custom approach could provide more flexibility, it would also introduce additional complexity and operational overhead. Packer, on the other hand, is a widely-used tool, but it would still require manual management of AMI distribution and lifecycle management.\n\nBy choosing AWS Image Builder, we can take advantage of its managed service features and reduce the operational overhead associated with AMI management, allowing us to focus on other critical aspects of the Modernisation Platform.\n\n## Consequences\n\n* Reduced operational overhead: By leveraging AWS Image Builder, we can automate many aspects of AMI management, reducing the need for manual intervention and minimizing the risk of human error.\n* Improved scalability: Image Builder provides a scalable solution for managing AMIs, allowing us to easily handle increased demand and growth.\n* Enhanced security: By using Image Builder, we can ensure that our AMIs are built and managed in a secure and compliant manner, reducing the risk of security breaches.\n\nOverall, we believe that AWS Image Builder is the best choice for managing AMI generation and management for the Modernisation Platform, providing a robust, scalable, and secure solution that aligns with our goals and objectives.","GenTime":"2024-08-10 14:55:39"}
{"File Name":"modernisation-platform\/0006-use-a-multi-account-strategy-for-applications.md","Context":"## Context\\nIn the Modernisation Platform, we want to reduce the blast radius and increase the scalability of how we create, maintain, and support applications in the context of what AWS account(s) they sit within.\\n","Decision":"We've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https:\/\/user-guide.modernisation-platform.service.justice.gov.uk\/concepts\/environments\/).\\n","tokens":44,"id":3147,"Prediction":"## Decision\n\n**Decision:** Implement a multi-account architecture with AWS Organizations and Service Catalog to manage application creation, maintenance, and support across multiple AWS accounts.\n\n## Context\n\nAs stated in the context, the Modernisation Platform aims to reduce the blast radius and increase scalability by managing applications across multiple AWS accounts. This decision is necessary to achieve these goals.\n\n## Problem\n\nCurrently, the Modernisation Platform lacks a scalable and manageable way to create, maintain, and support applications across multiple AWS accounts. This leads to:\n\n* Increased risk of unintended changes or outages due to the blast radius of a single account\n* Difficulty in scaling and managing applications across multiple accounts\n* Inefficient use of resources and lack of visibility into application performance\n\n## Solution\n\nTo address these issues, we will implement a multi-account architecture using AWS Organizations and Service Catalog. This will enable us to:\n\n* Create a centralized management structure for multiple AWS accounts\n* Use AWS Organizations to manage account creation, deletion, and access control\n* Utilize Service Catalog to provision and manage applications across multiple accounts\n* Reduce the blast radius by isolating applications and resources within individual accounts\n* Increase scalability by allowing for the creation of new accounts and applications as needed\n* Improve visibility and management of applications across multiple accounts\n\n## Consequences\n\n* Increased complexity in managing multiple accounts and applications\n* Requires additional training and expertise for administrators and developers\n* May require additional infrastructure and resources for account creation and management\n* Potential for increased costs due to the creation of new accounts and resources\n\n## Rationale\n\nThe benefits of implementing a multi-account architecture with AWS Organizations and Service Catalog outweigh the potential drawbacks. By reducing the blast radius and increasing scalability, we can improve the overall reliability, security, and manageability of the Modernisation Platform. This decision aligns with the goals of the platform and will enable us to better support the creation, maintenance, and support of applications across multiple AWS accounts.","GenTime":"2024-08-10 14:56:22"}
{"File Name":"modernisation-platform\/0011-use-vpc-flow-logs-to-gain-insight-into-network-state.md","Context":"## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n","Decision":"[VPC flow flows](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\\n","tokens":65,"id":3150,"Prediction":"## Decision\n\n**Title:** Choose a Network Monitoring and Testing Tool for the Modernisation Platform\n\n**Context:** As a core part of the Modernisation Platform, ensuring the network topology and configuration meets security, compliance, and best-practice requirements is crucial. We need a reliable and efficient way to monitor and test the network to maintain confidence in its desired state.\n\n**Decision:** We will use Ansible Network Automation and NetBox for network monitoring and testing capabilities.\n\n**Rationale:**\n\n1. Ansible Network Automation provides a comprehensive set of modules for network device management, allowing us to automate network configuration, deployment, and testing.\n2. NetBox is a popular open-source network management tool that provides a centralized repository for network devices, services, and configurations, enabling us to track and manage our network topology and configuration.\n3. Both tools are widely used and well-maintained, ensuring we can leverage a large community of users and developers for support and customization.\n4. Ansible's automation capabilities will help us streamline network configuration and deployment, reducing the risk of human error and increasing efficiency.\n5. NetBox's centralized repository will provide a single source of truth for our network configuration, making it easier to manage and troubleshoot issues.\n\n**Consequences:**\n\n1. We will need to invest time in setting up and configuring Ansible and NetBox, including integrating them with our existing infrastructure.\n2. We will need to develop custom playbooks and plugins for Ansible to automate specific network tasks and tests.\n3. We will need to ensure that our network devices and services are compatible with Ansible and NetBox.\n\n**Alternatives Considered:**\n\n1. Using a commercial network monitoring and testing tool, such as Riverbed or Riverbed SteelCentral, which would require a significant upfront investment and may not provide the same level of customization and flexibility as Ansible and NetBox.\n2. Building our own custom network monitoring and testing tool from scratch, which would require a significant amount of development time and resources.\n\n**Monitoring and Review:** We will regularly review the performance and effectiveness of Ansible and NetBox, and make adjustments as needed to ensure they continue to meet our network monitoring and testing needs.","GenTime":"2024-08-10 14:57:38"}
{"File Name":"alfa\/adr-003.md","Context":"## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n","Decision":"We will use [TypeScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\\n","tokens":195,"id":3153,"Prediction":"## Decision\n\n**Title:** Choose a statically-typed language for Alfa's core logic\n\n**Context:** As discussed in [ADR 2], the Alfa code base is expected to be larger and more complex, and we need a solution to ensure the correctness and maintainability of our code. Given the limitations of plain JavaScript, we need a language with a proper type system that can enforce API contracts. Additionally, Alfa must be able to run in a browser, requiring a language that can be transpiled to JavaScript.\n\n**Decision:** We will choose TypeScript as the primary language for Alfa's core logic. TypeScript is a statically-typed superset of JavaScript that provides optional static typing and other features that can help us catch errors and improve code maintainability. Its compatibility with JavaScript and its ability to be transpiled to JavaScript make it an ideal choice for our use case.\n\n**Rationale:**\n\n* TypeScript's type system will help us catch errors related to `undefined` or `null` values and incorrect API parameters, reducing the likelihood of bugs.\n* TypeScript's optional static typing will allow us to gradually introduce type annotations and improve code maintainability over time.\n* TypeScript's compatibility with JavaScript and its ability to be transpiled to JavaScript will enable us to run Alfa in a browser and leverage existing browser APIs.\n* TypeScript has a large and active community, ensuring that we will have access to a wide range of tools, libraries, and resources.\n\n**Consequences:**\n\n* We will need to invest time and resources in learning and adopting TypeScript.\n* We will need to update our development workflow and tools to support TypeScript.\n* We will need to ensure that our existing codebase is compatible with TypeScript and can be transpiled to JavaScript.\n\n**Alternatives Considered:**\n\n* We considered using Flow, another statically-typed JavaScript superset, but ultimately chose TypeScript due to its larger community and wider adoption.\n* We considered using a different language altogether, such as Kotlin or Rust, but ultimately chose TypeScript due to its compatibility with JavaScript and its ability to run in a browser.","GenTime":"2024-08-10 14:58:54"}
{"File Name":"alfa\/adr-002.md","Context":"## Context\\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https:\/\/github.com\/GoogleChrome\/accessibility-developer-tools), [aXe by Deque](https:\/\/github.com\/dequelabs\/axe-core), and [HTML_CodeSniffer by Squiz](https:\/\/github.com\/squizlabs\/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https:\/\/wicg.github.io\/aom\/).\\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\\n","Decision":"We will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\\n","tokens":437,"id":3156,"Prediction":"## Decision\n\n**Title:** Run Accessibility Conformance Testing Engine outside of the browser to ensure reproducible results\n\n**Context:** As described in the provided context, our proprietary accessibility conformance testing engine at Siteimprove currently runs within the context of a browser, which can lead to non-deterministic results due to various browser variables. We want to ensure that our tool can exactly replicate the results of a given accessibility assessment, which is currently difficult to achieve.\n\n**Decision:** We will run our accessibility conformance testing engine outside of the browser, using browser APIs only when absolutely necessary. This will allow us to isolate the testing environment from browser variables and ensure reproducible results.\n\n**Rationale:**\n\n* By running the engine outside of the browser, we can better control the testing environment and eliminate variables that can affect the results.\n* This approach will enable us to replicate the results of a given accessibility assessment more accurately and consistently, which is crucial for our customers.\n* While running the engine outside of the browser may require additional effort to integrate with the browser, the benefits of improved reproducibility and accuracy outweigh the costs.\n\n**Consequences:**\n\n* We will need to develop a new architecture for our testing engine that runs outside of the browser.\n* We will need to integrate our engine with the browser only when necessary, which may require additional development effort.\n* Our customers will benefit from more accurate and reproducible results, which will improve their trust in our tool.\n\n**Alternatives Considered:**\n\n* We could continue to run the engine within the browser and try to mitigate the effects of browser variables through additional testing and error handling.\n* We could use a hybrid approach, where the engine runs partially within the browser and partially outside of it.\n\n**Status:** This decision is approved and will be implemented in the next development cycle.","GenTime":"2024-08-10 14:59:50"}
{"File Name":"hmpps-interventions-service\/0002-represent-validation-errors-at-field-level.md","Context":"## Context\\nFor any user interface or client relying on our API, we need to define how we represent what was wrong with\\ninvalid client requests.\\n","Decision":"We will use field-level error validation.\\nWe will use meaningful codes per field.\\nExample:\\n```json\\n{\\n\"status\": 400,\\n\"error\": \"validation error\",\\n\"message\": \"draft referral update invalid\",\\n\"validationErrors\": [\\n{\\n\"field\": \"serviceUser.crn\",\\n\"error\": \"FIELD_CANNOT_BE_CHANGED\"\\n}\\n]\\n}\\n```\\n","tokens":31,"id":3159,"Prediction":"## Decision\n\nWe will use HTTP error codes and a standardized error response format to represent what went wrong with invalid client requests.\n\n## Context\n\nFor any user interface or client relying on our API, we need to define how we represent what was wrong with invalid client requests.\n\n## Problem\n\nCurrently, our API does not provide a consistent and standardized way to communicate errors to clients. This makes it difficult for clients to handle and display errors in a user-friendly manner. We need a solution that allows us to provide clear and concise information about what went wrong with an invalid client request.\n\n## Decision\n\nTo address this problem, we will use HTTP error codes to indicate the type of error that occurred, and a standardized error response format to provide additional information about the error. This will enable clients to handle and display errors in a consistent and user-friendly manner.\n\n## Rationale\n\nUsing HTTP error codes allows us to provide a clear and concise indication of the type of error that occurred, and allows clients to handle errors based on the HTTP status code. For example, a 400 Bad Request error code can indicate that the request was invalid or malformed.\n\nOur standardized error response format will include the following information:\n\n* `error`: a brief description of the error\n* `error_code`: a unique code that identifies the error\n* `message`: a detailed message that provides more information about the error\n* `details`: additional information that provides more context about the error\n\nThis format will allow clients to easily parse and display errors in a user-friendly manner.\n\n## Consequences\n\nBy using HTTP error codes and a standardized error response format, we will be able to provide clear and concise information about what went wrong with invalid client requests. This will make it easier for clients to handle and display errors, and will improve the overall user experience.\n\nAdditionally, this decision will allow us to provide more detailed and informative error messages, which will help developers to troubleshoot and fix issues more quickly.\n\n## Implementation\n\nWe will implement this decision by updating our API to return HTTP error codes and standardized error responses for invalid client requests. We will also provide documentation and examples of how to handle and display errors using our API.","GenTime":"2024-08-10 15:01:02"}
{"File Name":"heroes-board-game\/0002-tests-definition-and-scope.md","Context":"## Context and Problem Statement\\nAcceptance tests are embedded into game server application.\\nShould they be written one layer above (UI)?\\n## Decision Drivers\\n* Acceptance tests should be meaningful for end users\\n","Decision":"* Acceptance tests should be meaningful for end users\\nRedefine current acceptance tests and treat them like _component_ tests according to [this definition](https:\/\/www.simpleorientedarchitecture.com\/defining-test-boundaries\/).\\n* Good, because it promotes different tests for different audiences\\n* Good, because it sets a clear terminology\\n* Good, because end users don't care about layers below UI\\n","tokens":44,"id":3166,"Prediction":"## Decision\n\nWe will write acceptance tests at the same layer as the game server application, rather than one layer above (UI).\n\n## Context and Problem Statement\n\nAcceptance tests are embedded into the game server application. Should they be written one layer above (UI)?\n\n## Decision Drivers\n\n* Acceptance tests should be meaningful for end users\n\n## Rationale\n\nWriting acceptance tests at the same layer as the game server application allows us to focus on the core functionality of the game, which is the primary concern of the end users. By testing the game server directly, we can ensure that the game behaves as expected, including the logic and rules of the game. This approach also allows us to test the game server in isolation, without being influenced by the UI or other layers of the application.\n\nWriting acceptance tests at the UI layer would require us to test the UI itself, which may not be the primary concern of the end users. The UI is an implementation detail that can change without affecting the core functionality of the game. By testing the UI separately, we can ensure that it is correctly rendering the game state and responding to user input, but this would not guarantee that the game itself is functioning correctly.\n\n## Consequences\n\n* Acceptance tests will be more focused on the core functionality of the game, ensuring that it behaves as expected.\n* The game server will be tested in isolation, reducing the risk of UI-related issues affecting the game.\n* The acceptance tests will be more meaningful for end users, as they will be testing the game itself, rather than the UI.\n\n## Affected Components\n\n* Game server application\n* Acceptance tests\n\n## Implementation\n\nWe will implement the acceptance tests using a testing framework, such as JUnit or PyUnit, and write test cases that cover the core functionality of the game. We will use mocking or stubbing to isolate the game server from the UI and other layers of the application, and ensure that the tests are independent of the UI.","GenTime":"2024-08-10 15:03:32"}
{"File Name":"tul_cob\/0001-use-feature-flags-to-toggle-features.md","Context":"## Context\\nThere are features that exist in the codebase we are not yet ready to release in production. We would like to use feature flags to toggle the availability of certain features, which will help prevent development and production branches from drifting.\\n","Decision":"We've decided to implement very simple feature flags that can be toggled with environment variables.\\n","tokens":49,"id":3171,"Prediction":"## Decision\n\n**Feature Flag Implementation for Controlled Rollout of Unreleased Features**\n\n## Context\n\nAs discussed, we have features in our codebase that are not yet ready for production release. To ensure that our development and production branches remain in sync, we will implement feature flags to toggle the availability of these features. This will allow us to control the rollout of new features and prevent them from being accidentally deployed to production.\n\n## Decision\n\nWe will use a feature flagging library (e.g., Feature Flags by LaunchDarkly, Flagbit, or similar) to manage the toggling of features. This library will provide a centralized way to manage feature flags, allowing us to easily enable or disable features in different environments (e.g., development, staging, production).\n\nThe feature flagging library will be integrated into our codebase through a configuration file or environment variables, allowing us to easily switch features on or off based on the environment. This will enable us to:\n\n1. Prevent accidental deployment of unreleased features to production.\n2. Gradually roll out new features to a subset of users or environments.\n3. Monitor the performance and behavior of new features before deploying them to all users.\n\n## Consequences\n\n* Increased control over feature rollout and deployment.\n* Reduced risk of accidental deployment of unreleased features.\n* Improved ability to monitor and test new features before deploying them to all users.\n* Additional complexity in managing feature flags, which may require additional resources and expertise.\n\n## Rationale\n\nImplementing feature flags will provide a flexible and scalable way to manage the rollout of new features. By using a centralized feature flagging library, we can ensure that all features are properly toggled and monitored, reducing the risk of errors and improving the overall quality of our software.","GenTime":"2024-08-10 15:04:44"}
{"File Name":"service-stac\/2020_10_21_static_asset.md","Context":"## Context\\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\\n","Decision":"Because it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\\n","tokens":91,"id":3173,"Prediction":"## Decision\n\n**Decision:** Use Nginx as a reverse proxy to serve static assets for the admin pages in the `service-stac` application.\n\n**Context:** As mentioned in the context, `service-stac` needs to serve static assets such as CSS, images, and icons for the admin pages. Django is not suitable for serving static files in a production environment, and `gunicorn` should be served behind a reverse proxy to avoid issues with slow clients and Denial of Service attacks.\n\n**Rationale:** Nginx is a popular and widely-used reverse proxy server that can efficiently handle static asset serving, caching, and compression. It can be configured to serve static assets from a separate location, such as a dedicated server or a cloud storage service, and proxy requests to `gunicorn` for dynamic content. This approach allows us to offload the serving of static assets from `gunicorn`, reducing the load on the application server and improving overall performance and security.\n\n**Alternatives Considered:** Apache is another popular reverse proxy server that could be used for this purpose. However, Nginx is generally considered to be more lightweight and efficient, making it a better choice for serving static assets.\n\n**Consequences:**\n\n* Improved performance: By offloading static asset serving from `gunicorn`, we can reduce the load on the application server and improve overall performance.\n* Improved security: Serving static assets through a reverse proxy can help protect against Denial of Service attacks and other security threats.\n* Simplified configuration: Nginx provides a simple and intuitive configuration syntax, making it easy to set up and manage the reverse proxy.\n\n**Monitoring and Feedback:** We will monitor the performance and security of the reverse proxy configuration and adjust as necessary to ensure optimal performance and security. We will also provide feedback on the effectiveness of this approach to other teams and stakeholders.","GenTime":"2024-08-10 15:05:26"}
{"File Name":"react-transcript-editor\/2018-10-05-components-comunication.md","Context":"## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n","Decision":"* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026 -->\\n","tokens":71,"id":3187,"Prediction":"## Decision\n\nThe Transcript Editor's internal components will communicate with each other using a Publish-Subscribe pattern, facilitated by a centralized event bus.\n\n## Context and Problem Statement\n\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\n\n## Decision Drivers\n\n* Simple and straightforward way to reason around passing data between components\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\n\n## Rationale\n\nThe Publish-Subscribe pattern allows components to communicate with each other without having a direct reference to each other. This decouples the components, making it easier to maintain, test, and extend the system. The event bus acts as a centralized hub for events, allowing components to publish and subscribe to events of interest.\n\nThis approach provides several benefits:\n\n* Loose Coupling: Components are not tightly coupled, making it easier to change or replace individual components without affecting the rest of the system.\n* Scalability: The event bus can handle a large volume of events, making it suitable for systems with high traffic or complex event flows.\n* Flexibility: Components can publish and subscribe to events as needed, allowing for dynamic communication between components.\n\n## Trade-offs\n\n* Additional complexity: Introducing an event bus requires additional setup and configuration, which can add complexity to the system.\n* Performance Overhead: The event bus may introduce a small performance overhead due to the additional processing and serialization of events.\n\n## Consequences\n\n* Components will need to be designed with event-driven programming in mind, using events to communicate with each other.\n* The event bus will need to be configured and managed to ensure efficient and reliable event processing.\n* The system will be more scalable and flexible, allowing for easier maintenance and extension.\n\nBy using a Publish-Subscribe pattern with a centralized event bus, the Transcript Editor's internal components will be able to communicate effectively and efficiently, allowing for a more maintainable and scalable system.","GenTime":"2024-08-10 15:09:44"}
{"File Name":"aspan_micro_front_ends\/0002-derived-attributes.md","Context":"## Context\\nDerived attributes are read-only.\\n","Decision":"Following attributes are derived from file system:\\n1. name - file name without extension\\n2. contentType - file extension without dot, lowercased\\n","tokens":10,"id":3192,"Prediction":"## Decision\n\n**Decision:** Implement derived attributes as read-only to ensure data consistency and prevent accidental modification.\n\n## Context\n\nDerived attributes are read-only.\n\n## Problem\n\nDerived attributes are calculated based on other attributes and are not intended to be modified directly. However, if they were writable, it could lead to inconsistencies in the data and potentially cause bugs in the system.\n\n## Rationale\n\nMaking derived attributes read-only ensures that they are always accurate and consistent with the underlying data. This is particularly important when the derived attributes are used as input for other calculations or are relied upon by other parts of the system.\n\n## Consequences\n\n* Ensures data consistency and accuracy\n* Prevents accidental modification of derived attributes\n* Simplifies the system by reducing the complexity of handling derived attributes\n* May require additional checks and validation to ensure that derived attributes are correctly calculated\n\n## Decision Drivers\n\n* Data consistency and accuracy\n* Preventing accidental modification of derived attributes\n* Simplifying the system by reducing complexity\n\n## Variations Considered\n\n* Making derived attributes writable with validation checks to ensure consistency\n* Storing derived attributes as a separate entity to allow for modification\n* Using a different data structure to store derived attributes\n\n## Status\n\nApproved\n\n## Affected Components\n\n* Data model\n* Business logic\n* Validation logic","GenTime":"2024-08-10 15:10:52"}
{"File Name":"aspan_micro_front_ends\/0004-info-graphql-import.md","Context":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n","Decision":"Decision here...\\n","tokens":20,"id":3194,"Prediction":"## Decision\n\nTo define import and export statements in GraphQL SDL, we will use the `@import` and `@export` directives as specified in the GraphQL schema definition language (SDL).\n\n## Context\n\nIn GraphQL, SDL allows us to define the structure of our GraphQL schema. However, in larger and more complex projects, it's common to split the schema into multiple files or modules. This makes it easier to maintain and reuse code. To achieve this, we need a way to import and export definitions from these modules.\n\n## Problem\n\nCurrently, there is no built-in support for importing and exporting definitions in GraphQL SDL. This makes it difficult to modularize our schema and reuse code across different parts of the application.\n\n## Decision\n\nTo solve this problem, we will use the `@import` and `@export` directives as specified in the GraphQL SDL. These directives allow us to import and export definitions from other modules, making it possible to modularize our schema and reuse code.\n\n## Rationale\n\nUsing the `@import` and `@export` directives provides several benefits. It allows us to:\n\n* Modularize our schema, making it easier to maintain and reuse code.\n* Import and export definitions from other modules, making it possible to share code across different parts of the application.\n* Keep our schema organized and easy to understand.\n\n## Consequences\n\nUsing the `@import` and `@export` directives will require some changes to our existing codebase. We will need to refactor our schema to use these directives and update our build process to handle the imports and exports correctly.\n\n## Status\n\nThis decision is approved and will be implemented in our next schema update.\n\n## Links\n\n* GraphQL SDL documentation: https:\/\/graphql.org\/learn\/schema\/#directives\n* GraphQL `@import` and `@export` directives: https:\/\/graphql.org\/learn\/schema\/#import-and-export","GenTime":"2024-08-10 15:11:31"}
{"File Name":"heptaconnect-docs\/2020-04-30-contracts-and-interfaces.md","Context":"## Context\\nThere has to be a guide how to structure code to allow extendability.\\nWe have to ensure that functionality can be exchanged without interfering with adjacent packages within the package hierarchy.\\n","Decision":"The approved approach is using the language feature type hierarchy.\\nUsing interfaces, traits and contracts (abstract classes) is a good way to structure and allow replacements by ensuring certain behaviors.\\nWe use interfaces when multiple implementation will exist and are unknown at any time for the package that it is introduced in.\\nFor example, we do not know what kind of storage is used within the portal-base, but it will need some kind of storage.\\nTo supply some basic logic for commonly used interfaces we can provide traits for others to implement them easier.\\nWe use contracts similar to interfaces but use their advantages to contain any logic beforehand.\\nThis enables us to add additional code later with a reduced level of changes that can be non-breaking without removing the replacing option.\\nContracts are best without dependencies that have to be given in the constructor as this forces other implementations to follow this pattern regardless whether they need it.\\n","tokens":39,"id":3203,"Prediction":"## Decision\n\n**Title:** Modular Code Structure with Interface-Based Dependency Injection\n\n**Context:** As we strive to create a highly extendable and maintainable software system, we need to establish a guide for structuring our code to ensure that functionality can be easily exchanged without interfering with adjacent packages within the package hierarchy.\n\n**Decision:** We will adopt a modular code structure with interface-based dependency injection to achieve the desired level of extendability and maintainability.\n\n**Rationale:**\n\n* By using interfaces, we can decouple dependent components and make them interchangeable, allowing for easier replacement of functionality without affecting adjacent packages.\n* Interface-based dependency injection enables us to define the dependencies between components without hardcoding specific implementations, making it easier to swap out dependencies at runtime.\n* A modular code structure with clear boundaries between packages and components will help to reduce coupling and improve the overall maintainability of the system.\n\n**Consequences:**\n\n* We will need to refactor our existing codebase to adopt this new structure, which may require significant changes to our architecture and implementation.\n* We will need to define clear interfaces and contracts for each component, which may require additional documentation and testing.\n* We will need to ensure that our dependency injection mechanism is properly configured and managed to avoid issues with component interactions.\n\n**Status:** Accepted\n\n**Implementors:** [List the team members or individuals responsible for implementing this decision]\n\n**References:** [List any relevant resources, such as articles, books, or code examples, that support this decision]\n\nBy adopting this decision, we will be able to create a more modular and maintainable software system that is easier to extend and modify over time.","GenTime":"2024-08-10 15:14:38"}
{"File Name":"heptaconnect-docs\/2021-10-29-flow-components-are-not-crud.md","Context":"## Context\\nAt the time of writing we have explorers, emitters and receivers as three main flow components.\\nThey resemble CR and U from the well-known [CRUD](https:\/\/en.wikipedia.org\/wiki\/Create,_read,_update_and_delete).\\nMost APIs are CRUD or [BREAD](http:\/\/paul-m-jones.com\/post\/2008\/08\/20\/bread-not-crud\/) based and therefore match the three named flow components.\\nFor now, emitting and receiving entities can be also used differently as this \"just\" sends data from one portal node and is received by another portal node.\\nEmitters and receivers could send commands instead of entities.\\nAs previously mentioned we do not have a deletion flow component.\\nA receiver could receive an entity with a custom deletion command with any previous version of HEPTAconnect.\\nThis is discouraged but possible.\\nWe have already seen implementations, that receive data but don't write anything to the API the portal resembles.\\nThis is a misuse that is similar to described scenario above.\\nLooking at the other existing flow components we also have webhooks and status reporters.\\nThese are not related to CRUD at all, so we are not limited to CRUD.\\n","Decision":"Receivers are not meant to do everything, when it is about receiving a command.\\nReceivers are meant to be used for entities only.\\nGrouping explorers, emitters, receivers and \"deleters\" into a single CRUD flow component enforces structures that probably don't benefit APIs, that do not fall into this pattern.\\nGrouping flow components is not helpful when we do not know the possible groups in beforehand and therefore can't be done right.\\nEvery other transfer needs a new flow component.\\nAs routes connect emitters and receivers they need to learn how to decide which flow components to use on a route.\\nThis is described in a [different ADR](.\/2021-10-30-route-capabilities.md).\\n","tokens":250,"id":3205,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will introduce a new flow component, \"Handler\", to complement the existing \"Explorer\", \"Emitter\", and \"Receiver\" components. This decision is made to better reflect the flexibility and versatility of our system, which has already deviated from the traditional CRUD\/BREAD paradigm.\n\n**Context**\n\nAs described in the context, our system has evolved to include components that are not strictly CRUD\/BREAD-based. The \"Emitter\" and \"Receiver\" components, while similar to Create and Update, can also be used to send commands instead of entities. Additionally, we have other flow components like \"Webhooks\" and \"Status Reporters\" that are not related to CRUD at all. This diversity of use cases suggests that a more flexible and adaptable architecture is needed.\n\n**Problem**\n\nThe current architecture is too rigidly tied to the CRUD\/BREAD paradigm, which limits our ability to adapt to new use cases and requirements. The introduction of a new flow component will allow us to better accommodate the diverse needs of our system.\n\n**Decision Criteria**\n\n1. Flexibility: The new flow component should be able to accommodate a wide range of use cases and requirements.\n2. Adaptability: The architecture should be able to evolve and adapt to new use cases and requirements without significant changes.\n3. Scalability: The new flow component should be able to scale with the system and accommodate a large number of use cases and requirements.\n\n**Consequences**\n\n1. Increased flexibility and adaptability: The new flow component will allow us to better accommodate the diverse needs of our system.\n2. Simplified development: The new flow component will provide a clear and consistent way to handle different use cases and requirements.\n3. Improved scalability: The new flow component will be able to scale with the system and accommodate a large number of use cases and requirements.\n\n**Rationale**\n\nThe introduction of a new flow component, \"Handler\", will provide a clear and consistent way to handle different use cases and requirements. This will allow us to better accommodate the diverse needs of our system and provide a more flexible and adaptable architecture.","GenTime":"2024-08-10 15:15:22"}
{"File Name":"heptaconnect-docs\/2021-10-30-route-capabilities.md","Context":"## Context\\nRoutes define directions for data to flow.\\nThe interpretation or use-case for a flow can be different for various reasons.\\nIn general, we support read-multiple-times write-multiple-times scenarios, and they are very generous in options to work with but often needs to be limited in integrations.\\nLimitations like transferring data only once or transferring data for finding the equivalent on the target are missing but requested.\\nWe need a way to configure route behaviour in core without adding more work to the integrators.\\n","Decision":"All limitations (e.g. transferring once) will be implemented as skipping existing steps.\\nThese changes in behaviour can be represented by simple boolean flags.\\nEvery step that is not a limitation will result in further flow components that will get a boolean flag.\\n","tokens":106,"id":3210,"Prediction":"## Decision\n\n**Decision:** Implement a Route Configuration System to allow for customizable route behavior in the core.\n\n**Context:** As described, routes define directions for data to flow, and their interpretation or use-case can vary depending on different reasons. Currently, we support read-multiple-times write-multiple-times scenarios, but this flexibility can lead to limitations in integrations. To address this, we need a way to configure route behavior in the core without adding more work to the integrators.\n\n**Problem:** The current approach of allowing generous options for working with routes can lead to complexity and limitations in integrations. Integrators may need to handle different scenarios, such as transferring data only once or finding the equivalent on the target, which can be challenging.\n\n**Solution:** Implement a Route Configuration System that allows for customizable route behavior in the core. This system will provide a centralized way to define and configure route behavior, making it easier to manage and maintain integrations.\n\n**Rationale:** By implementing a Route Configuration System, we can:\n\n1. Decouple route behavior from integrations, making it easier to manage and maintain.\n2. Provide a centralized way to define and configure route behavior, reducing complexity and increasing flexibility.\n3. Allow integrators to focus on their specific use-cases, rather than worrying about the underlying route behavior.\n\n**Consequences:**\n\n1. Increased flexibility and customization options for route behavior.\n2. Simplified maintenance and management of integrations.\n3. Potential increase in development time and complexity for implementing the Route Configuration System.\n\n**Alternatives Considered:**\n\n1. Adding more work to integrators to handle different scenarios.\n2. Creating a separate module or library for route behavior customization.\n\n**Decision Owner:** [Your Name]\n\n**Reviewers:** [List of reviewers who approved this decision]\n\n**Approved Date:** [Date]","GenTime":"2024-08-10 15:17:06"}
{"File Name":"dlp-lux\/0003-Shibboleth.md","Context":"## Context\\nBoth Lux and Curate need to authenticate users to enforce authorization. Emory uses Shibboleth to manage\\nauthentication for University applications. Curate has gone through the lengthy process of requesting access to\\nShibboleth, and implemented user management through it. Part of the request was enumerating the \"attributes\" or\\nmetadata Curate needs to receive when a person logs in. Curate currently has three \"endpoints\", or applications,\\nthat can use its access to Shibboleth (prod, test, and arch).\\nLux also has access to Shibboleth, with the same three configured endpoints, and the same attributes.\\nLux does need to allow new users to sign in, where Curate does not.\\nIn production, neither application needs to allow users to authenticate from another source.\\n","Decision":"Lux will not use endpoints from Curate's Shibboleth access, instead using its own config. From the Lux side,\\na configuration option will be available to use database auth instead, for development, and to ease rollout.\\nIf the configuration option is not present, database auth will be used.\\n","tokens":167,"id":3214,"Prediction":"## Decision\n\n**Authentication and Authorization using Shibboleth**\n\n## Context\n\nAs both Lux and Curate need to authenticate users to enforce authorization, and both have access to Shibboleth for managing authentication, it makes sense to leverage Shibboleth for authentication and authorization.\n\n## Problem\n\nCurate has already implemented user management through Shibboleth and has enumerated the necessary attributes for user authentication. Lux also has access to Shibboleth with the same three configured endpoints and attributes. However, Lux needs to allow new users to sign in, whereas Curate does not.\n\n## Decision\n\nWe will use Shibboleth for authentication and authorization in both Lux and Curate, utilizing the same endpoints (prod, test, and arch) and attributes. This will simplify the authentication process and reduce the complexity of implementing separate authentication mechanisms for each application.\n\n## Rationale\n\n1. Shibboleth is already being used by Curate, and Lux can leverage this existing infrastructure to simplify the authentication process.\n2. Both applications have the same requirements for Shibboleth attributes, making it easy to implement a unified authentication mechanism.\n3. By using Shibboleth, we can ensure that both applications have a secure and standardized way of authenticating users.\n4. Since neither application needs to allow users to authenticate from another source in production, Shibboleth's single-source authentication mechanism meets our needs.\n\n## Consequences\n\n1. Simplified authentication process for both Lux and Curate.\n2. Reduced complexity in implementing separate authentication mechanisms for each application.\n3. Improved security through the use of a standardized and secure authentication mechanism.\n4. New users will be able to sign in to Lux, whereas Curate does not require this feature.\n\n## Status\n\nApproved\n\n## Links\n\n* Curate's Shibboleth implementation documentation\n* Lux's Shibboleth implementation documentation","GenTime":"2024-08-10 15:18:38"}
{"File Name":"dlp-lux\/0002-Blacklight-Version.md","Context":"## Context\\nIn order to facilitate work on dlp-lux, we need to make a decision about which verion of Blacklight to run.\\n","Decision":"We will use Blacklight 7.\\nThis allows us to avoid problems deploying the application, and sets us up for the future when Blacklight 7 is\\nsupported by Hyrax.\\nThis decision accepts that minor styling differences may occur between Lux and Curate.\\n","tokens":31,"id":3215,"Prediction":"Here is an Architectural Decision Record for the provided context:\n\n**Decision**\n\nWe will run Blacklight version 7.4.1 in the dlp-lux project.\n\n**Context**\n\nIn order to facilitate work on dlp-lux, we need to make a decision about which version of Blacklight to run.\n\n**Problem**\n\nThe dlp-lux project requires the use of Blacklight, a Solr-based search engine. However, there are multiple versions of Blacklight available, each with its own set of features and bug fixes. We need to decide which version to use in order to ensure compatibility with our project's requirements and to minimize technical debt.\n\n**Reasoning**\n\nWe have evaluated the different versions of Blacklight and have decided to run version 7.4.1. This version was chosen because it is the most recent version that has been thoroughly tested and has a stable set of features that meet our project's requirements. Additionally, version 7.4.1 has been shown to be compatible with the latest versions of Solr and other dependencies used in our project.\n\n**Consequences**\n\nBy running Blacklight version 7.4.1, we will be able to:\n\n* Ensure compatibility with our project's requirements\n* Minimize technical debt by avoiding the need to maintain multiple versions of Blacklight\n* Take advantage of the latest bug fixes and features in the Blacklight community\n\n**Alternatives Considered**\n\nWe considered running an earlier version of Blacklight, such as version 6.4.0, which is more lightweight and has fewer dependencies. However, this version is no longer supported and would require additional maintenance and testing to ensure compatibility with our project's requirements.\n\nWe also considered running a later version of Blacklight, such as version 8.0.0, which has additional features and improvements. However, this version is still in beta and has not been thoroughly tested, which could lead to compatibility issues and additional technical debt.\n\n**Status**\n\nThis decision is currently in effect and will be reviewed and updated as necessary in the future.","GenTime":"2024-08-10 15:19:00"}
{"File Name":"bookit-api\/0006-use-okta-as-identity-provider.md","Context":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n","Decision":"* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","tokens":279,"id":3217,"Prediction":"## Decision\n\n**Decision:** Integrate Okta as the identity provider for our application, utilizing their Microsoft OpenConnect ID provider to authenticate users and their Spring Boot and Spring Security integrations to simplify the authentication process.\n\n**Rationale:**\n\n* Okta provides a forever free developer account that can handle our anticipated usage, eliminating the need for us to manage our own identity infrastructure.\n* The Microsoft OpenConnect ID provider allows us to authenticate against Microsoft Azure AD, providing a seamless experience for our users.\n* Okta's integrations with Spring Boot and Spring Security simplify the authentication process and reduce the complexity of implementing identity management from scratch.\n* The React client SDK provided by Okta eliminates the need for us to write custom client code for Microsoft Azure AD.\n* The real value add comes from Okta's admin screens and dashboards, which provide a convenient way to manage users, roles, and groups, making it easier to manage our application's security and access control.\n* The flexibility to add additional social logins or support for AD directly in the future provides a scalable solution for our identity management needs.\n\n**Consequences:**\n\n* We will need to configure Okta and set up the Microsoft OpenConnect ID provider to integrate with our application.\n* We will need to update our application to use Okta's Spring Boot and Spring Security integrations.\n* We will need to modify our user management and authentication logic to work with Okta's API.\n* We will need to consider the trade-offs between using Okta's admin screens and dashboards versus implementing our own custom solution.\n\n**Status:** Approved\n\n**Owners:** [List the names of the people responsible for implementing and maintaining this decision]\n\n**History:**\n\n* [List the dates and changes made to this decision]\n\nNote: This is just a sample Architectural Decision Record, you can modify it according to your needs and company standards.","GenTime":"2024-08-10 15:19:37"}
{"File Name":"bookit-api\/0010-jpa-manages-schema.md","Context":"## Context\\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\\n","Decision":"* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging\/production databases (we will continue to drop\/recreate all other databases....local, integration).\\n* recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\\n","tokens":71,"id":3221,"Prediction":"## Decision\n\n**Database Initialization Strategy**\n\n### Context\n\nOriginally, we used Spring Boot's Database Initialization support to automatically create and initialize our database via schema.sql and data.sql scripts. Each deployment (application initialization) would execute these scripts. Our implementation would drop the database and recreate it each time. While this accelerated our development (avoid data migrations), it's not sustainable.\n\n### Problem\n\nThe current approach has several drawbacks:\n\n1. **Data Loss**: Dropping and recreating the database each time results in data loss, which is unacceptable for production environments.\n2. **Performance**: Recreating the database can be time-consuming and may impact application startup times.\n3. **Unreliable**: The database initialization process can fail due to various reasons, leading to inconsistent data and potential errors.\n\n### Decision\n\nWe will switch to a more sustainable and reliable approach for database initialization. We will use Liquibase, a popular database migration tool, to manage our database schema and data migrations. This will allow us to:\n\n1. **Preserve Data**: Migrations will be executed in a controlled manner, preserving existing data and avoiding data loss.\n2. **Improve Performance**: Migrations will be executed only when necessary, reducing the overhead of database recreation.\n3. **Ensure Reliability**: Liquibase will provide a robust and reliable way to manage database changes, reducing the risk of errors and inconsistencies.\n\n### Rationale\n\nLiquibase is a well-established and widely-used tool for database migration and management. It provides a flexible and customizable solution for managing database changes, including schema and data migrations. By using Liquibase, we can ensure a more sustainable and reliable approach to database initialization, which is essential for production environments.\n\n### Consequences\n\n1. **Development Time**: We will need to invest time in setting up and configuring Liquibase, as well as creating and managing migrations.\n2. **Database Complexity**: Our database schema and data migrations will become more complex, requiring careful planning and execution.\n3. **Testing**: We will need to update our testing strategy to accommodate the new database initialization approach.\n\n### Next Steps\n\n1. Set up Liquibase in our project.\n2. Create and manage migrations for schema and data changes.\n3. Update our testing strategy to accommodate the new database initialization approach.\n4. Monitor and refine the database initialization process as needed.","GenTime":"2024-08-10 15:21:06"}
{"File Name":"snippets-service\/0005-frequency-capping.md","Context":"## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n","Decision":"We decide to implement the Frequency Capping functionality into our\\nplatform to allow Content Managers to limit the number of Impressions,\\nClicks and Blocks per Job.\\nLocal or User Frequency Capping will be handled on the Browser level\\nby the Firefox Messaging Platform. The later supports only Impression\\nFrequency Capping.\\nThe Snippets Service will provide an interface (UI) for the Content\\nManagers to set upper limits on the number of Impressions a Job gets\\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\\nProfile Lifetime. This information is included in the JSON generated\\nfor each Job.\\nFor Global Frequency Capping the Snippets Service will provide an\\ninterface (UI) for the Content Managers to set the limits on total\\nworldwide number of Impressions, Clicks and Blocks per Job.\\nSnippets Service will query Mozilla's Redash for Telemetry data every\\nten minutes and will fetch current impressions, clicks, blocks for\\neach Job with set limits.\\nWhen the reported numbers exceed the set limits then, the Job will be\\nmarked COMPLETE and will be pulled out of the Bundles on the next run\\nof `update_jobs` cron job.\\nThe Frequency Capping functionality is additional to the Date\\nPublishing controls, therefore a Job can end on a specific Date and\\nTime or when its Global Frequency Capping Limits are met.\\n### Monitoring and Handling of Errors\\nSince Global Frequency Capping depends on an external system for\\nMetrics (Redash \/ Telemetry) it is possible that the latest numbers are\\nnot always available to the Snippets Service to make a decision. Such\\ncases include scheduled or unplanned service interruptions or network\\nerrors.\\nIn co-ordination with Snippet Content Owner we decided that for cases\\nwhere the Snippets Service cannot get the latest numbers for more than\\n24 hours, Jobs with Global Frequency Capping will get canceled. The\\ncancellation reason will state that the Jobs where prematurely\\nterminated due to missing metrics.\\nThe cron job responsible for fetching the Data from Telemetry is\\nmonitored by a Dead Man's Snitch.\\n","tokens":208,"id":3229,"Prediction":"## Decision\n\n**Decision:** Implement Frequency Capping functionality in the Snippet Service, enabling Content Managers to limit the number of impressions or interactions users have with content, both globally and locally.\n\n**Context:** The Snippet Service has been lacking this functionality due to technical limitations imposed by the way metrics were collected and content selection was handled on the client side. However, with the latest developments in Firefox Messaging Center and the Firefox Telemetry Pipeline, this capability is now unblocked.\n\n**Reasoning:** Frequency Capping is a widely available tool in Publishing Platforms, and its implementation in the Snippet Service will provide Content Managers with more control over how their content is consumed. This will enable them to optimize their content's reach and engagement, ultimately leading to better user experiences and more effective content delivery.\n\n**Consequences:**\n\n* Content Managers will have more control over content distribution and engagement.\n* The Snippet Service will become more competitive with other publishing platforms.\n* The implementation of Frequency Capping may require changes to the existing metrics collection and content selection mechanisms.\n* The development of this feature may introduce additional complexity to the system.\n\n**Alternatives Considered:** None, as the technical limitations imposed by the current system have been addressed by the latest developments in Firefox Messaging Center and the Firefox Telemetry Pipeline.\n\n**Status:** Accepted","GenTime":"2024-08-10 15:23:35"}
{"File Name":"cafebabel.com\/0005-flask-architecture.md","Context":"## Context\\nFlask allows you to do whatever you want on your app\/folders\/templates\/etc\\nand it can quickly become messy.\\nPlus, to deal with configurations within extensions is harder if you do not\\ndeclare the Flask app once and for all then load the configuration.\\n","Decision":"An architecture based on https:\/\/github.com\/stephane\/flask-skeleton\\n","tokens":58,"id":3237,"Prediction":"## Decision\n\nTo maintain a clean and organized architecture, we will use the \"Factory Pattern\" to create the Flask application, allowing us to decouple the application configuration from the application instance.\n\n## Context\n\nAs you mentioned, Flask's flexibility can sometimes lead to a messy and hard-to-maintain codebase. Additionally, dealing with configurations within extensions becomes more challenging when the Flask app is not properly instantiated and configured.\n\n## Problem\n\nWithout a clear and consistent way to create and configure the Flask app, our codebase may become disorganized and difficult to scale.\n\n## Decision\n\nTo address this issue, we will use the Factory Pattern to create a centralized point for creating and configuring the Flask app. This will allow us to:\n\n1. Decouple the application configuration from the application instance.\n2. Provide a clear and consistent way to create and configure the Flask app.\n3. Make it easier to manage and extend the application's functionality.\n\n## Solution\n\nWe will create a `app_factory.py` file that will contain the logic for creating and configuring the Flask app. This file will be responsible for:\n\n1. Importing the necessary extensions and configurations.\n2. Creating the Flask app instance.\n3. Configuring the app with the necessary settings.\n4. Returning the configured app instance.\n\nHere is an example of what the `app_factory.py` file might look like:\n```python\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_login import LoginManager\n\ndef create_app(config_name):\n    app = Flask(__name__)\n    app.config.from_object(f\"config.{config_name}\")\n\n    db = SQLAlchemy(app)\n    login_manager = LoginManager(app)\n\n    # Import and configure other extensions as needed\n\n    return app\n```\n## Rationale\n\nBy using the Factory Pattern, we can create a clear and consistent way to create and configure the Flask app, making it easier to maintain and extend our application. This approach also allows us to decouple the application configuration from the application instance, making it easier to manage and scale our application.\n\n## Consequences\n\nBy using the Factory Pattern, we will:\n\n1. Have a more organized and maintainable codebase.\n2. Be able to create and configure the Flask app in a consistent and predictable way.\n3. Be able to easily extend and modify the application's functionality.\n\nHowever, this approach may require some additional setup and configuration, and may require changes to our existing codebase.","GenTime":"2024-08-10 15:26:23"}
{"File Name":"html-diff-service\/0002-implement-using-spring-boot.md","Context":"## Context\\nThe HTML Diff Service is born out of need to increase performance of HTML\\ndiffing by not requiring a command line execution in Contently's web\\napplication. Additionally, we wanted to remove the Java dependency from our\\nRails application container to simplify and reduce the threat surface.\\n","Decision":"Spring Boot is used to allow for the service to be standalone with support for\\nembedded servers. The decision allows portability to deploy the service without\\na J2EE server.\\n","tokens":60,"id":3249,"Prediction":"## Decision\n\n**Decision:** Implement a standalone HTML diff service using a language-agnostic diffing library, rather than integrating the Java-based HTMLDiff library into the Rails application.\n\n**Context:** The HTML Diff Service is born out of need to increase performance of HTML diffing by not requiring a command line execution in Contently's web application. Additionally, we wanted to remove the Java dependency from our Rails application container to simplify and reduce the threat surface.\n\n**Problem:** The current implementation of HTML diffing in the Rails application requires a command line execution, which can lead to performance issues and introduces a Java dependency that increases the threat surface.\n\n**Reasoning:** By implementing a standalone HTML diff service using a language-agnostic diffing library, we can:\n\n* Improve performance by eliminating the need for command line execution\n* Reduce the threat surface by removing the Java dependency from the Rails application\n* Simplify the application architecture by decoupling the diffing functionality from the Rails application\n\n**Consequences:**\n\n* The HTML diff service will be a separate component that can be easily maintained and updated independently of the Rails application\n* The Rails application will no longer depend on the Java-based HTMLDiff library, reducing the risk of compatibility issues and security vulnerabilities\n* The performance of the HTML diffing functionality will be improved, leading to a better user experience\n\n**Alternatives Considered:** We considered integrating the Java-based HTMLDiff library into the Rails application, but this would have introduced additional complexity and risk. We also considered using a different diffing library that is written in Ruby, but this would have limited the flexibility and scalability of the diffing functionality.","GenTime":"2024-08-10 15:30:32"}
{"File Name":"html-diff-service\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3250,"Prediction":"Here is an Architectural Decision Record (ADR) for your project:\n\n**Decision**\n\n* **ADR-001: Use of Microservices Architecture**\n\n**Context**\n\nWe need to record the architectural decisions made on this project.\n\n**Problem**\n\nAs the project grows in complexity, we need to ensure that the system is scalable, maintainable, and adaptable to changing requirements. The traditional monolithic architecture is no longer suitable for our project, and we need to consider a more distributed and flexible approach.\n\n**Decision**\n\nWe will use a microservices architecture for our project. This approach will allow us to break down the system into smaller, independent services that can be developed, tested, and deployed independently. Each service will be responsible for a specific business capability, and they will communicate with each other using APIs.\n\n**Rationale**\n\nThe microservices architecture will provide several benefits, including:\n\n* Improved scalability: Each service can be scaled independently, allowing us to handle increased traffic and load.\n* Increased flexibility: With a microservices architecture, we can develop and deploy new services without affecting the entire system.\n* Better maintainability: Each service can be maintained and updated independently, reducing the complexity of the overall system.\n* Improved fault tolerance: If one service fails, the other services can continue to operate, reducing the impact on the overall system.\n\n**Consequences**\n\n* Increased complexity: With a microservices architecture, we will need to manage multiple services, each with its own infrastructure and dependencies.\n* Higher operational costs: We will need to invest in additional infrastructure and personnel to manage the multiple services.\n* Greater testing complexity: We will need to develop and maintain multiple test suites for each service, which can increase the testing complexity.\n\n**Status**\n\nThis decision is approved and will be implemented in the next release.\n\n**History**\n\n* 2023-02-20: Decision made\n* 2023-03-15: Design and implementation plan created\n* 2023-04-01: First release with microservices architecture\n\n**Links**\n\n* [Design and implementation plan](https:\/\/example.com\/design-plan)\n* [Release notes for first release with microservices architecture](https:\/\/example.com\/release-notes)","GenTime":"2024-08-10 15:30:55"}
{"File Name":"HES_pipeline\/data_storage_and_access.md","Context":"## Context\\nThe raw HES data is large, complex, unweildly and potentially dirty. A way to\\nclean, store and later easily access the data for analysis required, which means\\nthat repeated cleaning and prep of the raw data is avoided.\\nThere are multiple datasets (with different variables), which once cleaned, will\\nbe accompanied by additional derived variables, and additional derived datasets.\\n","Decision":"The raw data will be read into a SQLite database through R. R is The Health\\nFoundation's open-source language of choice, and is well equiped to\\nhandle later analysis of said data. An SQLite database is not stored in memory,\\navoiding holding very large datasets an R session. It is a standard database\\nformat which can be easily queried with SQL which can in turn be written within\\nmost other common languagses, including R.\\nIn an SQLite database the database consistents of all the data, and within that\\nthere are tables; here each table corresponds to a HES dataset or a derived\\ndataset.\\n","tokens":83,"id":3253,"Prediction":"## Decision\n\n**Decision:** Implement a Data Warehouse and Data Pipelines to manage and process the raw HES data.\n\n## Context\n\nThe raw HES data is large, complex, and potentially dirty, requiring a solution to clean, store, and easily access the data for analysis. To avoid repeated cleaning and prep of the raw data, we need a system that can handle multiple datasets with different variables, and generate additional derived variables and datasets.\n\n## Problem\n\nThe raw HES data is difficult to work with due to its size, complexity, and potential errors. Manually cleaning and processing the data is time-consuming and prone to errors. The data is also likely to be used by multiple teams and stakeholders, who may have different requirements and queries.\n\n## Decision Rationale\n\nImplementing a Data Warehouse and Data Pipelines will provide a centralized and scalable solution to manage and process the raw HES data. The Data Warehouse will store the cleaned and transformed data, making it easily accessible for analysis and reporting. The Data Pipelines will automate the data cleaning, processing, and transformation, ensuring that the data is consistent and accurate.\n\n**Benefits:**\n\n* Reduced data preparation time and effort\n* Improved data quality and consistency\n* Increased scalability and performance\n* Simplified data access and analysis for multiple teams and stakeholders\n* Automated data processing and transformation\n\n## Solution\n\nWe will implement a Data Warehouse using a cloud-based platform (e.g. Amazon Redshift, Google BigQuery) and design Data Pipelines using a workflow management tool (e.g. Apache Airflow, AWS Glue). The Data Pipelines will be responsible for:\n\n1. Data Ingestion: Extracting the raw HES data from various sources\n2. Data Cleaning: Cleaning and transforming the raw data\n3. Data Transformation: Creating additional derived variables and datasets\n4. Data Loading: Loading the transformed data into the Data Warehouse\n5. Data Quality Control: Monitoring and reporting on data quality and integrity\n\nThe Data Warehouse will be designed to support various data analysis and reporting requirements, including data visualization, machine learning, and statistical analysis.","GenTime":"2024-08-10 15:32:00"}
{"File Name":"mediawiki-extensions-Kartographer\/webgl_renderer.md","Context":"## Context\\nIn order to decrease maintenance overhead in the maps infrastructure, the tile server will receive improvements in the vector-tile generation using Tegola. Although Kartotherian already have a vector-tile endpoint, now there is some effort to evolve the platform as a whole, and it includes client-side rendering, that will take advantage of future capabilities supported by Tegola.\\nThe client-side render process will provide decoupling of some server-side components and will make it possible to replace mapnik, which is a maintenance overhead.\\nAfter analyzing all the open-source options in the market, we have reached the decision of using the mapbox-gl-style specification as the next-level in terms of maps rendering. The reason is because it can be easily integrated with the top client-side maps frameworks: maplibre-gl, Leaflet, and OpenLayers.\\nBecause of its flexibility and support across frameworks, it gives us freedom to experiment different engines in the early stages of this work.\\n","Decision":"Accepted (02-16-2021)\\n","tokens":198,"id":3286,"Prediction":"## Decision\n\n**Title:** Migrating to Tegola and Mapbox-GL-Style Specification for Vector-Tile Generation and Client-Side Rendering\n\n**Context:** In order to decrease maintenance overhead in the maps infrastructure, the tile server will receive improvements in the vector-tile generation using Tegola. The client-side render process will provide decoupling of some server-side components and make it possible to replace Mapnik, which is a maintenance overhead. After analyzing all the open-source options in the market, we have reached the decision of using the Mapbox-GL-Style specification as the next-level in terms of maps rendering.\n\n**Problem:** The current infrastructure has maintenance overhead due to the use of Mapnik, and the client-side rendering process will provide a decoupling of some server-side components.\n\n**Decision:** We will use Tegola for vector-tile generation and the Mapbox-GL-Style specification for client-side rendering. This decision was made after analyzing all the open-source options in the market and considering the flexibility and support across frameworks of the Mapbox-GL-Style specification.\n\n**Rationale:** The use of Tegola will improve the vector-tile generation process, and the use of the Mapbox-GL-Style specification will provide flexibility and support across frameworks, allowing us to experiment with different engines in the early stages of this work.\n\n**Consequences:**\n\n* Improved vector-tile generation using Tegola\n* Decoupling of server-side components and replacement of Mapnik\n* Flexibility and support across frameworks using the Mapbox-GL-Style specification\n* Possibility to experiment with different engines in the early stages of this work\n\n**Monitoring and Review:** The effectiveness of this decision will be monitored by tracking the maintenance overhead and the performance of the maps infrastructure. The decision will be reviewed after a period of time to ensure that it is still meeting the needs of the project.","GenTime":"2024-08-10 15:42:13"}
{"File Name":"paas-csls-splunk-broker\/ADR001-syslog-http-to-csls-adapter.md","Context":"## Context\\nWe want to provide a reliable method of shipping logs from applications (on\\nGOV.UK Paas) to Splunk so they can take advantage of the log storage, analytics\\nand protective monitoring provided by the GDS Cyber Security team.\\nGDS Cyber Security maintain an [AWS Kinesis][kinesis] based log shipping stream\\nthat accepts log events in the [AWS CloudWatch Logs][cloudwatch] format and\\nqueues them for delivery to Splunk.\\nGOV.UK PaaS supports forwarding log events from an application's stdout and\\nstderr streams in [syslog format][syslog] via [syslog drains][drains].\\nComponents such as [Fluentd][fluentd] are available that process and forwarding\\nlogs from various sources to various targets, but their configuration can\\nunweildly and hard to test.\\nGOV.UK PaaS Tenants can run [sidecar][sidecar] containers to handle custom log\\nshipping or instument their applications with [logging libraries that support\\nmultiple transports][winston], but this brings extra complexity and doesn't\\nmake use of supported logging infrastructure already provided by GOV.UK PaaS.\\n","Decision":"We will build an \"adapter\" application to deploy alongside the CSLS logging\\npipeline that accepts requests in the \"syslog over http\" format exported by the\\nGOV.UK PaaS syslog drain system, translates them into the CloudWatch Logs\\nformat and forwards them on to the CSLS Kinesis stream.\\n","tokens":248,"id":3290,"Prediction":"## Decision\n\n**Decision:** We will use Fluentd as the log shipping solution to forward log events from GOV.UK Paas applications to the GDS Cyber Security maintained AWS Kinesis stream.\n\n## Context\n\nAs described in the context, we need a reliable method to ship logs from applications on GOV.UK Paas to Splunk. The GDS Cyber Security team maintains an AWS Kinesis stream that accepts log events in AWS CloudWatch Logs format and queues them for delivery to Splunk. GOV.UK Paas supports forwarding log events from an application's stdout and stderr streams in syslog format via syslog drains.\n\n## Problem\n\nWe need a solution that can process and forward logs from various sources to various targets, while being easy to configure and test. Components like Fluentd are available, but their configuration can be unwieldy and hard to test. Running sidecar containers or instrumenting applications with logging libraries that support multiple transports brings extra complexity and doesn't make use of the supported logging infrastructure already provided by GOV.UK Paas.\n\n## Decision Criteria\n\n1. Ease of configuration and testing\n2. Ability to process and forward logs from various sources to various targets\n3. Compatibility with GOV.UK Paas and the GDS Cyber Security maintained AWS Kinesis stream\n4. Minimal complexity and extra overhead\n\n## Rationale\n\nWe chose Fluentd as the log shipping solution because it is designed to process and forward logs from various sources to various targets, making it a good fit for our use case. Fluentd is also highly configurable and has a large community of users and developers who contribute to its ecosystem. Additionally, Fluentd is compatible with GOV.UK Paas and can easily integrate with the GDS Cyber Security maintained AWS Kinesis stream. Finally, Fluentd is relatively easy to configure and test, making it a good choice for our needs.\n\n## Consequences\n\nBy using Fluentd as the log shipping solution, we will be able to reliably forward log events from GOV.UK Paas applications to the GDS Cyber Security maintained AWS Kinesis stream. This will allow us to take advantage of the log storage, analytics, and protective monitoring provided by the GDS Cyber Security team. The configuration and testing of Fluentd will be relatively straightforward, and we will be able to minimize complexity and extra overhead.","GenTime":"2024-08-10 15:43:26"}
{"File Name":"paas-csls-splunk-broker\/ADR007-paas-taking-ownership.md","Context":"## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov\/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n","Decision":"The GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\\n","tokens":94,"id":3291,"Prediction":"Here is an Architectural Decision Record (ADR) based on the provided context:\n\n**Decision:** Broker Service Refactoring\n\n**Context:** The RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov\/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\n\n**Problem:** The current implementation of the broker service is not scalable, maintainable, or easily extensible. The code is tightly coupled with the specific requirements of the Ireland and London regions, making it difficult to adapt to changes or new requirements. The pipeline for building and deploying the broker is also not automated, relying on manual intervention.\n\n**Decision:** To refactor the broker service to make it more scalable, maintainable, and extensible. This will involve:\n\n* Extracting the specific regional requirements into separate modules, allowing for easier adaptation to changes or new requirements.\n* Implementing automated testing and deployment pipelines to reduce manual intervention and improve reliability.\n* Implementing a more modular architecture to allow for easier extension or modification of the broker service.\n\n**Rationale:** By refactoring the broker service, we can improve its scalability, maintainability, and extensibility, making it easier to adapt to changing requirements and ensuring the continued delivery of high-quality services to our users.\n\n**Consequences:** The refactoring process will require significant changes to the codebase, which may introduce new technical debt. However, the benefits of improved scalability, maintainability, and extensibility will outweigh the costs in the long run.\n\n**Status:** Accepted\n\n**Date:** 2023-02-20\n\n**Responsible:** [Your Name]\n\n**Links:** [Any relevant links to documentation, code, or other resources]","GenTime":"2024-08-10 15:43:47"}
{"File Name":"paas-csls-splunk-broker\/ADR004-deploy-broker-as-lambda.md","Context":"## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n","Decision":"We will deploy the Broker as an AWS Lambda\\n","tokens":77,"id":3295,"Prediction":"## Decision\n\n**Deploy the Broker as a containerized application on a Kubernetes cluster**\n\n## Context\n\nWe need to deploy the Broker somewhere. The Broker implements the service broker API to generate per-application syslog drain URLs (Adapter URLs). The Adapter is written in Go. The Broker is written in Go. The Adapter runs as a lambda in AWS alongside the CSLS infrastructure. We have a pipeline to continuously build, test, deploy the Adapter to lambda.\n\n## Problem\n\nWe need to deploy the Broker in a way that allows for easy management, scalability, and maintainability. Considering the Adapter is already deployed as a lambda function, we want to ensure the Broker is deployed in a similar manner to maintain consistency and reduce complexity.\n\n## Decision\n\nWe will deploy the Broker as a containerized application on a Kubernetes cluster. This decision is based on the following reasons:\n\n* Containerization allows us to package the Broker with its dependencies and deploy it as a single unit, making it easier to manage and maintain.\n* Kubernetes provides a scalable and highly available platform for deploying and managing containerized applications.\n* By deploying the Broker on a Kubernetes cluster, we can leverage the same pipeline and infrastructure as the Adapter, reducing the complexity of managing multiple deployment mechanisms.\n* Containerization and Kubernetes also provide a flexible and scalable way to handle traffic and load balancing, which is important for a service broker that needs to handle multiple requests.\n\n## Consequences\n\n* We will need to containerize the Broker and create a Docker image for it.\n* We will need to set up a Kubernetes cluster and configure it to deploy and manage the Broker container.\n* We will need to update the pipeline to include the deployment of the Broker container to the Kubernetes cluster.\n* We will need to monitor and troubleshoot the Broker container and the Kubernetes cluster to ensure it is running correctly and efficiently.\n\n## Rationale\n\nDeploying the Broker as a containerized application on a Kubernetes cluster provides a scalable, maintainable, and consistent way to deploy and manage the service broker. It also allows us to leverage the same infrastructure and pipeline as the Adapter, reducing complexity and improving overall efficiency.","GenTime":"2024-08-10 15:45:20"}
{"File Name":"linshare-mobile-flutter-app\/0006-download-file-manager.md","Context":"## Context\\nIn LinShare Flutter application, we can download files one by one from \"My Space\", \"Received Shares\" or \"Shared Spaces\" with the library `flutter_downloader` by adding it to the queue :\\n```\\nTaskId enqueue(String url, String saveDir, String header...)\\n```\\nWe can also clear or cancel file queue with dedicated functions.\\n","Decision":"We decided that LinShare could currently download a file one by one.\\n","tokens":80,"id":3297,"Prediction":"## Decision\n\n**Decision:** Implement a file queue management system to handle file downloads in the LinShare Flutter application.\n\n## Context\n\nIn the LinShare Flutter application, we currently use the `flutter_downloader` library to download files one by one from \"My Space\", \"Received Shares\", or \"Shared Spaces\". This approach allows us to add files to the download queue using the `enqueue` method and manage the queue with dedicated functions to clear or cancel downloads.\n\n## Problem\n\nThe current approach has some limitations. For example, it does not provide a way to prioritize downloads, handle concurrent downloads, or provide a user-friendly interface to manage the download queue.\n\n## Decision Rationale\n\nTo address these limitations and improve the overall user experience, we will implement a file queue management system that allows users to manage their downloads more effectively. This system will provide the following features:\n\n* Prioritize downloads: Users will be able to set the priority of each download, allowing them to focus on the most important files first.\n* Manage concurrent downloads: The system will handle concurrent downloads, ensuring that multiple files are downloaded simultaneously without interfering with each other.\n* User-friendly interface: A dedicated interface will be provided to manage the download queue, allowing users to pause, resume, or cancel individual downloads.\n\n## Consequences\n\nImplementing a file queue management system will require significant changes to the current implementation. We will need to refactor the code to use a queue data structure and implement the necessary logic to manage the queue. Additionally, we will need to design and implement a user-friendly interface to manage the download queue.\n\nHowever, this change will provide several benefits, including:\n\n* Improved user experience: Users will be able to manage their downloads more effectively, with more control over the download process.\n* Increased efficiency: The system will be able to handle concurrent downloads, reducing the overall download time.\n* Enhanced functionality: The system will provide additional features, such as prioritization and cancellation, to improve the overall user experience.","GenTime":"2024-08-10 15:46:01"}
{"File Name":"linshare-mobile-flutter-app\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3308,"Prediction":"## Decision\n\n**Decision:** Implement a Microservices Architecture for the Project\n\n**Context:** We need to record the architectural decisions made on this project.\n\n**Problem:** The project requires scalability, flexibility, and maintainability to accommodate the increasing user base and growing complexity of the system.\n\n**Reasoning:** A microservices architecture allows for the decomposition of the system into smaller, independent services that can be developed, tested, and deployed independently. This approach enables us to:\n\n* Improve scalability by allowing each service to scale independently\n* Enhance flexibility by allowing services to be developed in different programming languages and frameworks\n* Improve maintainability by allowing services to be updated and deployed independently without affecting the entire system\n\n**Consequences:**\n\n* Increased complexity in terms of communication and integration between services\n* Potential for increased latency due to network communication between services\n* Requires a robust and scalable infrastructure to support the microservices architecture\n\n**Alternatives Considered:**\n\n* Monolithic architecture: A single, self-contained application that performs all the necessary functions.\n* Service-oriented architecture (SOA): A distributed architecture where services are designed to be reusable and loosely coupled.\n\n**Decision Criteria:**\n\n* Scalability\n* Flexibility\n* Maintainability\n* Complexity\n\n**Status:** Approved\n\n**Implementors:** [List the names of the team members responsible for implementing this decision]\n\n**Links:** [List any relevant links to documentation, code, or other resources]\n\nBy recording this decision, we ensure that the architectural direction of the project is documented and that all stakeholders are aware of the chosen approach. This will help to facilitate communication, reduce misunderstandings, and ensure that the project is developed in a way that aligns with the chosen architecture.","GenTime":"2024-08-10 15:49:29"}
{"File Name":"cukeness\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3316,"Prediction":"Here is an Architectural Decision Record (ADR) for your software project:\n\n**Decision**\n\nTitle: Use a Microservices Architecture for the New Project\n\n**Context**\n\nAs we begin this new project, we need to decide on an architecture that will allow us to scale and maintain the system efficiently. We have considered various options, including a monolithic architecture and a service-oriented architecture (SOA). However, after careful evaluation, we have decided to adopt a microservices architecture for this project.\n\n**Decision Drivers**\n\n* Scalability: Microservices allow us to scale individual services independently, which is essential for our project's expected high traffic and user growth.\n* Flexibility: With microservices, we can develop and deploy services in different programming languages and frameworks, giving us more flexibility in our technology choices.\n* Resilience: If one service experiences issues, it will not affect the entire system, as each service is decoupled and can operate independently.\n* Maintainability: Microservices make it easier to maintain and update individual services without affecting the entire system.\n\n**Consequences**\n\n* Increased complexity: With more services to manage, there will be a higher level of complexity in the system, requiring more expertise and resources to maintain.\n* Higher operational overhead: Microservices require more infrastructure and operational overhead, including multiple databases, message queues, and service discovery mechanisms.\n* Potential for inconsistencies: With multiple services, there is a risk of inconsistencies between services, which will require careful design and testing to mitigate.\n\n**Rationale**\n\nWe believe that the benefits of microservices outweigh the drawbacks. The increased complexity and operational overhead will be manageable with proper design, testing, and deployment practices. Additionally, the flexibility and scalability offered by microservices will allow us to adapt to changing requirements and user needs more effectively.\n\n**Status**\n\nThis decision is currently in effect and has been implemented in the project.\n\n**References**\n\n* [1] \"Microservices: A Critical Review of the Literature\" by Ian Robinson\n* [2] \"Designing Distributed Systems\" by Brendan Burns\n* [3] \"Microservices Architecture\" by Martin Fowler","GenTime":"2024-08-10 15:52:15"}
{"File Name":"react-template\/0003-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":125,"id":3321,"Prediction":"## Decision\n\n### Decision: Adopt Prettier as the official code formatter across all projects\n\n### Context\n\nWe want to ensure we're all using one code style, that is familiar across projects. As of writing, Prettier is an opinionated code formatter with support for most, if not all, of the languages in the JavaScript ecosystem. With over 1 million repositories on GitHub, including React itself, Prettier has become a standard.\n\n### Problem\n\nWithout a standardized code formatting tool, our codebase may contain inconsistencies in formatting, making it harder to read and maintain. This can lead to errors, bugs, and increased development time.\n\n### Decision Criteria\n\n1. **Wide language support**: Prettier supports most, if not all, languages in the JavaScript ecosystem, making it a versatile choice.\n2. **Opinionated formatting**: Prettier's opinionated formatting ensures that code is formatted consistently, reducing the risk of inconsistencies.\n3. **Widespread adoption**: With over 1 million repositories on GitHub using Prettier, it has become a de facto standard in the industry.\n4. **Easy integration**: Prettier is easy to integrate with various development tools and workflows.\n\n### Decision\n\nWe will adopt Prettier as the official code formatter across all projects. This decision ensures consistency in code formatting, reduces errors, and increases development efficiency.\n\n### Consequences\n\n1. **Code consistency**: All code will be formatted according to Prettier's standards, making it easier to read and maintain.\n2. **Reduced errors**: Consistent formatting reduces the risk of errors and bugs caused by formatting inconsistencies.\n3. **Increased development efficiency**: Developers will spend less time formatting code and more time focusing on writing high-quality code.\n\n### Rationale\n\nBy adopting Prettier as our official code formatter, we can ensure consistency in code formatting across all projects, reducing errors and increasing development efficiency.","GenTime":"2024-08-10 15:54:02"}
{"File Name":"react-template\/0005-use-jest.md","Context":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":3323,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use Jest as our test framework for our React application with TypeScript support. This decision is based on the following reasons:\n\n* Jest is the standard and recommended test framework for React applications, which ensures compatibility and a large community of users and contributors.\n* Jest has excellent support for TypeScript, with built-in type checking and support for TypeScript's type annotations.\n* Jest provides a wide range of features and plugins for testing, including snapshot testing, code coverage, and mocking.\n* Jest has a large and active community, which ensures that issues are quickly addressed and new features are added regularly.\n* Jest is widely used in the React ecosystem, which makes it easier to integrate with other tools and libraries.\n\n## Context\n\nWe want a test framework that has good support for React and TypeScript.\n\n## Decision\n\nWe will use Jest as our test framework for our React application with TypeScript support.\n\n## Status\n\nApproved\n\n## Implemented\n\nThe implementation of Jest as our test framework will involve setting up a new project with Jest, configuring it to work with our React application and TypeScript code, and writing tests using Jest's API. We will also explore using Jest's plugins and features to improve our testing experience.\n\n## Rationale\n\nThe rationale for this decision is to ensure that we have a robust and widely-supported testing framework that can effectively test our React application with TypeScript. By using Jest, we can take advantage of its built-in support for TypeScript and its wide range of features and plugins for testing.\n\n## Consequences\n\nThe consequences of this decision are:\n\n* We will need to learn and adapt to Jest's API and syntax.\n* We will need to set up and configure Jest to work with our React application and TypeScript code.\n* We will need to write tests using Jest's API and syntax.\n* We will need to explore and integrate Jest's plugins and features to improve our testing experience.\n\nOverall, using Jest as our test framework will provide us with a robust and widely-supported testing framework that can effectively test our React application with TypeScript.","GenTime":"2024-08-10 15:54:41"}
{"File Name":"react-template\/0014-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":3328,"Prediction":"## Decision\n\n**Title:** Implement Dependabot to automate dependency updates\n\n**Context:** We want to ensure all dependencies stay up to date, and Dependabot offers a service where a bot opens PRs on GitHub when new versions of dependencies are released.\n\n**Decision:** We will integrate Dependabot into our GitHub repository to automate the process of updating dependencies.\n\n**Rationale:**\n\n* Dependabot is a widely-used and well-established service that can help us keep our dependencies up to date, reducing the risk of security vulnerabilities and ensuring our codebase remains compatible with the latest versions of dependencies.\n* Automating the process of updating dependencies will save us time and effort, allowing us to focus on more critical tasks.\n* Dependabot's integration with GitHub will enable us to review and merge updates in a controlled manner, ensuring that changes are properly tested and validated before being deployed.\n\n**Consequences:**\n\n* We will need to set up and configure Dependabot in our GitHub repository, which may require some technical expertise.\n* We will need to review and merge updates generated by Dependabot, which may require some additional testing and validation.\n* We will need to ensure that our CI\/CD pipeline is configured to build and test our codebase with the updated dependencies.\n\n**Status:** Accepted\n\n**Implementation:** We will set up Dependabot in our GitHub repository and configure it to monitor our dependencies. We will also update our CI\/CD pipeline to build and test our codebase with the updated dependencies.","GenTime":"2024-08-10 15:56:07"}
{"File Name":"knot-documentation\/ARC-002-fog-cloud-sync.md","Context":"## Context\\nOne of the main questions that appear when discussing the new fog service development\/integration was how the device contextual representation and data should be synchronized between fog and cloud. Before, when we were using the Meshblu we decided to send all the messages (device registration, schema, and data) to the cloud independently of the message type, data relevance at time or cloud connectivity state. This solution was simple for that moment but was increasing the internal message queues size and obviously isn't optimal mainly when dealing with a constrained environment. Therefore, our goal is to start with a simple but optimal approach in the first stage of the new fog service integration.\\n","Decision":"* Retry to register and update the device's schema on the cloud five times and if the error persists retry with ten minutes interval.\\n* Retry to publish only the last device's data.\\n* Retry to execute only the last commands sent to the device, avoiding to store a lot of pending commands on the message queue (which could lead to inconsistencies).\\n","tokens":132,"id":3337,"Prediction":"## Decision\n\n**Title:** Synchronize Device Contextual Representation and Data between Fog and Cloud using a Two-Phase Approach\n\n**Context:** As we discussed, the current approach of sending all messages (device registration, schema, and data) to the cloud independently of the message type, data relevance at time, or cloud connectivity state is not optimal, especially in a constrained environment. We need a more efficient and scalable approach to synchronize device contextual representation and data between fog and cloud.\n\n**Decision:** We will implement a two-phase approach to synchronize device contextual representation and data between fog and cloud. In the first phase, we will use a lightweight, event-driven mechanism to send only the necessary information to the cloud, focusing on device registration and schema updates. In the second phase, we will use a more robust, data-driven mechanism to send device data to the cloud, prioritizing data relevance and cloud connectivity state.\n\n**Rationale:**\n\n1. The two-phase approach allows us to separate the critical device registration and schema updates from the device data updates, reducing the load on the cloud and improving overall system performance.\n2. The event-driven mechanism in the first phase ensures that device registration and schema updates are processed efficiently, even in the presence of network connectivity issues.\n3. The data-driven mechanism in the second phase enables us to prioritize device data updates based on relevance and cloud connectivity state, reducing the likelihood of data loss or corruption.\n4. This approach provides a scalable and flexible solution that can be easily extended or modified as the fog service evolves.\n\n**Consequences:**\n\n1. Improved system performance and reduced load on the cloud.\n2. Enhanced reliability and resilience in the presence of network connectivity issues.\n3. Better support for device data prioritization and relevance.\n4. Simplified implementation and maintenance of the fog service.\n\n**Status:** Implemented\n\n**Owner:** [Your Name]\n\n**Created:** [Date]\n\n**Updated:** [Date]","GenTime":"2024-08-10 15:59:14"}
{"File Name":"knot-documentation\/ARC-003-amqp-exc-name.md","Context":"## Context\\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\\nThe goal is to define names that make it clear what this Exchange is used for.\\n","Decision":"* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\\n* The reference for choosing the queue name will be the client.\\n* The queue where BabelTower publishes messages will be called 'fogOut'.\\n* The queue where BabelTower consumes messages will be called 'fogIn'.\\n* The queue where Connector publishes messages will be called 'connOut'.\\n* The queue where Connector consumes messages will be called 'connIn'.\\n","tokens":131,"id":3339,"Prediction":"## Decision\n\n**Exchange Naming Convention for BabelTower Architecture**\n\n### Context\n\nAs defined by CloudAMQP Blog, exchanges are message routing agents that route messages to different queues based on header attributes, bindings, and routing keys. In the BabelTower architecture, we need to define exchange names that clearly indicate their purpose, considering the client-Connector interactions.\n\n### Problem\n\nThe exchange name should be defined in a way that is easy to understand and maintain, especially when dealing with multiple clients and services. The current architecture involves clients communicating with BabelTower, which acts as a Connector client, or communicating directly with the Connector. This raises the question of how to name the exchange in a way that accurately reflects its purpose.\n\n### Decision\n\nTo address this issue, we will adopt the following exchange naming convention:\n\n* For exchanges related to the client-Connector interactions within BabelTower, we will prefix the exchange name with `bt_` (short for BabelTower). For example: `bt_client_to_connector`.\n* For exchanges related to direct client-Connector interactions, we will prefix the exchange name with `direct_`. For example: `direct_client_to_connector`.\n* For exchanges related to internal BabelTower services, we will prefix the exchange name with `bt_service_`. For example: `bt_service_notification`.\n\nThis convention will help to clearly identify the purpose of each exchange, making it easier to understand and maintain the message routing configuration.\n\n### Rationale\n\nThe chosen convention is simple, consistent, and easy to understand. It separates the exchange names into three categories, making it clear which exchanges are related to client-Connector interactions, direct client-Connector interactions, or internal BabelTower services. This will reduce confusion and make it easier to manage the exchange configuration.\n\n### Consequences\n\nThis decision will ensure that the exchange names are clear, concise, and easy to understand. It will also help to maintain consistency across the system, making it easier for developers to understand and work with the message routing configuration. However, it may require some updates to existing code and documentation to reflect the new naming convention.","GenTime":"2024-08-10 15:59:54"}
{"File Name":"plant-shop\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3349,"Prediction":"Here is an Architectural Decision Record for your project:\n\n**Decision**: Implement a Microservices Architecture for the Project\n\n**Context**: As our project grows in complexity and scale, we need a scalable and maintainable architecture that allows for independent development, deployment, and scaling of each component. We also need to ensure that each service can be developed, tested, and deployed independently without affecting the entire system.\n\n**Problem**: Our current monolithic architecture is becoming difficult to maintain and scale, making it challenging to add new features and improve performance.\n\n**Decision**: We will implement a microservices architecture for the project, breaking down the monolithic application into smaller, independent services that communicate with each other using APIs.\n\n**Rationale**: A microservices architecture provides several benefits, including:\n\n* Scalability: Each service can be scaled independently, allowing us to optimize resources for each component.\n* Flexibility: Services can be developed and deployed independently, allowing for faster development and deployment of new features.\n* Resilience: If one service experiences issues, it will not affect the entire system, ensuring higher uptime and reliability.\n* Easier maintenance: With a smaller, more focused codebase, maintenance and debugging become easier and more efficient.\n\n**Consequences**:\n\n* Increased complexity: Implementing a microservices architecture requires additional infrastructure, such as service discovery and communication protocols.\n* Higher operational overhead: We will need to manage and monitor multiple services, which can increase operational overhead.\n* Potential for increased latency: Depending on the communication protocol and network topology, there may be increased latency between services.\n\n**Status**: Implemented\n\n**Links**: [Insert links to relevant documentation, code, or other resources]\n\n**Notes**: [Insert any additional notes or comments about the decision]\n\nThis Architectural Decision Record provides a clear and concise record of the decision, its context, rationale, and consequences. It also provides a link to relevant documentation and code, making it easy to reference and review the decision in the future.","GenTime":"2024-08-10 16:03:28"}
{"File Name":"mediawiki-extensions-Wikispeech\/0001-store-synthesized-audio-in-mediawiki-extension.md","Context":"## Context\\nThe original implementation of Wikispeech stored the synthesized audio\\nas files in a folder within the Speechoid service (in the\\nwikispeech-server sub-service). The paths to these files, together\\nwith the related metadata were then passed on as a response to the\\nMediaWiki extension.\\nThis implementation had a few identified drawbacks: Wikimedia\\ninfrastructure expects files to be stored in [Swift] rather than as\\nfiles on disk, supporting this would require implementing Swift\\nstorage in the Speechoid service.  There is a desire to keep the\\nSpeechoid service stateless, persistent storage of synthesized files\\nwithin the service runs counter to this.  The utterance metadata was\\nnot stored, requiring that each sentence always be re-synthesized\\nunless cached together with the file path.\\nWhile Wikimedia requires Swift many other MediaWiki installations\\nmight not be interested in that. It is therefore important with a\\nsolution where the file storage backend can be changed as desired\\nthrough the configs.\\nDue to [RevisionDelete] none of the content (words) of any segment\\nanywhere should be stored anywhere, e.g. in a table, since these must\\nthen not be publicly queryable, and to include mechanisms preventing\\nnon-public segments from being synthesized.\\nWe have an interest in storing the utterance audio for a long time to\\navoid the expensive operation of synthesizing segments on demand, but\\nwe still want a mechanism that flush stored utterances after a given\\nperiod of time. If a user makes a change to a text segment, it is\\nunlikely that the previous revision of that segment is used in another\\narticle and could thus be instantly flushed. There is also the case\\nwhere we want to flush to trigger re-synthesizing segments when a word\\nis added to or updated in the phonetic lexicon, as that would improve\\nthe resulting synthesized speech.\\nRe-use of utterance audio across a site (or many sites) is desirable,\\nbut likely to be rare (largely limited to headings and shorter\\nphrases). What will likely be more common is re-use of utterance audio\\nacross multiple revisions of the same page. If a single segment is\\nedited then all other segments, and their corresponding audio, remain\\nvalid. For this reason utterance audio should not be tied to a given\\npage or revision.\\n","Decision":"Files are only temporarily stored within Speechoid.\\nWhen a segment is synthesized, or when the audio is retrieved, a check\\nmust be performed to ensure it corresponds to a page revision which\\nhas not been suppressed through RevisionDelete. A segment is\\nrepresented by a hash to satisfy RevisionDelete requirements on public\\ntables. The segment hash should only be constructed from its\\ncontents. For the sake of RevisionDelete the link to the synthesised\\naudio should never be exposed to the end user.\\nThe MediaWiki extension parses the response from Speechoid, fetches\\nthe synthesized audio and stores this as a file using the provided\\n[FileBackend] functionality. The corresponding utterance metadata is\\nstored as a JSON file. Both files share the same base filename.\\nAn expiry date is attached to each stored Speechoid response to allow\\nlexicon updates to propagate and for the flushing of outdated\\nsegments.\\n","tokens":488,"id":3351,"Prediction":"## Decision\n\nTo address the drawbacks of the original implementation, we will store the synthesized audio as objects in a NoSQL database, such as MongoDB or Cassandra, rather than as files on disk. This will allow us to decouple the storage of audio files from the Speechoid service, making it stateless and scalable. The database will also enable us to store metadata about each audio object, such as the timestamp and revision ID, and provide a mechanism for flushing stored utterances after a given period of time.\n\n## Context\n\nThe original implementation of Wikispeech stored synthesized audio files in a folder within the Speechoid service, which had several drawbacks, including the need for implementing Swift storage, persistence of audio files, and lack of metadata storage. Additionally, Wikimedia infrastructure expects files to be stored in Swift, and there is a desire to keep the Speechoid service stateless. The utterance metadata was not stored, requiring re-synthesis of sentences unless cached together with the file path. The solution should also comply with RevisionDelete requirements, ensuring that content is not publicly queryable, and provide a mechanism for flushing stored utterances after a given period of time.\n\n## Decision Drivers\n\n* Decouple storage of audio files from the Speechoid service to make it stateless and scalable\n* Comply with Wikimedia infrastructure requirements for Swift storage\n* Store metadata about each audio object, such as timestamp and revision ID\n* Provide a mechanism for flushing stored utterances after a given period of time\n* Ensure compliance with RevisionDelete requirements\n\n## Alternatives Considered\n\n* Implementing Swift storage in the Speechoid service, which would require significant changes to the service architecture\n* Storing audio files on disk, which would require implementing a file system and would not provide a scalable solution\n* Using a relational database to store audio objects, which would require complex queries and would not provide a scalable solution\n\n## Rationale\n\nStoring audio objects in a NoSQL database provides a scalable and flexible solution that meets the requirements of the project. It allows us to decouple the storage of audio files from the Speechoid service, making it stateless and scalable. The database also enables us to store metadata about each audio object, providing a mechanism for flushing stored utterances after a given period of time. Additionally, the solution complies with RevisionDelete requirements, ensuring that content is not publicly queryable.","GenTime":"2024-08-10 16:04:11"}
{"File Name":"ethnicity-facts-and-figures-publisher\/0002-use-heroku-run-detached-for-static-site-build.md","Context":"## Context\\nAt the moment, we have a set of scheduled tasks on Heroku that run every 10 minutes, hour, or day. These range from building the static site (every 10 minutes), to reporting stalled builds (every hour), to refreshing the database from production data (in dev\/staging, every night). The main job here, building the static site, has started to fail because it occasionally runs longer than 10 minutes. The Heroku scheduler will only allow a scheduled instance to live for as long as the interval on the task. Heroku is therefore killing the instance before it can finish, leading to the incomplete builds and frequent reports of 'stalled' (in this instance, killed) builds.\\nWe need to schedule static site builds in a way that removes this strict time constraint so that they will always finish building.\\n","Decision":"We will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.\\nThe command we will use is `heroku run:detached -a <app_name> .\/manage.py build_static_site`.\\nPros\\n----\\n* Very easy to implement.\\n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).\\nCons\\n----\\n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.\\n* We have to expose a Heroku API key and install the Heroku CLI buildpack on our main Publisher app.\\n","tokens":172,"id":3361,"Prediction":"## Decision\n\nTo remove the strict time constraint on the static site builds and ensure they always finish building, we will implement a Quartz-based scheduling system instead of relying on Heroku's scheduler.\n\n## Context\n\nAs described, the current Heroku scheduler is killing the instance of the static site build before it can finish, leading to incomplete builds and frequent reports of'stalled' builds. This is due to the strict time constraint imposed by the scheduler, which only allows a scheduled instance to live for as long as the interval on the task.\n\n## Problem\n\nThe strict time constraint imposed by the Heroku scheduler is causing the static site builds to fail, resulting in incomplete builds and frequent reports of'stalled' builds.\n\n## Decision\n\nWe will implement a Quartz-based scheduling system to schedule the static site builds. Quartz is a job scheduling library that allows us to define jobs that can run for an arbitrary amount of time, without the constraints imposed by the Heroku scheduler. This will ensure that the static site builds always finish building, even if they take longer than the initial interval.\n\n## Rationale\n\nQuartz is a widely-used and reliable job scheduling library that provides a lot of flexibility and customization options. It allows us to define jobs that can run for an arbitrary amount of time, and also provides features such as job retries, job failures, and job scheduling.\n\n## Consequences\n\nBy implementing a Quartz-based scheduling system, we will be able to remove the strict time constraint imposed by the Heroku scheduler, ensuring that the static site builds always finish building. This will improve the reliability and stability of our build process, and reduce the frequency of reports of'stalled' builds.\n\n## Pros and Cons\n\nPros:\n\n* Improved reliability and stability of the build process\n* Reduced frequency of reports of'stalled' builds\n* Flexibility and customization options provided by Quartz\n\nCons:\n\n* Additional complexity and overhead of implementing a new scheduling system\n* Potential for additional errors or issues if not implemented correctly","GenTime":"2024-08-10 16:08:35"}
{"File Name":"dos-server\/adr-14-persistent-file-links.md","Context":"## Context\\nDOS returns internally generated unique identifiers for files it ingests.\\nThe URLs are roughly of the form (at the time of this ADR, plain integer identifiers are returned):\\nhttps:\/\/dos-stage.mitlib.net\/file?id=123e4567-e89b-12d3-a456-556642440000\\nOnce files are ingested, the identifiers are stored in ArchivesSpace.\\nThis may result in a situation where if URLs to files need to change (e.g., if\\nthe AWS bucket name for DOS changes), all links in ArchivesSpace need to be updated for\\nall ingested files.\\nAlthough DOS can be made to generate persistent identifies, it is desirable to externalize\\nthis functionality for reuse and portability considerations.\\nThe Handle System can be used for this purpose. Handles are already being used in\\nDome and DSpace, and there is organizational familiarity with the system.\\n","Decision":"Handle System will be used to generate persistent links. DOS will use the\\nHandle server API to generate handles. Handles will be returned when the\\nrelevant DOS end point is invoked.\\n","tokens":187,"id":3369,"Prediction":"## Decision\n\nTo externalize the generation of unique identifiers for files ingested by DOS, we will use the Handle System to provide persistent and reusable identifiers.\n\n## Context\n\nAs described, DOS currently generates internal unique identifiers for files it ingests, which are stored in ArchivesSpace. However, this approach has limitations, such as the need to update all links in ArchivesSpace if the AWS bucket name for DOS changes. Additionally, DOS is not designed to generate persistent identifiers, which can be a problem for long-term preservation and reuse of the files.\n\n## Problem\n\nThe current approach to generating unique identifiers for files ingested by DOS has the following problems:\n\n* Internal identifiers are not persistent and may change if the AWS bucket name changes\n* The identifiers are not reusable across different systems or applications\n* The identifiers are not easily portable to other systems or environments\n\n## Decision Rationale\n\nWe have chosen to use the Handle System to generate unique identifiers for files ingested by DOS because:\n\n* Handles are persistent and can be used across different systems and applications\n* Handles are reusable and can be used to identify the same resource across different contexts\n* Handles are widely used and have a established community and infrastructure\n* Handles are already being used in Dome and DSpace, and there is organizational familiarity with the system\n\n## Consequences\n\nBy using the Handle System to generate unique identifiers for files ingested by DOS, we will:\n\n* Provide persistent and reusable identifiers for files\n* Reduce the risk of identifier changes due to changes in the AWS bucket name or other system changes\n* Improve the portability and reusability of the files across different systems and applications\n* Align with existing practices and infrastructure in Dome and DSpace\n\n## Alternatives Considered\n\nWe considered the following alternatives:\n\n* Using a different identifier system, such as UUIDs or DOI\n* Implementing a custom identifier system within DOS\n* Not using a persistent identifier system at all\n\nHowever, we chose the Handle System because of its established community, infrastructure, and wide adoption in the digital preservation and repository communities.","GenTime":"2024-08-10 16:11:16"}
{"File Name":"dos-server\/adr-11-exception-monitoring.md","Context":"## Context\\nAn application is needed for real time production error discovery and reporting. Sentry is currently being\\nused by DLS for various applications.\\n","Decision":"We will use Sentry for application exception monitoring.\\n","tokens":30,"id":3370,"Prediction":"## Decision\n\n**Error Reporting and Real-time Production Error Discovery**\n\n## Context\n\nAn application is needed for real-time production error discovery and reporting. Sentry is currently being used by DLS for various applications.\n\n## Problem\n\nThe current error reporting system is not providing real-time error discovery, and the existing Sentry integration is not sufficient to meet the requirements. The lack of real-time error discovery is causing delays in identifying and resolving production errors, resulting in decreased system reliability and increased downtime.\n\n## Decision\n\nTo address the problem, we will implement a new error reporting system that provides real-time production error discovery and reporting. We will leverage the existing Sentry integration and build upon it to provide a more comprehensive and real-time error reporting solution.\n\n## Rationale\n\nThe decision to implement a new error reporting system is based on the following factors:\n\n* Real-time error discovery is critical for identifying and resolving production errors quickly, which is essential for maintaining system reliability and minimizing downtime.\n* The existing Sentry integration is not sufficient to meet the requirements, and a new system is needed to provide the necessary functionality.\n* The new system will be designed to be scalable and flexible, allowing it to adapt to changing requirements and integrate with other systems as needed.\n\n## Consequences\n\nThe implementation of a new error reporting system will have the following consequences:\n\n* The system will provide real-time error discovery and reporting, enabling faster identification and resolution of production errors.\n* The system will be designed to be scalable and flexible, allowing it to adapt to changing requirements and integrate with other systems as needed.\n* The system will require additional resources, including development time and infrastructure, to implement and maintain.\n\n## Status\n\nThe decision has been made, and the implementation of the new error reporting system has begun. The system is currently in the development phase, and is expected to be completed within the next 6 months.","GenTime":"2024-08-10 16:11:36"}
{"File Name":"dos-server\/adr-1-metadata.md","Context":"## Context\\nDigital objects need to have associated metadata for various use cases (refer to\\nthe requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural.\\nTo avoid \"duplication\" of descriptive metadata, it is desired that DOS not store descriptive metadata.\\n","Decision":"Descriptive metadata will not be stored by DOS.\\n","tokens":59,"id":3372,"Prediction":"## Decision\n\nTo avoid duplication of descriptive metadata and to maintain a scalable and maintainable architecture, we will not store descriptive metadata in the Digital Object Store (DOS). Instead, we will leverage existing metadata management systems and services to retrieve and store descriptive metadata.\n\n## Context\n\nDigital objects need to have associated metadata for various use cases (refer to the requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural. To avoid \"duplication\" of descriptive metadata, it is desired that DOS not store descriptive metadata.\n\n## Problem\n\nStoring descriptive metadata in DOS would lead to data duplication and potential inconsistencies across different systems and services. This would also increase the complexity and maintenance burden of the DOS architecture.\n\n## Solution\n\nTo address this issue, we will integrate the DOS with existing metadata management systems and services that provide access to descriptive metadata. This will allow us to retrieve and store descriptive metadata in a centralized and standardized manner, without duplicating the data in the DOS.\n\n## Rationale\n\nBy not storing descriptive metadata in DOS, we can:\n\n* Avoid data duplication and potential inconsistencies\n* Reduce the complexity and maintenance burden of the DOS architecture\n* Improve scalability and performance of the system\n* Ensure consistency with existing metadata management systems and services\n\n## Consequences\n\n* The DOS will need to be integrated with existing metadata management systems and services to retrieve and store descriptive metadata.\n* The architecture of the DOS will need to be adjusted to accommodate the use of external metadata management systems and services.\n* The system will require additional infrastructure and resources to support the integration with external metadata management systems and services.\n\n## Alternatives Considered\n\n* Storing descriptive metadata in DOS: This would lead to data duplication and potential inconsistencies across different systems and services.\n* Using a separate metadata management system: This would require additional infrastructure and resources, and may lead to data silos and inconsistencies.\n\n## Decision Owner\n\n[Your Name]\n\n## Date\n\n[Date]","GenTime":"2024-08-10 16:12:20"}
{"File Name":"dos-server\/adr-2a-java.md","Context":"## Context\\nAs DOS will be an important component of our collections platform,\\nthe software stack (web server, application container, programming language, database)\\nDOS uses to realize core functionality and to fulfill non-functional requirements\\nof scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and\\nmature ecosystem libraries can help meet the functional requirements easily. The Spring\\nframework also makes it relatively easy to create RESTful web services.\\n","Decision":"The core service will rely on Java and Spring framework as the basic stack for implementing core services.\\n","tokens":114,"id":3374,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\n* We will use Java as the primary programming language for the DOS component of our collections platform, and leverage the Spring framework to create RESTful web services.\n\n**Context**\n\nAs DOS will be an important component of our collections platform, the software stack (web server, application container, programming language, database) DOS uses to realize core functionality and to fulfill non-functional requirements of scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and mature ecosystem libraries can help meet the functional requirements easily. The Spring framework also makes it relatively easy to create RESTful web services.\n\n**Rationale**\n\nUsing Java as the primary programming language and Spring framework for creating RESTful web services will allow us to take advantage of Java's built-in support for scalability, performance, and reliability, as well as the extensive ecosystem of libraries and tools available for Java. This will enable us to meet the non-functional requirements of the DOS component, such as scalability, performance, and reliability, while also making it easier to create and maintain the functional requirements of the system.\n\n**Consequences**\n\n* We will need to invest time and resources in learning and implementing the Java programming language and the Spring framework.\n* We will need to ensure that the Java and Spring ecosystem libraries and tools are properly integrated and configured to meet the specific requirements of the DOS component.\n* We will need to monitor and test the system to ensure that it meets the non-functional requirements of scalability, performance, reliability, security, and backward compatibility.\n\n**Status**\n\nThis decision is currently active and in use.\n\n**References**\n\n* [1] Java documentation\n* [2] Spring framework documentation\n* [3] Java ecosystem libraries and tools documentation","GenTime":"2024-08-10 16:12:56"}
{"File Name":"support-rota\/0004-add-rollbar-for-application-monitoring.md","Context":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n","Decision":"Use Rollbar to collect and manage our application errors.\\n","tokens":187,"id":3388,"Prediction":"## Decision\n\n**Error Management Mechanism: Rollbar vs. Other Options**\n\n## Context\n\nAs described, we currently use Rollbar for error management across 14 applications. However, we have projects that use other error tracking tools, such as Sentry. We also face issues with Rollbar environment names not matching our Rails environments, which can lead to confusion and difficulties in debugging.\n\n## Problem\n\nOur current error management mechanism, Rollbar, has been used for some time without a thorough review. We need a more robust and flexible solution to capture and manage application errors, taking into account the diversity of tools used across projects.\n\n## Decision\n\nWe will continue to use Rollbar as our primary error management mechanism, but with the following modifications:\n\n1. **Explicit Environment Configuration**: We will introduce a consistent and explicit way to configure Rollbar environments, using the `ROLLBAR_ENV` variable to manage environment names and ensure they match our Rails environments.\n2. **Tool Agnosticism**: We will allow for the use of other error tracking tools, such as Sentry, on a project-by-project basis, while still maintaining a consistent error management mechanism across all applications.\n3. **Monitoring and Review**: We will regularly review and monitor our error management mechanism to ensure it remains effective and efficient, and to identify areas for improvement.\n\n## Rationale\n\nBy continuing to use Rollbar, we can leverage our existing investment and expertise. The explicit environment configuration will improve clarity and ease of use. Allowing for tool agnosticism will enable us to adapt to changing project needs and preferences. Regular monitoring and review will ensure we stay up-to-date with the latest best practices and technologies.\n\n## Consequences\n\n* We will need to implement the explicit environment configuration and monitoring mechanisms.\n* We may need to refactor existing code to accommodate the new error management mechanism.\n* We will need to review and adapt our error management strategy as new projects and tools are introduced.\n\n## Status\n\nThis decision is currently in effect, with implementation and monitoring to follow.","GenTime":"2024-08-10 16:17:40"}
{"File Name":"support-rota\/0003-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":3389,"Prediction":"## Decision\n\nWe will use the `dotenv` gem to manage environment variables and provide a centralized way to access and stub environment variables. This decision is made to address the limitations of accessing ENV directly, ensure fast failure with explicit messages when environment variables are missing, and provide a way to stub environment variables in our test suite.\n\n## Context\n\nAccessing ENV directly without a wrapper is limited and can introduce problems. We want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\n\n## Rationale\n\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches. The `dotenv` gem is a popular and well-maintained alternative that provides the same functionality as Figaro. It allows us to load environment variables from a `.env` file and provides a centralized way to access and stub environment variables.\n\n## Consequences\n\nUsing the `dotenv` gem will provide a centralized way to manage environment variables and ensure that we fail fast with explicit messages when environment variables are missing. It will also provide a way to stub environment variables in our test suite, making it easier to test our code. However, it may require some changes to our existing codebase to adapt to the new gem.\n\n## Alternatives Considered\n\nWe considered using a custom solution to manage environment variables, but this would require a significant amount of development and maintenance effort. We also considered using a different gem, but `dotenv` is a popular and well-maintained alternative that provides the same functionality as Figaro.","GenTime":"2024-08-10 16:18:03"}
{"File Name":"mario\/0008-use-one-s3-bucket-per-source.md","Context":"## Context\\nEach data source will need to upload one or more files to S3 in order to trigger processing. S3 events, which will drive Lambda execution (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), are configured at the bucket level. We may or may not have much control over the environment which is sending data to S3, for example, if it came directly from a vendor. At minimum we must be able to specify a bucket, but we should not assume we will have much more control than this.\\nEach data source will also need different processing. This implies the need to identify which source a data file came from.\\n","Decision":"Use one S3 bucket per data source.\\n","tokens":137,"id":3416,"Prediction":"## Decision\n\n**Use a hybrid approach for handling file uploads to S3 and processing**\n\n### Context\n\nEach data source will need to upload one or more files to S3 in order to trigger processing. S3 events, which will drive Lambda execution, are configured at the bucket level. We may or may not have much control over the environment which is sending data to S3, for example, if it came directly from a vendor. At minimum, we must be able to specify a bucket, but we should not assume we will have much more control than this. Each data source will also need different processing, implying the need to identify which source a data file came from.\n\n### Problem\n\nWe need to handle file uploads to S3 and trigger processing for each data source, while also accommodating varying levels of control over the environment sending data to S3.\n\n### Decision\n\nWe will use a hybrid approach to handle file uploads to S3 and processing. Specifically:\n\n1. Each data source will be responsible for uploading files to a designated S3 bucket.\n2. We will use S3 events to trigger Lambda functions, which will process the uploaded files.\n3. To identify which data source a file came from, we will use a metadata approach, where each data source will include a unique identifier (e.g., a header or a custom metadata field) in the uploaded files.\n\n### Rationale\n\nThis approach allows us to:\n\n* Decouple the file upload process from the processing logic, making it more scalable and flexible.\n* Accommodate varying levels of control over the environment sending data to S3, as we can still identify the data source based on the metadata.\n* Support different processing requirements for each data source, as we can tailor the Lambda function logic accordingly.\n\n### Consequences\n\n* We will need to implement metadata handling and processing logic in our Lambda functions.\n* We will need to ensure that each data source is configured to include the required metadata in their uploaded files.\n* We will need to monitor and troubleshoot S3 events and Lambda function execution to ensure that processing is triggered correctly.","GenTime":"2024-08-10 16:27:11"}
{"File Name":"saas-plaform-tenant-identity-provider\/0002-use-a-hosted-identity-solution-over-a-self-hosted-one.md","Context":"## Context\\nWe need an identity provider for our platform. This could be a host (SaaS) solution vs a self hosted solution. While the Cost & Lockin is higher at scale with a SaaS the self hosted has a higher Management Effort & Less Security & Expensive Bootstraping. From an evolutionary architecture perspective it is not core to value generation and does not change rapidly[source](https:\/\/www.youtube.com\/watch?v=8bEsNT7jdC4&t=112s&index=57&list=WL) - identity will mostlikly the same (OpenIDConnect, Login page ...) but it will be hard to change in the future if you decided for one provider.\\n","Decision":"We will use a SaaS solution as we don't have the skill to host our own solution in the efficient quality. And from a evolutionary architecture perspective the system is commondity or support.\\n","tokens":142,"id":3421,"Prediction":"## Decision\n\n**Identity Provider Solution: SaaS-based OpenID Connect**\n\n## Context\n\nAs discussed, we need an identity provider for our platform, weighing the pros and cons of a SaaS-based solution versus a self-hosted solution. While a SaaS-based solution may have higher costs and lock-in at scale, it offers a lower management effort, increased security, and less expensive bootstrapping. Additionally, from an evolutionary architecture perspective, identity is not core to value generation and does not change rapidly, making a SaaS-based solution a suitable choice.\n\n## Problem\n\nWe need to decide on an identity provider solution that balances cost, management effort, security, and scalability.\n\n## Decision\n\nWe will choose a SaaS-based OpenID Connect solution as our identity provider. This decision is based on the following factors:\n\n* Reduced management effort: With a SaaS-based solution, we can focus on our core business logic and outsource the management of the identity provider.\n* Increased security: SaaS-based identity providers typically have advanced security measures in place, reducing the risk of security breaches.\n* Lower bootstrapping costs: By using a SaaS-based solution, we can avoid the upfront costs of setting up and maintaining a self-hosted identity provider.\n* Scalability: SaaS-based solutions are designed to scale with our growing user base, ensuring that our identity provider can handle increased traffic and user growth.\n\n## Consequences\n\n* Higher costs at scale: While the costs of a SaaS-based solution may be lower upfront, they may increase at scale.\n* Lock-in: By choosing a SaaS-based solution, we may be locked into a specific provider and may face difficulties in switching to a different provider in the future.\n* Limited customization: With a SaaS-based solution, we may have limited ability to customize the identity provider to our specific needs.\n\n## Rationale\n\nWhile the decision to use a SaaS-based OpenID Connect solution comes with some trade-offs, we believe that the benefits of reduced management effort, increased security, and lower bootstrapping costs outweigh the drawbacks. Additionally, we can mitigate the risks of lock-in and limited customization by carefully evaluating the SaaS provider's terms and conditions and building a flexible architecture that allows for future changes.","GenTime":"2024-08-10 16:28:58"}
{"File Name":"mediawiki-extensions-Popups\/0008-enable-minification-of-bundle-with-uglifyjs.md","Context":"## Context\\nSources are currently minified by JSMin producing a bundle bigger than\\nnecessary. We want to produce a JS bundle as small as possible to ship less\\ncode to clients.\\nGiven we have a pre-compilation step in Popups, we can leverage node based\\ntooling without penalty, so using UglifyJS to minify the code is an option.\\nWe performed some analysis to see the differences in asset size between the two\\ntools. See:\\n* [Minifying assets with uglifyjs][1]\\nResults with the current codebase as of today 12th of May 2017 are that uglify\\nproduces a 40% smaller bundle, and a 25% smaller bundle when using gzip.\\nThis results are stable through the last 3 months, the same test has been\\nperformed multiple times. See [Compressed JS comparison][2]\\n","Decision":"Webpack's production mode has been enabled for `npm run build`, and thus the\\ncompiled bundle will be minified in production with UglifyJS.\\nSee [Enable production settings for the production bundle][3]\\n","tokens":189,"id":3428,"Prediction":"## Decision\n\nAfter careful consideration of the context, I have decided to use UglifyJS to minify the JavaScript sources, instead of the current JSMin tool. The analysis has shown that UglifyJS produces a 40% smaller bundle without compression and a 25% smaller bundle with gzip compression, which is a significant improvement in terms of reducing the size of the shipped code.\n\n## Context\n\nAs mentioned in the context, sources are currently minified by JSMin, resulting in a bundle that is larger than necessary. With the pre-compilation step in Popups, we can leverage node-based tooling without penalty, making UglifyJS a viable option. The analysis has shown that UglifyJS produces a smaller bundle than JSMin, making it a better choice for our use case.\n\n## Status\n\nApproved\n\n## Rationale\n\nThe main reason for this decision is to reduce the size of the shipped code, which will result in faster page loads and improved user experience. The analysis has shown that UglifyJS is capable of producing a smaller bundle than JSMin, which makes it a better choice for our use case. Additionally, the use of UglifyJS will allow us to take advantage of its advanced minification capabilities, such as dead code elimination and tree shaking.\n\n## Decision Drivers\n\n* The need to reduce the size of the shipped code\n* The analysis showing that UglifyJS produces a smaller bundle than JSMin\n* The ability to leverage node-based tooling without penalty\n\n## Pros and Cons\n\nPros:\n\n* Reduced size of the shipped code\n* Improved user experience\n* Advanced minification capabilities\n\nCons:\n\n* Potential complexity in integrating UglifyJS with our existing build process\n* Potential impact on debugging and development workflows\n\n## Alternatives Considered\n\n* Continuing to use JSMin\n* Using another minification tool, such as Google's Closure Compiler\n\n## Implementation\n\nThe implementation of this decision will involve updating our build process to use UglifyJS instead of JSMin. This will require integrating UglifyJS with our existing build process and testing to ensure that it does not introduce any issues.","GenTime":"2024-08-10 16:31:35"}
{"File Name":"k8s-vagrant-centos-cluster\/0002-why-not-use-kubeadm.md","Context":"## Context\\nkubeadm can be used to setup Kubernetes cluster with apiserver, etcd, controller, scheduler...\\n","Decision":"Setup Kubernetes cluster with apiserver, etcd, controller, scheduler without using kubeadm and docker\\n","tokens":26,"id":3440,"Prediction":"## Decision\n\n**Use kubeadm to set up Kubernetes cluster for simplicity and ease of deployment**\n\n## Context\n\nkubeadm can be used to setup Kubernetes cluster with apiserver, etcd, controller, scheduler...\n\n## Problem\n\nSetting up a Kubernetes cluster from scratch can be a complex and time-consuming process, requiring manual configuration of multiple components, including the API server, etcd, controller, and scheduler.\n\n## Decision\n\nTo simplify the process of setting up a Kubernetes cluster, we will use kubeadm to automate the deployment of the necessary components. This will reduce the complexity and effort required to set up a new cluster, making it easier to deploy and manage our Kubernetes environment.\n\n## Rationale\n\nUsing kubeadm will provide several benefits, including:\n\n* Simplified deployment: kubeadm automates the deployment of the necessary components, reducing the complexity and effort required to set up a new cluster.\n* Consistency: kubeadm ensures consistency across clusters, making it easier to manage and maintain multiple clusters.\n* Flexibility: kubeadm supports a wide range of hardware and software configurations, making it easy to adapt to different environments.\n\n## Consequences\n\n* Reduced complexity: The deployment process will be simplified, reducing the risk of human error and making it easier to maintain the cluster.\n* Faster deployment: kubeadm will automate the deployment process, reducing the time required to set up a new cluster.\n* Increased consistency: kubeadm will ensure consistency across clusters, making it easier to manage and maintain multiple clusters.\n\n## Status\n\nApproved\n\n## Implementation\n\nWe will use kubeadm to set up our Kubernetes cluster, following the official documentation and best practices. We will also ensure that all necessary components are properly configured and tested before deploying the cluster to production.","GenTime":"2024-08-10 16:35:21"}
{"File Name":"community\/dr-017-Application-Integration-without-Wormhole-Connector.md","Context":"## Context\\nThe Wormhole Connector is a part of the Application Integration. It is used by external systems deployed inside customers' private datacenter to establish a secure connection tunnel to the Kyma platform.\\n","Decision":"The decision is to get rid of the Wormhole Connector at this stage of the project. It can be an optional component of the Application Connector but a secure tunnel. For now, the Application Connector should focus on stability and on providing the core business features.\\n","tokens":42,"id":3448,"Prediction":"## Decision\n\n**Decision:** Implement a Mutual TLS (mTLS) authentication mechanism for the Wormhole Connector to ensure secure communication between the external systems and the Kyma platform.\n\n**Context:** The Wormhole Connector is a critical component of the Application Integration, enabling secure communication between external systems deployed in customers' private datacenters and the Kyma platform. To ensure the confidentiality, integrity, and authenticity of the data exchanged between these systems, we need to implement a robust authentication mechanism.\n\n**Problem:** The current implementation relies on a simple username\/password authentication, which is vulnerable to various attacks, such as password cracking and man-in-the-middle attacks. This poses a significant risk to the security of the data exchanged between the external systems and the Kyma platform.\n\n**Reasoning:** Implementing a Mutual TLS (mTLS) authentication mechanism provides a more secure and robust way to authenticate the communication between the Wormhole Connector and the external systems. mTLS ensures that both parties verify each other's identity using digital certificates, preventing unauthorized access and ensuring the integrity of the data exchanged.\n\n**Consequences:**\n\n* Improved security: mTLS provides end-to-end encryption and mutual authentication, ensuring that only authorized systems can establish a connection to the Kyma platform.\n* Compliance: Implementing mTLS aligns with industry standards for secure communication, such as PCI-DSS and HIPAA.\n* Increased complexity: Implementing mTLS requires additional infrastructure and configuration, including certificate management and truststore setup.\n\n**Decision Drivers:**\n\n* Security: The primary driver for this decision is to ensure the confidentiality, integrity, and authenticity of the data exchanged between the external systems and the Kyma platform.\n* Compliance: Implementing mTLS aligns with industry standards for secure communication, ensuring that the Wormhole Connector meets the necessary security requirements.\n* Customer trust: By implementing a robust authentication mechanism, we can increase customer trust in the security of the Wormhole Connector and the Kyma platform.\n\n**Alternatives Considered:**\n\n* Implementing a username\/password authentication mechanism with additional security measures (e.g., rate limiting, IP blocking)\n* Using a third-party authentication service\n* Implementing a different encryption mechanism (e.g., SSL\/TLS)\n\n**Implementation:**\n\n* Configure the Wormhole Connector to use mTLS authentication\n* Set up a certificate authority (CA) to issue digital certificates for the external systems and the Kyma platform\n* Configure the truststore to trust the digital certificates issued by the CA\n* Implement certificate revocation list (CRL) and online certificate status protocol (OCSP) checks to ensure certificate validity","GenTime":"2024-08-10 16:38:08"}
{"File Name":"community\/dr-012-Prettier_as_JavaScript_code_formatter.md","Context":"## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https:\/\/prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n","Decision":"The decision is to use Prettier as the only code formatter for JavaScript-based projects.\\n","tokens":301,"id":3451,"Prediction":"## Decision\n\n**Use Prettier as the code formatter for Kyma projects**\n\n## Context\n\nAs discussed, the Code Style Guide is essential for maintaining a unified and readable codebase. Prettier is a popular and well-maintained tool that enforces consistent formatting rules, saving time and energy for developers.\n\n## Problem\n\nWithout a unified code style, our codebase may become difficult to read and maintain, leading to increased development time and potential errors.\n\n## Decision\n\nAfter considering the benefits of Prettier, we have decided to use it as the code formatter for Kyma projects. Its simplicity, ease of configuration, and support for various file types (JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON) make it an ideal choice.\n\n## Consequences\n\n* Consistent code formatting across the project, improving readability and maintainability.\n* Reduced noise in diffs and discussions around code style.\n* Simplified code review and development process.\n* Increased developer productivity.\n\n## Rationale\n\nPrettier's simplicity and ease of configuration make it an attractive choice for our project. Its support for various file types ensures that all our code files can be formatted consistently. Additionally, its ability to be used as a plugin, pre-commit hook, or standalone CLI tool provides flexibility in how it can be integrated into our development workflow.\n\n## Implementation\n\nWe will integrate Prettier into our development workflow by setting it up as a pre-commit hook in our `.git\/hooks\/pre-commit` file. This will ensure that all code changes are formatted according to our Code Style Guide before they are committed to the repository.","GenTime":"2024-08-10 16:38:47"}
{"File Name":"community\/0007-github-actions.md","Context":"## [Context](https:\/\/docs.google.com\/document\/d\/1zRy8OCZ_JJpbs6scwrvx5PYkWwaWbR-5DLuq3AujhBA\/edit)\\nLibero products need automated builds for pull requests and release candidates.\\nTravis CI has been [acquired](https:\/\/techcrunch.com\/2019\/01\/23\/idera-acquires-travis-ci\/) by a private equity firm and has an [uncertain future](https:\/\/twitter.com\/ReinH\/status\/1098663375985229825). We also have run into performance bottlenecks of [5 concurrent jobs](https:\/\/travis-ci.com\/plans) in the open source offer.\\nGithub Actions is a Github-native general workflow system that can provide CI\/CD capabilities; it has been in general availability [since November](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/). It offers fully managed, sandboxed environments and a per-repository limit to concurrency.\\n","Decision":"We will create Github Actions builds for all new projects.\\nWe will port existing Travis CI builds of maintained projects onto Github Actions, on an as-needed basis.\\n","tokens":222,"id":3456,"Prediction":"## Decision\n\n**Automated Builds for Libero Products using GitHub Actions**\n\n## Context\n\nAs Libero products require automated builds for pull requests and release candidates, we need to explore alternative solutions to Travis CI due to its uncertain future and performance bottlenecks. GitHub Actions, a Github-native general workflow system, has been in general availability since November and offers fully managed, sandboxed environments and a per-repository limit to concurrency, making it a suitable replacement.\n\n## Problem\n\nWe need to ensure that our automated builds are reliable, scalable, and secure for our Libero products, and Travis CI's uncertain future and performance limitations are no longer viable options.\n\n## Decision\n\nWe will migrate our automated builds from Travis CI to GitHub Actions, leveraging its fully managed, sandboxed environments and per-repository concurrency limits to ensure reliable and scalable builds for our Libero products.\n\n## Rationale\n\n* GitHub Actions is a Github-native solution, providing seamless integration with our existing infrastructure.\n* It offers fully managed, sandboxed environments, ensuring secure and reliable builds.\n* The per-repository concurrency limit allows us to scale our builds without worrying about performance bottlenecks.\n* GitHub Actions has been in general availability since November, providing a stable and reliable solution.\n\n## Consequences\n\n* We will need to refactor our existing Travis CI configurations to work with GitHub Actions.\n* We will need to test and validate our builds on GitHub Actions to ensure compatibility.\n* We will need to monitor and adjust our GitHub Actions configurations as needed to ensure optimal performance and reliability.\n\nBy migrating to GitHub Actions, we can ensure reliable, scalable, and secure automated builds for our Libero products, and take advantage of a native Github solution with a stable and reliable future.","GenTime":"2024-08-10 16:40:03"}
{"File Name":"community\/dr-013-Allowed_UI_technology_in_the_Console_UI.md","Context":"## Context\\nThe Console UI is the administration view of Kyma and is a core part of the product.\\nTo simplify the development process in Kyma, the number of used programming languages and frameworks is limited. It makes the collaboration easier and allows the developers to specialize in a chosen technology.\\nFollowing this principle, the Go language is used as the only programming language for the back-end functionality of the core Kyma features.\\nBy design, the Console UI can be extended using plugins written in any framework, developed either internally or by customers or partners. The interface between the Console UI and the plugins is based on HTML and JavaScript but the approach is to provide libraries for the most popular and well-adapted UI frameworks, and gather knowledge and experience in using them.\\n","Decision":"The decision is to allow the [React](https:\/\/reactjs.org) and [Angular](https:\/\/angular.io) frameworks in combination with the JavaScript and TypeScript programming languages to build the core views in the Kyma Console UI.\\n","tokens":159,"id":3460,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nAs of [current date], the Console UI of Kyma will use a plugin-based architecture, allowing for the extension of the UI using plugins written in any framework, developed either internally or by customers or partners. The interface between the Console UI and the plugins will be based on HTML and JavaScript, with libraries provided for the most popular and well-adapted UI frameworks.\n\n**Context**\n\nThe Console UI is a core part of the Kyma product, and simplifying the development process is crucial. By limiting the number of used programming languages and frameworks, collaboration and specialization become easier. The Go language is used for the back-end functionality of the core Kyma features. The Console UI can be extended using plugins, and the interface between the UI and the plugins will be based on HTML and JavaScript.\n\n**Rationale**\n\nThe plugin-based architecture allows for the extension of the Console UI without requiring significant changes to the core codebase. This approach enables customers and partners to develop plugins using their preferred frameworks, increasing the flexibility and adaptability of the Kyma ecosystem. By providing libraries for popular UI frameworks, we can ensure that the plugins are well-integrated and easy to maintain.\n\n**Consequences**\n\n* The Console UI will be more extensible and adaptable to different UI frameworks and technologies.\n* Developers will have more flexibility in choosing the frameworks and technologies they use for plugin development.\n* The Kyma ecosystem will become more diverse and open to innovation.\n* The complexity of the Console UI codebase will be reduced, making it easier to maintain and evolve.\n\n**Alternatives Considered**\n\n* A monolithic architecture, where the Console UI is tightly coupled with a specific framework or technology.\n* A micro-frontends architecture, where the Console UI is composed of multiple, independent front-end applications.\n\n**Status**\n\nApproved\n\n**Links**\n\n* [Link to relevant documentation or code]","GenTime":"2024-08-10 16:41:17"}
{"File Name":"community\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3466,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context:\n\n**Decision**\nADR-001: Use a Microservices Architecture\n\n**Context**\nWe need to record the architectural decisions made on this project.\n\n**Problem**\nThe project requires a scalable and maintainable architecture to handle a large volume of users and data. The monolithic architecture currently in place is not sufficient to meet these requirements.\n\n**Decision**\nWe will adopt a microservices architecture, where the application is broken down into a collection of small, independent services that communicate with each other using APIs. This will allow us to:\n\n* Scale individual services independently\n* Improve maintainability by allowing developers to work on specific services without affecting the entire application\n* Increase fault tolerance by allowing services to fail without affecting the entire application\n\n**Rationale**\nA microservices architecture is well-suited for our project because it allows us to:\n\n* Handle a large volume of users and data by scaling individual services independently\n* Improve maintainability by allowing developers to work on specific services without affecting the entire application\n* Increase fault tolerance by allowing services to fail without affecting the entire application\n\n**Status**\nApproved\n\n**Implementation**\nThe implementation of the microservices architecture will involve:\n\n* Breaking down the monolithic application into a collection of small, independent services\n* Designing and implementing APIs for communication between services\n* Implementing service discovery and load balancing\n* Implementing circuit breakers and retries for fault tolerance\n\n**References**\n* \"Microservices Architecture\" by Martin Fowler\n* \"Designing Distributed Systems\" by Brendan Burns\n\n**Changelog**\n* 2023-02-20: Initial approval and implementation plan\n* 2023-03-15: Service discovery and load balancing implementation\n* 2023-04-10: Circuit breakers and retries implementation\n\nNote: This is a sample ADR, and the details may vary based on the specific project requirements and context.","GenTime":"2024-08-10 16:43:23"}
{"File Name":"gp-finder\/0006-use-prometheus-for-exposing-metrics.md","Context":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n","Decision":"We will use Prometheus to monitor and alert on the state of the application.\\n","tokens":86,"id":3469,"Prediction":"## Decision\n\n**Title:** Implement a Lightweight Telemetry System for Real-time Application Monitoring\n\n**Context:** As discussed, we need a more lightweight and efficient way to monitor the application's behavior and performance, beyond logging and scraping. This is crucial to ensure the health of the application and provide a better service to our users.\n\n**Decision:** We will implement a lightweight telemetry system that collects and aggregates key performance indicators (KPIs) and metrics in real-time, allowing us to monitor the application's health and performance. This system will provide us with actionable insights to proactively identify and address potential issues before they impact the user experience.\n\n**Rationale:**\n\n1. **Lightweight**: The telemetry system will be designed to minimize performance overhead and avoid impacting the application's responsiveness.\n2. **Real-time monitoring**: The system will collect and process data in real-time, enabling us to quickly identify and respond to issues as they arise.\n3. **KPIs and metrics**: We will focus on collecting relevant KPIs and metrics that provide insights into the application's performance, such as request latency, error rates, and user engagement.\n4. **Aggregation and analysis**: The system will aggregate and analyze the collected data to provide a comprehensive view of the application's health and performance.\n5. **Alerting and notification**: The system will trigger alerts and notifications when thresholds are exceeded or anomalies are detected, enabling us to take proactive measures to address issues.\n\n**Consequences:**\n\n1. **Improved application health**: By monitoring KPIs and metrics in real-time, we will be able to identify and address issues before they impact the user experience.\n2. **Enhanced user satisfaction**: By proactively addressing issues, we will be able to provide a better service to our users, leading to increased satisfaction and loyalty.\n3. **Increased efficiency**: The telemetry system will help us to identify and optimize areas of the application that are impacting performance, leading to increased efficiency and reduced costs.\n\n**Monitoring and Feedback**: We will regularly review and refine the telemetry system to ensure it is meeting our monitoring and analysis needs. We will also provide feedback to the development team on areas for improvement and optimization.","GenTime":"2024-08-10 16:44:32"}
{"File Name":"gp-finder\/0002-use-express-web-framework.md","Context":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum. The decision of using Express and Node.js\\nwhere made at (platform level)[https:\/\/github.com\/search?q=topic%3Aconnecting-to-services+org%3Anhsuk&type=Repositories],\\nand the team had experience in creating these types of applications.\\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":96,"id":3470,"Prediction":"## Decision\n\nThe web framework to be used is Express.js, built on top of Node.js, to provide a level of abstraction over low-level language features and to minimize the effort spent solving non-domain specific problems.\n\n## Context\n\nA web framework is needed to provide a level of abstraction over low-level language features. Effort spent solving non-domain specific problems should be kept to an absolute minimum. The decision of using Express and Node.js was made at the platform level, and the team had experience in creating these types of applications.\n\n## Rationale\n\nThe use of Express.js and Node.js was chosen due to the team's existing experience and knowledge in building web applications with these technologies. Express.js provides a lightweight and flexible framework for building web applications, allowing the team to focus on solving domain-specific problems. By using a well-established and widely-used framework, the team can leverage the community-driven ecosystem and reduce the time spent on solving non-domain specific problems.\n\n## Consequences\n\n* The team will be able to focus on solving domain-specific problems, reducing the time spent on solving non-domain specific problems.\n* The use of Express.js and Node.js will allow the team to leverage the community-driven ecosystem and reduce the time spent on learning and implementing new technologies.\n* The team's existing experience and knowledge in building web applications with Express.js and Node.js will enable a faster development process and reduce the risk of errors.\n\n## Alternatives Considered\n\n* Other web frameworks such as Koa.js, Hapi, or Django were considered, but the team's existing experience and knowledge in building web applications with Express.js and Node.js made it the most suitable choice.\n* Building a custom web framework from scratch was also considered, but the team decided to use an existing and well-established framework to reduce the time spent on solving non-domain specific problems.","GenTime":"2024-08-10 16:44:52"}
{"File Name":"gp-finder\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3475,"Prediction":"Here is a sample Architectural Decision Record (ADR):\n\n**Decision**\n\nTitle: Use of Microservices Architecture for Scalability and Flexibility\n\n**Context**\n\nAs our project grows in complexity and user base, we need to ensure that our system can scale horizontally and vertically to meet the increasing demands. We also want to maintain the flexibility to make changes to individual components without affecting the entire system. Microservices architecture seems like a promising approach to achieve these goals.\n\n**Problem**\n\nOur current monolithic architecture is becoming a bottleneck, making it difficult to scale and maintain. We are experiencing performance issues and downtime due to the high traffic and large data sets. We need a more scalable and flexible architecture to handle the increasing demands.\n\n**Decision**\n\nWe will adopt a microservices architecture for our project, breaking down the monolithic application into smaller, independent services that communicate with each other using APIs. Each service will be responsible for a specific business capability, and will be developed, tested, and deployed independently.\n\n**Consequences**\n\n* Pros:\n\t+ Improved scalability: Microservices can be scaled independently, allowing us to scale specific services as needed.\n\t+ Increased flexibility: With separate services, we can make changes to individual components without affecting the entire system.\n\t+ Easier maintenance: With smaller, independent services, maintenance and updates will be easier and less prone to errors.\n* Cons:\n\t+ Increased complexity: With multiple services, we will need to manage more complexity, including communication between services and data consistency.\n\t+ Higher operational costs: With more services, we will need to invest in additional infrastructure and personnel to manage and maintain the system.\n\n**Status**\n\nThis decision is currently in the implementation phase. We are working on designing and developing the individual services, and will continue to monitor and evaluate the effectiveness of this architecture.\n\n**References**\n\n* [1] \"Microservices: A Critical Perspective\" by Sam Newman\n* [2] \"Designing Distributed Systems\" by Brendan Burns","GenTime":"2024-08-10 16:46:42"}
{"File Name":"devops-challenge\/0010-feat-ci-cd-with-circleci.md","Context":"## Context\\nTime to work on the CI\/CD solution.\\nI need a job manager to build, test and deploy the apps to the kubernetes\\ncluster.\\nI know there is a lot of documentation about jenkins, but I have been working\\nin the last years with other solutions like:\\n* SolanoCI (now closed)\\n* CircleCI\\nSo I need to see if there is a way to use my knowledge in CircleCI or not.\\nBesides, CircleCI has a free plan very useful for testing.\\nI know there is a jenkins-x product, but I don't have a clear idea about\\nit.  I installed it and spent couple of hours to make it work (jx) but I got\\nan error downloading kops, and I couldn't go further.\\n","Decision":"Try first with CircleCI to see if I can deploy the apps to kubernetes.\\nAs I'm using a monorepo, all the apps are within the same repository, so we\\ncannot separate the building process of each microservice.  All them will be\\nbuild and deployed as one.\\nBut if nothing has change in the app, then the building process will be faster.\\nTesting will be peformed on all apps secuentially, and some tests could be\\nperformed on all services without implementing mockups.\\nThis way, when a event is received by the CI, a script will be executed to\\nexecute the actions on each app.\\n","tokens":168,"id":3482,"Prediction":"## Decision\n\n**Job Manager for CI\/CD Solution**\n\n## Context\n\nAs we are about to implement a CI\/CD solution, we need a job manager to build, test, and deploy our apps to a Kubernetes cluster. Considering our team's experience with CircleCI and its free plan, we want to explore the possibility of using CircleCI as our job manager.\n\n## Problem\n\nWe need a reliable and efficient way to automate our build, test, and deployment process. We have experience with CircleCI and other solutions like SolanoCI, but we are unsure if we can leverage our existing knowledge to implement our CI\/CD solution.\n\n## Decision\n\nWe will use CircleCI as our job manager for the CI\/CD solution. This decision is based on the following factors:\n\n* Familiarity: Our team has experience with CircleCI, which will reduce the learning curve and implementation time.\n* Free plan: CircleCI offers a free plan that is suitable for testing and development, which will help us to evaluate the solution without incurring significant costs.\n* Kubernetes support: CircleCI provides native support for Kubernetes, making it an ideal choice for deploying our apps to the cluster.\n\n## Consequences\n\n* We will need to familiarize ourselves with CircleCI's features and configuration options.\n* We will need to integrate CircleCI with our existing tools and infrastructure.\n* We may need to invest time in learning CircleCI's best practices and optimization techniques.\n\n## Rationale\n\nUsing CircleCI as our job manager will allow us to leverage our existing knowledge and experience, reducing the risk and complexity of implementing a new solution. Additionally, CircleCI's free plan provides a cost-effective way to test and evaluate the solution before scaling up.","GenTime":"2024-08-10 16:48:20"}
{"File Name":"devops-challenge\/0009-doc-about-monorepo.md","Context":"## Context\\nMicroservices are complex because they are small and can use different languages\\nand different stacks to run: java, spring, jakarta, python, flask, tornado, ruby\\nrails, sinatra, mysql, postgresql, mongodb, redis...\\nThere are so heterogeneous that sometimes we need to set a common way to work with\\nthem.  Each microservice has a different life-cycle, some are update more\\nfrequently, others are not.\\nUsually, when apps differs in their life-cycle speed, incompatibilities will come\\nup.  Those incompatibilities can make the system get down.  And keeping an\\nupdated and acurated matrix of compatibilities is a pain.\\nA way to minimize this risks is to put all code in the same repository, but not as\\na monolithic application, but in separate folders.  This has some advantages\\nlike:\\n- all code is tested at the same time\\n- transversal refactors can be easy\\n- compatibility matrices are simplified\\n","Decision":"To use a monorepo to keep all code together and deploy it at the same time.\\n","tokens":210,"id":3483,"Prediction":"## Decision\n\n**Decision:** To use a monorepo approach with separate folders for each microservice, rather than a traditional monolithic repository or multiple separate repositories for each microservice.\n\n**Context:** As described, microservices can be complex due to their heterogeneous nature, using different languages and stacks. This complexity can lead to incompatibilities and difficulties in maintaining a matrix of compatibilities.\n\n**Problem:** The current approach of using multiple separate repositories for each microservice can lead to difficulties in maintaining compatibility, testing, and refactoring across services.\n\n**Rationale:** The proposed monorepo approach with separate folders for each microservice offers several advantages, including:\n\n* All code is tested at the same time, reducing the complexity of testing and ensuring that changes to one service do not break other services.\n* Transversal refactors can be easily performed, allowing for improvements to be made across multiple services at once.\n* Compatibility matrices are simplified, reducing the effort required to maintain and update them.\n\n**Consequences:**\n\n* All code will be stored in a single repository, which may require additional infrastructure and tooling to manage.\n* The structure of the repository will need to be carefully planned to ensure that each microservice has its own separate folder and that dependencies between services are managed effectively.\n* The development workflow may need to be adjusted to accommodate the monorepo approach, potentially requiring changes to version control systems, build tools, and testing frameworks.\n\n**Alternatives Considered:** The alternative to this decision would be to continue using multiple separate repositories for each microservice, which would likely lead to the same issues described in the problem statement. Another alternative could be to use a hybrid approach, where some services are stored in separate repositories and others are stored in the monorepo, but this would likely introduce additional complexity and challenges.\n\n**Status:** This decision is approved and will be implemented in the next release.","GenTime":"2024-08-10 16:48:41"}
{"File Name":"devops-challenge\/0001-record-architecture-decisions.md","Context":"## Context\\nThe goal of this challenge is to implement a support plaform for a\\nmicroservices architecture with several requirements:\\n* is scalable on load\\n* is fully automated\\n* identifies Single Point of Failures\\n* implements resilience and fault tolerance mechanisms\\nWe need to record the architectural decisions made on this project as\\na way to document all the thinking and building process.  This will help\\nothers to answer their wondering questions about why and how this solution was\\nimplemented.\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":112,"id":3488,"Prediction":"## Decision\n\n**Decision:** Implement a distributed, cloud-based infrastructure with a service mesh architecture to support the microservices architecture.\n\n**Context:** As part of the support platform for a microservices architecture, we need to ensure scalability, automation, identification of Single Point of Failures, and resilience and fault tolerance mechanisms.\n\n**Problem:** Traditional monolithic architectures are not suitable for modern microservices-based systems, which require a high degree of autonomy, scalability, and fault tolerance.\n\n**Reasoning:** A distributed, cloud-based infrastructure with a service mesh architecture provides the following benefits:\n\n* **Scalability**: By deploying services across multiple nodes and regions, we can ensure that the system can handle increased traffic and load.\n* **Automation**: With a service mesh, we can automate service discovery, traffic management, and security policies, reducing the need for manual intervention.\n* **Single Point of Failure (SPOF) identification**: By decoupling services and using a service mesh, we can identify and mitigate SPOFs, ensuring that the system remains operational even in the event of failures.\n* **Resilience and fault tolerance**: With a distributed infrastructure and service mesh, we can ensure that services can be easily restarted or re-routed in the event of failures, minimizing downtime and ensuring high availability.\n\n**Consequences:**\n\n* Increased complexity: Implementing a distributed, cloud-based infrastructure with a service mesh architecture requires significant expertise and resources.\n* Higher costs: Deploying and maintaining a distributed infrastructure can be more expensive than a traditional monolithic architecture.\n* Improved scalability and resilience: By using a distributed, cloud-based infrastructure with a service mesh architecture, we can ensure that the system is highly available, scalable, and resilient.\n\n**Alternatives:**\n\n* Implementing a monolithic architecture with a load balancer and autoscaling.\n* Using a container orchestration platform (e.g., Kubernetes) to manage a distributed infrastructure.\n\n**Decision Date:** [Insert date]\n\n**Responsible Person:** [Insert name]\n\n**Discussion:** This decision was discussed and agreed upon by the team during a meeting on [Insert date]. The team considered the pros and cons of each option and decided that a distributed, cloud-based infrastructure with a service mesh architecture best meets the requirements of the support platform.","GenTime":"2024-08-10 16:50:12"}
{"File Name":"git-en-boite\/0016-handle-concurrent-git-operations","Context":"## Context\\nConcurrent git operations can fail because git is locking the repo. The current implementation is waiting for jobs to finish, but multiple workers in different containers can still take jobs in the queue and proccess them in parallel.\\n","Decision":"Reduce worker concurrency to 1 by using a single container (for now).\\n","tokens":47,"id":3497,"Prediction":"## Decision\n\nTo mitigate the issue of concurrent git operations failing due to locking the repository, we will implement a distributed lock mechanism to ensure that only one worker can access the repository at a time.\n\n## Context\n\nConcurrent git operations can fail because git is locking the repo. The current implementation is waiting for jobs to finish, but multiple workers in different containers can still take jobs in the queue and process them in parallel.\n\n## Decision Drivers\n\n* The current implementation does not handle concurrent access to the repository correctly, leading to failures when multiple workers try to access the repository simultaneously.\n* We need a solution that ensures only one worker can access the repository at a time to avoid conflicts and ensure data consistency.\n* A distributed lock mechanism is a suitable solution for this problem, as it allows multiple workers to coordinate access to the repository without requiring a centralized lock manager.\n\n## Solution\n\nWe will use a distributed lock mechanism, such as Redis or ZooKeeper, to ensure that only one worker can access the repository at a time. Each worker will attempt to acquire the lock before accessing the repository, and if the lock is already held by another worker, the worker will wait until the lock is released.\n\n## Consequences\n\n* This solution will ensure that only one worker can access the repository at a time, preventing conflicts and ensuring data consistency.\n* The solution will add some overhead in terms of communication with the distributed lock mechanism, but this overhead is expected to be minimal.\n* The solution will require some additional configuration and setup to integrate with the existing system.\n\n## Rationale\n\nThe distributed lock mechanism provides a simple and effective way to ensure that only one worker can access the repository at a time. This solution is scalable and can be easily integrated with the existing system. It also provides a clear and consistent way to handle concurrent access to the repository, which is essential for ensuring data consistency and preventing conflicts.","GenTime":"2024-08-10 16:53:04"}
{"File Name":"git-en-boite\/0009-prefer-dugite-over-raw-git-commands-in-tests.md","Context":"## Context\\nThe build failed because the version of git used in CircleCI was not what we expected.\\n","Decision":"Use `GitProcess.exec` even in tests that need to lean on the git process.\\n","tokens":22,"id":3504,"Prediction":"## Decision\n\n**Decision:** Upgrade the version of Git used in CircleCI to a specific version that is compatible with our project's dependencies.\n\n**Context:** The build failed because the version of Git used in CircleCI was not what we expected.\n\n**Problem:** The unexpected version of Git caused issues with our project's dependencies, leading to a failed build.\n\n**Reasoning:** We need to ensure that the version of Git used in CircleCI is compatible with our project's dependencies to avoid build failures. Upgrading to a specific version of Git will provide a consistent and reliable environment for our builds.\n\n**Consequences:**\n\n* The build will no longer fail due to Git version issues.\n* We will have a consistent and reliable environment for our builds.\n* We will need to update our CircleCI configuration to use the new version of Git.\n\n**Alternatives Considered:**\n\n* We could have tried to use a different version of Git that is compatible with our project's dependencies. However, this would have introduced additional complexity and potential issues.\n* We could have left the build as is and tried to work around the issues caused by the unexpected Git version. However, this would have been a temporary solution and would not have addressed the root cause of the problem.\n\n**Status:** Implemented\n\n**Implementors:** [List the names of the implementors]\n\n**Links:** [List any relevant links or resources]\n\nNote: This is just a sample, please adjust the details according to your specific use case.","GenTime":"2024-08-10 16:55:08"}
{"File Name":"git-en-boite\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3505,"Prediction":"Here is an Architectural Decision Record (ADR) for your software:\n\n**Decision**\n\n**ADR-001: Use of Microservices Architecture**\n\n**Context**\n\nAs we are building a complex software system with multiple features and integrations, we need to decide on the architecture that will best support the development and maintenance of the system.\n\n**Problem**\n\nThe current monolithic architecture is becoming increasingly difficult to maintain and scale, with tight coupling between components making it hard to make changes without affecting other parts of the system.\n\n**Decision**\n\nWe will adopt a microservices architecture, breaking down the system into smaller, independent services that communicate with each other using APIs. This will allow for greater flexibility, scalability, and maintainability.\n\n**Rationale**\n\n* Microservices architecture allows for greater autonomy and independence of each service, enabling teams to work on individual services without affecting the entire system.\n* Each service can be developed, tested, and deployed independently, reducing the complexity and risk of changes.\n* Microservices architecture enables greater scalability, as each service can be scaled independently to meet changing demands.\n* The use of APIs for communication between services provides a clear and standardized interface, making it easier to integrate new services and third-party systems.\n\n**Consequences**\n\n* The transition to microservices architecture will require significant changes to the existing codebase and development processes.\n* There may be additional complexity and overhead in managing the communication between services.\n* The use of APIs will require careful consideration of data consistency and integrity across services.\n\n**Status**\n\nThis decision has been made and is currently being implemented.\n\n**History**\n\n* 2023-02-15: Decision made\n* 2023-03-01: Initial implementation of microservices architecture\n* 2023-04-15: Review and refinement of microservices architecture\n\nI hope this helps! Let me know if you have any further requests.","GenTime":"2024-08-10 16:55:28"}
{"File Name":"caia\/0011-pin-mbtest-library-to-v2.5.1.md","Context":"## Context\\nOn March 22, 2021, it was discovered that the \"caia\" Jenkins builds were\\nfailing. All the failing tests were failing with the following error, related\\nto a \"get_actual_requests\" method call:\\n```\\nTypeError: 'generator' object is not subscriptable\\n```\\nThe \"caia\" build was last successful in Jenkins on October 7, 2020. No builds\\nwere performed again until March 22, 2021, as there was no development work\\nbeing done on the project.\\nBuilds were made on March 22, 2021 because of a move to the\\n\"GitHub organization\" pipeline in LIBITD-1880, which triggered rebuilds in all\\nexisting projects.\\nWhen the last successful build was made in October, the \"mbtest\" library\\n([https:\/\/github.com\/brunns\/mbtest](mbtest)) was at v2.5.1. In v2.5.2, the\\n\"src\/mbtest\/server.py\" file was modified, changing the \"get_actual_requests\"\\nmethod signature (see [this commit e398f2f1f32420](mbtest_commit)). from:\\n```\\ndef get_actual_requests(self) -> Mapping[int, JsonStructure]:\\n```\\nto\\n```\\ndef get_actual_requests(self) -> Iterable[Request]:\\n```\\nThe change from a Mapping to an Iterable is the cause of the error in the tests.\\n","Decision":"The simplest solution for the moment is to \"pin\" the version of the \"mbtest\"\\nlibrary to v2.5.1 in the \"setup.py\" file. This will preserve the current\\nbehavior, until further \"caia\" development warrants additional testing.\\n","tokens":315,"id":3507,"Prediction":"## Decision\n\n**Problem:** The \"caia\" Jenkins builds are failing due to a TypeError: 'generator' object is not subscriptable error when calling the \"get_actual_requests\" method.\n\n**Root Cause:** The change in the \"get_actual_requests\" method signature in the \"mbtest\" library from a Mapping to an Iterable is the cause of the error.\n\n**Decision:** To resolve the issue, we will update the \"mbtest\" library in the project to the latest version (v2.5.2) to reflect the changes made in the \"get_actual_requests\" method signature.\n\n**Rationale:** Updating the \"mbtest\" library to the latest version will ensure that the project is using the correct method signature for the \"get_actual_requests\" method, which will resolve the TypeError: 'generator' object is not subscriptable error.\n\n**Consequences:**\n\n* The \"caia\" Jenkins builds will no longer fail due to the TypeError: 'generator' object is not subscriptable error.\n* The project will be using the latest version of the \"mbtest\" library, which includes the changes made in the \"get_actual_requests\" method signature.\n\n**Status:** This decision is approved and will be implemented as soon as possible.\n\n**Implementation:** The implementation will involve updating the \"mbtest\" library in the project to the latest version (v2.5.2) and rebuilding the \"caia\" Jenkins builds to ensure that they are successful.","GenTime":"2024-08-10 16:56:07"}
{"File Name":"tracking-consent-frontend\/0007-use-eslint-for-linting-and-formatting.md","Context":"## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n","Decision":"* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\nChosen option: ESLint + Airbnb\\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https:\/\/github.com\/airbnb\/javascript)\\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\\nit was overcome with config updates.\\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\\nchose to use Typescript for Tracking Consent but we usually use Javascript.\\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\\nthis is also a common feature which most IDEs have.\\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\\nwere committing.\\n### Positive Consequences\\n* Code style within a file will be consistent\\n* Files within a project will be consistent with each other\\n* When widely applied this Javascript\/Typescript files between projects will be consistent\\n* IDE will reformat for us which takes away the need for us to do it ourselves\\n* Code reviews will focus less on code format and more on the details of the changes made\\n* Builds will fail if format isn't followed which guarentees consistency\\n### Negative Consequences\\n* Builds will fail if format isn't followed which can disrupt focus\/workflow (mitigated by pre-commit hooks)\\n","tokens":133,"id":3521,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Introduce a standardized JavaScript code formatting style and use a linter to enforce it.\n\n**Context and Problem Statement:** We found that without a standardised format, our JavaScript files ended up with different formats in different files or even multiple formats in the same file. Additionally, our IDEs had different configurations, which meant that using an autoformat tool would give different results when each of us did it.\n\n**Decision Drivers:**\n\n* We wanted to spend less time doing manual formatting\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\n* We wanted to see easily which lines had actually changed when reviewing PRs\n* We wanted to avoid discussions about individual's preferences for particular formatting styles\n\n**Decision Description:** To address these issues, we will introduce a standardized JavaScript code formatting style and use a linter to enforce it. This will ensure that all code is formatted consistently, making it easier to read and maintain. The linter will also prevent manual formatting and autoformatting conflicts, reducing the time spent on formatting and reviewing code. Additionally, the standardized formatting will make it easier to identify changes in code reviews.\n\n**Decision Status:** Implemented\n\n**Pros and Cons:**\n\nPros:\n\n* Consistent code formatting across the project\n* Reduced time spent on formatting and reviewing code\n* Easier to identify changes in code reviews\n\nCons:\n\n* Initial setup and configuration required\n* May require changes to existing code to conform to the new formatting style\n\n**Rationale:** By introducing a standardized JavaScript code formatting style and using a linter to enforce it, we can improve code readability, reduce formatting conflicts, and increase productivity. This decision aligns with our goals of reducing manual formatting time, avoiding formatting conflicts, and making code reviews more efficient.","GenTime":"2024-08-10 17:00:26"}
{"File Name":"tracking-consent-frontend\/0005-use-data-attribute-for-language.md","Context":"## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https:\/\/github.com\/hmrc\/assets-frontend\/blob\/97c638289e23bee255ac30724a8572c6efa96817\/assets\/patterns\/help-users-when-we-time-them-out-of-a-service\/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n","Decision":"* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings\/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui\/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n","tokens":409,"id":3526,"Prediction":"## Decision\n\nWe will remove the reading of PLAY_LANG in tracking consent and accept a `data-language` attribute instead.\n\n## Rationale\n\nThe main driver for this decision is the need to support classic services, which do not set PLAY_LANG. Since PLAY_LANG is not set for these services, the banner will not be translated into Welsh, which is not acceptable. By removing the reading of PLAY_LANG and instead using a `data-language` attribute, we can ensure that the banner is translated correctly for all services.\n\nAnother important consideration is the time-sensitive nature of this issue. With many services already integrated, we need to deploy a solution quickly to avoid having to upgrade services a second time.\n\nAdditionally, we prefer to avoid further changes to tracking consent, which is a non-breaking change. Removing the reading of PLAY_LANG is a simpler solution than advising teams to add exemptions to ZAP tests across MDTP.\n\nFinally, consistency in language settings across our frontend components is important, and using a `data-language` attribute instead of PLAY_LANG will ensure that language settings are handled consistently across all components.\n\n## Trade-offs\n\nOne potential trade-off is the need to update the `timeout dialog` in assets-frontend to use the new `data-language` attribute. However, this is a relatively simple change, and the benefits of removing the reading of PLAY_LANG outweigh the costs.\n\n## Consequences\n\nBy removing the reading of PLAY_LANG and using a `data-language` attribute instead, we will:\n\n* Ensure that the banner is translated correctly for all services, including classic services\n* Avoid having to advise teams to add exemptions to ZAP tests across MDTP\n* Maintain consistency in language settings across our frontend components\n* Simplify the tracking consent code and reduce the risk of errors\n\nHowever, we will also need to update the `timeout dialog` in assets-frontend to use the new `data-language` attribute.","GenTime":"2024-08-10 17:02:01"}
{"File Name":"tracking-consent-frontend\/0002-use-data-attribute-for-gtm-container.md","Context":"## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n","Decision":"* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\nChosen option: \"Use a data attribute\", because based on the benefits listed below the team\\nbelieves this is the best way forward.\\n### Positive Consequences\\n* The Javascript bundle creation process is simplified.\\n* The Scala Play routing is simplified\\n* Only one endpoint needs to be managed in production\\n* Future additional containers can be supported more easily\\n* Service developers only have to configure a single URL to tracking consent and use\\na data attribute to configure the container.\\n* The central common configuration repo (owned by a separate team) only requires a single URL to\\ntracking consent defining for each environment, rather than one for each GTM container.\\n### Negative Consequences\\n* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPT\\ntag when integrating.\\n","tokens":78,"id":3529,"Prediction":"## Decision\n\nWe will configure the GTM container used by tracking consent via data attributes rather than separate bundles.\n\n## Context and Problem Statement\n\nIn order to simplify the tracking consent build and deploy process and make integrating with tracking consent less surprising, should the configuration of the GTM container used by tracking consent be via data attributes rather than separate bundles?\n\n## Decision Drivers\n\n* The need to keep things simple for service developers: By using data attributes, service developers will not need to manage separate bundles, which will reduce the complexity of the tracking consent integration.\n* The need to improve the operability of tracking consent: Data attributes will provide a more straightforward and intuitive way to configure the GTM container, making it easier to troubleshoot and maintain.\n\n## Rationale\n\nUsing data attributes to configure the GTM container will provide a more streamlined and user-friendly experience for service developers. This approach will also reduce the likelihood of errors and inconsistencies that can occur when managing separate bundles. Additionally, data attributes will provide a more explicit and transparent way to configure the GTM container, making it easier to understand and debug.\n\n## Consequences\n\n* Service developers will have a simpler and more intuitive way to configure the GTM container, reducing the complexity of the tracking consent integration.\n* The operability of tracking consent will be improved, making it easier to troubleshoot and maintain.\n* The risk of errors and inconsistencies will be reduced, ensuring a more reliable and accurate tracking consent experience.\n\n## Alternatives Considered\n\n* Using separate bundles to configure the GTM container: This approach would require service developers to manage additional files and configurations, increasing the complexity of the tracking consent integration.\n* Using a combination of data attributes and separate bundles: This approach would add complexity to the tracking consent integration, making it more difficult for service developers to understand and maintain.\n\n## Status\n\nApproved\n\n## Links\n\n* Related issues: #1234, #5678\n* Related documentation: [link to documentation]","GenTime":"2024-08-10 17:03:02"}
{"File Name":"rotc\/0002-use-aws-as-example-cloud-platform.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3546,"Prediction":"Here is a potential Architectural Decision Record:\n\n**Decision**\nUse a Service-Oriented Architecture (SOA) to structure our application's codebase.\n\n**Context**\n\nOur application has grown to become a complex system with multiple features and integrations. As a result, our codebase has become increasingly difficult to maintain and extend. We have multiple teams working on different parts of the application, and communication and coordination between teams has become a challenge. Additionally, our application relies on multiple third-party services, which can be difficult to integrate and manage.\n\n**Problem**\nOur current monolithic architecture makes it difficult to:\n\n* Scale individual components independently\n* Integrate with third-party services\n* Manage complexity and maintainability\n* Allow multiple teams to work on different parts of the application without conflicts\n\n**Rationale**\nA Service-Oriented Architecture (SOA) will allow us to:\n\n* Break down the application into smaller, independent services that can be developed, tested, and deployed independently\n* Use standardized interfaces and APIs to communicate between services\n* Improve scalability and fault tolerance by allowing services to be scaled and deployed independently\n* Reduce coupling and improve maintainability by minimizing dependencies between services\n* Enable multiple teams to work on different parts of the application without conflicts\n\n**Consequences**\nImplementing a SOA will require significant changes to our application's architecture and codebase. This will likely require:\n\n* Breaking down the application into smaller services\n* Defining and implementing standardized interfaces and APIs\n* Developing and deploying services independently\n* Integrating services with third-party services\n* Implementing monitoring and logging for services\n\n**Alternatives**\nWe considered alternative approaches, such as:\n\n* Microfrontends: This approach would involve breaking down the application into smaller, independent frontend components. However, this approach may not provide the same level of scalability and fault tolerance as a SOA.\n* Event-driven architecture: This approach would involve using events to communicate between components. However, this approach may be more complex to implement and manage than a SOA.\n\n**Decision Date**\n2023-02-15\n\n**Responsible Parties**\n[Name], [Name], [Name]\n\n**Links to Documentation**\n[Link to design documents, architecture diagrams, etc.]\n\nNote: This is just an example, you should adjust it to your specific use case and needs.","GenTime":"2024-08-10 17:08:54"}
{"File Name":"blueprint\/0003-feature-selection-policy.md","Context":"## Context and Problem Statement\\nOpen Data Hub has release v0.8.0, some of the Elyra features required by Thoth Station experiments are\\nnot part of this ODH release. This would require to update certain components to the HEAD of main branch of ODH upstream\\nproject.\\n## Decision Drivers\\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\n","Decision":"* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\nChosen option: \"deploy specific versions of components\", because this will give the most efficient deployment to\\nOperate First operators and users.\\n### Positive Consequences\\n* operators can gain a maximum of experience, enabling feedback on component versions that might have not been tested\\nby the upstream project\\n* users get the feature set they need to get the most out of an Operate First deployment\\n### Negative Consequences\\n* additional deployment\/manifest customizations that are not valuable to upstream project, as they are out of scope\\nfor them\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":90,"id":3551,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** \"Sync Open Data Hub components with upstream HEAD for Elyra features required by Thoth Station experiments\"\n\n**Context:** Open Data Hub has released v0.8.0, but some Elyra features required by Thoth Station experiments are not part of this release. This would require updating certain components to the HEAD of the main branch of the ODH upstream project.\n\n**Decision Drivers:**\n\n* Operational complexity of an environment diverging from an upstream release: By keeping components up-to-date with the upstream HEAD, we can reduce the operational complexity of our environment and ensure that our system is consistent with the latest developments.\n* User needs of more current software components: Thoth Station experiments require specific Elyra features that are not available in the current ODH release. By updating our components to the latest version, we can meet the user needs and ensure that our system is able to support the required features.\n\n**Decision Description:** We will update the components that require Elyra features to the HEAD of the main branch of the ODH upstream project. This will ensure that our system is up-to-date and able to support the required features for Thoth Station experiments.\n\n**Rationale:** By syncing our components with the upstream HEAD, we can ensure that our system is consistent with the latest developments and able to support the required features. This will also reduce the operational complexity of our environment and improve the overall user experience.\n\n**Consequences:**\n\n* We will need to update our components regularly to keep them in sync with the upstream HEAD.\n* We will need to ensure that our system is able to handle any potential breaking changes introduced by the upstream updates.\n* We will need to communicate the changes to the users and ensure that they are aware of the updates.\n\n**Status:** [Insert status, e.g. \"Proposed\", \"Accepted\", \"Rejected\", \"Implemented\"]","GenTime":"2024-08-10 17:10:56"}
{"File Name":"blueprint\/0019-org-management.md","Context":"## Context and Problem Statement\\nManaging GitHub organizations via UI is restricted to few individuals only and does not offer any review mechanisms. The same apply to repository creation, organization membership management and label management. This ADR focuses on bringing transparency into this aspect of housekeeping within a community and aims to provide a way to individual contributors, so they can influence and propose changes themselves.\\nADR seeks a way to manage GitHub organization as a code, declaratively via GitOps as a natural extension of the core paradigm of Operate First movement.\\n## Decision Drivers\\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\n","Decision":"Improve transparency, provide auditing and reviews and allow individual contributors to propose changes.\\nChosen option: _\"1. Prow's Peribolos\"_, because it's a declarative solution that is widely embraced in upstream communities like Kubernetes.\\n### Positive Consequences\\n* Repositories can be created declaratively.\\n* Even external users can request themselves to be added to the organization via a pull request.\\n* Teams can be created and maintained declaratively.\\n* Labels can be centrally managed in a unified way as well as updated per repository bases.\\n* Solution can be easily automated via post-submit Prow jobs.\\n### Negative Consequences\\n* Using a declarative configuration may not be as straightforward as clicking buttons in the UI.\\n* Some users can still do manual changes to repositories they own, these changes are undone by the automation.\\n","tokens":128,"id":3554,"Prediction":"## Decision\n\nTo address the issue of restricted GitHub organization management and lack of review mechanisms, we will implement a GitOps-based solution that allows individual contributors to propose and manage changes to GitHub organizations, repositories, organization memberships, and labels in a transparent and auditable manner.\n\n## Context\n\nThe current process of managing GitHub organizations is restricted to a few individuals, which limits transparency, accountability, and the ability for individual contributors to propose and influence changes. The lack of review mechanisms also increases the risk of errors or unauthorized changes. By adopting a GitOps approach, we can bring transparency and accountability to the management of GitHub organizations, enabling individual contributors to propose and manage changes in a declarative and auditable manner.\n\n## Problem Statement\n\nThe current process of managing GitHub organizations is:\n\n* Restricted to a few individuals, limiting transparency and accountability\n* Lacks review mechanisms, increasing the risk of errors or unauthorized changes\n* Does not provide a way for individual contributors to propose and manage changes\n\n## Decision Drivers\n\n* Improve transparency by making changes to GitHub organizations visible and auditable\n* Provide auditing and reviews to ensure accountability and minimize errors\n* Allow individual contributors to propose changes to GitHub organizations, repositories, organization memberships, and labels\n\n## Decision Criteria\n\n* Scalability: The solution should be able to handle a large number of GitHub organizations and changes\n* Security: The solution should ensure the security and integrity of GitHub organizations and changes\n* Transparency: The solution should provide a clear and transparent record of changes to GitHub organizations\n* Auditability: The solution should provide a clear and auditable record of changes to GitHub organizations\n* Reviewability: The solution should provide a mechanism for reviewing and approving changes to GitHub organizations\n\n## Decision Outcome\n\nWe will implement a GitOps-based solution that allows individual contributors to propose and manage changes to GitHub organizations, repositories, organization memberships, and labels in a transparent and auditable manner. This solution will provide a clear and transparent record of changes, ensure the security and integrity of GitHub organizations, and provide a mechanism for reviewing and approving changes.","GenTime":"2024-08-10 17:11:58"}
{"File Name":"adr-tools\/0005-help-comments.md","Context":"## Context\\nThe tool will have a `help` subcommand to provide documentation\\nfor users.\\nIt's nice to have usage documentation in the script files\\nthemselves, in comments.  When reading the code, that's the first\\nplace to look for information about how to run a script.\\n","Decision":"Write usage documentation in comments in the source file.\\nDistinguish between documentation comments and normal comments.\\nDocumentation comments have two hash characters at the start of\\nthe line.\\nThe `adr help` command can parse comments out from the script\\nusing the standard Unix tools `grep` and `cut`.\\n","tokens":64,"id":3566,"Prediction":"## Decision\n\n**Decision:** We will use a combination of docstrings and a separate documentation file to provide documentation for users.\n\n**Context:** The tool will have a `help` subcommand to provide documentation for users. It's nice to have usage documentation in the script files themselves, in comments. When reading the code, that's the first place to look for information about how to run a script.\n\n**Problem:** We need to provide documentation for users, and we want to make it easily accessible and maintainable.\n\n**Reasoning:**\n\n* Using docstrings in the script files themselves will provide immediate access to documentation for users who are reading the code. This will make it easy for them to understand how to run the script.\n* A separate documentation file will provide a centralized location for documentation, making it easier to maintain and update. This will also allow us to include additional information, such as usage examples and troubleshooting tips, that may not be relevant to the code itself.\n\n**Consequences:**\n\n* The code will be more readable and maintainable, with clear documentation provided in the script files themselves.\n* The separate documentation file will provide a centralized location for documentation, making it easier to update and maintain.\n* The documentation will be easily accessible to users, both in the code and in a separate file.\n\n**Alternatives Considered:**\n\n* Only using a separate documentation file, without including docstrings in the script files. This would make the code less readable and require users to look elsewhere for documentation.\n* Only using docstrings in the script files, without a separate documentation file. This would make it difficult to maintain and update the documentation, and may not provide enough information for users.\n\n**Status:** Accepted\n\n**Implementation:** We will use Python's built-in docstring support to include documentation in the script files themselves. We will also create a separate documentation file, using a tool such as Sphinx, to provide a centralized location for documentation.","GenTime":"2024-08-10 17:16:23"}
{"File Name":"adr-tools\/0004-markdown-format.md","Context":"## Context\\nThe decision records must be stored in a plain text format:\\n* This works well with version control systems.\\n* It allows the tool to modify the status of records and insert\\nhyperlinks when one decision supercedes another.\\n* Decisions can be read in the terminal, IDE, version control\\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\n","Decision":"browser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n","tokens":114,"id":3568,"Prediction":"## Decision\n\n**Decision:** Use Markdown format for storing Architectural Decision Records.\n\n**Context:** The decision records must be stored in a plain text format, allowing for version control, tool modification, and readability in various environments. Additionally, users will want to use formatting such as lists, code examples, and hyperlinks to make the records more readable and printable.\n\n**Rationale:**\n\n1. Markdown is a lightweight and easy-to-read format that can be converted to HTML for viewing in a more formatted way.\n2. Markdown is widely supported and can be read in various environments, including terminals, IDEs, and version control browsers.\n3. Markdown allows for the use of formatting such as headings, bold text, lists, and code blocks, making it easy to create readable and visually appealing decision records.\n4. Markdown can be easily converted to other formats, such as PDF, making it suitable for printing and sharing.\n\n**Consequences:**\n\n1. Decision records will be easy to read and understand, even in plain text format.\n2. Users will be able to easily create and edit decision records using Markdown syntax.\n3. Decision records can be easily converted to other formats, such as HTML or PDF, for sharing and printing.\n4. The use of Markdown will make it easy to add hyperlinks and code examples to decision records, making them more informative and useful.\n\n**Alternatives:**\n\n1. Using a proprietary format, which would limit the flexibility and readability of decision records.\n2. Using a rich text format, which would be more difficult to read and edit in plain text environments.\n\n**Status:** Approved\n\n**Implementation:** The team will use Markdown format for storing Architectural Decision Records, and will provide tools and guidelines for creating and editing decision records in Markdown.","GenTime":"2024-08-10 17:16:59"}
{"File Name":"adr-tools\/0009-help-scripts.md","Context":"## Context\\nCurrently help text is generated by extracting specially formatted comments from the top of the command script.\\nThis makes it easy for developers of the tool:  documentation and code is all in one place.\\nBut, it means that help text cannot include calculated values, such as the location of files.\\n","Decision":"Where necessary, help text can be generated by a script.\\nThe script will be called _adr_help_<command>_<subcommand>\\n","tokens":63,"id":3571,"Prediction":"## Decision\n\n**Decide to separate help text generation from the command script**\n\n## Context\n\nCurrently, help text is generated by extracting specially formatted comments from the top of the command script. This makes it easy for developers of the tool, as documentation and code are all in one place. However, it means that help text cannot include calculated values, such as the location of files.\n\n## Problem\n\nThe current approach limits the ability to include dynamic information in the help text, which can make it less informative and less useful for users.\n\n## Decision\n\nTo address this limitation, we will separate the help text generation from the command script. Instead of extracting comments from the script, we will create a separate help text file that can be easily updated and maintained.\n\n## Rationale\n\nThis decision will allow us to include calculated values and dynamic information in the help text, making it more informative and useful for users. It will also decouple the help text from the command script, making it easier to maintain and update.\n\n## Consequences\n\n* We will need to create a new help text file and update the existing code to use this new file.\n* The help text file will need to be maintained separately from the command script.\n* This change will require additional development effort to implement.\n\n## Alternatives Considered\n\n* We considered keeping the current approach and finding alternative ways to include calculated values in the help text, but this would likely be more complex and difficult to maintain.\n* We also considered using a templating engine to generate the help text, but this would add additional complexity and overhead.\n\n## Status\n\nThis decision is approved and will be implemented in the next release.","GenTime":"2024-08-10 17:18:03"}
{"File Name":"konfetti\/0004-use-lazy-loading.md","Context":"## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n","Decision":"We will use a lazy evaluation approach, similar to [implemented in Django](https:\/\/github.com\/django\/django\/blob\/master\/django\/conf\/__init__.py#L42)\\n","tokens":37,"id":3572,"Prediction":"## Decision\n\nTo avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`, we will implement a strategy of lazy loading and configuration caching.\n\n## Context\n\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\n\n## Problem\n\nThe current configuration loading approach is causing side effects and making it necessary to fully configure the settings to run a subset of tests. This is leading to increased complexity and maintenance overhead.\n\n## Decision\n\nWe will implement a strategy of lazy loading and configuration caching to address the problem. This will allow us to load only the necessary configuration settings for the tests being run, without affecting the global configuration. This will also enable us to cache the loaded configuration settings, reducing the need for repeated loading and improving performance.\n\n## Rationale\n\nLazy loading will allow us to load the configuration settings only when they are actually needed, reducing the overhead of loading unnecessary settings. Configuration caching will enable us to store the loaded configuration settings and reuse them when the same settings are needed again, reducing the need for repeated loading.\n\n## Consequences\n\n* Improved performance: By loading only the necessary configuration settings and caching the loaded settings, we can reduce the overhead of loading unnecessary settings and improve performance.\n* Reduced complexity: By avoiding the need to fully configure the settings to run a subset of tests, we can reduce the complexity and maintenance overhead of the configuration.\n* Improved testability: By allowing us to load only the necessary configuration settings for the tests being run, we can improve the testability of the system.\n\n## Alternatives Considered\n\n* Implementing a separate configuration for each test: This would require significant changes to the existing configuration system and would likely lead to increased complexity and maintenance overhead.\n* Loading all configuration settings for each test: This would lead to increased overhead and unnecessary loading of configuration settings, making it less efficient.\n\n## Status\n\nApproved\n\n## Links\n\n* Related issues: #123, #456\n* Related PR: #789","GenTime":"2024-08-10 17:18:25"}
{"File Name":"court-case-service\/0005-guiding-principles.md","Context":"## Context\\nThe implementation of [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) raised questions about what is the appropriate approach for serving new data to the prepare-a-case app, and in particular stressed the need for error handling by `prepare-a-case` where graceful degradation from partial failures is needed. The question was raised as to whether `court-case-service` should be coupled closely with `prepare-a-case` and is notionally a dedicated gateway for it or whether it should be treated as a general purpose API which may be consumed by multiple consumers.\\n","Decision":"1. All data stored by `court-case-service` is considered its own domain and should be treated as a general purpose api\\n2. All endpoints composing data on behalf of `prepare-a-case` are not intended for consumption by other clients and should provide all data needed on a given `prepare-a-case` page as efficiently as possible\\n3. The exception to point 2 is the case where errors in the retrieval of certain pieces of data are expected and need to be isolated. In this case data to be isolated should be served as a separate endpoint which will fail fast. `prepare-a-case` can then handle it simply as a failed HTTP call. See [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) for an example of this.\\n","tokens":129,"id":3579,"Prediction":"## Decision\n\n**Decision:** Implement a general-purpose API for the court-case-service, allowing it to be consumed by multiple consumers, including prepare-a-case.\n\n**Context:** The implementation of PIC-1135 raised questions about the approach for serving new data to the prepare-a-case app, specifically the need for error handling and graceful degradation from partial failures. This decision aims to address these concerns.\n\n**Problem:** Coupling the court-case-service too closely with prepare-a-case would limit its flexibility and make it difficult to adapt to changing requirements or integrate with other services. Additionally, a dedicated gateway for prepare-a-case would add complexity and create a single point of failure.\n\n**Reasoning:** By treating the court-case-service as a general-purpose API, we can:\n\n1. Increase flexibility: The service can be easily integrated with other consumers, reducing the risk of vendor lock-in.\n2. Improve fault tolerance: If one consumer experiences issues, the service can continue to operate and serve other consumers.\n3. Simplify maintenance: The service can be maintained and updated independently of prepare-a-case, reducing the risk of cascading failures.\n\n**Consequences:**\n\n* The court-case-service will need to implement robust error handling and graceful degradation mechanisms to ensure that partial failures do not affect other consumers.\n* Prepare-a-case will need to implement retry mechanisms and handle errors gracefully when consuming the court-case-service.\n* The service will require additional documentation and testing to ensure that it is properly consumed and integrated with other services.\n\n**Alternatives Considered:**\n\n* Implementing a dedicated gateway for prepare-a-case, which would add complexity and create a single point of failure.\n* Coupling the court-case-service too closely with prepare-a-case, which would limit flexibility and make it difficult to adapt to changing requirements.\n\n**Decision History:**\n\n* This decision was discussed and agreed upon during the PIC-1135 implementation meeting on [Date].\n* The decision was reviewed and confirmed by the architecture team on [Date].","GenTime":"2024-08-10 17:20:45"}
{"File Name":"api-catalogue\/0001-remove-tech-docs-gem-dependency.md","Context":"## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https:\/\/github.com\/alphagov\/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n","Decision":"Remove the TDG as a dependency by vendoring the code relevant to\\nthe API catalogue directly into the project itself.\\n","tokens":228,"id":3586,"Prediction":"## Decision\n\nBased on the provided context, I propose the following decision:\n\n**Decision:** Migrate away from the Tech Docs Gem (TDG) and use a custom solution for rendering the API catalogue website.\n\n**Rationale:**\n\n1. The TDG is not a good fit for the API catalogue project, which has a non-documentation data source (CSV) and requires customization that is difficult to achieve with the gem.\n2. The TDG's dependency version restrictions limit the team's ability to control version upgrades, which is necessary to remain on supported versions and receive security patches.\n3. The TDG's Table of Contents rendering is slow and adds over a minute to build times, which is unacceptable for the project.\n4. A custom solution would provide more flexibility and control over the rendering process, allowing the team to tailor the layout and styling to the specific needs of the API catalogue.\n\n**Consequences:**\n\n1. The team will need to develop a custom solution for rendering the API catalogue website, which may require additional resources and time.\n2. The team will need to handle the rendering of the Table of Contents and other features that were previously handled by the TDG.\n3. The team will need to ensure that the custom solution is properly tested and maintained to ensure the stability and security of the website.\n\n**Alternatives Considered:**\n\n1. Continue using the TDG and try to work around its limitations.\n2. Use a different static site generator that is more flexible and customizable.\n3. Use a templating engine to render the API catalogue website, but this would likely require more development effort.\n\n**Monitoring and Feedback:**\n\nThe effectiveness of this decision will be monitored through metrics such as build time, website performance, and user feedback. The team will also conduct regular reviews to ensure that the custom solution is meeting the needs of the API catalogue and to identify areas for improvement.","GenTime":"2024-08-10 17:22:46"}
{"File Name":"libmemory\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3590,"Prediction":"Here is an Architectural Decision Record (ADR) for the software project:\n\n**Decision**\n\n**Title:** Use a Microservices Architecture for the New Project\n\n**Status:** Accepted\n\n**Context:** As we begin this new project, we need to decide on an architecture that will allow us to build a scalable, maintainable, and flexible system. The project requires integrating multiple services and systems, and we need to ensure that each component can be developed, tested, and deployed independently.\n\n**Problem:** Our previous projects have shown that monolithic architectures can lead to tight coupling, making it difficult to maintain and scale the system. We need an architecture that allows for loose coupling, scalability, and flexibility.\n\n**Decision:** We will use a Microservices Architecture (MSA) for the new project. This will allow us to break down the system into smaller, independent services that can be developed, tested, and deployed separately. Each service will be responsible for a specific business capability, and they will communicate with each other using APIs or message queues.\n\n**Rationale:** MSA provides several benefits, including:\n\n* Loose coupling between services, making it easier to maintain and update individual services without affecting the entire system.\n* Scalability, as each service can be scaled independently based on its specific needs.\n* Flexibility, as new services can be added or removed without affecting the overall system.\n* Improved fault tolerance, as a failure in one service will not bring down the entire system.\n\n**Consequences:**\n\n* Increased complexity, as we will need to manage multiple services and their interactions.\n* Higher operational costs, as we will need to manage multiple services and their infrastructure.\n* Potential for increased latency, as services may need to communicate with each other over a network.\n\n**Alternatives Considered:** We also considered using a Monolithic Architecture, but this would likely lead to the same issues we experienced in our previous projects. We also considered using a Service-Oriented Architecture (SOA), but this would require a significant investment in infrastructure and governance.\n\n**Implementation:** We will start by identifying the individual services that make up the system, and then design and implement each service using a suitable programming language and framework. We will use APIs or message queues to enable communication between services. We will also establish a governance model to ensure that services are developed and maintained in a consistent manner.\n\n**Reevaluation:** We will reevaluate this decision in 6 months to assess its effectiveness and identify any areas for improvement.\n\n---\n\nI hope this helps! Let me know if you have any questions or if you'd like me to add anything else.","GenTime":"2024-08-10 17:24:16"}
{"File Name":"axiomatic\/0001-enabling-configuration-repo-access.md","Context":"## Context and Problem Statement\\nHow should Axiomatic ensure that dir2consul, when run as a Nomad batch job, has access to configuration repos hosted on GitHub?\\n## Decision Drivers\\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\n","Decision":"* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\nChosen option: Option 2, Axiomatic Retrieves Deployment Keys\\n### Positive Consequences\\n* Allows us to provide a repeatable solution for configurationn repos in the short term\\n* We will default to using read-only ssh access\\n### Negative Consequences\\n* Requires codifying a key management process and a potential automation effort\\n","tokens":161,"id":3595,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Axiomatic will use Option 1: Store the SSH key pair in the Nomad job configuration, and use the private key to clone the configuration repos hosted on GitHub.\n\n**Context and Problem Statement:** How should Axiomatic ensure that dir2consul, when run as a Nomad batch job, has access to configuration repos hosted on GitHub?\n\n**Decision Drivers:**\n\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\n* There is no appetite for spending money on Service User accounts.\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\n* Option 2 setup is an infrequent activity as would be key rotation\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\n* Option 4 was eliminated due to possible violation of our acceptable use policies\n\n**Rationale:** Option 1 is chosen because it is the simplest and most straightforward solution, requiring minimal setup and maintenance. By storing the SSH key pair in the Nomad job configuration, we can ensure that the job has access to the necessary configuration repos without having to create a separate Service User account or use a third-party solution like Vault.\n\n**Consequences:**\n\n* The SSH key pair will be stored in plain text in the Nomad job configuration, which may be a security risk if the Nomad job is compromised.\n* The private key will need to be rotated regularly to maintain security.\n* This solution may not be scalable if multiple Nomad jobs need to access the same configuration repos.\n\n**Status:** Approved\n\n**Date:** [Insert Date]\n\n**Responsible:** [Insert Name]\n\n**Related Decisions:** [Insert Related Decisions]","GenTime":"2024-08-10 17:26:10"}
{"File Name":"alfresco-anaxes-shipyard\/0002-docker-registry-for-internal-and-protected-images.md","Context":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n","Decision":"We will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).\\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\\nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.\\nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\\n","tokens":130,"id":3598,"Prediction":"## Decision\n\n**Decision:** Standardize on Google Container Registry (GCR) as the Docker Image Registry for Alfresco Digital Business Platform.\n\n**Context:** As part of our efforts around containerized deployment of the Alfresco Digital Business Platform, we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images. We can describe those as 'internal' and 'protected' tiers. The current Nexus3 implementation does not meet our requirements around access control, security scanning, scalability, global performance, usability, or maintainability.\n\n**Problem:** The current Nexus3 implementation is not scalable, has poor global performance, and lacks essential features for access control and security scanning. Additionally, our IT resources are already stretched thin and we need to avoid adding another system for them to deploy and maintain.\n\n**Reasoning:** Google Container Registry (GCR) meets our requirements for access control, security scanning, scalability, global performance, usability, and maintainability. GCR is a highly available and scalable registry that provides robust access control and security scanning features. It also integrates well with other Google Cloud services, making it a good choice for our containerized deployment. Furthermore, GCR is a managed service, which means that Google is responsible for deploying and maintaining it, reducing the burden on our IT resources.\n\n**Consequences:**\n\n* All Alfresco engineering teams, internal groups, customers, and partners will use GCR as the standard Docker Image Registry.\n* GCR will provide robust access control and security scanning features, ensuring the security and integrity of our Docker images.\n* Our IT resources will not be burdened with deploying and maintaining an additional system.\n* We will be able to take advantage of GCR's scalability and global performance, ensuring a smooth and reliable containerized deployment.\n\n**Alternatives Considered:** We considered alternative Docker Image Registries such as Docker Hub, Azure Container Registry, and Amazon Elastic Container Registry. However, GCR met our requirements and provided the best overall solution.","GenTime":"2024-08-10 17:26:53"}
{"File Name":"alfresco-anaxes-shipyard\/0004-k8s-cluster-creation.md","Context":"## Context\\nWe need to recommend a tool to those customers that do not have an existing Kubernetes cluster.\\nA tool called minikube can be used on laptops for development and evaluation purposes. For a production ready stack we need to provide a way for customers to provision a cluster in AWS using a mechanism a less technically literate person can follow.\\nA list of candidate solutions can be found [here](https:\/\/github.com\/kubernetes\/community\/blob\/master\/sig-aws\/kubernetes-on-aws.md).\\n","Decision":"We will be informing customers that we use kops internally and it's the cluster upon which we perform all our testing.\\nWe have chosen kops as it's the tool the most closely fulfils our current and future requirements, it\u2019s produced by the same team that build Kubernetes and it\u2019s been [endorsed](https:\/\/aws.amazon.com\/blogs\/compute\/kubernetes-clusters-aws-kops) by AWS.\\n","tokens":101,"id":3600,"Prediction":"## Decision\n\nTo recommend a tool to customers who do not have an existing Kubernetes cluster, we will use a combination of minikube for development and evaluation purposes and AWS CloudFormation for provisioning a production-ready cluster in AWS.\n\n## Context\n\nAs discussed, we need to provide a way for customers to provision a cluster in AWS using a mechanism that a less technically literate person can follow. We have identified minikube as a suitable tool for development and evaluation purposes, but we also need a solution for provisioning a production-ready cluster.\n\n## Problem\n\nWe need to recommend a tool that allows customers to easily provision a Kubernetes cluster in AWS, without requiring extensive technical expertise.\n\n## Decision Criteria\n\n1. Ease of use: The tool should be easy to use, even for those with limited technical expertise.\n2. Scalability: The tool should be able to handle clusters of varying sizes.\n3. Security: The tool should provide a secure way to provision and manage clusters.\n4. Cost-effectiveness: The tool should be cost-effective and provide a good balance between cost and functionality.\n\n## Decision\n\nBased on the decision criteria, we will use AWS CloudFormation to provision a production-ready cluster in AWS. CloudFormation is a service that allows users to use templates to define and deploy infrastructure as code. This approach provides several benefits, including:\n\n* Ease of use: CloudFormation provides a visual interface for creating and managing templates, making it easy for users to define and deploy infrastructure.\n* Scalability: CloudFormation can handle clusters of varying sizes, and can be used to provision and manage multiple clusters.\n* Security: CloudFormation provides a secure way to provision and manage clusters, using features such as IAM roles and access controls.\n* Cost-effectiveness: CloudFormation provides a cost-effective way to provision and manage clusters, by allowing users to define and deploy infrastructure as code.\n\n## Consequences\n\nBy using AWS CloudFormation to provision a production-ready cluster in AWS, we will provide customers with a easy-to-use and cost-effective solution for deploying a Kubernetes cluster. This will help to improve customer satisfaction and reduce the technical burden on our support team.\n\n## Rationale\n\nWe chose AWS CloudFormation because it provides a secure and cost-effective way to provision and manage clusters, and is easy to use even for those with limited technical expertise. Additionally, CloudFormation integrates well with other AWS services, such as Kubernetes, making it a suitable choice for provisioning a production-ready cluster.","GenTime":"2024-08-10 17:27:40"}
{"File Name":"alfresco-anaxes-shipyard\/0006-helm-chart-repository.md","Context":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to decide on a Helm Chart Repository that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Helm Charts. When we looked for a solution, we considered that this is going to be a public-facing customer repository. We also took into consideration the need for a custom domain for it.\\nThe criterias we looked for when we investigated each option are: if it uses AWS and Quay, if it is a PaaS\/Cloud Storage solution, if the project is mature enough for our use case, if it is a personal project or if it is maintained by a company and if it requires IT support (if it does can it be avoided by altering the pipeline).  We considered that authentication and SSL is not needed.\\nWe had several options that we considered. The full list of pros and cons for each option can be found [here](https:\/\/issues.alfresco.com\/jira\/secure\/attachment\/97743\/DEPLOY-150%20Helm%20Chart%20Repos.xlsx). More comments on the investigation we have done can be found [here](https:\/\/issues.alfresco.com\/jira\/browse\/DEPLOY-150).\\n","Decision":"We will use Github Pages to store the Helm Charts. The reasons why we elected this solution are: it is a cloud storage solution, it is mature project that has been actively mantained, doesn't require IT support and it offers an easy, well-known pipeline.\\n","tokens":266,"id":3602,"Prediction":"## Decision\n\nAfter careful consideration of the options, we have decided to use AWS CodeArtifact as the Helm Chart Repository for the Alfresco Digital Business Platform. The decision is based on the following reasons:\n\n* AWS CodeArtifact is a fully managed service that supports Helm Chart repositories, making it a suitable choice for a public-facing customer repository.\n* It uses AWS and Quay, which aligns with our existing infrastructure and reduces the complexity of managing multiple services.\n* AWS CodeArtifact is a PaaS\/Cloud Storage solution, which eliminates the need for us to manage underlying infrastructure.\n* The project is mature enough for our use case, with a large user base and a growing ecosystem of Helm Chart repositories.\n* It is a company-maintained project, which means that it receives regular updates and support from AWS.\n* IT support is not required, as AWS CodeArtifact provides a managed service that can be easily integrated into our pipeline.\n\nWe believe that AWS CodeArtifact meets our requirements for a Helm Chart Repository, and we are confident that it will provide a reliable and scalable solution for our use case.","GenTime":"2024-08-10 17:27:52"}
{"File Name":"alfresco-anaxes-shipyard\/0008-applying-extensions-to-docker-images.md","Context":"## Context\\nOne of the main advantages of Docker images is their immutability. This means once an image has been tested and verified there is a high confidence it will work as intended in other environments as it doesn't change.\\nAlfresco allows the core product to the enhanced via external modules in the form of [AMPs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-amps.html) or [simple JARs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-jar-files.html).\\nThis results in two big problems, how do we release containers with every combination of AMP available and how do customers apply their own extensions?\\nWe have three options; apply extensions at build time (thus retaining the immutability advantage), apply extensions at runtime using a mechanism that doesn't change the contents of the container or apply extensions as the container initializes (breaking immutability).\\nApplying extensions at build time means we will be forcing customers to build their own images depending on which official and custom extensions they require.\\nHowever, there are a number of disadvantages of applying extensions at runtime; The immutability advantage is lost (the WAR file is changed), an extension could fail to apply and prevent the container from starting, it has the potential to introduce performance issues as the AMP or JAR has to be fetched and applied and finally it opens a potential security hole as code can be applied to a container via a simple environment variable.\\nWe investigated potential approaches to keep the main container immutable by using volumes and initContainers but this increases the complexity of the solution and doesn't resolve all the issues outlined above.\\n","Decision":"We will proceed with the build time option as there are a number of disadvantages to applying extensions at runtime, customers are already used to applying their own extensions and they're having to learn a new deployment mechanism anyway.\\nWe will release a small number of images with and without common AMPs applied.\\n","tokens":351,"id":3604,"Prediction":"## Decision\n\nGiven the context of using Docker images for Alfresco, which requires applying external modules (AMPs and JARs) to the core product, we have three options to consider:\n\n1. Apply extensions at build time\n2. Apply extensions at runtime using a mechanism that doesn't change the contents of the container\n3. Apply extensions as the container initializes\n\nAfter careful evaluation, we decide to:\n\n**Apply extensions at runtime using a mechanism that doesn't change the contents of the container**\n\nWe choose this option because it allows us to maintain the immutability advantage of Docker images, while still enabling customers to apply their own extensions. This approach also avoids the potential security risks and performance issues associated with applying extensions at build time or as the container initializes.\n\nTo achieve this, we will investigate and implement a mechanism that allows extensions to be applied to the container without modifying its contents. This may involve using a separate layer for extensions, or utilizing a container orchestration tool that supports extension management.\n\nBy choosing this option, we can strike a balance between immutability, flexibility, and security, ultimately providing a more robust and maintainable solution for our customers.","GenTime":"2024-08-10 17:28:27"}
{"File Name":"cloud-sdk-js\/0021-odata-url-builder.md","Context":"## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET \/People('scottketchum')\/Friends`\\n- getting \"raw value\" of a property `\/People('scottketchum')\/$value`\\n","Decision":"- Implement A for now as a powerful workaround.\\n- Proposal B\/C\/variant will be a `2.0` task, where it seems C might be the winner and we might review the decision later as they close to each other.\\nAt least, implement it as a separate task so we have a workaround for custom URL.\\n### Proposal B\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends, 'scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend) \/\/ single item can continue linking\\n.navigationProp(People.BestFriend); \/\/ single item can continue linking\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Better fluent API (compared to `asChildOf`) with builder pattern.\\n- Can be extended for supporting problem 5-7.\\n- Typed.\\n##### Cons:\\n- Lots of effort to build the new structure, which seems to be a `2.0` task.\\n### Proposal C\\nBasically, the same idea but with different API in terms of reaching single items.(e.g., \"getByKey\" and 1-to-1 navigation properties)\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends) \/\/ multi item can call \"key\" to become a single item\\n.key('scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n### Proposal C variants\\n```\\n\/\/frank\\nPeople.requestBuilder()\\n.getByKey('key') \/\/ xxxRequestBuilder\\n.toFriend('abc')\\n.toBestFriend()\\n.toFriends()\\n.getBuilder()\/\/create\\n\/\/marika\\nPeople.requestBuilder()\\n.key('scottketchum')\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination)\\n```\\n#### Pros and cons:\\nSame as `Proposal B`, but with more methods instead of overloading functions with more parameters.\\n### Proposal D\\nUse the similar API like `asChildOf`\\n```ts\\n\/\/ \/People(personKey)\/Friends\\nFriends.requestBuilder().getAll().asChildOf(person, People.Friends);\\n```\\n```ts\\n\/\/ \/People(personKey)\/Friends(friendKey)\\nFriends.requestBuilder().getByKey(friendKey).asChildOf(person, People.Friends);\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Consistent with `asChildOf` for generating same URL.\\n- Medium complexity\\n- Typed.\\n##### Cons:\\n- Cannot be extended for supporting problem 5-7, so we need to find solution for them.\\n- The ugly API `asChildOf` is used with additional use cases.\\n- Different order: `Entity -> NavigationProp` (url) V.S. `NavigationProp -> Entity` (API usage)\\n#### Decision:\\nNot chosen due to the lack of extension and confusing API.\\n### previous docs\\nFind related discussion [here](..\/implementation-documentation\/api-improvements.md)\\n","tokens":61,"id":3630,"Prediction":"## Decision\n\n**Decision:** Introduce a new OData Request Builder API that can handle complex OData requests, including query navigation properties and getting \"raw value\" of a property.\n\n## Context\n\nThe current request builder APIs are not able to handle some OData requests, specifically:\n\n* Query navigation properties (e.g. `GET \/People('scottketchum')\/Friends`)\n* Getting \"raw value\" of a property (e.g. `\/People('scottketchum')\/$value`)\n\n## Problem\n\nThe current request builder APIs are not flexible enough to handle these types of requests, which are common in OData scenarios. This limitation is causing issues for users who need to make these types of requests.\n\n## Rationale\n\nIntroducing a new OData Request Builder API that can handle complex OData requests will provide a more comprehensive solution for users. This API will be able to handle a wider range of OData requests, including query navigation properties and getting \"raw value\" of a property. This will improve the usability and flexibility of the request builder APIs.\n\n## Consequences\n\n* The new OData Request Builder API will require additional development and testing to ensure it meets the requirements.\n* The new API will need to be documented and supported, which will add to the overall maintenance burden.\n* The new API will provide a more comprehensive solution for users, which will improve the overall user experience.\n\n## Alternative\n\nAn alternative solution could be to modify the existing request builder APIs to handle these types of requests. However, this would likely require significant changes to the underlying architecture and could potentially introduce unintended consequences. Introducing a new API allows for a more targeted solution that can be designed and implemented with the specific requirements in mind.\n\n## Status\n\nThis decision is approved and will be implemented in the next release.","GenTime":"2024-08-10 17:36:42"}
{"File Name":"adrflow\/5-Utility_Context_Lazily_Initialized.md","Context":"# 5 Utility Context Lazily Initialized\\n## Context\\nThe [synchronous utils](..\/..\/src\/adr_util_sync.js) are intended to be used by commands to work on ADRs, with the given context of the project, e.g. the [adr marker file](1-ADR_directory_identified_by_marker_file.md).\\nSince the context is mandatory (the ADR directory, existing ADRs), it is initialized at the beginning, in the object's constructor.\\nSeveral commands (scripts) already include that script, and initialize the object on the script loading, as part of the `require` statement.\\nThis resulted in loading the context whenever a command is loaded.\\nThis might have performance implications, but the more immediate functional lacuna is that this then assumes that the context *can* be loaded every time.\\nOf course, this assumption breaks - when running the `init` command, where by definition there is no context to load - the `init` command creates that.\\nThis resulted in trying to run the `init` command, on an existing directory, in an error.\\n","Decision":"The ADR project context will be loaded lazily, only when needed.\\nThis assumes the `init` command doesn't require it, and hence will not invoke any commands that require that context (e.g. the existing files).\\nSo any command that is loaded can still create the context object and create an instance of, but the actual context will be loaded easily.\\nThis assumption seems safe enough since `init` command indeed creates the context (the `.adr` file).\\nAnd this will probably hold for similar commands.\\nCommands that will require the ADR context, will continue to use it as today, and it will be loaded lazily.\\nWe need of course to make sure that the context isn't created\/calculated every time it is called; e.g. searching for the ADR dir every time one of the utility functions is called. In other words - memoize the context-creating functions.\\n","tokens":227,"id":3635,"Prediction":"## Decision\n\n### Decision: Lazy Initialization of Context\n\n### Context\n\nThe synchronous utilities are intended to be used by commands to work on ADRs, with the given context of the project, e.g. the ADR marker file. Since the context is mandatory (the ADR directory, existing ADRs), it is initialized at the beginning, in the object's constructor. Several commands (scripts) already include that script, and initialize the object on the script loading, as part of the `require` statement. This resulted in loading the context whenever a command is loaded. This might have performance implications, but the more immediate functional lacuna is that this then assumes that the context *can* be loaded every time. Of course, this assumption breaks - when running the `init` command, where by definition there is no context to load - the `init` command creates that. This resulted in trying to run the `init` command, on an existing directory, in an error.\n\n### Problem\n\nThe current implementation of loading the context whenever a command is loaded is not suitable for the `init` command, which creates the context. This results in an error when trying to run the `init` command on an existing directory.\n\n### Solution\n\nLazy initialization of the context will solve this problem. The context will only be initialized when it is actually needed, which is when the command is executed. This will ensure that the context is only created when it is necessary, and will not interfere with the `init` command.\n\n### Rationale\n\nLazy initialization is a design pattern that defers the initialization of an object until it is actually needed. This approach is suitable for our use case because it ensures that the context is only created when it is necessary, and will not interfere with the `init` command.\n\n### Consequences\n\n* The context will only be loaded when it is actually needed, which will improve performance.\n* The `init` command will no longer result in an error when trying to run it on an existing directory.\n* The design pattern of lazy initialization will make the code more flexible and easier to maintain.\n\n### Status\n\nThis decision is approved and will be implemented in the next release.","GenTime":"2024-08-10 17:38:31"}
{"File Name":"adrflow\/8-Remove_ADR_Util_Sync_Module.md","Context":"supersedes [5](5-Utility_Context_Lazily_Initialized.md)\\nmentions [6](6-Centralize_Definition_of_Filename.md)\\n## Context\\nThe ADR Util module grew as an attempt to centralize a list of utility functions that require the context of the ADR file and directory.\\nThe resulting module was very central to the logic of the entire program, but resulted in a module that had too much differing concerns (too many responsibilities\/reasons to change).\\nThis has eventually resulted in tangled code, and sometimes to (unintentional) circular dependencies, e.g. as when trying to isolate the handling of links.\\nAt the same time, the usage of the shared context - ADR files and directory - hasn't proved that useful in most case.\\nIn the cases where it should be used, it can be defined and cached.\\n","Decision":"Divide the utilities defined in this module to other, more focused modules (under `core` directory):\\n1. `files.js`: handles all file reading and writing\\n2. `adrobj.js`: handles all ADR logical-level operations. Essentially linking the ID to the file.\\n3. `links.js`: handles logic around linking ADRs\\nIn cases where caching the ADR files is needed, e.g. in `adrobj.js`, it is done in that module.\\n","tokens":180,"id":3637,"Prediction":"## Decision\n\nThe Util module's responsibility will be split into smaller, more focused modules, each with a single responsibility. The Util module will be refactored to only contain utility functions that are truly reusable across the entire program, and not specific to the ADR file and directory context.\n\nThis decision is made to address the issue of the Util module having too many responsibilities, leading to tangled code and unintentional circular dependencies. By breaking down the module into smaller, more focused modules, we can improve the maintainability and scalability of the codebase.\n\n## Context\n\nThe Util module grew as an attempt to centralize a list of utility functions that require the context of the ADR file and directory. The resulting module was very central to the logic of the entire program, but resulted in a module that had too many differing concerns (too many responsibilities\/reasons to change). This has eventually resulted in tangled code, and sometimes to (unintentional) circular dependencies, e.g. as when trying to isolate the handling of links. At the same time, the usage of the shared context - ADR files and directory - hasn't proved that useful in most cases. In the cases where it should be used, it can be defined and cached.\n\n## Rationale\n\nThe Util module's growth was an attempt to centralize a list of utility functions, but it has become a monolithic module that is difficult to maintain and extend. By breaking it down into smaller modules, we can create a more modular and scalable codebase that is easier to understand and modify. Additionally, this decision aligns with the principles of Single Responsibility Principle (SRP) and Separation of Concerns (SoC), which promote modular and maintainable code.\n\n## Decision Drivers\n\n* Improve maintainability and scalability of the codebase\n* Reduce complexity and tangled code\n* Improve reusability of utility functions\n* Align with principles of Single Responsibility Principle (SRP) and Separation of Concerns (SoC)\n\n## Pros and Cons\n\nPros:\n\n* Improved maintainability and scalability of the codebase\n* Reduced complexity and tangled code\n* Improved reusability of utility functions\n\nCons:\n\n* Increased number of modules to manage\n* Potential for increased coupling between modules\n* Requires careful consideration of dependencies and interfaces between modules\n\n## Conclusion\n\nBy splitting the Util module into smaller, more focused modules, we can improve the maintainability and scalability of the codebase, reduce complexity and tangled code, and improve the reusability of utility functions. This decision aligns with the principles of Single Responsibility Principle (SRP) and Separation of Concerns (SoC), and will help to create a more modular and maintainable codebase.","GenTime":"2024-08-10 17:39:20"}
{"File Name":"corona-hackathon\/0010-branching-strategy.md","Context":"## Context\\nWe have to make a decision on the branching strategy for development.\\n","Decision":"[Git Flow](https:\/\/danielkummer.github.io\/git-flow-cheatsheet\/) it will be.\\n","tokens":17,"id":3653,"Prediction":"## Decision\n\n**Branching Strategy for Development**\n\n### Context\n\nAs we have to make a decision on the branching strategy for development, we need to consider the pros and cons of different strategies and choose the one that best fits our team's workflow and project requirements.\n\n### Decision\n\nAfter careful consideration, we have decided to adopt a Git Flow branching strategy for our development process. This strategy is a variation of the Git Flow model, which is a popular branching strategy used by many teams.\n\nIn our implementation, we will have the following branches:\n\n* `main`: The main branch that contains the production-ready code.\n* `develop`: The main development branch where new features are developed.\n* `feature\/*`: Feature branches that are created from the `develop` branch and are used for developing new features.\n* `release\/*`: Release branches that are created from the `develop` branch and are used for preparing releases.\n* `hotfix\/*`: Hotfix branches that are created from the `main` branch and are used for fixing critical issues in production.\n\n### Rationale\n\nWe chose the Git Flow branching strategy because it provides a clear separation of concerns between development and production, allows for easy tracking of changes, and enables us to maintain a stable production environment. The strategy also provides a clear process for developing, testing, and releasing new features and fixes.\n\n### Consequences\n\nBy adopting the Git Flow branching strategy, we will need to create a new branch for each feature or release, which may add some overhead to our development process. However, this strategy provides a clear and structured approach to development, which will help us to maintain a high level of quality and reduce the risk of introducing bugs into production.\n\n### Next Steps\n\nWe will implement the Git Flow branching strategy in our development process and provide training to the team on how to use it effectively. We will also monitor the effectiveness of the strategy and make adjustments as needed to ensure it continues to meet our team's needs.","GenTime":"2024-08-10 17:44:03"}
{"File Name":"corona-hackathon\/0011-postgres.md","Context":"## Context\\nWe have to make a decision on how\/where to host the DB.\\n","Decision":"We use a freely managed [Postgres on Heroku](https:\/\/www.heroku.com\/postgres)\\n","tokens":19,"id":3657,"Prediction":"## Decision\n\nAfter careful consideration of the options, we have decided to host the database on a cloud-based relational database management system (RDBMS) service, specifically Amazon Aurora.\n\n## Context\n\nWe have to make a decision on how\/where to host the DB.\n\n## Problem\n\nWe need to decide on a suitable hosting solution for our database to ensure scalability, reliability, and cost-effectiveness. We have considered several options, including self-hosting, hosting on a third-party managed service, and hosting on a cloud-based RDBMS service.\n\n## Decision\n\nWe have decided to host the database on Amazon Aurora, a cloud-based RDBMS service provided by Amazon Web Services (AWS). This decision is based on the following factors:\n\n* Scalability: Amazon Aurora provides automatic scaling, allowing us to easily increase or decrease the database instance size as needed.\n* Reliability: Amazon Aurora is designed to provide high availability and durability, with automatic failover and replication to ensure minimal downtime.\n* Cost-effectiveness: Amazon Aurora provides a cost-effective solution, with pricing based on the instance size and usage.\n* Security: Amazon Aurora provides robust security features, including encryption at rest and in transit, and compliance with major regulatory standards.\n* Integration: Amazon Aurora integrates seamlessly with other AWS services, making it easy to incorporate into our existing architecture.\n\n## Consequences\n\nBy hosting the database on Amazon Aurora, we can:\n\n* Ensure high availability and scalability to meet the demands of our growing user base.\n* Reduce the administrative burden of managing a database, allowing us to focus on developing new features and improving the overall user experience.\n* Benefit from the cost-effective pricing model and scalability features of Amazon Aurora.\n* Improve the security and compliance of our database by leveraging the robust security features and regulatory compliance of Amazon Aurora.\n\n## Rationale\n\nWe believe that hosting the database on Amazon Aurora is the best decision for our project, as it provides a scalable, reliable, and cost-effective solution that meets our needs and allows us to focus on developing new features and improving the overall user experience.","GenTime":"2024-08-10 17:45:37"}
{"File Name":"corona-hackathon\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3663,"Prediction":"## Decision\n\n**Title:** Handling Large Amounts of Data in the System\n\n**Context:** We need to record the architectural decisions made on this project.\n\n**Problem:** Our system is expected to handle large amounts of data, which poses a challenge in terms of performance, scalability, and maintainability.\n\n**Decision:** We will use a distributed database system to store and manage large amounts of data. This will allow us to scale horizontally and handle high volumes of data.\n\n**Rationale:** A distributed database system provides several benefits, including:\n\n* Scalability: We can add more nodes to the system as needed to handle increasing amounts of data.\n* High availability: If one node fails, the system can continue to function without interruption.\n* Fault tolerance: The system can continue to operate even if some nodes fail.\n* Better performance: Distributed databases can provide better performance than traditional relational databases.\n\n**Consequences:**\n\n* We will need to develop additional infrastructure to support the distributed database system.\n* We will need to implement data replication and consistency mechanisms to ensure data integrity.\n* We will need to monitor and maintain the distributed database system to ensure optimal performance.\n\n**Status:** Accepted\n\n**Implementation:** The distributed database system will be implemented using a combination of open-source and commercial tools. The system will be designed to be highly available and scalable, with multiple nodes and replication mechanisms in place to ensure data integrity.\n\n**Reevaluation:** This decision will be reevaluated in 6 months to assess its effectiveness and identify any areas for improvement.","GenTime":"2024-08-10 17:47:48"}
{"File Name":"nhsuk-prototype-kit-version-one\/0001-use-node-and-express.md","Context":"## Context\\nThe prototype kit application will need to operate similarly to a live service, depending on the level of fidelity required for testing. This means that a server is required. To make the prototype kit easy to use for different levels of coding experience we want to implement auto routing (or route matching) for template files, the simplest way to achieve this this dynamically is to have an application running on a server.\\n","Decision":"We will use Node.js with the Express framework to run a Node.js server for the prototype kit on both local development environments and production environment when published on Heroku. Node.js is written in javascript and is therefore one the most common languages between designers who have some coding experience and software engineers. It is also the easiest runtime to install on both Mac and PC.\\n","tokens":83,"id":3681,"Prediction":"## Decision\n\nTo enable the prototype kit to operate similarly to a live service and provide auto-routing for template files, we will implement a server-based solution. This will allow for dynamic routing and make the prototype kit more accessible to users with varying levels of coding experience.\n\n## Context\n\nAs you mentioned, the prototype kit will need to operate similarly to a live service to achieve the required level of fidelity for testing. Additionally, implementing auto-routing for template files will require a dynamic solution, and running an application on a server is the simplest way to achieve this.\n\n## Status\n\nAccepted\n\n## Description\n\nWe will use a server-side technology such as Node.js or Python to create a simple web server that can serve the prototype kit's template files. This will allow us to implement auto-routing for the template files using a routing library such as Express.js or Flask. The server will also provide a simple API for users to interact with the prototype kit.\n\n## Rationale\n\nImplementing a server-based solution will provide several benefits, including:\n\n* Dynamic routing: The server will be able to dynamically route requests to the correct template files, making it easier for users to access the prototype kit's features.\n* Simplified development: By using a server-side technology, we can focus on developing the prototype kit's features without worrying about the underlying infrastructure.\n* Improved scalability: The server-based solution will allow us to scale the prototype kit more easily, making it easier to handle a large number of users.\n\n## Consequences\n\n* Additional complexity: Implementing a server-based solution will add additional complexity to the prototype kit, requiring more development time and resources.\n* Dependence on infrastructure: The prototype kit will require a server to operate, which may introduce dependencies on external infrastructure.\n\n## Pros and Cons\n\nPros:\n\n* Simplified development\n* Dynamic routing\n* Improved scalability\n\nCons:\n\n* Additional complexity\n* Dependence on infrastructure","GenTime":"2024-08-10 17:54:04"}
{"File Name":"nhsuk-prototype-kit-version-one\/0003-use-npm-scripts-and-gulp-for-running-tasks.md","Context":"## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n","Decision":"We will use a mixture on NPM scripts and [Gulp](https:\/\/gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\\n","tokens":49,"id":3682,"Prediction":"## Decision\n\n**Decision:** Implement a task runner to manage the various tasks required to set up and run the prototype kit.\n\n**Context:** There are multiple tasks that need to be processed to get the prototype kit up and running, including installing dependencies, moving files, and running the application. Manually executing these tasks can be error-prone, time-consuming, and difficult to reproduce.\n\n**Problem:** Without a task runner, the setup and deployment process for the prototype kit will be complex, prone to errors, and difficult to maintain.\n\n**Reasoning:** A task runner will allow us to define and execute these tasks in a consistent and reliable manner, making it easier to set up and deploy the prototype kit. This will improve the overall development experience and reduce the risk of errors.\n\n**Decision Criteria:**\n\n* Ease of use: The task runner should be easy to use and understand.\n* Flexibility: The task runner should be able to handle a variety of tasks and dependencies.\n* Reproducibility: The task runner should ensure that the setup and deployment process is consistent and reproducible.\n* Maintainability: The task runner should be easy to maintain and update.\n\n**Alternatives Considered:**\n\n* Manual execution of tasks: This would require manually executing each task, which would be time-consuming and prone to errors.\n* Custom scripting: This would require writing custom scripts to execute each task, which would be inflexible and difficult to maintain.\n\n**Conclusion:** Implementing a task runner will simplify the setup and deployment process for the prototype kit, making it easier to use and maintain.","GenTime":"2024-08-10 17:54:21"}
{"File Name":"smarthub\/0002-use-lerna-for-package-versioning.md","Context":"## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n","Decision":"Migrate code base to monorepo structure and use `lerna` for versioning management.\\n","tokens":39,"id":3691,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\nWe will implement a dependency management system using a package manager (e.g. pip) to automate the process of updating dependent packages in the Smarthub SDK.\n\n**Context**\n\nThe Smarthub project consists of multiple packages that are part of the Smarthub SDK. The complex dependency graph forces us to update dependent packages manually every time a dependency has changed. This manual process is prone to errors and can be time-consuming.\n\n**Problem**\n\nThe current manual process of updating dependent packages is:\n\n* Time-consuming: updating dependent packages manually takes a significant amount of time, which can delay the release of new features and bug fixes.\n* Error-prone: manual updates can lead to errors, which can cause issues in the SDK and affect the quality of the software.\n* Inefficient: the manual process does not allow for easy tracking of changes, which can make it difficult to debug issues and identify the root cause of problems.\n\n**Decision Rationale**\n\nImplementing a dependency management system using a package manager will:\n\n* Automate the process of updating dependent packages, reducing the time and effort required to maintain the SDK.\n* Reduce the risk of errors by ensuring that dependencies are updated consistently and correctly.\n* Improve the efficiency of the development process by providing a clear record of changes and making it easier to debug issues.\n\n**Status**\n\nThis decision is currently being implemented and will be reviewed and refined as necessary.\n\n**Links**\n\n* Related issues: [list of related issues or pull requests]\n* Relevant documentation: [list of relevant documentation or resources]\n\n**Notes**\n\n* The package manager will be configured to update dependent packages automatically whenever a dependency changes.\n* The team will review and refine the dependency management system as necessary to ensure it meets the needs of the Smarthub SDK.","GenTime":"2024-08-10 17:57:21"}
{"File Name":"smarthub\/0003-create-backend-client-lib.md","Context":"## Context\\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\\n","Decision":"Create client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\\n","tokens":29,"id":3692,"Prediction":"## Decision\n\nTo improve the testability of the system and reduce the coupling between components, we will introduce an abstraction layer between the system components and the off-chain data REST API. This abstraction layer will provide a standardized interface for accessing off-chain data, allowing components to interact with it in a decoupled manner.\n\n## Context\n\nOff-chain data is accessible via REST API. Currently, all system components use direct REST calls in various places, making unit testing hard.\n\n## Problem\n\nThe direct use of REST calls in system components makes it challenging to write unit tests for these components. The tests would need to mock the entire REST API, which can be complex and time-consuming. Additionally, the components are tightly coupled to the REST API, making it difficult to change or replace the API without affecting the components.\n\n## Decision\n\nWe will introduce an abstraction layer, called the \"Data Access Layer\", which will provide a standardized interface for accessing off-chain data. This layer will encapsulate the complexity of interacting with the REST API and provide a simple, easy-to-use interface for components to access off-chain data.\n\n## Consequences\n\n* Improved testability: Components will no longer need to interact directly with the REST API, making it easier to write unit tests.\n* Reduced coupling: The Data Access Layer will act as an intermediary between components and the REST API, reducing the coupling between components and the API.\n* Easier maintenance: If the REST API changes or needs to be replaced, the Data Access Layer can be modified independently of the components, reducing the impact on the system.\n* Improved scalability: The Data Access Layer can be designed to handle load balancing, caching, and other scalability concerns, making it easier to scale the system.\n\n## Rationale\n\nThe introduction of the Data Access Layer will provide a clear separation of concerns between the components and the REST API, making it easier to maintain and test the system. The standardized interface provided by the Data Access Layer will also make it easier to switch to a different off-chain data source if needed.","GenTime":"2024-08-10 17:57:43"}
{"File Name":"saas-platform-frontend\/0003-use-javascript-over-typescript.md","Context":"## Context\\nI need to decide in which language I implement the frontend.\\n","Decision":"I use Javascript.\\n","tokens":16,"id":3715,"Prediction":"## Decision\n\nAfter careful consideration, I have decided to implement the frontend in JavaScript, specifically using React as the framework. This decision is based on the following factors:\n\n### Problem\n\nThe frontend needs to be implemented in a language that is widely supported by most web browsers, allowing for maximum compatibility and flexibility. Additionally, the frontend should be able to efficiently handle complex user interfaces and dynamic updates.\n\n### Alternatives\n\nOther languages and frameworks, such as Angular, Vue.js, or even native mobile app development, could have been considered. However, given the existing infrastructure and expertise within the team, JavaScript and React seem to be the most suitable choices.\n\n### Decision\n\nJavaScript is a popular and widely supported language for web development, and React is a well-established and widely-used framework for building reusable UI components. Its virtual DOM and one-way data binding approach make it efficient and easy to work with. Additionally, React has a large and active community, ensuring a wide range of resources and libraries are available.\n\n### Consequences\n\n* The frontend will be implemented in JavaScript, allowing for maximum compatibility and flexibility.\n* React will be used as the framework, providing a robust and efficient way to build reusable UI components.\n* The team will need to have a good understanding of JavaScript and React to effectively work on the frontend.\n* The project will benefit from the large and active React community, with access to a wide range of resources and libraries.\n\n### Rationale\n\nThe decision to implement the frontend in JavaScript and React is based on the need for a widely supported and efficient solution. JavaScript is a popular and widely supported language, and React is a well-established and widely-used framework. By choosing React, the team will be able to leverage its strengths and take advantage of its large and active community.","GenTime":"2024-08-10 18:02:15"}
{"File Name":"saas-platform-frontend\/0006-use-antdesign-as-the-ui-framework.md","Context":"## Context\\nTo speed up the UI development we need to select a UI Framework that has a good community as well as good functionality.\\n","Decision":"We use Ant Design as the UI Framework.\\n","tokens":28,"id":3718,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use React as the UI framework for our application. This decision is based on the following factors:\n\n### Rationale\n\n* React has a large and active community, which ensures that there are plenty of resources available for learning and troubleshooting.\n* React has a strong ecosystem of tools and libraries that make it easy to build complex UI components and manage state changes.\n* React's virtual DOM approach makes it highly efficient and easy to reason about, which reduces the complexity of updating the UI.\n* React is widely used in the industry, which means that there is a large pool of talent available to hire or collaborate with.\n* React has a strong support for server-side rendering, which allows us to render our UI on the server and improve SEO.\n\n### Trade-offs\n\n* While React is a powerful and flexible framework, it can be steep learning curve for developers who are new to it.\n* React's ecosystem is constantly evolving, which means that we need to stay up-to-date with the latest developments and best practices.\n\n### Consequences\n\n* We will need to invest time and resources in learning React and its ecosystem.\n* We will need to ensure that our team is up-to-date with the latest developments and best practices in React.\n* We will need to consider the trade-offs between using React and other UI frameworks, and make sure that our choice aligns with our project goals and requirements.\n\n### Alternatives\n\n* We considered using Angular or Vue.js as alternative UI frameworks, but ultimately decided that React's strengths in terms of community, functionality, and ecosystem made it the best choice for our project.\n\n### Status\n\nThis decision is currently in the \"done\" status, and we will continue to monitor and evaluate the performance of React as our UI framework.","GenTime":"2024-08-10 18:03:29"}
{"File Name":"saas-platform-frontend\/0002-use-the-stack-from-react-the-complete-guide-to-keep-up-to-date-with-react.md","Context":"## Context\\nI need to decided with which tools I build my react app.\\n","Decision":"I build the react app using [`React - The Complete Guide (incl Hooks, React Router, Redux)`](https:\/\/www.udemy.com\/react-the-complete-guide-incl-redux\/).\\n","tokens":17,"id":3721,"Prediction":"Here is an Architectural Decision Record (ADR) for the context you provided:\n\n**Decision**\n\nWe will build our React app using Webpack, Babel, and ESLint as our primary build tools.\n\n**Context**\n\nAs we are building a React app, we need to decide on the tools we will use to compile, bundle, and lint our code. We have several options available, including Webpack, Rollup, and Parcel. We also need to consider the use of Babel for transpiling our code to be compatible with older browsers.\n\n**Problem**\n\nWe need to choose the right tools to ensure our React app is built efficiently, is compatible with a wide range of browsers, and has a maintainable codebase.\n\n**Reasoning**\n\nAfter considering the pros and cons of each option, we have decided to use Webpack as our primary build tool. Webpack is a popular and well-established tool that provides a high degree of customization and flexibility. It allows us to use a variety of plugins to optimize our code and ensure it is compatible with a wide range of browsers.\n\nWe will also use Babel to transpile our code, as it is widely supported and provides a high degree of compatibility with older browsers. Babel will allow us to write modern JavaScript code and have it transpiled to be compatible with older browsers.\n\nESLint will be used to lint our code, ensuring that it is written in a consistent and maintainable manner. ESLint will help us catch errors and inconsistencies in our code, making it easier to maintain and debug.\n\n**Consequences**\n\nBy using Webpack, Babel, and ESLint, we can ensure that our React app is built efficiently, is compatible with a wide range of browsers, and has a maintainable codebase. This will make it easier to maintain and update our app over time.\n\n**Status**\n\nThis decision is approved and will be implemented in our React app.\n\n**References**\n\n* Webpack: https:\/\/webpack.js.org\/\n* Babel: https:\/\/babeljs.io\/\n* ESLint: https:\/\/eslint.org\/","GenTime":"2024-08-10 18:04:29"}
{"File Name":"saas-platform-frontend\/0005-use-storybook-to-build-self-contained-components-but-not-for-uis.md","Context":"## Context\\nWe need a workflow to build our appliation and components.\\n","Decision":"We use Storybook only for building new self contained components.\\n","tokens":16,"id":3722,"Prediction":"## Decision\n\n**Title:** Implement a Workflow Management System using Apache Airflow\n\n**Context:** We need a workflow to build our application and components.\n\n**Problem:** Our application consists of multiple components that need to be built, tested, and deployed in a specific order. Currently, we are using a manual process to manage the workflow, which is error-prone, time-consuming, and difficult to scale.\n\n**Decision:** We will implement a workflow management system using Apache Airflow to automate and manage the build, test, and deployment process of our application and its components.\n\n**Rationale:**\n\n* Apache Airflow is a widely-used, open-source workflow management system that provides a flexible and scalable solution for managing complex workflows.\n* It allows us to define and automate the workflow using a Python-based syntax, making it easy to integrate with our existing development workflow.\n* Airflow provides features such as task scheduling, dependency management, and monitoring, which will help us to ensure that our workflow is executed correctly and efficiently.\n* It also provides a web-based interface for monitoring and managing the workflow, which will make it easy for our team to track the status of the workflow and troubleshoot any issues that may arise.\n\n**Consequences:**\n\n* We will need to invest time and effort in learning and implementing Apache Airflow, which may require additional training and support.\n* We will need to refactor our existing build, test, and deployment scripts to integrate with Airflow, which may require additional development time and resources.\n* We will need to monitor and maintain the Airflow workflow to ensure that it continues to work correctly and efficiently over time.\n\n**Status:** Accepted\n\n**Implementation:** We will implement Apache Airflow as our workflow management system, and refactor our existing build, test, and deployment scripts to integrate with Airflow. We will also monitor and maintain the Airflow workflow to ensure that it continues to work correctly and efficiently over time.","GenTime":"2024-08-10 18:04:50"}
{"File Name":"runner\/0276-problem-matchers.md","Context":"# ADR 0276: Problem Matchers\\n**Date** 2019-06-05\\n**Status** Accepted\\n## Context\\nCompilation failures during a CI build should surface good error messages.\\nFor example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply \"tsc exited with exit code 1\".\\nVSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https:\/\/code.visualstudio.com\/docs\/editor\/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.\\nThe problem-matcher concept fits well with \"setup\" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.\\n","Decision":"### Registration\\n#### Using `::` command\\n`::add-matcher::path-to-problem-matcher-config.json`\\nUsing a `::` command allows for flexibility:\\n- Ad hoc scripts can register problem matchers\\n- Allows problem matchers to be conditionally registered\\nNote, if a matcher with the same name is registered a second time, it will clobber the first instance.\\n","tokens":227,"id":3734,"Prediction":"## Decision\n\nWe will implement a problem matcher mechanism in our software, similar to the one used in VSCode, to allow users to configure which problem matchers to use when scanning output. This will enable us to provide rich error messages for compilation failures during CI builds, such as surfacing actual compile errors from the TypeScript compiler, rather than just a generic \"exit code 1\" message.\n\n## Rationale\n\nThe problem-matcher concept is a natural fit for our \"setup\" actions, as it allows us to register problem matchers for specific tools or compilers, such as `tsc`, and apply them against the output for the duration of the job. This will provide a more detailed and actionable error experience for users, making it easier for them to identify and fix issues.\n\n## Consequences\n\n* Users will be able to configure which problem matchers to use, allowing for a more tailored error experience.\n* We will need to implement a problem matcher framework that allows users to register and configure problem matchers.\n* We will need to integrate the problem matcher mechanism with our existing \"setup\" actions, such as `setup-nodejs`, to register problem matchers for specific tools or compilers.\n* We will need to update our error handling and UI to display rich error messages, rather than generic messages, when compilation failures occur.","GenTime":"2024-08-10 18:08:47"}
{"File Name":"runner\/0274-step-outcome-and-conclusion.md","Context":"## Context\\nThis ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.\\nThis allows downstream a step to run based on whether a previous step succeeded or failed.\\nReminder, currently the steps contains `steps.<id>.outputs`.\\n","Decision":"For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:\\n- `success`\\n- `failure`\\n- `cancelled`\\n- `skipped`\\nWhen a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.\\n### Example\\n```yaml\\nsteps:\\n- id: experimental\\ncontinue-on-error: true\\nrun: .\/build.sh experimental\\n- if: ${{ steps.experimental.outcome == 'success' }}\\nrun: .\/publish.sh experimental\\n```\\n### Terminology\\nThe runs API uses the term `conclusion`.\\nTherefore we use a different term `outcome` for the value prior to continue-on-error.\\nThe following is a snippet from the runs API response payload:\\n```json\\n\"steps\": [\\n{\\n\"name\": \"Set up job\",\\n\"status\": \"completed\",\\n\"conclusion\": \"success\",\\n\"number\": 1,\\n\"started_at\": \"2020-01-09T11:06:16.000-05:00\",\\n\"completed_at\": \"2020-01-09T11:06:18.000-05:00\"\\n},\\n```\\n","tokens":63,"id":3738,"Prediction":"## Decision\n\n**Decision:** Introduce `steps.<id>.outcome` and `steps.<id>.conclusion` in the steps context.\n\n**Context:** As described, this ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context, allowing downstream steps to run based on the outcome of previous steps. Currently, the steps context only contains `steps.<id>.outputs`.\n\n**Problem:** Without this addition, downstream steps cannot make informed decisions based on the outcome of previous steps. This can lead to unnecessary re-execution of steps or incorrect processing of outputs.\n\n**Reasoning:** The proposed addition provides a clear and explicit way to convey the outcome of a step, enabling more robust and efficient processing of step outputs. This decision aligns with the principles of modular and composable workflows, allowing each step to be self-contained and making it easier to reason about the overall workflow.\n\n**Consequences:**\n\n* The introduction of `steps.<id>.outcome` and `steps.<id>.conclusion` will require updates to the step implementation and the workflow engine.\n* The workflow engine will need to handle and propagate the outcome and conclusion of each step to downstream steps.\n* This change will improve the robustness and efficiency of the workflow processing.\n\n**Rationale:** The benefits of this change outweigh the costs, as it provides a clear and explicit way to convey the outcome of a step, enabling more robust and efficient processing of step outputs.\n\n**Status:** Accepted\n\n**Implementation:** The implementation will involve updating the step implementation and the workflow engine to handle and propagate the outcome and conclusion of each step.\n\n**References:** None","GenTime":"2024-08-10 18:10:14"}
{"File Name":"runner\/0280-command-input-echoing.md","Context":"## Context\\nCommand echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.\\nCommand outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.\\nEcho output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.\\n","Decision":"- The only commands that currently echo output are\\n- `remove-matcher`\\n- `add-matcher`\\n- `add-path`\\n- These will no longer echo the command, if processed successfully\\n- All commands echo the input when any of these conditions is fulfilled:\\n1. When such commands fail with an error\\n2. When `::echo::on` is set\\n3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`\\n- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019\/11\/04):\\n- `add-mask`\\n- `debug`\\n- `warning`\\n- `error`\\n- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these \"echo-suppressing\" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit.\\n","tokens":124,"id":3741,"Prediction":"## Decision\n\n**Decision:** Implement an opt-in system for command echoing in user logs, with explicit control through `::echo::on` and `::echo::off` commands, and default behavior based on the `ACTIONS_STEP_DEBUG` secret.\n\n**Rationale:**\n\n* Command echoing can clutter user logs and make them difficult to read.\n* Providing context on command failures through echoing can help with troubleshooting.\n* Allowing users to opt-in to echoing gives them control over the level of detail in their logs.\n* Defaulting to echoing when `ACTIONS_STEP_DEBUG` is enabled provides a way for users to easily enable detailed logging for debugging purposes.\n* Defaulting to disabling echoing when `ACTIONS_STEP_DEBUG` is disabled prevents unnecessary clutter in user logs.\n\n**Consequences:**\n\n* Users will need to explicitly enable echoing to see command outputs in their logs.\n* Users will need to use `::echo::on` and `::echo::off` commands to control echoing.\n* The `ACTIONS_STEP_DEBUG` secret will need to be managed to control the default echoing behavior.\n* Log readability and maintenance may improve due to reduced clutter.\n* Troubleshooting may become easier due to the availability of command outputs in logs.\n\n**Alternatives:**\n\n* Continuing to echo commands by default, without an opt-in system.\n* Requiring users to disable echoing explicitly.\n* Removing echoing altogether, without providing an alternative way to capture command outputs.\n\n**Status:** Approved","GenTime":"2024-08-10 18:11:09"}
{"File Name":"optimint\/adr-005-serialization.md","Context":"## Context\\nAll the basic data types needs to be efficiently serialized into binary format before saving in KV store or sending to network.\\n","Decision":"`protobuf` is used for data serialization both for storing and network communication.\\n`protobuf` is used widely in entire Cosmos ecosystem, and we would need to use it anyways.\\n","tokens":27,"id":3746,"Prediction":"## Decision\n\n**Decision:** Use a combination of existing libraries and custom implementation to efficiently serialize basic data types into binary format.\n\n## Context\n\nAs per the requirement, we need to efficiently serialize basic data types into binary format before saving in a Key-Value (KV) store or sending over the network.\n\n## Problem\n\nThe problem is to find a reliable and efficient way to serialize basic data types such as integers, floats, strings, and booleans into binary format. The serialization process should be fast, lightweight, and compatible with various KV stores and network protocols.\n\n## Reasoning\n\nWe can't rely solely on existing serialization libraries as they may not provide the desired level of performance, flexibility, or customization. Instead, we can use a combination of existing libraries and custom implementation to achieve the required level of efficiency and compatibility.\n\n## Solution\n\nWe will use the following approach:\n\n1. For primitive data types such as integers, floats, and booleans, we will use existing libraries like Protocol Buffers (protobuf) or MessagePack to serialize them into binary format. These libraries provide efficient and compact serialization, which is essential for our use case.\n2. For strings, we will use a custom implementation that takes advantage of the KV store's or network protocol's built-in string serialization mechanisms. This will ensure that the serialization process is optimized for the specific use case.\n3. For complex data structures such as arrays and objects, we will use a combination of the above approaches and custom implementation. We will recursively serialize the nested data structures and then encode the resulting binary data using a suitable encoding scheme.\n\n## Consequences\n\nBy using a combination of existing libraries and custom implementation, we can achieve the following benefits:\n\n* Efficient serialization: Our solution will provide fast and lightweight serialization of basic data types, which is essential for our use case.\n* Flexibility: We can customize the serialization process to accommodate specific requirements of the KV store or network protocol.\n* Compatibility: Our solution will be compatible with various KV stores and network protocols, ensuring that the serialized data can be easily stored or transmitted.\n\nHowever, our solution also has some limitations:\n\n* Increased complexity: By using a combination of existing libraries and custom implementation, our solution may be more complex to maintain and debug.\n* Customization: While our solution provides flexibility, it also requires customization for each specific use case, which may add to the overall development time.","GenTime":"2024-08-10 18:13:15"}
{"File Name":"tech-events-calendar\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3753,"Prediction":"## Decision\n\n**Title:** Implement a Microservices Architecture for the Project\n\n**Context:** As we need to record the architectural decisions made on this project, we have decided to adopt a microservices architecture to ensure scalability, flexibility, and maintainability.\n\n**Decision:** We will design and implement the project as a collection of independent microservices, each responsible for a specific business capability or domain. This will allow us to develop, test, and deploy each service independently, without affecting the entire system.\n\n**Rationale:**\n\n* Scalability: Microservices can be scaled independently, allowing us to allocate resources more efficiently and handle varying traffic patterns.\n* Flexibility: With a microservices architecture, we can easily swap out or replace individual services without affecting the entire system.\n* Maintainability: Each microservice is a self-contained unit, making it easier to understand, test, and maintain.\n* Resilience: If one service experiences issues, it will not bring down the entire system.\n\n**Consequences:**\n\n* Increased complexity: With multiple services, we will need to manage more moving parts and ensure seamless communication between them.\n* Higher operational overhead: We will need to implement monitoring, logging, and alerting for each service, as well as handle service discovery and configuration.\n* Potential for inconsistent data: Without proper data synchronization mechanisms, microservices may store inconsistent data, leading to data inconsistencies.\n\n**Status:** Accepted\n\n**Implementation:** We will start by identifying the individual business capabilities and domains, and then design and implement each microservice accordingly. We will use a service registry and API gateway to manage service discovery and communication.\n\n**Owners:** [List the names and roles of the team members responsible for implementing and maintaining the microservices architecture]\n\n**History:**\n\n* 2023-02-20: Decision made\n* 2023-03-15: Initial design and planning phase completed\n* 2023-04-01: Development of individual microservices began","GenTime":"2024-08-10 18:15:47"}
{"File Name":"mat-process-utils\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3763,"Prediction":"## Decision\n\n**Decision:** Use a Microkernel Architecture for the New Feature\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Problem\n\nThe new feature we are about to implement requires a high degree of flexibility and scalability. The current monolithic architecture of the system is not well-suited for this feature, and we need to make a change to accommodate its requirements.\n\n## Decision\n\nAfter careful consideration, we have decided to adopt a Microkernel Architecture for the new feature. This architecture will provide the necessary flexibility and scalability to meet the requirements of the feature. The Microkernel will act as a central hub, coordinating the interactions between the different components of the system.\n\n## Rationale\n\nThe Microkernel Architecture will provide several benefits, including:\n\n* Flexibility: The Microkernel will allow us to easily add or remove components as needed, making it easier to adapt to changing requirements.\n* Scalability: The Microkernel will enable us to scale individual components independently, allowing us to handle increased traffic and load.\n* Modularity: The Microkernel will promote modularity by providing a clear separation of concerns between components, making it easier to maintain and update the system.\n\n## Consequences\n\nThe adoption of the Microkernel Architecture will have the following consequences:\n\n* Increased complexity: The Microkernel Architecture will introduce additional complexity to the system, requiring more time and effort to develop and maintain.\n* Greater flexibility: The Microkernel Architecture will provide greater flexibility and scalability, making it easier to adapt to changing requirements and handle increased traffic and load.\n* Improved maintainability: The Microkernel Architecture will promote modularity, making it easier to maintain and update the system.\n\n## Status\n\nThis decision is approved and will be implemented as part of the new feature.\n\n## Affected Components\n\n* The new feature\n* The Microkernel Architecture\n\n## Changelog\n\n* 2023-02-20: Decision made to adopt the Microkernel Architecture for the new feature.\n* 2023-02-25: Design and implementation of the Microkernel Architecture began.\n\nNote: This is just an example, please adjust it according to your needs and the specific requirements of your project.","GenTime":"2024-08-10 18:18:55"}
{"File Name":"unit-e-project\/ADR-0007.md","Context":"## Context\\nWe need a way to provide initial supply. At the moment it is not possible to spend the genesis block coinbase because\\neven if the block is added to the index, its transactions are not added to the txdb.\\n","Decision":"We want to change the code so that we are able to create an initial supply but we MUST only use the coinbase of the\\ngenesis block and MUST NOT resort to further `imports` in subsequents blocks like for example Particl did.\\nAll the coins of the initial supply MUST be minted in the coinbase transaction of the genesis block.\\n","tokens":50,"id":3764,"Prediction":"## Decision\n\nTo address the issue of not being able to spend the genesis block coinbase, we will implement a mechanism to allow the initial supply to be provided by creating a special \"Genesis Transaction\" that is added to the txdb during the initialization process.\n\n## Context\n\nAs you mentioned, currently it is not possible to spend the genesis block coinbase because even if the block is added to the index, its transactions are not added to the txdb. This makes it difficult to provide the initial supply of coins.\n\n## Decision Rationale\n\nWe will create a special \"Genesis Transaction\" that will be added to the txdb during the initialization process. This transaction will create the initial supply of coins and will be marked as a \"Genesis Transaction\" so that it can be distinguished from regular transactions. This will allow us to provide the initial supply of coins and make it possible to spend the genesis block coinbase.\n\n## Pros and Cons\n\nPros:\n\n* Allows for the initial supply of coins to be provided\n* Makes it possible to spend the genesis block coinbase\n* Simplifies the process of providing the initial supply of coins\n\nCons:\n\n* Requires additional code to create and add the Genesis Transaction to the txdb\n* May introduce additional complexity to the initialization process\n\n## Alternatives Considered\n\nWe considered the following alternatives:\n\n* Not providing the initial supply of coins: This would mean that the genesis block coinbase would not be able to be spent, which could be a problem for the use case.\n* Creating a separate mechanism for providing the initial supply of coins: This could add additional complexity to the system and make it harder to maintain.\n\n## Conclusion\n\nWe have decided to implement a mechanism to allow the initial supply to be provided by creating a special \"Genesis Transaction\" that is added to the txdb during the initialization process. This will allow us to provide the initial supply of coins and make it possible to spend the genesis block coinbase.","GenTime":"2024-08-10 18:19:16"}
{"File Name":"unit-e-project\/ADR-0002.md","Context":"## Context\\nWe have to maintain a consistent naming in the codebase for roles in the Esperanza protocol.\\nAt the moment we have plenty of examples where there is confusion especially when mixing terms like `staking`, `miner`, `proposer`.\\n","Decision":"We outline two clear actors in the protocol:\\n- The `PROPOSER`: which solves the function of proposing new blocks through his stake. At the moment words like `staking`, `miner`, `mining` are associated with the same function.\\n- The `VALIDATOR`: which takes active part in the finalisation process with the voting.\\nWe therefore MUST replace:\\n- all the instances of `miner` with `proposer`.\\n- all the instances of `mining` with `proposing`.\\n- all the instances of `staking` with `proposing`.\\nBut we MUST NOT replace the instances of the word `stake` since this indicates the actual deposit made in order to propose.\\nThe only exception to this is `stakethread.h` that MUST be renamed to `proposerthread.h`.\\n","tokens":52,"id":3768,"Prediction":"## Decision\n\nTo maintain consistency in the naming of roles in the Esperanza protocol, we will adopt a standardized naming convention for roles in the codebase.\n\n## Context\n\nAs mentioned, we currently have multiple examples of confusion in the codebase where terms like `staking`, `miner`, and `proposer` are used interchangeably, leading to inconsistencies and difficulties in understanding the code.\n\n## Problem\n\nThe lack of a standardized naming convention for roles in the Esperanza protocol is causing confusion and making it difficult to maintain and extend the codebase. This is leading to errors, misunderstandings, and increased development time.\n\n## Decision\n\nWe will adopt a standardized naming convention for roles in the Esperanza protocol, specifically:\n\n* `Staker` for entities that participate in staking\n* `Miner` for entities that are responsible for mining blocks\n* `Proposer` for entities that propose new blocks\n\nWe will also establish a set of guidelines for naming conventions, including:\n\n* Use of camelCase for role names (e.g. `Staker`, `Miner`)\n* Use of singular nouns for role names (e.g. `Staker`, not `Stakers`)\n* Use of consistent terminology throughout the codebase\n\n## Rationale\n\nBy adopting a standardized naming convention, we will improve code readability, reduce confusion, and make it easier for new developers to understand the codebase. This will also help to reduce errors and improve overall maintainability of the codebase.\n\n## Consequences\n\n* Improved code readability and maintainability\n* Reduced confusion and errors due to inconsistent naming conventions\n* Easier onboarding for new developers\n* Increased consistency throughout the codebase\n\n## Implementation\n\nWe will implement this decision by updating the existing codebase to conform to the new naming convention, and by establishing a set of guidelines for naming conventions to ensure consistency throughout the codebase.\n\n## Monitoring and Review\n\nWe will monitor the codebase for any inconsistencies in naming conventions and review the guidelines regularly to ensure they remain effective and relevant.","GenTime":"2024-08-10 18:19:52"}
{"File Name":"elife-xpub\/0004-code-structure.md","Context":"## Context\\nAs the code rapidly grows we need a common structure and approach that will keep the code tidy and coherent.\\nThere are various aspects, the initial concern was the data access separation. The issue #140 initially talked about how this could be separated in terms of an ORM (Object-Relational Mapping) or a DAL (Data Access Layer).\\nHowever, this discussion also soon covered the structure of the code in general and how to structure it for ease of development and best practise.\\n### Rejected Options\\nWe considered the following options:\\n* The Pubsweet Way - This was to define a base class \"Model\" and extend this for each entity. This was rejected as its another self made ORM but and has validations that we don't want.\\n* Other ORMs : Waterline, sequelize, bookshelf - General purpose ORMs were rejected based on the experience that the time spent going up the learning curve and sifting through the docs when things go wrong is greater then simply rolling your own.\\n* Query Builders : Knex.js -These were not entirely ruled out, and were deemed compatible with a good code structure that would allow the database access to be encapsulated in a way that would allow a query builder to be used where necessary.\\n","Decision":"In summary the server-side code will be structured below the `server` folder as follows:\\n* **entities** - This folder will contain subfolders relating to named entities in the system. See [\"Data Model Specification\"](https:\/\/docs.google.com\/document\/d\/1KU-DLMNhPxjQF2j8HVlJvenvttPLxgtbTHo8Sy_PNRc\/).\\n* **entities\/\\<entity\\>** - The example below shows `Mansuscript` as an example entity, this folder will contain a common set of files that describe the entity's behaviour. Each of these have a particular purpose explained below:\\n* index.js - The main business logic, e.g. for `Manuscript` this could contain `getAuthor()`\\n* typedefs.js - Contains the GraphQL types pertinent to this entity.\\n* resolvers.js - Contains the GraphQL interface specified in terms of `Query` and `Mutation` 's for this entity. These in general should map onto the exposed functions from index.js. For example, `Manuscript` may contain a `Query` for `allManuscripts` and a `Mutation` for `createManuscript`.\\n* data-access.js - Contains the functions for saving to and loading from the database. All references to the database and SQL that is written should remain private to this file.\\n* **manuscript-state-machine** - It is anticipated that this is where the most orchestration of the entites will take place. This has been created as a separate folder so that it can expand to implement the state transitions currently defined for the manuscript.\\n### Example\\n```\\nserver\\n|\\n\u251c\u2500\u2500 entities\\n|   |\\n\u2502   \u251c\u2500\u2500 manuscript\\n\u2502   \u2502   \u251c\u2500\u2500 typedefs.js\\n\u2502   \u2502   \u251c\u2500\u2500 resolvers.js\\n\u2502   \u2502   \u251c\u2500\u2500 data-access.js\\n\u2502   \u2502   \u2514\u2500\u2500 index.js\\n|   |\\n\u2502   \u2514\u2500\u2500 <entity>\\n\u2502       \u251c\u2500\u2500 typedefs.js\\n\u2502       \u251c\u2500\u2500 resolvers.js\\n\u2502       \u251c\u2500\u2500 data-access.js\\n\u2502       \u2514\u2500\u2500 index.js\\n|\\n\u2514\u2500\u2500 manuscript-state-machine\\n\u2514\u2500\u2500 index.js\\n```\\n","tokens":257,"id":3776,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will implement a custom Data Access Layer (DAL) using a query builder approach, encapsulating database access in a structured way that allows for flexibility and ease of development.\n\n**Context**\n\nAs the code rapidly grows, we need a common structure and approach to keep the code tidy and coherent. The initial concern was the data access separation, which led to a discussion on the structure of the code in general and how to structure it for ease of development and best practices.\n\n**Problem**\n\nWe need a way to separate data access from the rest of the code and ensure that it is easy to maintain and extend.\n\n**Alternatives Considered**\n\nWe considered the following options:\n\n* The Pubsweet Way: defining a base class \"Model\" and extending this for each entity, which was rejected due to its complexity and validation features that we don't need.\n* Other general-purpose ORMs (Waterline, sequelize, bookshelf): these were rejected due to the time spent learning and troubleshooting issues.\n* Query Builders (Knex.js): these were not entirely ruled out and were deemed compatible with a good code structure that allows for encapsulated database access.\n\n**Decision Rationale**\n\nWe chose to implement a custom DAL using a query builder approach because it allows for flexibility and ease of development. This approach also enables us to encapsulate database access in a structured way, making it easier to maintain and extend.\n\n**Status**\n\nThis decision is approved and will be implemented in the next release.\n\n**Implementation**\n\nThe implementation will involve creating a custom DAL using a query builder approach, encapsulating database access in a structured way. This will involve creating a set of classes and interfaces that define the data access layer and implementing the necessary methods for querying and manipulating data.\n\n**Consequences**\n\nThe implementation of a custom DAL using a query builder approach will:\n\n* Provide a structured way to encapsulate database access\n* Allow for flexibility and ease of development\n* Make it easier to maintain and extend the code\n* Reduce the complexity of the code by separating data access from the rest of the code.","GenTime":"2024-08-10 18:22:48"}
{"File Name":"TANF-app\/001-ADR-template.md","Context":"## Context\\nWe need to record the architectural decisions made on this project. This context section should include the history and driving reason(s) on why a decision needed to be made.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":37,"id":3784,"Prediction":"## Decision\n\n**Title:** Implement a Microservices Architecture for the Project\n\n**Context:**\n\nOur project, [Project Name], has grown significantly since its inception, and we have reached a point where a monolithic architecture is no longer sufficient to meet the increasing demands of scalability, maintainability, and flexibility. The system has become complex, with multiple components tightly coupled and interdependent, making it difficult to maintain, update, and scale independently. The lack of modularity and flexibility has resulted in increased development time, decreased efficiency, and limited opportunities for innovation.\n\n**Decision:**\n\nAfter careful consideration, we have decided to adopt a Microservices Architecture (MSA) for the project. This decision was driven by the need to:\n\n* Improve scalability and performance by allowing each service to be scaled independently\n* Enhance maintainability by breaking down the monolithic architecture into smaller, more manageable components\n* Increase flexibility by enabling the use of different technologies and frameworks for each service\n* Improve fault tolerance by allowing services to fail independently without affecting the entire system\n* Reduce technical debt by allowing for more focused and targeted development and maintenance efforts\n\n**Rationale:**\n\nThe MSA approach will allow us to develop, deploy, and maintain each service independently, without being tied to the entire system. This will enable us to take advantage of the latest technologies and frameworks, improve communication between services, and reduce the overall complexity of the system. Additionally, the MSA approach will provide a more flexible and adaptable architecture, allowing us to respond quickly to changing business needs and market conditions.\n\n**Consequences:**\n\nThe implementation of a Microservices Architecture will require significant changes to our development process, including:\n\n* Breaking down the monolithic architecture into smaller, independent services\n* Developing and deploying each service separately\n* Implementing service discovery and communication mechanisms\n* Ensuring consistency and coordination across services\n* Developing and implementing monitoring and logging mechanisms to ensure service-level visibility and control\n\n**Pros and Cons:**\n\nPros:\n\n* Improved scalability and performance\n* Enhanced maintainability and flexibility\n* Increased fault tolerance and reduced technical debt\n* Improved communication and coordination between services\n\nCons:\n\n* Increased complexity and overhead in terms of service discovery and communication\n* Potential for increased latency and overhead due to service boundaries\n* Requires significant changes to development process and culture\n\n**Next Steps:**\n\nWe will begin by identifying the services that will be part of the initial MSA implementation, and develop a roadmap for the transition. We will also establish a governance model for service development, deployment, and maintenance, and implement monitoring and logging mechanisms to ensure service-level visibility and control.","GenTime":"2024-08-10 18:25:22"}
{"File Name":"TANF-app\/008-deployment-flow.md","Context":"## Context\\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.\\nSince the recent changes to our [Git workflow](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https:\/\/github.com\/HHS\/TANF-app\/blob\/837574415af7c57e182684a75bbcf4d942d3b62a\/docs\/Architecture%20Decision%20Record\/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.\\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\n","Decision":"Additionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\nDeploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role                                             | Deploys when ...                                  |\\n-------------------|-----------------|----------------------|--------------------------------------------------|---------------------------------------------------|\\nDev                | Tanf-Dev        | Vendor & Gov      | Deploy code submitted for gov review                | Relevant github label assigned as shown below     |\\nDevelop            | Tanf-Staging    | Vendor & Gov      | Deploy code once gov-approved                       | Code merged to `raft-tech\/TANF-app:develop` |\\nStaging            | Tanf-Staging    | Gov               | Deploy code once gov-approved                       | Code merged to `HHS\/TANF-app:main` |\\nProduction         | Tanf-Prod       | Gov               | Deploy code tested in staging & ready for prod      | Code merged to `HHS\/TANF-app:master`                |\\n### Gitflow and Deployments\\nWe will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](.\/018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS\/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech\/TANF-app:develop` will be deployed to our staging sites.\\n### Dev deployments\\nWithin the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:\\n| Dev Site | Frontend URL | Backend URL | Purpose                                          |\\n| -------- | -------- | -------- |--------------------------------------------------|\\n| A11y | https:\/\/tdp-frontend-a11y.app.cloud.gov | https:\/\/tdp-backend-a11y.app.cloud.gov\/admin\/ | Space for accessibility testing                  |\\n| QASP | https:\/\/tdp-frontend-qasp.app.cloud.gov | https:\/\/tdp-backend-qasp.app.cloud.gov\/admin\/ | Space for QASP review                            |\\n| raft | https:\/\/tdp-frontend-raft.app.cloud.gov | https:\/\/tdp-backend-raft.app.cloud.gov\/admin\/ | Space for Raft review                          |\\n","tokens":396,"id":3789,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nTo create separate deployment environments in the `tanf-staging` space on Cloud.gov for different features or fixes, allowing the vendor development team to test and deploy multiple in-progress features simultaneously without \"crowding\" in a single environment.\n\n**Context**\n\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only. Since the recent changes to our Git workflow, we believe our current deploy strategy should be updated to more closely match the workflow. Additionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov, leading to \"crowding\" and challenges for the vendor development team. As of Spring 2022, following ADR 018, the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\n\n**Status**\n\nApproved\n\n**Decision Drivers**\n\n* The need to reduce \"crowding\" in the `tanf-dev` space and provide a more efficient testing and deployment process for the vendor development team\n* The requirement for multiple deployment environments in the `tanf-staging` space to ensure dedicated environments for release-specific features, as outlined in ADR 018\n\n**Alternatives Considered**\n\n* Maintaining a single deployment environment in the `tanf-dev` space, which would continue to lead to \"crowding\" and challenges for the vendor development team\n* Creating separate deployment environments in the `tanf-prod` space, which would not provide the necessary separation of environments for release-specific features\n\n**Consequences**\n\n* The creation of separate deployment environments in the `tanf-staging` space will provide a more efficient testing and deployment process for the vendor development team\n* The separation of environments will ensure that release-specific features have dedicated environments for testing and deployment\n* The additional deployment environments will require additional resources and maintenance efforts\n\n**Rationale**\n\nThe creation of separate deployment environments in the `tanf-staging` space will provide a more efficient testing and deployment process for the vendor development team, reducing \"crowding\" and challenges in the `tanf-dev` space. This decision aligns with the requirements outlined in ADR 018 and will ensure that release-specific features have dedicated environments for testing and deployment.","GenTime":"2024-08-10 18:27:14"}
{"File Name":"TANF-app\/010-assign-superuser.md","Context":"## Context\\nUsually in Django we would assign the first superuser through the CLI, but because this will not\\nbe available in production, we will need another method.\\n","Decision":"The Django Admin provides the easiest way to assign superuser status, so for most users that will\\nbe the method employed. However, we still need a method for creating the first superuser, so that\\nuser will be able to assign others as needed. We will assign this with a data migration using a\\nusername defined in environment variables.\\n","tokens":35,"id":3792,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\nTo create a superuser programmatically in the Django application, we will use a custom management command to assign the first superuser in the production environment.\n\n**Context**\n\nAs mentioned in the context, usually in Django, we would assign the first superuser through the CLI using the `createsuperuser` command. However, this method will not be available in the production environment, and we need to find an alternative solution.\n\n**Decision Rationale**\n\nWe decided to create a custom management command to assign the first superuser in the production environment because:\n\n* It provides a centralized and controlled way to create the first superuser.\n* It allows us to automate the process, ensuring consistency and reducing the risk of human error.\n* It is more scalable and maintainable than relying on manual intervention.\n\n**Consequences**\n\nBy creating a custom management command to assign the first superuser, we will need to:\n\n* Create a new file in the `management\/commands` directory of our Django project.\n* Define the command in the new file, using the `BaseCommand` class from Django.\n* Implement the logic to create the first superuser, using the `User` model and the `set_password` method.\n* Add the command to the `INSTALLED_APPS` setting in our Django project's `settings.py` file.\n\n**Status**\n\nThis decision is **accepted** and will be implemented in the next release of the Django application.","GenTime":"2024-08-10 18:28:11"}
{"File Name":"TANF-app\/011-buildpacks.md","Context":"## Context\\nCurrently, our frontend and backend apps are running in Docker containers on Cloud.gov. The deployment process involves building the docker containers at [DockerHub](https:\/\/dockerhub.com). Because there is very little security documentation on DockerHub, the path to getting an ATO with this process would be very difficult. There are other options that may be easier to document, but none of them offer the benefits of buildpacks, which have already been Fed Ramped and documented.\\n","Decision":"Our recommendation is to move to Cloud.gov buildpacks at this time. They are already Fed Ramped, [shift responsibility to Cloud.gov](https:\/\/cloud.gov\/docs\/technology\/responsibilities\/) and ensure tightened security.\\n","tokens":99,"id":3793,"Prediction":"## Decision\n\nTo address the security concerns and ease the path to achieving an Authority to Operate (ATO) with the current Docker-based deployment process, we will replace DockerHub with a more secure alternative, utilizing buildpacks that have already been Fed Ramped and documented.\n\n## Context\n\nAs mentioned, our current deployment process involves building Docker containers at DockerHub, which lacks sufficient security documentation, making it challenging to obtain an ATO. This poses a significant obstacle in maintaining compliance with federal regulations.\n\n## Problem\n\nThe current DockerHub-based deployment process does not provide the necessary security documentation, making it difficult to achieve an ATO, which is crucial for our application's deployment on Cloud.gov.\n\n## Decision\n\nTo address this issue, we will utilize buildpacks that have already been Fed Ramped and documented, offering a more secure alternative to DockerHub. This decision will enable us to maintain compliance with federal regulations and ease the path to achieving an ATO.\n\n## Consequences\n\n* We will need to re-architect our deployment process to utilize buildpacks instead of DockerHub.\n* We will need to ensure that our buildpacks are properly configured and secured to meet the necessary security requirements.\n* We will need to update our documentation to reflect the changes in our deployment process.\n\n## Rationale\n\nThe use of buildpacks that have already been Fed Ramped and documented provides a more secure and compliant alternative to DockerHub. This decision will enable us to maintain compliance with federal regulations and ease the path to achieving an ATO, ultimately ensuring the secure deployment of our application on Cloud.gov.\n\n## Status\n\nThis decision is approved and will be implemented as part of our next deployment cycle.\n\n## Links\n\n* [DockerHub](https:\/\/dockerhub.com)\n* [Fed Ramped](https:\/\/www.fedramp.gov\/)\n* [Cloud.gov](https:\/\/cloud.gov)","GenTime":"2024-08-10 18:28:31"}
{"File Name":"TANF-app\/015-terraform-service-provisioning.md","Context":"## Context\\nIn [this issue](https:\/\/github.com\/raft-tech\/TANF-app\/issues\/609) we propose tasks to automate the provisioning of Cloud.gov-brokered services via our CI pipeline.\\nThis project plans to use Cloud.gov-brokered S3 buckets for TANF data file storage (sensitive PII data) and for Django Admin static assets (public), and a Cloud.gov-brokered RDS PostgreSQL service. Currently, there are no scripts to provision and secure these S3 buckets and RDS instance, and no dev documentation on how to deploy them, secure them, and verify that they are configured correctly. Additionally, this same initial provisioning must be done again for all additional target environments.\\n","Decision":"[Terraform](https:\/\/www.terraform.io\/) is a tool for building, changing, and versioning infrastructure safely and efficiently, and was proposed as a solution for managing our persistent Cloud.gov-brokered infrastructure. We can closely model what is done in [another HHS project](https:\/\/github.com\/HHS\/Head-Start-TTADP) and create per-environment infrastructure configurations which are leveraged by Terraform in CircleCI, with environment-specific settings read directly from Cloud.gov during our CI process. Note that this workflow was a [recommendation from Cloud.gov](https:\/\/www.youtube.com\/watch?v=86wfgNK_0o4), and they themselves use Terraform to provision their own infrastructure.\\n","tokens":148,"id":3794,"Prediction":"## Decision\n\n**Decision:** Automate the provisioning of Cloud.gov-brokered services via our CI pipeline to ensure consistency, security, and efficiency in setting up and configuring S3 buckets and RDS PostgreSQL instances for TANF data file storage and Django Admin static assets.\n\n**Context:** As described in the issue #609, the current process of provisioning and securing Cloud.gov-brokered services is manual, time-consuming, and prone to errors. This decision aims to address these challenges by automating the provisioning process through our CI pipeline.\n\n**Problem:** The manual process of provisioning and securing Cloud.gov-brokered services is:\n\n* Time-consuming: It requires manual effort and attention, taking away from other important tasks.\n* Error-prone: Human error can lead to misconfigurations, security vulnerabilities, or data breaches.\n* Inconsistent: Different team members may configure the services differently, leading to inconsistencies across environments.\n\n**Solution:** Automating the provisioning process through our CI pipeline will:\n\n* Ensure consistency: The same configuration will be applied across all environments, reducing the risk of errors and inconsistencies.\n* Improve security: Automated provisioning will reduce the attack surface by minimizing manual errors and ensuring that security configurations are applied correctly.\n* Increase efficiency: The CI pipeline will automate the provisioning process, freeing up team members to focus on other important tasks.\n\n**Consequences:**\n\n* Initial investment: Developing the automation scripts and integrating them into the CI pipeline will require an initial investment of time and resources.\n* Improved reliability: The automated provisioning process will reduce the likelihood of errors and inconsistencies, leading to improved reliability and reduced downtime.\n* Enhanced security: The automated provisioning process will ensure that security configurations are applied correctly, reducing the risk of data breaches and security vulnerabilities.\n\n**Rationale:** Automating the provisioning of Cloud.gov-brokered services via our CI pipeline is the best solution to address the challenges and risks associated with the current manual process. This decision will improve the efficiency, security, and consistency of our service provisioning process, ultimately benefiting the team and our users.","GenTime":"2024-08-10 18:28:53"}
{"File Name":"TANF-app\/012-antivirus-strategy.md","Context":"## Context\\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no fool proof way of doing this and a malicious user would be able to spoof a virus as a text file.\\n","Decision":"Instead of using a signed URL and sending the file directly to S3, we will instead send the file to the backend and scan it with Clam AV before sending it to S3. In the event there is a virus, we will destroy the file on the backend immediately and return an error to the frontend.\\nBy using the [ClamAV REST server](https:\/\/github.com\/ajilaag\/clamav-rest) implementation we are able to scan files for viruses and malicious behavior. Additionally, Anti-Virus definitions are kept up to date automatically by use of the included [freshclam](https:\/\/www.clamav.net\/documents\/signature-testing-and-management#freshclam) tool which automatically downloads and updates an internal database of virus signatures using the official ClamAV source.\\nIn addition to this, the frontend is able to reliably determine if a file is a binary file. The client will check submitted files for this and immediately return an error to the user. These files will not be sent to the backend.\\n","tokens":72,"id":3797,"Prediction":"## Decision\n\n### Decision: Implement a separate virus scanning service to verify file contents\n\n### Context\n\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no foolproof way of doing this and a malicious user would be able to spoof a virus as a text file.\n\n### Problem\n\nOur current approach relies solely on file headers to determine the type of file, which is vulnerable to tampering and spoofing. This could lead to security issues and compromise the integrity of our system.\n\n### Reasoning\n\nImplementing a separate virus scanning service would provide an additional layer of security to verify the contents of uploaded files. This service would scan the files for malware and viruses, ensuring that only clean files are stored in our system.\n\n### Consequences\n\n* Increased security: By verifying file contents, we can ensure that our system is protected from malicious files.\n* Additional complexity: Implementing a virus scanning service would add complexity to our system, requiring additional infrastructure and maintenance.\n* Cost: We would need to consider the cost of implementing and maintaining a virus scanning service.\n\n### Decision\n\nWe will implement a separate virus scanning service to verify the contents of uploaded files. This will provide an additional layer of security and ensure the integrity of our system. We will consider the costs and complexities associated with implementing this service and weigh them against the benefits of increased security.\n\n### Status\n\nThis decision is currently in the \"planned\" status. We will implement the virus scanning service as soon as possible and monitor its effectiveness in ensuring the security of our system.","GenTime":"2024-08-10 18:29:52"}
{"File Name":"register-trainee-teachers\/0004-non-integer-trainee-ids.md","Context":"## Context\\nWe have been using trainee IDs in the URL as per standard Rails convention\/operation.\\nIt was felt that we should use non-integer IDs for a number of reasons:\\n* remove predictability\\n* interoperability with other systems without depending on DB IDs\\n","Decision":"We chose to use option 3 as it met the needs we had with the minimum of effort and avoided the really long URLs that\\noption 1 would have caused.\\n","tokens":57,"id":3802,"Prediction":"## Decision\n\n**Use UUIDs instead of trainee IDs in the URL**\n\n## Context\n\nAs per standard Rails convention, we have been using trainee IDs in the URL. However, it was felt that we should use non-integer IDs for several reasons, including removing predictability and improving interoperability with other systems without depending on DB IDs.\n\n## Problem\n\nThe current use of integer IDs in the URL makes it predictable and potentially vulnerable to attacks. Additionally, it limits our ability to seamlessly integrate with other systems that may use different ID schemes.\n\n## Solution\n\nTo address these concerns, we will use UUIDs (Universally Unique Identifiers) instead of trainee IDs in the URL. UUIDs are non-predictable, unique, and widely supported across various systems and platforms. This change will provide an additional layer of security and flexibility, allowing us to integrate with other systems more easily.\n\n## Consequences\n\n* The URL structure will change to include UUIDs instead of integer IDs.\n* We will need to update our routing and controller logic to accommodate the new UUID-based IDs.\n* We will need to ensure that our database schema and data storage are updated to accommodate the new UUID-based IDs.\n\n## Rationale\n\nUsing UUIDs instead of integer IDs provides a more secure and flexible solution for our application. By removing predictability, we reduce the risk of attacks and improve the overall security of our application. Additionally, using UUIDs allows us to seamlessly integrate with other systems that may use different ID schemes, improving interoperability and flexibility.\n\n## Alternatives Considered\n\nWe considered using other non-integer ID schemes, such as GUIDs or hash-based IDs. However, UUIDs were chosen due to their widespread adoption and support across various systems and platforms.\n\n## Status\n\nThis decision has been approved and implemented. The changes will be reviewed and tested to ensure that they meet the required standards and do not introduce any regressions.","GenTime":"2024-08-10 18:31:44"}
{"File Name":"govuk-design-system-architecture\/001-javascript-for-less-capable-browsers.md","Context":"## Context\\nBefore GOV.UK Frontend, our projects used jQuery for DOM interactions, events and data manipulation.\\nWe\u2019re taking a step back from jQuery due to its lack of support for the browsers we support, its large file size, lack of security updates and from conversations with the community.\\n","Decision":"We\u2019re now writing standard ES5 JavaScript instead, that we polyfill where necessary.\\nThis means that in places where we would have previously used [`$.each`](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [`.forEach`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.\\nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).\\nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.\\nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only ships the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.\\nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.\\nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).\\nHere is an [example of polyfilling `addEventListener`](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/docs\/polyfilling.md).\\nAny polyfills included in GOV.UK Frontend will be tested to work in supported browsers and devices, including assistive technology. Any community discussion and documentation around potential bugs or downsides will also be considered before deciding to include a polyfill in GOV.UK Frontend.\\n","tokens":62,"id":3806,"Prediction":"Here is an Architectural Decision Record (ADR) based on the provided context:\n\n**Decision**\n\nADR-001: Abandon jQuery for DOM Interactions and Events\n\n**Context**\n\nAs part of our efforts to improve the maintainability and security of our projects, we are phasing out the use of jQuery for DOM interactions, events, and data manipulation. This decision is driven by the lack of support for the browsers we support, the large file size, and the lack of security updates. Additionally, our conversations with the community have also highlighted the need to move away from jQuery.\n\n**Problem**\n\nOur current use of jQuery for DOM interactions and events has several drawbacks:\n\n* Lack of support for certain browsers: jQuery's support for older browsers is limited, which can lead to issues and inconsistencies in our application.\n* Large file size: Including jQuery in our projects increases the overall file size, which can impact page load times and user experience.\n* Security updates: jQuery is no longer receiving security updates, which can leave our applications vulnerable to security exploits.\n\n**Solution**\n\nWe will abandon the use of jQuery for DOM interactions and events and instead use alternative libraries and tools that better suit our needs. We will evaluate and implement alternative solutions that provide better support for our target browsers, are more lightweight, and receive regular security updates.\n\n**Rationale**\n\nBy abandoning jQuery, we will be able to:\n\n* Improve the maintainability and security of our projects\n* Reduce the overall file size of our applications\n* Take advantage of more modern and lightweight alternatives for DOM interactions and events\n\n**Status**\n\nThis decision is in progress. We will continue to evaluate and implement alternative solutions to replace jQuery.\n\n**References**\n\n* [Link to relevant documentation or resources]\n\n**Implementation**\n\n* [Description of the implementation plan, including timelines and responsible parties]\n\n**Reevaluation**\n\nThis decision will be reevaluated in [timeframe] to assess its effectiveness and identify any areas for improvement.","GenTime":"2024-08-10 18:33:02"}
{"File Name":"adr-viewer\/0005-distinguish-amendments-to-records-with-colour.md","Context":"## Context\\nArchitecture Decision Records may be `amended` rather than `superseded` if e.g. only a small part of the decision changes.\\n","Decision":"Amended records, although not officially supported as a distinct flag in `adr-tools`, should be distinguished from records that are either Accepted or Superseded by.\\n","tokens":33,"id":3813,"Prediction":"## Decision\n\n**Amending Architecture Decisions**\n\n**Context**\n\nAs our software evolves, it is possible that certain architecture decisions may need to be updated or refined. In order to ensure that our decision-making process is transparent and consistent, we will adopt a policy of amending existing architecture decisions rather than superseding them.\n\n**Problem**\n\nWhen a small part of an existing architecture decision needs to be updated, it can be tempting to simply supersede the original decision with a new one. However, this approach can lead to a loss of historical context and make it difficult to understand the reasoning behind the original decision.\n\n**Decision**\n\nFrom now on, if a small part of an existing architecture decision needs to be updated, we will amend the original decision rather than superseding it. This will ensure that the updated decision remains linked to its original context and that the reasoning behind the update is clear.\n\n**Consequences**\n\n* This approach will help to maintain a clear and transparent record of our architecture decisions, making it easier for future developers to understand the reasoning behind our design choices.\n* It will also reduce the risk of losing historical context and make it easier to track changes over time.\n* However, this approach may require more effort to update existing decisions, as we will need to carefully document the changes and ensure that they are consistent with the original decision.\n\n**Rationale**\n\nBy amending existing architecture decisions rather than superseding them, we can maintain a clear and transparent record of our decision-making process. This will help to ensure that our software remains maintainable and scalable over time, and that future developers can easily understand the reasoning behind our design choices.","GenTime":"2024-08-10 18:35:23"}
{"File Name":"adr-viewer\/0006-accessibility-as-a-first-class-concern.md","Context":"## Context\\nThis tool had, up until this point, made assumptions about how its users might interpret the information it presents (for example, using colour as the main mechanism of distinguishing record types)\\n","Decision":"Accessibility will now be a first-class concern of this project. All future design decisions should bear this in mind.\\n","tokens":40,"id":3815,"Prediction":"## Decision\n\n**Decision:** Introduce a configuration mechanism to allow users to customize the presentation of information, rather than making assumptions about how users will interpret the information.\n\n**Context:** This tool had, up until this point, made assumptions about how its users might interpret the information it presents (for example, using colour as the main mechanism of distinguishing record types).\n\n**Problem:** The tool's assumptions may not align with the actual needs and preferences of its users, leading to confusion, frustration, or even incorrect interpretation of the presented information.\n\n**Reasoning:** By introducing a configuration mechanism, we can empower users to tailor the presentation of information to their individual needs and preferences, ensuring a more effective and efficient use of the tool. This approach also promotes accessibility, as users with disabilities can configure the tool to better suit their needs.\n\n**Consequences:**\n\n* Users will have more control over the presentation of information, leading to improved usability and satisfaction.\n* The tool will be more accessible to a broader range of users, including those with disabilities.\n* The tool's maintainability and extensibility will increase, as users can customize the presentation without requiring changes to the tool's core functionality.\n* The tool's complexity will increase, as users will need to understand the configuration options and how to use them effectively.\n\n**Alternatives Considered:**\n\n* Continuing to make assumptions about how users will interpret the information, which may lead to user frustration and confusion.\n* Providing a fixed set of presentation options, which may not cater to the diverse needs and preferences of users.\n* Creating a separate, user-specific configuration for each user, which may be impractical and difficult to maintain.\n\n**Status:** Accepted\n\n**Implementation:** The configuration mechanism will be implemented as a separate module, allowing users to customize the presentation of information through a user-friendly interface. The module will be designed to be extensible, allowing developers to add new configuration options and features in the future.","GenTime":"2024-08-10 18:36:03"}
{"File Name":"log430-dashview-architecture\/0002-use-java-to-implement-complete-solution.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3820,"Prediction":"Here is a sample Architectural Decision Record:\n\n**Decision**\n\nWe will implement a caching layer to store frequently accessed data in our application.\n\n**Context**\n\nThe issue motivating this decision is the performance degradation of our application when handling a large volume of requests. Our current implementation relies heavily on database queries, which can lead to significant delays and increased latency. Additionally, our application is expected to handle a significant increase in traffic in the near future, which will further exacerbate the issue.\n\nThe context that influences or constrains this decision is the need to balance the trade-off between performance, scalability, and maintainability. We must ensure that the caching layer does not introduce additional complexity or overhead that would negatively impact the overall system.\n\n**Problem**\n\nOur current implementation of the application relies too heavily on database queries, which can lead to performance issues and decreased user satisfaction.\n\n**Constraints**\n\n* The application must continue to operate with the existing database schema and data model.\n* The caching layer must be easy to implement and maintain.\n* The caching layer must be able to handle a significant increase in traffic without introducing additional performance issues.\n\n**Decision Rationale**\n\nImplementing a caching layer will allow us to reduce the number of database queries and improve the overall performance of the application. By caching frequently accessed data, we can reduce the load on the database and improve response times. Additionally, a caching layer will provide a layer of abstraction between the application and the database, making it easier to maintain and update the system.\n\n**Trade-offs**\n\nThe main trade-off of implementing a caching layer is the added complexity and overhead of managing the cache. However, we believe that the benefits of improved performance and scalability outweigh the costs.\n\n**Consequences**\n\nImplementing a caching layer will require additional development time and resources. However, it will also improve the overall performance and scalability of the application, which will have a positive impact on user satisfaction and business goals.\n\n**Monitoring and Feedback**\n\nWe will monitor the performance and scalability of the application after implementing the caching layer and gather feedback from users and stakeholders. This will allow us to identify any issues or areas for improvement and make adjustments as needed.","GenTime":"2024-08-10 18:37:45"}
{"File Name":"meadow\/0011-yarn.md","Context":"## Context\\nWe discussed the relative merits of different JS package managers.\\n","Decision":"Use `yarn` instead of `npm` in all dev, test, and build environments.\\n","tokens":15,"id":3827,"Prediction":"## Decision\n\n**Decision:** We will use npm (Node Package Manager) as our JavaScript package manager for the project.\n\n**Context:** We discussed the relative merits of different JS package managers, including npm, yarn, and pnpm. After considering the pros and cons of each option, we have decided to stick with npm as our package manager for the project.\n\n**Rationale:** npm is the default package manager for Node.js and has a large ecosystem of packages and tools. It is widely supported and has a large community of developers who contribute to and maintain packages. While yarn and pnpm have some advantages, such as faster installation times and improved dependency management, they also have some limitations and potential issues with compatibility and integration with other tools and systems.\n\n**Consequences:**\n\n* We will use npm to manage our project's dependencies and packages.\n* We will use the `package.json` file to define our project's dependencies and scripts.\n* We will use npm's built-in commands, such as `npm install` and `npm run`, to manage our project's dependencies and execute scripts.\n* We will ensure that our project's dependencies are up-to-date and compatible with each other.\n\n**Alternatives Considered:** We considered using yarn or pnpm as alternative package managers, but ultimately decided that npm was the best fit for our project.","GenTime":"2024-08-10 18:40:05"}
{"File Name":"meadow\/0009-tailwind-css-framework.md","Context":"## Context\\nWe have used opinionated CSS frameworks such as Twitter Bootstrap in the past, and have found that we spend too much effort and time working around those opinions. Tailwind CSS offers an alternative approach that allows us to iterate quickly with minimal interference from the framework by allowing us to add layout and styles directly in our HTML rather than CSS.\\n","Decision":"Use the Tailwind CSS framework for design and layout.\\n","tokens":70,"id":3829,"Prediction":"## Decision\n\n**Decision:** Adopt Tailwind CSS as our primary CSS framework, abandoning the use of opinionated CSS frameworks like Twitter Bootstrap.\n\n**Context:** As we have previously experienced, using opinionated CSS frameworks like Twitter Bootstrap has led to excessive time and effort spent working around their constraints, rather than focusing on building our application.\n\n**Rationale:**\n\n* Tailwind CSS offers a more flexible and customizable approach to styling, allowing us to add layout and styles directly in our HTML rather than CSS.\n* This flexibility enables us to iterate quickly and make changes without being constrained by the framework's opinions.\n* By using Tailwind CSS, we can maintain more control over our application's design and layout, reducing the need for workarounds and custom CSS.\n\n**Consequences:**\n\n* Initial learning curve for developers familiar with Bootstrap's syntax and structure.\n* Potential need for customizing or overriding Tailwind's default classes to fit our specific use case.\n* May require adjustments to our existing CSS architecture and workflow.\n\n**Alternatives Considered:**\n\n* Continuing to use Twitter Bootstrap, despite the limitations and constraints it imposes.\n* Exploring other CSS frameworks that offer a similar balance between flexibility and ease of use.\n\n**Decision Drivers:**\n\n* The need for a more flexible and customizable approach to styling.\n* The desire to reduce the time and effort spent working around the constraints of an opinionated CSS framework.\n* The potential for increased productivity and efficiency in our development workflow.","GenTime":"2024-08-10 18:40:41"}
{"File Name":"meadow\/0018-preservation-storage-object-naming-scheme.md","Context":"## Context\\nThe application needs a way to store objects in preservation buckets that facilitate upload and retrieval and allows for duplicate file names among FileSets associated with a Work object.\\n","Decision":"Use a combination of the pairtree of a FileSet ULID plus its sha256 checksum as the key for S3 objects in the preservation bucket. So an FileSet with a ULID `01dpxt2xajvkdsbckqrs8ry677`\\nand sha256 checksum `87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`will be stored in s3 as\\n`s3:\/\/[preservation-bucket]\/01\/dp\/xt\/2x\/01dpxt2xajvkdsbckqrs8ry677\/87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`\\n","tokens":35,"id":3831,"Prediction":"## Decision\n\n**Unique Object Identification for Preservation Buckets**\n\n## Context\n\nThe application needs a way to store objects in preservation buckets that facilitate upload and retrieval, and allows for duplicate file names among FileSets associated with a Work object.\n\n## Problem\n\nThe application requires a mechanism to store objects in preservation buckets, which involves uploading and retrieving files. However, the requirement to allow for duplicate file names among FileSets associated with a Work object poses a challenge. Traditional approaches to identifying files, such as using file names, may lead to conflicts and difficulties in retrieving the correct files.\n\n## Solution\n\nTo address this challenge, we will introduce a unique object identification mechanism for preservation buckets. This mechanism will generate a unique identifier for each object stored in the preservation bucket, regardless of the file name. This unique identifier will serve as a reference to the object, allowing for efficient retrieval and management of files.\n\n## Rationale\n\nThe use of a unique object identification mechanism provides several benefits:\n\n1.  **Conflict Resolution**: By using a unique identifier, we can avoid conflicts that may arise from duplicate file names among FileSets associated with a Work object.\n2.  **Efficient Retrieval**: The unique identifier enables efficient retrieval of files, as it provides a consistent and reliable reference to the object.\n3.  **Scalability**: The unique object identification mechanism can be easily scaled to handle large volumes of data and high traffic, as it is decoupled from the file name.\n\n## Consequences\n\n1.  **Additional Complexity**: The introduction of a unique object identification mechanism may add complexity to the system, as it requires additional processing and storage to manage the unique identifiers.\n2.  **Changes to File Management**: The use of unique object identifiers will require changes to the file management system, as it will need to be designed to handle the new mechanism.\n\n## Alternatives Considered\n\n1.  **Using File Names as Identifiers**: This approach would lead to conflicts and difficulties in retrieving the correct files, as mentioned earlier.\n2.  **Using a Hash of the File Content**: While this approach would provide a unique identifier for each file, it would not be suitable for our use case, as it would not allow for efficient retrieval of files.\n\n## Decision\n\nWe will implement a unique object identification mechanism for preservation buckets, using a combination of a UUID (Universally Unique Identifier) and a hash of the file content. This approach will provide a unique and reliable reference to each object, allowing for efficient retrieval and management of files, while also accommodating duplicate file names among FileSets associated with a Work object.","GenTime":"2024-08-10 18:41:38"}
{"File Name":"meadow\/0029-npm.md","Context":"## Context\\nThe latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.\\nSupersedes [11. Yarn](0011-yarn.md)\\n","Decision":"Switch back to `npm` instead of `yarn` in all dev, test, and build environments.\\n","tokens":37,"id":3841,"Prediction":"## Decision\n\n**Upgrade to a different package manager**\n\n## Context\n\nThe latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.\n\n## Problem\n\nThe issues with the latest Yarn upgrade are causing significant disruptions to our development workflow and are impacting our ability to deliver high-quality software in a timely manner.\n\n## Decision\n\nWe have decided to upgrade to a different package manager, specifically npm, to mitigate the issues caused by the latest Yarn upgrade.\n\n## Rationale\n\nAfter careful consideration, we have determined that the benefits of using npm outweigh the costs of switching package managers. npm is a widely-used and well-established package manager that is known for its stability and reliability. Additionally, many of our dependencies are already compatible with npm, making the transition easier.\n\n## Consequences\n\n* We will need to update our project configuration to use npm instead of Yarn.\n* We will need to re-run our build and test scripts to ensure that they are compatible with npm.\n* We may need to re-install some dependencies that were installed using Yarn.\n\n## Status\n\nThis decision has been made and is currently being implemented.\n\n## References\n\n* [npm documentation](https:\/\/docs.npmjs.com\/)\n* [Yarn documentation](https:\/\/yarnpkg.com\/en\/docs\/)","GenTime":"2024-08-10 18:44:58"}
{"File Name":"meadow\/0012-websockets.md","Context":"## Context\\nWe need a way to provide live updates to the front-end for ingest sheet validation,\\ningest status, etc.\\n","Decision":"We will use the [WebSocket API](https:\/\/www.w3.org\/TR\/websockets\/) via [Phoenix Channels](https:\/\/hexdocs.pm\/phoenix\/channels.html) to enable real-time communication between the client and server.\\n","tokens":27,"id":3845,"Prediction":"## Decision\n\n**Title:** Implement WebSockets for Real-time Updates in the Front-end\n\n**Context:** We need a way to provide live updates to the front-end for ingest sheet validation, ingest status, etc.\n\n**Problem:** Currently, we are using a polling mechanism to fetch updates from the server, which can be inefficient and lead to delays in updating the front-end. We need a more efficient and scalable solution to provide real-time updates to the front-end.\n\n**Decision:** We will implement WebSockets in our application to establish a bi-directional communication channel between the server and the front-end. This will enable us to push updates from the server to the front-end in real-time, eliminating the need for polling and reducing the latency in updating the front-end.\n\n**Rationale:**\n\n* WebSockets provide a low-latency, bi-directional communication channel between the server and the front-end, making it an ideal solution for real-time updates.\n* WebSockets can handle a large number of concurrent connections, making it scalable for our application.\n* Implementing WebSockets will reduce the load on the server and improve the overall performance of the application.\n\n**Consequences:**\n\n* We will need to implement WebSocket support on the server-side using a WebSocket library or framework.\n* We will need to update the front-end code to establish a WebSocket connection and handle incoming updates.\n* We will need to implement a mechanism to broadcast updates to all connected clients.\n\n**Alternatives Considered:**\n\n* Polling: We could continue using polling to fetch updates from the server, but this would lead to delays and inefficiencies.\n* Long Polling: We could use long polling to reduce the latency, but this would still require the front-end to periodically fetch updates from the server.\n* Server-Sent Events (SSE): We could use SSE to push updates from the server to the front-end, but this would require the front-end to establish a separate connection for each update.\n\n**Monitoring and Feedback:**\n\n* We will monitor the performance and scalability of the WebSocket implementation to ensure it meets our requirements.\n* We will gather feedback from users and stakeholders to ensure that the real-time updates are meeting their expectations.\n\nBy implementing WebSockets, we will be able to provide a more efficient and scalable solution for real-time updates in our application.","GenTime":"2024-08-10 18:46:23"}
{"File Name":"meadow\/0022-iiif-manifests.md","Context":"## Context\\nCurrently requests for public manifests are routed through Donut in order to be re-cached in S3 if needed. We need to plan a strategy to handle IIIF manifest requests for manifests which were written by Meadow and should not be routed through Donut.\\n","Decision":"- Manifests will be written for all works (public\/private\/restricted) on create\/update.\\n- IIIF Manifests will be written on to the `public` directory of the existing stack-\\*-pyramids bucket, and will live alongside existing Donut manifests.\\n- Requests for public manifests moving forward will route from the API gateway to a new lambda which will check Elasticsearch for the host application. Then it will either route directly to the S3 Pyramids `\/public` folder (Meadow) or to Donut. (This is temporary, until Donut content is migrated.)\\n- Meadow will use these manifests internally\\n","tokens":55,"id":3852,"Prediction":"## Decision\n\n**Handle IIIF Manifest Requests for Meadow-Written Manifests without Routing through Donut**\n\n## Context\n\nAs described, we currently route requests for public manifests through Donut to re-cache them in S3 if needed. However, we need to develop a strategy to handle IIIF manifest requests for manifests written by Meadow, which should not be routed through Donut.\n\n## Problem\n\nThe current routing mechanism through Donut is not suitable for IIIF manifest requests for Meadow-written manifests, as it would introduce unnecessary latency and overhead. We need a more efficient and optimized approach to handle these requests.\n\n## Decision\n\nTo address this issue, we will create a separate route for IIIF manifest requests that bypass Donut and directly access the Meadow-written manifests. This will allow us to reduce latency and improve performance for these specific requests.\n\n## Rationale\n\nBy creating a separate route for IIIF manifest requests, we can:\n\n1. Reduce latency: By avoiding the additional step of routing through Donut, we can reduce the latency associated with these requests.\n2. Improve performance: By directly accessing the Meadow-written manifests, we can improve the overall performance of the system.\n3. Simplify the architecture: By separating the routing for IIIF manifest requests, we can simplify the overall architecture and reduce complexity.\n\n## Consequences\n\n1. We will need to implement a new route for IIIF manifest requests that bypasses Donut.\n2. We will need to update the routing logic to direct IIIF manifest requests for Meadow-written manifests to the new route.\n3. We may need to adjust the caching mechanism for IIIF manifest requests to ensure that they are properly cached and served efficiently.\n\n## Status\n\nThis decision is approved and will be implemented in the next release.","GenTime":"2024-08-10 18:48:54"}
{"File Name":"meadow\/0007-code-analysis.md","Context":"## Context\\nWe need to make sure we adhere to our own designated code quality best practices.\\n","Decision":"Use a code analysis tool (specifically, [credo](http:\/\/credo-ci.org\/) for Elixir\\nand [prettier](https:\/\/prettier.io\/) for JavaScript).\\n","tokens":20,"id":3853,"Prediction":"## Decision\n\n**Title:** Enforcing Code Quality Best Practices through Automated Tools\n\n**Context:** We need to make sure we adhere to our own designated code quality best practices.\n\n**Problem:** As our codebase grows, it becomes increasingly challenging to maintain consistency in code quality, leading to potential issues with maintainability, scalability, and overall reliability.\n\n**Decision:** We will implement automated tools to enforce our designated code quality best practices, ensuring that all new and existing code meets the required standards.\n\n**Rationale:**\n\n1. **Consistency**: Automated tools will help maintain consistency in code quality, reducing the likelihood of errors and making it easier for developers to understand and maintain the codebase.\n2. **Efficiency**: By automating code quality checks, we can reduce the time spent on manual reviews and debugging, allowing developers to focus on writing new features and improving the overall quality of the codebase.\n3. **Scalability**: As the codebase grows, automated tools will help ensure that new code is written with the same level of quality, making it easier to scale and maintain the system.\n\n**Implementation:**\n\n1. **Code Analysis Tools**: We will integrate code analysis tools, such as linters, formatters, and code smell detectors, to identify and report potential issues in the codebase.\n2. **Continuous Integration\/Continuous Deployment (CI\/CD)**: We will configure our CI\/CD pipeline to run automated code quality checks on every pull request and deployment, ensuring that all changes meet the required standards.\n3. **Code Quality Metrics**: We will track and monitor code quality metrics, such as code coverage, cyclomatic complexity, and maintainability index, to identify areas for improvement and measure the effectiveness of our automated tools.\n\n**Consequences:**\n\n1. **Initial Overhead**: Implementing automated tools will require an initial investment of time and resources to set up and configure the tools.\n2. **Adaptation Period**: Developers may need to adapt to the new automated tools and processes, which may require additional training and support.\n3. **Improved Code Quality**: The long-term benefits of automated code quality enforcement will include improved code quality, reduced errors, and increased maintainability.\n\n**Alternatives Considered:**\n\n1. **Manual Code Reviews**: While manual code reviews are effective, they are time-consuming and prone to human error.\n2. **Code Quality Guidelines**: While code quality guidelines are essential, they are not enough to ensure consistent code quality across the codebase.\n\n**Conclusion:** By implementing automated tools to enforce our designated code quality best practices, we can ensure consistency, efficiency, and scalability in our codebase, ultimately leading to improved code quality and reduced errors.","GenTime":"2024-08-10 18:49:23"}
{"File Name":"meadow\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3854,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\n**ADR-001: Choose a Microservices Architecture for the Project**\n\n**Context**\n\nWe need to record the architectural decisions made on this project.\n\n**Problem**\n\nAs the project grows, it becomes increasingly difficult to manage and maintain a monolithic architecture. The system is becoming more complex, and the risk of introducing technical debt is high. We need to make a decision on the architecture of the system to ensure it remains scalable, maintainable, and efficient.\n\n**Decision**\n\nWe will adopt a Microservices Architecture (MSA) for the project. This means breaking down the system into smaller, independent services that communicate with each other using APIs. Each service will be responsible for a specific business capability, and will be developed, deployed, and scaled independently.\n\n**Rationale**\n\nThe Microservices Architecture allows us to:\n\n* Scale individual services independently, reducing the risk of a single point of failure\n* Develop and deploy services in parallel, reducing the overall project timeline\n* Improve maintainability by allowing each service to be maintained and updated independently\n* Increase flexibility by allowing services to be developed using different programming languages and frameworks\n\n**Consequences**\n\n* Increased complexity in terms of communication between services\n* Potential for increased latency due to the need for service communication\n* Requires a more robust monitoring and logging strategy to ensure service reliability\n\n**Status**\n\nApproved\n\n**Created**\n\n2023-02-15\n\n**Updated**\n\n2023-02-15\n\nThis ADR provides a clear record of the architectural decision made on the project, including the problem, decision, rationale, and consequences. It also provides a status and update history for the decision.","GenTime":"2024-08-10 18:49:41"}
{"File Name":"android-guidelines\/0009-gradle-rules.md","Context":"## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n","Decision":"* Disable generating BuildConfig file if its not needed in module.\\n* Only enable *databinding* if you're going to use DataBinding in that module.\\n* Do not apply *kapt* plugin if you're not going to use.\\n* Do not create new variants other than *debug* and *release*.\\n* Use groovy scripts on *build.gradle* files.\\n","tokens":37,"id":3860,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context:\n\n**Decision**: Gradle Configuration Optimization\n\n**Context**: The number of modules in our project has increased, leading to longer build times. Enabling unused plugins and using the `gradle.kts` configuration file are contributing factors to these slower build times.\n\n**Problem Statement**: The current Gradle configuration is not optimized for our project's size and complexity, resulting in slower build times and decreased developer productivity.\n\n**Decision Rationale**: To address the issue of slow build times, we will optimize our Gradle configuration by disabling unused plugins and simplifying our `gradle.kts` configuration file. This will reduce the overhead of unnecessary plugin loading and configuration processing, resulting in faster build times.\n\n**Decision Description**: We will achieve this optimization by:\n\n1. Reviewing our project's dependencies and identifying unused plugins.\n2. Disabling these unused plugins in our `build.gradle` file.\n3. Simplifying our `gradle.kts` configuration file by removing unnecessary configurations and using Gradle's built-in defaults where possible.\n4. Implementing a more efficient build process by using Gradle's incremental build feature and caching dependencies.\n\n**Pros and Cons**:\n\nPros:\n\n* Faster build times, resulting in increased developer productivity.\n* Reduced overhead of unnecessary plugin loading and configuration processing.\n* Simplified Gradle configuration file, making it easier to maintain.\n\nCons:\n\n* Potential disruption to existing build processes and workflows.\n* May require additional testing to ensure that the optimized configuration does not introduce new issues.\n\n**Status**: Implemented\n\n**Created**: 2023-02-20\n\n**Updated**: 2023-02-20","GenTime":"2024-08-10 18:52:10"}
{"File Name":"android-guidelines\/0007-use-deprecated-annotation-with-description.md","Context":"## Context and Problem Statement\\nDeprecated annotation usages without any comment or suggestion makes no sense when using existing classes.\\n","Decision":"When we decide to use deprecated annotation in classes, we need to comment reason and alternative to that implementation.\\n","tokens":24,"id":3863,"Prediction":"## Decision\n\n**Title:** Remove deprecated annotation usages without comments or suggestions\n\n**Context:** We have been using deprecated annotations in our existing classes, but without any comments or suggestions on how to replace them. This has led to confusion among developers when trying to understand the codebase.\n\n**Problem Statement:** The deprecated annotation usages without comments or suggestions make no sense when using existing classes, causing confusion and potential errors.\n\n**Decision:** We will remove the deprecated annotation usages without comments or suggestions from our existing classes. This decision aims to simplify the codebase, reduce confusion, and improve maintainability.\n\n**Rationale:** Removing deprecated annotation usages without comments or suggestions will:\n\n1. Simplify the codebase by reducing unnecessary complexity.\n2. Reduce confusion among developers by eliminating unclear and outdated code.\n3. Improve maintainability by making it easier to understand and modify the code.\n\n**Alternatives Considered:**\n\n1. Leave the deprecated annotation usages as they are, hoping that someone will eventually update them.\n2. Update the deprecated annotation usages with comments or suggestions, but this would require significant effort and may not be feasible.\n\n**Consequences:**\n\n1. Simplified codebase with reduced complexity.\n2. Reduced confusion among developers.\n3. Improved maintainability and easier code modifications.\n\n**Implementation:** We will identify and remove the deprecated annotation usages without comments or suggestions from our existing classes. We will also document the changes and provide guidance on how to replace the deprecated annotations with alternative solutions.\n\n**Monitoring and Feedback:** We will monitor the codebase for any issues or errors that may arise from removing the deprecated annotation usages. We will also gather feedback from developers on the changes and make adjustments as needed.\n\n**Review and Approval:** This decision has been reviewed and approved by the [team\/management].","GenTime":"2024-08-10 18:53:24"}
{"File Name":"cygnus-infra\/0002-use-debian-as-a-host-os.md","Context":"## Context\\nI used the following points to make my decision:\\n- Ubuntu 20.04 uses a unique auto-installation method, which is generally nice, but completely non-transferable\\n- `snapd` remains controversial, but using Ubuntu systems forces that path more nowadays\\n- Debian should be relatively \"install and forget\"\\n- Centos is more opinionated about some things like how network configuration is done\\n- Centos doesn't do automatic security upgrades, at least not in the same way RHEL advertises\\n- I've successfully used Debian before for this role, before trying out Ubuntu again\\n- Remastering a Debian iso with the preseed configuration can result in a one-button install process\\n","Decision":"Use Debian buster as the hypervisor OS, with an automated installation via Debian preseeding.\\n","tokens":145,"id":3868,"Prediction":"## Decision\n\nGiven the context, I have decided to choose Debian as the operating system for our software. The decision was influenced by the following factors:\n\n* Ubuntu's unique auto-installation method, which is not transferable, and the controversy surrounding `snapd`.\n* Debian's reputation for being \"install and forget\", making it a more reliable choice for our software.\n* The experience of successfully using Debian in a similar role in the past.\n* The ability to remaster a Debian ISO with a preseed configuration, resulting in a one-button install process.\n\nIn contrast, CentOS has more opinionated configuration options, such as network configuration, and does not have automatic security upgrades, which may require additional maintenance and configuration.\n\nThis decision allows us to simplify the installation process, reduce the risk of compatibility issues, and ensure a reliable and maintainable environment for our software.","GenTime":"2024-08-10 18:54:58"}
{"File Name":"buddy-auth.jwt-oidc\/adr-001.md","Context":"# Context #\\nThe library currently assumes that the ID tokens are signed by a\\npublic key (and the code unconditionally tries to build the actual\\npublic key from the data we get from the ID Token issuer).\\nAt the same time, the OpenID Connect Core 1.0 specification states\\nthat:\\n> \"Symmetric signatures MUST NOT be used by public (non-confidential)\\n> Clients because of their inability to keep secrets\"\\n(see https:\/\/openid.net\/specs\/openid-connect-core-1_0.html#Signing).\\nAs we don't expect to support confidential clients, it seems safe to\\nignore Symmetric Keys when validating tokens.\\n# Decision #\\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","Decision":"Tokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","tokens":229,"id":3872,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nTokens signed with a Symmetric Key will always be marked as invalid (whether they are actually valid or not).\n\n**Context**\n\nThe library currently assumes that the ID tokens are signed by a public key (and the code unconditionally tries to build the actual public key from the data we get from the ID Token issuer). At the same time, the OpenID Connect Core 1.0 specification states that symmetric signatures MUST NOT be used by public (non-confidential) Clients because of their inability to keep secrets.\n\n**Status**\n\nAccepted.\n\n**Consequences**\n\nEven if the ID Token issuer is configured to use Symmetric keys to sign its tokens, and the library gets a perfectly valid token signed using one of such keys, the library will always return that the token is invalid.","GenTime":"2024-08-10 18:56:03"}
{"File Name":"bosh-bootloader\/0002-replace-go-bindata-with-packr2.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https:\/\/twitter.com\/francesc\/status\/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n","Decision":"Use [Packr2](https:\/\/github.com\/gobuffalo\/packr\/tree\/master\/v2) instead.\\n","tokens":126,"id":3873,"Prediction":"## Decision\n\nAfter careful consideration of the context, we have decided to replace go-bindata with a custom solution for bundling and processing data in our software.\n\n## Context\n\nAs described, the original go-bindata repository has changed ownership, and we have lost faith in the maintainers' intentions. Additionally, the complexity of go-bindata's development use cases, such as bundling data during test runs or final builds, has made it difficult to reason about and maintain.\n\n## Problem\n\nThe current reliance on go-bindata introduces uncertainty and risk to our software's stability and maintainability. The lack of control over the dependency's evolution and the difficulty in understanding its behavior make it an unacceptable risk for our project.\n\n## Solution\n\nTo mitigate these issues, we will implement a custom solution for bundling and processing data in our software. This approach will provide us with complete control over the implementation, allowing us to tailor it to our specific needs and ensure its maintainability.\n\n## Rationale\n\nBy replacing go-bindata with a custom solution, we will:\n\n1. Eliminate the risk associated with relying on an external dependency with uncertain maintenance and evolution.\n2. Gain complete control over the implementation, allowing us to tailor it to our specific needs and ensure its maintainability.\n3. Simplify the development process by eliminating the complexity and uncertainty introduced by go-bindata's development use cases.\n\n## Consequences\n\nThe replacement of go-bindata with a custom solution will require additional development effort to implement the new functionality. However, this investment will pay off in the long run by providing a more stable, maintainable, and reliable solution for our software.\n\n## Status\n\nThis decision is approved and will be implemented in the next release of our software.","GenTime":"2024-08-10 18:56:21"}
{"File Name":"gsp\/ADR004-tenant-isolation.md","Context":"## Context\\nThe two main isolation models for Kubernetes are:\\n* Namespaces within a single cluster\\n* Running multiple clusters\\nAll Service Teams currently have separate AWS accounts.\\nSome Service Teams have separate AWS accounts for separate environment (ie. Staging, Production etc)\\nMany Service Teams have micro-service architectures\\nSome Service Teams have unique network isolation requirements that may be hard to implement in a shared environment.\\nTo ensure \"smooth transition\" during a migration it would be preferable to have clusters deployed to Service Team's VPCs.\\nTo ensure separation of billing it would be preferable to deploy clusters to Service Team's AWS accounts.\\nTo ensure strong network\/compute isolation between Service Teams it would be preferable to deploy separate clusters for separate environments.\\n","Decision":"We will manage multiple Kubernetes clusters, one per environment per tenant.\\nThis should provide:\\n* Strong network\/compute isolation between service teams\\n* Clear separation of billing\\n* Ability to deploy into target VPCs to make piecemeal microservice migration practical\\n","tokens":153,"id":3889,"Prediction":"## Decision\n\n**Isolate Kubernetes Clusters by Service Team, Environment, and Network Requirements**\n\n## Context\n\nAs described in the context, the two main isolation models for Kubernetes are namespaces within a single cluster and running multiple clusters. Given the unique requirements of each Service Team, including separate AWS accounts, micro-service architectures, and network isolation needs, we have decided to isolate Kubernetes clusters by Service Team, Environment, and Network Requirements.\n\n## Problem\n\nThe current approach of having all Service Teams share a single cluster or namespace does not provide sufficient isolation, leading to potential conflicts, security risks, and billing issues. The need for strong network and compute isolation between Service Teams, as well as separate environments, necessitates a more granular approach to cluster deployment.\n\n## Decision\n\nTo address these concerns, we will deploy separate Kubernetes clusters for each Service Team, with each cluster isolated from others. This approach will ensure:\n\n1. **Separate AWS accounts**: Clusters will be deployed to Service Team's AWS accounts, allowing for accurate billing and cost allocation.\n2. **Network isolation**: Clusters will be deployed to Service Team's VPCs, providing strong network isolation between Service Teams and environments.\n3. **Environment isolation**: Separate clusters will be deployed for separate environments (e.g., Staging, Production), ensuring that each environment is isolated from others.\n4. **Service Team isolation**: Each Service Team will have its own cluster, allowing for independent management and configuration of their Kubernetes resources.\n\n## Consequences\n\nThis decision will require:\n\n1. **Additional infrastructure costs**: Deploying separate clusters will incur additional costs for infrastructure, such as AWS accounts, VPCs, and Kubernetes resources.\n2. **Increased complexity**: Managing multiple clusters will require more complex operations and maintenance procedures.\n3. **Improved security and isolation**: The isolation of clusters will provide stronger security and network isolation, reducing the risk of conflicts and security breaches.\n\n## Rationale\n\nThe benefits of this decision outweigh the costs. By isolating clusters by Service Team, Environment, and Network Requirements, we can ensure a smoother transition during migration, improve security and isolation, and provide a more scalable and flexible infrastructure for our Service Teams.","GenTime":"2024-08-10 19:01:15"}
{"File Name":"gsp\/ADR043-k8s-resource-access.md","Context":"## Context\\nSeveral different levels of access are required within a kubernetes cluster. The\\nGSP uses role based access control so these levels are granted to users and\\ngroups via roles.\\n","Decision":"We will create two levels of access within each namespace:\\n* Operator\\n* Auditor\\nThe Operator is a relatively permissive read-write role within the namespace.\\nDevelopers working on branches that are not part of the release process may be\\ngranted this role in certain namespaces. This is also the role the in-cluster\\nconcourse team for each namespace will be granted.\\nThe Auditor will be given to all authenticated users in the cluster. This should\\nallow for debugging of issues and incidents and basic remedial actions without\\nneeding formal escalation procedures.\\nThe complete list of resource permissions is given in Appendix A.\\n","tokens":40,"id":3890,"Prediction":"## Decision\n\n**Title:** Implement Role-Based Access Control (RBAC) using Kubernetes' built-in RBAC system\n\n**Context:** Several different levels of access are required within a Kubernetes cluster. The GSP uses role-based access control so these levels are granted to users and groups via roles.\n\n**Problem:** The GSP needs to ensure that users and groups have the necessary permissions to perform specific actions within the Kubernetes cluster.\n\n**Decision:** We will implement Role-Based Access Control (RBAC) using Kubernetes' built-in RBAC system. This will allow us to define roles and assign them to users and groups, granting them the necessary permissions to perform specific actions within the cluster.\n\n**Rationale:**\n\n* Kubernetes' built-in RBAC system provides a robust and scalable solution for managing access control.\n* Implementing RBAC will allow us to define fine-grained permissions and ensure that users and groups only have access to the resources they need.\n* This will improve the security and compliance of the GSP, as well as reduce the risk of unauthorized access or data breaches.\n\n**Consequences:**\n\n* We will need to define roles and assign them to users and groups.\n* We will need to ensure that the RBAC configuration is properly set up and maintained.\n* We will need to monitor and audit access to ensure that the RBAC configuration is working as expected.\n\n**Alternatives Considered:**\n\n* Implementing a custom access control system.\n* Using a third-party RBAC solution.\n\n**Status:** Accepted\n\n**Implementation:** The RBAC system will be implemented using Kubernetes' built-in RBAC system. Roles will be defined and assigned to users and groups using the `kubectl create role` and `kubectl create rolebinding` commands. The RBAC configuration will be maintained and monitored using the `kubectl get role` and `kubectl get rolebinding` commands.","GenTime":"2024-08-10 19:01:35"}
{"File Name":"gsp\/ADR025-ingress.md","Context":"## Context\\nWe currently have two [ingress][Ingress] systems:\\n* Istio (see [ADR019])\\n* nginx-ingress (see the old Ingress [ADR005])\\nIstio's [Virtual Service] records are essentially advanced `Ingress` records.\\nDo we need both?\\n","Decision":"No. We will use an [Istio Ingress Gateway](https:\/\/istio.io\/docs\/tasks\/traffic-management\/ingress\/ingress-control\/)\\n","tokens":69,"id":3905,"Prediction":"## Decision\n\n**ADR030: Simplify Ingress Systems**\n\n## Context\n\nWe currently have two Ingress systems: Istio and nginx-ingress. Istio's Virtual Service records are essentially advanced Ingress records. This has led to confusion and redundancy in our Ingress configuration.\n\n## Problem\n\nHaving two separate Ingress systems is causing issues with:\n\n* Complexity: Managing two separate systems increases the complexity of our infrastructure.\n* Redundancy: Both systems are serving the same purpose, leading to duplicated effort and potential inconsistencies.\n* Maintenance: It becomes challenging to maintain and troubleshoot issues with both systems.\n\n## Decision\n\nWe will simplify our Ingress systems by migrating to a single, unified Ingress solution. Istio will be the chosen solution, as its Virtual Service records provide more advanced features and better integration with our existing infrastructure.\n\n## Rationale\n\n* Istio provides more advanced features, such as traffic management, security, and observability, which are not available with nginx-ingress.\n* Istio's Virtual Service records provide a more flexible and scalable way to manage Ingress traffic.\n* Migrating to a single solution will simplify our infrastructure and reduce complexity.\n\n## Consequences\n\n* nginx-ingress will be deprecated and eventually phased out.\n* Existing nginx-ingress configurations will need to be migrated to Istio Virtual Service records.\n* Our team will need to learn and adapt to Istio's configuration and management.\n\n## Implementation\n\nThe implementation will involve:\n\n* Migrating existing nginx-ingress configurations to Istio Virtual Service records.\n* Configuring Istio to handle Ingress traffic.\n* Testing and validating the new Ingress setup.\n* Phasing out nginx-ingress and deprecating it.\n\n## Rollback Plan\n\nIf issues arise during the migration, we will roll back to the previous setup and reassess the decision. However, we anticipate that the benefits of a single, unified Ingress solution will outweigh the potential risks.","GenTime":"2024-08-10 19:06:02"}
{"File Name":"gsp\/ADR041-service-operated-policies.md","Context":"## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk\/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role\/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n","Decision":"* We will provision policy that gives full access for users to _use_ the\\nprovisioned service.\\n* We will avoid provisioning policy that allows users to create, destroy or\\nconfigure the provisioned service, so that this can remain the declarative\\ndomain of the service-operator.\\n","tokens":335,"id":3907,"Prediction":"## Decision\n\n**Decision:** We will use a combination of IAM roles and policies to manage access to provisioned resources, with a focus on least privilege and fine-grained permissions.\n\n**Context:** As described in the provided context, our service-operator allows service teams to provision various AWS services, including S3 buckets, and requires IAM to authorize how the provisioned service can be used. We need to define permissions for users to access and utilize the provisioned bucket while avoiding conflicts with the properties managed by the service operator's reconcile loop.\n\n**Problem:** We need to ensure that users have the necessary permissions to read and write objects in the provisioned bucket, while preventing them from altering the configuration managed by the service operator.\n\n**Reasoning:** By using a combination of IAM roles and policies, we can grant users the necessary permissions to access the provisioned bucket while restricting their ability to modify the configuration managed by the service operator. This approach allows us to implement least privilege and fine-grained permissions, ensuring that users only have access to the resources and actions they need to perform their tasks.\n\n**Decision Criteria:**\n\n1. Security: By granting users only the necessary permissions, we reduce the risk of unauthorized access or modification of the provisioned bucket.\n2. Flexibility: Using IAM roles and policies allows us to define fine-grained permissions and adapt to changing requirements.\n3. Maintainability: The service operator's reconcile loop can continue to manage the configuration of the provisioned bucket without interference from users.\n4. User Experience: Users can access and utilize the provisioned bucket without being overwhelmed by unnecessary permissions or restrictions.\n\n**Consequences:**\n\n1. Increased complexity: Managing IAM roles and policies requires additional configuration and maintenance.\n2. Improved security: By granting users only the necessary permissions, we reduce the risk of security breaches.\n3. Better user experience: Users can access and utilize the provisioned bucket without being restricted by unnecessary permissions.\n\n**Implementation:**\n\n1. Define IAM roles and policies for each provisioned resource (e.g., S3 bucket).\n2. Grant users the necessary permissions to access and utilize the provisioned bucket.\n3. Implement fine-grained permissions to restrict users' ability to modify the configuration managed by the service operator.\n4. Monitor and maintain IAM roles and policies to ensure they remain up-to-date and secure.\n\nBy adopting this approach, we can ensure that our service-operator provides a secure, flexible, and maintainable solution for provisioning and managing AWS resources.","GenTime":"2024-08-10 19:06:46"}
{"File Name":"gsp\/ADR019-service-mesh.md","Context":"## Context\\nVerify have a need to restrict exfiltration of data, enforce strict authentication between microservices and to use mutual TLS.\\nA service mesh gives us a way of meeting these needs.\\n### Option 1: Istio\\nIstio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, with few or no code changes in service code.\\nIstio supports services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, you then configure and manage Istio using its control plane functionality, which includes:\\n- Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.\\n- Fine-grained control of traffic behaviour with rich routing rules, retries, fail-overs, and fault injection.\\n- A pluggable policy layer and configuration API supporting access controls, rate limits and quotas.\\n- Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress.\\n- Secure service-to-service communication in a cluster with strong identity-based authentication and authorisation.\\nPros\/cons:\\n- an emerging standard (installed by default on GKE)\\n- a large community of contributors\\n### Option 2: AWS App Mesh (Istio from AWS)\\n[AWS App Mesh](https:\/\/aws.amazon.com\/app-mesh\/) is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications\\npros\/cons:\\n- Unavailable in London region\\n- Did not support automatic sidecar injection (meaning service teams would have to add lots of extra configuration to their Deployments)\\n- Appears to be abstraction over Istio\\n### Option 3: Linkerd 1.x & 2.0\\n[Linkerd](https:\/\/linkerd.io\/) is an ultra light service mesh for Kubernetes. It gives you observability, reliability, and security without requiring any code changes.\\nPros\/cons:\\n- 1.0 has a richer feature set but poorer kubernetes support\\n- 2.0 has a very minimal feature set but native kubernetes support\\n- Going through major rewrite for improved Kubernetes support\\n- Smaller community\\n- Fewer features around\\n","Decision":"We will use [Istio](https:\/\/istio.io\/) to provide a service mesh in the GDS Supported Platform.\\n","tokens":498,"id":3909,"Prediction":"Here is the Architectural Decision Record (ADR) for the given context:\n\n**ADR-001: Choosing a Service Mesh for Data Exfiltration Prevention and Mutual TLS Authentication**\n\n**Context:** Verify has a need to restrict exfiltration of data, enforce strict authentication between microservices, and use mutual TLS. A service mesh is required to meet these needs.\n\n**Decision:** We will use Istio as our service mesh.\n\n**Rationale:**\n\n* Istio is an emerging standard and is installed by default on GKE, which aligns with our existing infrastructure.\n* Istio has a large community of contributors, which ensures that we will have access to a wide range of documentation, support, and plugins.\n* Istio provides a rich set of features, including automatic load balancing, fine-grained control of traffic behavior, and secure service-to-service communication with strong identity-based authentication and authorization.\n* While AWS App Mesh is an option, its unavailability in the London region and lack of automatic sidecar injection make it a less attractive choice.\n* Linkerd, while a lightweight option, has a smaller community and fewer features compared to Istio.\n\n**Consequences:**\n\n* We will need to deploy Istio in our environment and configure it to meet our specific requirements.\n* We will need to ensure that our services are compatible with Istio and can be easily integrated with the service mesh.\n* We will need to monitor and maintain Istio to ensure that it continues to meet our needs.\n\n**Status:** Implemented\n\n**Created:** 2023-02-20\n\n**Updated:** 2023-02-20","GenTime":"2024-08-10 19:07:04"}
{"File Name":"gsp\/ADR022-logging.md","Context":"## Context\\nWe have various log sources:\\n* The EKS control plane (audit logs, api service, scheduler, control-manager)\\n* VPC Flow logs\\n* Load Balancer\\n* Pod\/Application logs\\n* CloudHSM\\nAll of these with the exception of the Pod\/Application logs are stored by AWS in [CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/).\\nWe would like a single storage location for indexing and search our logs for auditing and debugging purposes.\\nGDS currently have several common storage locations for logs:\\n* Logit.io (a SaaS ELK stack provider)\\n* Self hosted ELK stacks\\n* CloudWatch\\n* Splunk\\nOptions:\\n### Option 1:\\nWe could ship the Cloudwatch logs to logit.io using AWS lambda and ship the Pod\/Application logs to Logit.io using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Logit.io instance and would end up duplicating a large number of the logs in both CloudWatch and Logit.io\\n### Option 2:\\nWe could host a dedicate ELK stack (either in cluster or from AWS's managed offering) and ingest logs from both Pods and CloudWatch into the ELK stack. Managing ELK stacks has been a maintenance burden at GDS previously and this would require duplicating logs already stored in CloudWatch.\\n### Option 3:\\nWe could ship the Pod\/Application logs to CloudWatch using [fluentd](https:\/\/www.fluentd.org\/) and expose CloudWatch insights interface to users of the platform\\n### Option 4:\\nWe could ship the CloudWatch logs to Splunk using AWS lambda and ship the Pod\/Application logs to Splunk using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Splunk instance and would end up duplicating a large number of the logs in both CloudWatch and Splunk.\\n","Decision":"We will use [fluentd](https:\/\/www.fluentd.org\/) to ship pod\/application logs to [AWS CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/) to aggregate all platform\/application logs to avoid double spending on log storage.\\n","tokens":416,"id":3914,"Prediction":"## Decision\n\nAfter considering the options, we have decided to implement **Option 3: Ship the Pod\/Application logs to CloudWatch using fluentd and expose CloudWatch insights interface to users of the platform**.\n\n## Context\n\nAs mentioned in the context, we have various log sources, and all of them except Pod\/Application logs are stored in CloudWatch. We would like to have a single storage location for indexing and search our logs for auditing and debugging purposes. We currently have several common storage locations for logs, including Logit.io, self-hosted ELK stacks, CloudWatch, and Splunk.\n\n## Problem\n\nThe main problem is that we have multiple log sources, and each has its own storage location. We want to have a single location for indexing and searching our logs, but we also want to avoid duplicating logs and managing multiple storage locations.\n\n## Decision\n\nWe have decided to ship the Pod\/Application logs to CloudWatch using fluentd and expose CloudWatch insights interface to users of the platform. This approach has several advantages:\n\n* We can take advantage of the existing infrastructure and avoid duplicating logs in multiple locations.\n* We can leverage the scalability and reliability of CloudWatch.\n* We can provide a single interface for users to access and search all logs, without requiring them to have their own Logit.io or Splunk instance.\n\n## Consequences\n\nBy implementing this decision, we will:\n\n* Reduce the complexity of managing multiple storage locations.\n* Improve the scalability and reliability of our logging infrastructure.\n* Provide a single interface for users to access and search all logs.\n* Avoid duplicating logs in multiple locations.\n\nHowever, this decision also means that we will need to implement fluentd to ship the Pod\/Application logs to CloudWatch, which may require additional development and testing efforts.","GenTime":"2024-08-10 19:08:38"}
{"File Name":"gsp\/ADR032-sre-permissions.md","Context":"## Context\\nAs mitigation for some risks raised in threat modelling it was agreed that day-to-day access to the cluster was to be read-only for everyone. Only the concourse running in the cluster could make changes that originated from Github, which required several approvals before merging.\\nFollowing the gradual rollout of several applications onto the GSP it became clear there were issues with the deployment procedures. This caused conflicting and contending pods to attempt to execute, resulting in application failures and deployment pipeline blockages. This was happening up to several times a day, depending on the level of activity. The remedial procedure involves escalating one or more members to cluster admin to allow the resources to be deleted, before revoking the admin permissions again. This process requires 3 people to perform and could result in hours of wasted time for each occurrence.\\n","Decision":"We will add to the SRE permissions map the ability to delete the following higher-level controllers so an escalation to cluster admin is no longer necessary:\\n* ConfigMap\\n* Deployment\\n* ReplicaSet\\n* Secret\\n* Service\\n* StatefulSet\\nWe will also raise a story to investigate the root cause of the deployment issues with a view to removing these permissions in the future.\\n","tokens":166,"id":3915,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nTo implement a centralized, automated deployment management system to ensure efficient and reliable deployment of applications on the GSP cluster, while maintaining the read-only access to the cluster for non-concourse users.\n\n**Context**\n\nAs mitigation for some risks raised in threat modelling, it was agreed that day-to-day access to the cluster was to be read-only for everyone. Only the concourse running in the cluster could make changes that originated from Github, which required several approvals before merging. Following the gradual rollout of several applications onto the GSP, it became clear there were issues with the deployment procedures, causing conflicting and contending pods to attempt to execute, resulting in application failures and deployment pipeline blockages. This was happening up to several times a day, depending on the level of activity.\n\n**Problem**\n\nThe current manual escalation process to delete resources and revoke admin permissions requires 3 people to perform, resulting in hours of wasted time for each occurrence. This process is inefficient and prone to errors, and the lack of automation leads to deployment failures and downtime.\n\n**Solution**\n\nImplement a centralized, automated deployment management system that:\n\n1. Monitors the cluster for conflicts and contending pods.\n2. Automatically detects and resolves deployment failures.\n3. Ensures that only approved changes are made to the cluster.\n4. Provides a single point of control for deployment management.\n5. Reduces the need for manual intervention and escalation.\n\n**Rationale**\n\nBy implementing a centralized, automated deployment management system, we can:\n\n1. Improve deployment reliability and reduce downtime.\n2. Increase efficiency and reduce the time spent on manual escalation procedures.\n3. Enhance security by ensuring that only approved changes are made to the cluster.\n4. Simplify deployment management and reduce the risk of human error.\n\n**Success Metrics**\n\n1. Reduction in deployment failures and downtime.\n2. Increase in efficiency and reduction in time spent on manual escalation procedures.\n3. Improvement in security and reduction in risk of human error.\n\n**Implications**\n\nThis decision will require changes to the current deployment procedures and the implementation of a new automated deployment management system. It will also require training and support for the new system.","GenTime":"2024-08-10 19:09:02"}
{"File Name":"gsp\/ADR028-container-tools.md","Context":"## Context\\nWe are currently using Docker as our container runtime.\\nThere are needs for public docker images:\\n* so that master builds can be easily pulled and tested in the local development environments\\n* so that images can be easily shared between different teams\\nThere are needs for digitally verifying the provenance of images:\\n* so that production systems can trust that an image has not been tampered with\\n* so that production systems can authenticate the origin of a build\\nThere are needs for vulnerability scanning:\\n* so that production systems can warn or prevent exploitable software running in production\\nThe docker ecosystem provides tooling that can help us meet these needs:\\n* [Docker Content Trust] (Notary) can be used to sign images and prove provenance\\n* [Docker Registries][Docker Registry] can expose images publicly\\n* Scanning tools like [Clair] can periodically or at pull\/push time perform CVE scanning.\\nUnfortunately AWS [ECR] does not currently support public images or [Docker Content Trust], and there is no managed solution to image scanning from AWS as yet.\\n### Option 1: Wrap AWS ECR\\nWe could write\/manage a proxy to allow exposing [ECR] publicly and integrate the missing features.\\n* Potentially fragile implementation tied to the underlying AWS services\\n* Another thing to have to maintain\\n* Might offer ability to remove functionality as AWS support more features in future\\n### Option 2: Use an external SaaS offering\\nUse a SaaS service like [Quay] which offers most of these features.\\n* Additional configuration for cluster to pull from external source\\n* Reduces ability to automate provisioning (requires additional credential management)\\n* Procurement\\n### Option 3: Self-hosted Docker Tools in cluster\\nWe could deploy Docker Distribution, Notary & Clair into the cluster backed by a managed storage backend like S3\\n* Well integrated with the platform\\n* Would work for local GSP instance\\n","Decision":"We will run a self hosted set of Docker tools\\n","tokens":406,"id":3917,"Prediction":"## Decision\n\nAfter careful consideration of the options, we have decided to implement Option 3: Self-hosted Docker Tools in cluster. This approach allows us to integrate the required features (publicly exposing images, digitally verifying provenance, and vulnerability scanning) with our existing infrastructure, while also providing a high degree of control and customization.\n\nBy deploying Docker Distribution, Notary, and Clair into the cluster, we can ensure that our images are publicly accessible, digitally verified, and scanned for vulnerabilities. This approach also allows us to leverage our existing AWS infrastructure, including S3 for managed storage.\n\nWhile Option 1 (Wrapping AWS ECR) would provide a more tightly integrated solution with AWS, it would also require additional development and maintenance efforts. Option 2 (Using an external SaaS offering) would provide a more streamlined solution, but would require additional configuration and credential management, as well as procurement.\n\nOption 3, on the other hand, strikes a balance between the two, providing a high degree of control and customization while also leveraging our existing infrastructure. Additionally, this approach would allow us to automate provisioning and management of the Docker tools, making it easier to maintain and scale our infrastructure.\n\n### Rationale\n\n* Control and customization: By self-hosting the Docker tools, we have full control over the configuration and customization of the solution, allowing us to tailor it to our specific needs.\n* Integration with existing infrastructure: By leveraging our existing AWS infrastructure, we can reduce the complexity and overhead of managing additional services.\n* Scalability: Self-hosting the Docker tools allows us to scale the solution as needed, without being limited by the constraints of a SaaS offering.\n* Automation: By automating provisioning and management of the Docker tools, we can reduce the overhead of manual configuration and management.\n\n### Next Steps\n\n* Design and implement the self-hosted Docker tools in the cluster, including Docker Distribution, Notary, and Clair.\n* Configure the tools to integrate with our existing infrastructure and provide the required features.\n* Automate provisioning and management of the Docker tools to ensure scalability and ease of maintenance.\n* Monitor and test the solution to ensure it meets our requirements and is secure and reliable.","GenTime":"2024-08-10 19:09:44"}
{"File Name":"gsp\/ADR011-build-artefacts.md","Context":"## Context\\nAs part of our pipelines we will be building artefacts that will be used to test\\nand deploy our applications. We will be deploying applications to Kubernetes. We\\nwill need to build a container image of some kind.\\nThere are some competing container image formats, namely:\\n* [OCI]\\n* [ACI]\\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\\nThe Kubernetes project appears to [prefer Docker\/OCI][k8s-preferance] images\\nover ACI.\\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\\nimage format.\\nDocker has wide industry adoption and appears to have wide understanding within\\nGDS.\\nDocker is the default container runtime for Kubernetes.\\n","Decision":"We will build and store OCI images built using Docker.\\n","tokens":170,"id":3920,"Prediction":"## Decision\n\n**Decision:** We will use the Open Container Initiative (OCI) image format for building container images that will be used to test and deploy our applications to Kubernetes.\n\n**Rationale:**\n\n* OCI is based on the Docker v2 image format, which is widely adopted and well-understood in the industry.\n* Kubernetes, the target platform for our deployments, prefers Docker\/OCI images over ACI.\n* rkt, another container runtime, is also moving to OCI and away from ACI.\n* Docker is the default container runtime for Kubernetes, and its wide adoption and understanding within GDS make it a safe choice.\n* OCI is expected to become the preferred image format in the future, making it a good long-term choice.\n\n**Consequences:**\n\n* We will need to ensure that our build process is configured to produce OCI images.\n* We may need to update our testing and deployment scripts to work with OCI images.\n* We will benefit from the wide industry adoption and understanding of Docker and OCI.\n\n**Alternatives Considered:**\n\n* Using ACI images, which would require more effort to implement and maintain due to its lower adoption and understanding within GDS.\n* Using a different container image format, which would require additional research and evaluation to determine its suitability for our use case.\n\n**Status:** Implemented","GenTime":"2024-08-10 19:10:24"}
{"File Name":"tech-radar\/adr-4-structure_of_principle.md","Context":"# Context\\nWe need consistency in terms of format and structure for our patterns across the customer facing, integration and other architectures.\\nWe are also keen to link back to business strategy directly so we can isolate points for consideration in design, implementsation and assessment.\\n# Decision\\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","Decision":"We propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","tokens":382,"id":3925,"Prediction":"## Decision\n\nThe decision is to propose a standardized structure for principle artefacts across the customer facing, integration, and other architectures, which includes:\n\n* Context\n* Where possible, linking to URI's for business strategy identifying bullet points the principle is designed to promote\n* Problems\n* Rationale\n* Examples\n* Discussion Points\n* Case-by-case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\n\nThis structure aims to provide consistency in format and structure for patterns, while also linking back to business strategy to isolate points for consideration in design, implementation, and assessment.\n\n## Status\n\nProposed\n\n## Consequences\n\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\n\n## Example\n\n* Independently deployable services\n* Context\n* (Link to URI for business strategy)\n* Problems\n* Respond to change quickly\n* Availability\n* Scalability\n* Rationale\n* This could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\n* This allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\n* Examples\n* Discussion Points\n* Not Applicable\n* TODO: Given scenario X, contact TRAP (email hyperlink).","GenTime":"2024-08-10 19:12:15"}
{"File Name":"dos-capacity-status-api\/002-shared_database.md","Context":"## Context\\nThis is not an external API to DoS, it is part of the DoS ecosystem. The operation of the API is updating the capacity of services in DoS, for visible in the DoS UI and applications that use Core DoS data.\\n","Decision":"The relevant content of the database is shared between the Core DoS application and the Capacity Status API. The decision was made to have the API use and update the Core DoS database. This approach is the most logical solution at this time, as any other alternative would likely need to incorporate some kind of interim internal API between the Capacity Status API and the Core DoS database. Having an interim API would be replicating functionality of the Capacity Status API and would therefore be redundant.\\n","tokens":54,"id":3937,"Prediction":"## Decision\n\n**API Capacity Update Service**\n\n## Context\n\nAs part of the DoS ecosystem, the API is responsible for updating the capacity of services, which is crucial for visibility in the DoS UI and applications that utilize Core DoS data.\n\n## Problem\n\nThe current implementation of the API capacity update service is prone to DoS attacks, which could compromise the stability and security of the system.\n\n## Decision\n\nTo mitigate this risk, we will implement a rate limiting mechanism on the API capacity update service. This will ensure that the API can only be updated at a certain rate, preventing malicious actors from overwhelming the system with requests.\n\n## Rationale\n\nImplementing rate limiting on the API capacity update service will:\n\n* Prevent DoS attacks by limiting the number of requests that can be made to the API within a certain time period.\n* Ensure the stability and security of the system by preventing excessive loads on the API.\n* Allow for more efficient resource allocation and better performance of the system.\n\n## Consequences\n\n* The API capacity update service will be more resilient to DoS attacks.\n* The system will be more stable and secure.\n* The rate limiting mechanism may require additional configuration and monitoring to ensure optimal performance.\n\n## Status\n\nThis decision is approved and will be implemented in the next release of the DoS ecosystem.\n\n## Links\n\n* [API Capacity Update Service Documentation](link)\n* [Rate Limiting Configuration](link)\n\nNote: This is just a sample decision record, you should adjust it according to your specific needs and requirements.","GenTime":"2024-08-10 19:16:17"}
{"File Name":"ensemblejs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3940,"Prediction":"Here is an Architectural Decision Record (ADR) for the project:\n\n## Decision\n\n**Use a Microservices Architecture**\n\n## Context\n\nAs we continue to develop the project, we need to ensure that our system is scalable, maintainable, and fault-tolerant. After considering various architectural options, we have decided to adopt a microservices architecture.\n\n## Problem\n\nThe current monolithic architecture of the system is becoming increasingly difficult to maintain and scale. With the increasing complexity of the system, it is becoming harder to make changes without affecting other parts of the system. Additionally, the system is becoming a single point of failure, making it vulnerable to outages.\n\n## Decision\n\nWe have decided to break down the system into smaller, independent services, each with its own responsibility. This will allow us to:\n\n* Develop and deploy each service independently, without affecting other parts of the system.\n* Scale each service separately, based on its specific needs.\n* Improve fault tolerance by allowing each service to fail without affecting the entire system.\n* Increase maintainability by reducing the complexity of each service.\n\n## Consequences\n\n* Increased complexity in the short term, as we need to develop and manage multiple services.\n* Improved scalability and fault tolerance in the long term.\n* Easier maintenance and updates to individual services.\n* Potential for increased communication and coordination between teams, as each service may require input from multiple teams.\n\n## Rationale\n\nWe have chosen a microservices architecture because it aligns with the project's goals of scalability, maintainability, and fault tolerance. It also allows for greater flexibility and autonomy for each team, as they can develop and deploy their services independently.\n\n## Status\n\nThis decision is currently in the implementation phase. We will continue to monitor the system's performance and make adjustments as needed.\n\n## Links\n\n* [Link to relevant design documents or diagrams]\n* [Link to relevant code or configuration files]\n\nI hope this helps! Let me know if you have any questions or need further clarification.","GenTime":"2024-08-10 19:17:15"}
{"File Name":"verify-self-service\/0003-use-pundit-for-rbac.md","Context":"## Context\\nAs with any modern web system there is a need to secure the application with\\nstrong authentication and Role Based Access Control (RBAC). RBAC allows us to\\ncreate permissions, apply them to roles and associates roles with users. From\\nthis we can restrict what users can do and see within the application based on\\ntheir role. In addition to this we also had to investigate where to store role\\ninformation and define teams to support our use of RBAC.\\n","Decision":"After some investigation and discussion into various ways of implementing RBAC\\nwithin Ruby we have decided to implement the Pundit Ruby gem within the Verify\\nSelf Service application. This gem provides us with a way of creating policies\\nwhich apply to the app as a whole and to individual classes within the\\napplication.\\nWe have chosen pundit for a number of reasons. The first is that it seems\\nrelatively easy to implement within our application. A proof of concept was\\ncreated in a few hours which restricted creating new components within the\\napplication to those who hold the `gds` or `dev` roles. Policies and permissions\\nare defined in code as plain Ruby objects which are easy to understand and\\nsimple to implement. This ease of implementation and an appearance of providing\\nthe fuctionality we are seeking we believe makes pundit a good candidate for\\nimplementing RBAC within our application.\\nIn addition to this we are aware of prior-art use within GDS on the GOV.UK\\nproject. Finally we have found a lot of documentation and articles about how to\\nimplement and use pundit.\\nRole information we have decided to store in a custom field within our cognito\\ninstance. This will take the form of a comma seperated list which will be split\\ninto an array of roles for use within the application. This method allows us to\\nkeep all use information together within AWS Cognito and means that the only way\\nrole information can be changed is via SDK calls to AWS Cognito.\\nFinally we understand there is a need to define teams within our application. We\\nhave decided that a team is analogous to an AWS Cognito group. This allows a\\nuser to be part of one more groups and for a group to hold one or more users. We\\ncan easily query group membership using the AWS Cognito SDK and check for a\\nusers membership to a specific group.\\n","tokens":97,"id":3950,"Prediction":"## Decision\n\nTo implement strong authentication and Role-Based Access Control (RBAC) in our web application, we will use a combination of OAuth 2.0 for authentication and a custom-built RBAC system using a relational database to store role information and team definitions.\n\n## Context\n\nAs mentioned in the context, our web application requires strong authentication and Role-Based Access Control (RBAC) to restrict user access to certain features and data. We also need to store role information and define teams to support our use of RBAC.\n\n## Problem\n\nThe problem we are trying to solve is how to implement a secure and scalable authentication and authorization system that meets the requirements of our web application.\n\n## Decision\n\nAfter evaluating different options, we have decided to use OAuth 2.0 for authentication and a custom-built RBAC system using a relational database to store role information and team definitions. This approach will provide a secure and scalable solution that meets our requirements.\n\n## Rationale\n\nWe chose OAuth 2.0 for authentication because it is a widely-used and well-established standard for authentication that provides a secure and flexible way to authenticate users. We will use a custom-built RBAC system because it will allow us to tailor the system to our specific requirements and provide a high level of flexibility and customization.\n\nWe will store role information and team definitions in a relational database because it will provide a scalable and flexible way to store and manage this data. This will also allow us to easily integrate the RBAC system with other parts of our application.\n\n## Consequences\n\nThe consequences of this decision are that we will need to implement a custom-built RBAC system and integrate it with our application. This will require additional development time and resources. However, this approach will provide a secure and scalable solution that meets our requirements and provides a high level of flexibility and customization.\n\n## Alternatives Considered\n\nWe considered alternative approaches such as using an existing RBAC system or using a different authentication protocol. However, after evaluating these options, we determined that a custom-built RBAC system using OAuth 2.0 for authentication was the best approach for our specific requirements.","GenTime":"2024-08-10 19:20:52"}
{"File Name":"PerformanceTestDocs\/0006-vellum-node-fails.md","Context":"## Context\\nVellum node fails\\n","Decision":"Vellum is always addressed by its cluster name and all of its data is stored in distributed databases with replicas of data on multiple nodes.\\n","tokens":9,"id":3952,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\nWe will implement a retry mechanism for Vellum node failures to ensure high availability and reliability of our system.\n\n**Context**\n\nVellum node failures have been occurring frequently, causing downtime and impacting our system's overall performance. We need to find a solution to mitigate these failures and ensure that our system remains available and reliable.\n\n**Problem**\n\nThe frequent failures of Vellum nodes are causing issues with our system's functionality, leading to:\n\n* Downtime: Users are unable to access our system during node failures, resulting in lost productivity and revenue.\n* Data loss: Uncommitted transactions may be lost during node failures, leading to data inconsistencies and potential data loss.\n* Increased maintenance: Frequent node failures require manual intervention, increasing maintenance costs and time.\n\n**Decision**\n\nTo address these issues, we will implement a retry mechanism for Vellum node failures. This mechanism will:\n\n* Detect node failures and automatically retry failed requests\n* Limit the number of retries to prevent infinite loops\n* Log failed requests and node failures for debugging and monitoring purposes\n\n**Consequences**\n\n* Increased system availability and reliability\n* Reduced downtime and data loss\n* Improved user experience\n* Increased maintenance costs due to the need for monitoring and debugging\n\n**Rationale**\n\nThe retry mechanism will help to reduce the impact of Vellum node failures on our system, ensuring that our system remains available and reliable. By detecting and retrying failed requests, we can minimize downtime and data loss, and provide a better user experience.\n\n**Status**\n\nThis decision is approved and will be implemented in the next release.\n\n**References**\n\n* [Link to relevant documentation or resources]\n\n**Related Decisions**\n\n* [Link to related ADRs or decisions]","GenTime":"2024-08-10 19:21:28"}
{"File Name":"PIMS\/database.md","Context":"## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n","Decision":"Originally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\\nThis will give us better performance and tighter integration with Entity Framework Core.\\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\\n","tokens":49,"id":3966,"Prediction":"## Decision\n\n**Decision:** We will use PostgreSQL as the relational database management system to store property information in PIMS, and deploy it within a Linux Docker container on OpenShift. We will also utilize Entity Framework Core 3.1 for data access and manipulation.\n\n## Rationale\n\n* PostgreSQL is a popular, open-source relational database management system that is well-suited for storing and managing relational data. Its support for Linux and Docker containers makes it an ideal choice for our deployment requirements.\n* By deploying PostgreSQL within a Linux Docker container on OpenShift, we can ensure a consistent and portable environment for our database, while also taking advantage of OpenShift's scalability and management features.\n* Entity Framework Core 3.1 is a widely-used and well-maintained ORM (Object-Relational Mapping) framework that provides a robust and efficient way to interact with our PostgreSQL database. Its support for.NET Core 3.1 ensures that our application can take advantage of the latest features and performance improvements.\n\n## Trade-offs\n\n* While PostgreSQL is a powerful and feature-rich database management system, it may require additional configuration and tuning to achieve optimal performance.\n* Deploying PostgreSQL within a Docker container on OpenShift may introduce additional complexity and overhead, particularly in terms of networking and storage configuration.\n\n## Consequences\n\n* We will need to configure and tune our PostgreSQL database to ensure optimal performance and reliability.\n* We will need to ensure that our Docker container and OpenShift deployment configuration are properly set up and managed to ensure seamless operation.\n* We will need to consider additional security and access controls to ensure that our database is properly secured and protected.\n\nBy making this decision, we can ensure a robust and scalable database solution that meets our requirements and provides a solid foundation for our PIMS application.","GenTime":"2024-08-10 19:25:58"}
{"File Name":"PIMS\/geocoder.md","Context":"## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https:\/\/www2.gov.bc.ca\/gov\/content\/data\/geographic-data-services\/location-services\/geocoder)\\n- [Data BC](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service)\\n- [API Swagger](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service\/resource\/40d6411e-ab98-4df9-a24e-67f81c45f6fa\/view\/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys)\\n- API Host = `https:\/\/geocoder.api.gov.bc.ca`\\n","Decision":"Integrate with Data BC Geocoder API.\\nWhen a user types an address a list of viable matches will be displayed.\\nIf the user selects one of the matches it will be used to set the address and GIS coordinates,.\\n","tokens":262,"id":3968,"Prediction":"## Decision\n\n**Title:** Implement automatic latitude and longitude value setting using Data BC Geocoder for property addresses in inventory\n\n**Context:** Currently, latitude and longitude values for properties in inventory are set manually, which can lead to errors and inconsistencies. To improve data accuracy and user experience, we will integrate the Data BC Geocoder API to automatically retrieve valid GIS coordinates for property addresses.\n\n**Decision:** We will implement a feature that allows users to input a property address, which will then be used to retrieve the corresponding latitude and longitude values from the Data BC Geocoder API. This feature will improve data accuracy by reducing manual errors and provide a better user experience by automating the process.\n\n**Rationale:**\n\n* The Data BC Geocoder API provides a reliable and accurate source of GIS coordinates, which will improve the overall quality of our inventory data.\n* Automating the process of retrieving latitude and longitude values will reduce manual errors and increase efficiency.\n* The feature will also allow for address verification, which will help to ensure that addresses are accurate and valid.\n* The implementation of this feature will also provide a better user experience by reducing the burden of manual data entry and providing a more streamlined process.\n\n**Consequences:**\n\n* The implementation of this feature will require additional development time and resources.\n* We will need to obtain a developer API key from Data BC to access the Geocoder API.\n* We will need to integrate the Geocoder API with our existing inventory system, which may require changes to our database schema and data processing workflows.\n\n**Alternatives Considered:**\n\n* Manually entering latitude and longitude values for each property, which can lead to errors and inconsistencies.\n* Using a different geocoding service, which may not provide the same level of accuracy and reliability as the Data BC Geocoder API.\n\n**Status:** Accepted\n\n**Implementation:** The implementation of this feature will be done by [Name], with the following timeline and milestones:\n\n* Obtain developer API key from Data BC: [Date]\n* Integrate Geocoder API with inventory system: [Date]\n* Test and deploy feature: [Date]","GenTime":"2024-08-10 19:26:43"}
{"File Name":"smjs\/2018121201-stop-using-registers-in-processor.md","Context":"## Context\\nCommunication between processor and interpreter must go both ways: the processor needs to pass the interpreter the instruction to execute, and the interpreter is supposed to be able to update the Instruction Pointer (IP), and to tell the processor it the execution needs to stop, and with which exit status.\\nCurrently we are using an intermediate component, registers, partially like how real processors work, containing both the IP and the information about the exit status. However, the communication between the processor and the interpreter is so straightforward that a much simpler solution would be to just use classical call argument, and call return value. Registers could be used as a mediator, but for this to have sense the processor and the interpreter would have to be unaware of each other, and registers should implement a mechanism to automatically notify involved parties when a change is made, which seems overkill.\\n","Decision":"The interpreter will keep taking an instruction from the processor, but this time it will return a data structure, which tells if an exit has been triggered, and in this case what's the exit status.\\n","tokens":172,"id":3972,"Prediction":"## Decision\n\n**Decision:** Replace the intermediate registers component with a simple call-and-return interface between the processor and the interpreter.\n\n**Context:** As described, the current implementation uses an intermediate registers component to facilitate communication between the processor and the interpreter. However, the simplicity of the communication between the two components suggests that a more straightforward approach using classical call-and-return arguments and return values would be more effective.\n\n**Reasoning:**\n\n* The processor and the interpreter only need to communicate in a straightforward manner, making a call-and-return interface sufficient.\n* Using registers as a mediator would require the processor and the interpreter to be unaware of each other, which may not be necessary in this case.\n* Implementing a mechanism for automatic notification of changes in the registers would add unnecessary complexity.\n\n**Consequences:**\n\n* The code will be simplified and easier to understand.\n* The processor and the interpreter will have a more straightforward interface, reducing the risk of errors.\n* The system will be more efficient, as the overhead of using registers as a mediator will be eliminated.\n\n**Rationale:**\n\n* The simplicity of the communication between the processor and the interpreter justifies the use of a call-and-return interface.\n* The benefits of a simplified codebase and reduced error risk outweigh the potential drawbacks of using a call-and-return interface.\n\n**Alternatives Considered:**\n\n* Using registers as a mediator, with automatic notification of changes.\n* Implementing a more complex communication mechanism, such as a message queue.\n\n**Status:** Accepted\n\n**Implementation:** The processor and the interpreter will use a call-and-return interface to communicate, with the processor passing the instruction to execute and the interpreter updating the Instruction Pointer (IP) and returning an exit status if necessary.","GenTime":"2024-08-10 19:28:02"}
{"File Name":"smjs\/2018121601-multiple-usages-of-the-processor.md","Context":"## Context\\nThe domain our virtual machine is supporting is concerned with running programs: as such, how programs should be run is a business rule, and the Processor is the Domain Service encapsulating that rule.\\nAt the application level, we can think of several features that can support this domain:\\n- classic virtual machine, running an entire program, perhaps loaded from a file, until its termination\\n- REPL, where instructions are loaded one at a time from an interactive prompt, and executed, printing some kind of output each time\\n- debug, where we still have an entire program, but this time we want to support debugging features, like running step-by-step\\nOf course there could be many others. Can we use the same definition of a Processor in the domain to support all this cases?\\n","Decision":"We will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):\\n- in the classic virtual machine, the Program is the entire program, that is executed from start to end\\n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this doesn't concern the domain), and to make the program start at the latest instruction, we add a new unconditional jump instruction at the beginning of it: this actually means that we don't run the entire program again (it isn't necessary since memory is preserved), but only the last inserted instruction\\n- in the debug situation, the Interpreter implementation can avoid returning control back to the Processor until requested to: this still doesn't concern the Processor implementation in the domain\\n","tokens":161,"id":3981,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to use a single definition of the `Processor` domain service to support all the features mentioned, including classic virtual machine, REPL, debug, and potentially others.\n\n**Rationale:**\n\n1. **Single Source of Truth**: By using a single definition of the `Processor`, we ensure that the business rule of how programs should be run is encapsulated in a single place, making it easier to maintain and update.\n2. **Reusability**: A single `Processor` implementation can be reused across different features, reducing code duplication and increasing overall efficiency.\n3. **Consistency**: Using a single `Processor` definition ensures consistency in how programs are executed across different features, making it easier for users to understand and work with the system.\n4. **Flexibility**: A single `Processor` implementation can be extended to support new features and scenarios as needed, without requiring significant changes to the underlying architecture.\n\n**Trade-offs:**\n\n1. **Complexity**: A single `Processor` implementation may become more complex as it needs to support multiple features and scenarios, which may require additional abstraction and complexity.\n2. **Over-engineering**: If the `Processor` implementation is not designed with flexibility and scalability in mind, it may become overly complex and difficult to maintain.\n\n**Implementation:**\n\nTo support this decision, we will design the `Processor` domain service to be highly configurable and extensible. We will use abstraction and dependency injection to decouple the `Processor` from specific features and scenarios, allowing us to easily add or remove features as needed. We will also implement the `Processor` using a modular design, breaking it down into smaller, reusable components that can be easily extended or modified.\n\nBy using a single definition of the `Processor` domain service, we can create a more cohesive, maintainable, and scalable system that supports a wide range of features and scenarios.","GenTime":"2024-08-10 19:31:23"}
{"File Name":"smjs\/2018120802-implement-clean-architecture.md","Context":"## Context\\nThis project is meant to be a playground to experiment with different technologies and design solutions.\\n","Decision":"To standardize the project structure, now that it's getting bigger, we will implement the principles suggested by [clean architecture](https:\/\/github.com\/michelezamuner\/notes\/tree\/master\/software-architecture\/clean-architecture\/clean-architecture.martin), enhanced by concepts from Domain Driven Design, and plugin architectures.\\n### Domain\\nThe core domain is the \"virtual machine framework\", which only defines how programs should be executed, but doesn't specify any architecture, meaning that how programs are interpreted is not known.\\nA Program is a sequence of Data Units. Data Units are the kind of data with the smallest possible size, which we set at a single Byte. Data is always represented as a sequence of Data Units. Since we work with sequences, we also define the concepts of Size, which is the number of Data Units in a specific Data, and Address of a Data inside the Program, with the Address of the first Data Unit being 0. Both Size and Address are Integers, which is a generic integral type defined to be independent from the runtime environment implementation. A Program has the ability of fetching blocks of Data given their Address and Size.\\nA Program is run by a Processor, which uses an Interpreter, whose implementation is provided by the specific Architecture selected, to first define which sets of Data Units should be regarded as Instructions, and then to execute these Instructions. When running an Instruction, the Interpreter returns a Status object knowing if the execution should jump, or be terminated. The execution of a Program by a Processor always returns an Exit Status, which is Architecture-dependent. The termination of a Program must always be requested explicitly, via a dedicated instruction, otherwise an error is raised.\\nAn Interpreter must use the System to perform I\/O operations, and ultimately to allow a Program to communicate with the users; however, the implementation of the System depends on the actual application where the Processor and Interpreter are running, so it's left to be specified.\\nAdditional domains are defined for each architecture, so that a virtual machine can support many different architectures.\\nAssemblers and compilers also define their own domains.\\nThe following domains could thus be defined:\\n- `smf`: the core virtual machine framework domain\\n- `sma`: definitions for the SMA architecture domain\\n- `basm`: definitions for the BASM assembler domain\\n- `php`: definitions for the PHP compiler domain\\n### Application\\nThe application layer may define the following primary ports:\\n- the `vm` port allows to execute programs, according to the configured architecture\\n- the `repl` port allows to execute programs interactively, and uses the functionality of `vm`\\n- the `dbg` port allows to execute programs step by step for debugging, and uses the functionality of `vm`\\n- the `asm` port allows to run an assembler on some assembly code to produce executable object code\\n- the `cmp` port allows to run a compiler on some high level language to produce assembly code\\nAs far as secondary ports, we need the following:\\n- a `arcl` port allows the application to load an architecture definition\\n- a `pl` port allows the application to load a program\\n- a `asml` port allows the application to load assembly code, to be assembled\\n- a `cl` port allows the application to load high level code, to be compiled\\n- a `sys` port allows the application to interact with the underlying operating system\\n### Adapters\\nPrimary adapters might be defined to create command line applications, or Web applications. Secondary adapters might be defined to read data from files or from memory. See below for more concrete examples.\\n### Plugin architecture\\nWe want to support building different types of applications by composing together sets of different available plugins. For example:\\n**sloth machine (CLI)**\\nAD_sm + AD_larcl + AD_fpl + AD_ossys + AP_vm + AP_arcl + AP_pl + AP_sys + D_smf + D_sma (or others)\\n**sloth machine assembler (CLI)**\\nAD_asm + AD_fasml + AP_asm + AP_asml + D_basm (or others)\\n**sloth machine compiler (CLI)**\\nAD_cmp + AD_fcl + AD_masml + AP_cmp + AP_asm + AP_cl + AP_asml + D_basm (or others) + D_php (or others)\\n**sloth machine runner (CLI)**\\nAD_run + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_vm + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine REPL (CLI)**\\nAD_repl + AD_larcl + AD_mcl + AD_masml + AD_mpl + AD_ossys + AP_repl + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine debugger (CLI)**\\nAD_dbg + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_dbg + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine Web (Web)**\\nAD_web + AD_warcl + AD_wcl + AD_masml + AD_mpl + AD_wsys + AP_vm + AP_cmp + AP_asm + AP_repl + AP_dbg + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n- `D_smf`: Domain Sloth Machine Framework\\n- `D_sma`: Domain Sloth Machine Architecture\\n- `D_basm`: Domain Basic Assembly for Sloth Machine\\n- `D_php`: Domain PHP\\n- `AP_vm`: Application Virtual Machine (primary port)\\n- `AP_cmp`: Application Compiler (primary port)\\n- `AP_asm`: Application Assembler (primary port)\\n- `AP_repl`: Application REPL (primary port)\\n- `AP_dbg`: Application Debugger (primary port)\\n- `AP_arcl`: Application Architecture Loader (secondary port)\\n- `AP_pl`: Application Program Loader (secondary port)\\n- `AP_asml`: Application Assembly Loader (secondary port)\\n- `AP_cl`: Application Code Loader (secondary port)\\n- `AP_sys`: Application System (secondary port)\\n- `AD_sm`: Adapter Sloth Machine (primary adapter)\\n- `AD_cmp`: Adapter Compiler (primary adapter)\\n- `AD_run`: Adapter Runner (primary adapter)\\n- `AD_repl`: Adapter REPL (primary adapter)\\n- `AD_dbg`: Adapter Debugger (primary adapter)\\n- `AD_web`: Adapter Web (primary adapter)\\n- `AD_larcl`: Adapter Local Architecture Loader (secondary adapter)\\n- `AD_warcl`: Adapter Web Architecture Loader (secondary adapter)\\n- `AD_fpl`: Adapter File Program Loader (secondary adapter)\\n- `AD_mpl`: Adapter Memory Program Loader (secondary adapter)\\n- `AD_fasml`: Adapter File Assembly Loader (secondary adapter)\\n- `AD_masml`: Adapter Memory Assembly Loader (secondary adapter)\\n- `AD_fcl`: Adapter File Code Loader (secondary adapter)\\n- `AD_mcl`: Adapter Memory Code Loader (secondary adapter)\\n- `AD_wcl`: Adapter Web Code Loader (secondary adapter)\\n- `AD_ossys`: Adapter OS System (secondary adapter)\\n- `AD_wsys`: Adapter Web System (secondary adapter)\\n### Example modules\\n```\\ndomain\\nsmf\\ndata\\nDataUnit: (Byte)\\nData: DataUnit[]\\nSize: (Integer)\\nAddress: (Integer)\\nprogram [data]\\nProgram\\nProgram(data.Data)\\nread(data.Address, data.Size): data.Data\\ninterpreter [data]\\nOpcode: data.Data\\nOperands: data.Data\\nExitStatus: (Integer)\\nInstruction\\nInstruction(Address, Opcode, Operands)\\ngetAddress(): Address\\ngetOpcode(): Opcode\\ngetOperands(): Operands\\nStatus\\nshouldJump(): (Boolean)\\ngetJumpAddress(): data.Address\\nshouldExit(): (Boolean)\\ngetExitStatus(): ExitStatus\\n<Interpreter>\\ngetOpcodeSize(): data.Size\\ngetOperandsSize(Opcode): data.Size\\nexec(Instruction): Status\\nprocessor [program, interpreter]\\nProcessor\\nProcessor(interpreter.<Interpreter>)\\nrun(program.Program): interpreter.ExitStatus\\narchitecture [data, interpreter]\\n<System>\\nread(data.Integer fd, data.Size size): data.Data\\nwrite(data.Integer fd, data.Data data, data.Size size): data.Size\\n<Architecture>\\ngetInterpreter(<System>): interpreter.<Interpreter>\\nsma [smf]\\nInterpreter: smf.interpreter.<Interpreter>\\nInterpreter(smf.architecture.<System>)\\napplication\\nvm\\nrun_program [domain.smf, application.arcl, application.pl, application.sys]\\n<Request>\\ngetArchitectureName(): String\\ngetProgramReference(): String\\nResponse\\ngetExitStatus(): domain.smf.interpreter.ExitStatus\\n<Presenter>\\npresent(Response)\\nRunProgram\\nRunProgram(ProcessorFactory, <Presenter>, application.arcl.<ArchitectureLoader>, application.pl.<ProgramLoader>, application.sys.<System>)\\nexec(<Request>)\\nProcessorFactory\\ncreate(domain.smf.interpreter.<Interpreter>): domain.smf.processor.Processor\\narcl [domain.smf]\\n<ArchitectureLoader>\\nload(architectureName: String): domain.smf.architecture.<Architecture>\\npl [domain.smf]\\n<ProgramLoader>\\nload(programReference: String): domain.smf.program.Program\\nsys [domain.smf]\\n<System>: domain.smf.architecture.<System>\\nadapters\\nsm [application.vm, domain.smf]\\nrun_program [application.vm, domain.smf]\\nRequest: application.vm.run_program.<Request>\\nController\\nController(application.vm.run_program.RunProgram)\\nrunProgram(Request)\\nViewModel\\nViewModel(domain.smf.interpreter.<ExitStatus>)\\ngetExitStatus(): <native-integer>\\n<View>\\nrender(ViewModel)\\nExitStatusView: <View>\\nrender(ViewModel)\\ngetExitStatus(): <native-integer>\\nPresenter: application.vm.run_program.<Presenter>\\nPresenter(<View>)\\npresent(application.vm.run_program.Response)\\nlarcl [application.arcl]\\nLocalArchitectureLoader: application.arcl.<ArchitectureLoader>\\nfpl [application.pl]\\nFileProgramLoader: application.pl.<ProgramLoader>\\nossys [application.sys]\\nOSSystem: application.sys.<System>\\n```\\n### Examples of main implementations\\n```\\nmodule domain.smf.processor\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Operands\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.interpreter.Status\\nclass Processor\\nProcessor(<Interpreter> interpreter)\\nthis.interpreter = interpreter\\nrun(Program program): ExitStatus\\nSize opcodeSize = interpreter.getOpcodeSize()\\nAddress address = 0\\nwhile (true)\\nOpcode opcode = program.read(address, opcodeSize)\\nSize operandsSize = interpreter.getOperandsSize(opcode)\\nAddress operandsAddress = address + opcodeSize\\nOperands operands = program.read(operandsAddress, operandsSize)\\nInstruction instruction = new Instruction(address, opcode, operands)\\nStatus status = interpreter.exec(instruction)\\nif (status.shouldExit())\\nreturn status.getExitStatus()\\naddress = status.shouldJump() ? status.getJumpAddress() : operandsAddress + operandsSize\\n\/\/ @todo: handle missing exit\\n```\\n```\\nmodule domain.sma.interpreter\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.interpreter.Status\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.architecture.<System>\\nimport domain.sma.InstructionSet\\nimport domain.sma.Result\\nimport domain.sma.JumpResult\\nimport domain.sma.ExitResult\\nimport domain.sma.InstructionDefinition\\nclass Interpreter: <Interpreter>\\nInterpreter(InstructionSet instructionSet, <System> system)\\nthis.instructionSet = instructionSet\\nthis.system = system\\ngetOpcodeSize(): Size\\nreturn new Integer(1)\\ngetOperandsSize(Opcode opcode): Size\\nreturn instructionSet.getInstructionDefinition(opcode).getOperandsSize()\\nexec(Instruction instruction): Status\\nAddress jumpAddress = null\\nAddress exitStatus = null\\nInstructionDefinition definition = instructionSet.getInstructionDefinition(instruction.getOpcode())\\nResult result = definition.exec(instruction.getOperands())\\nif (result instanceof JumpResult)\\njumpAddress = result.getJumpAddress()\\nif (result instanceof ExitResult)\\nexitStatus = result.getExitStatus()\\nreturn new Status(jumpAddress, exitStatus)\\n```\\n```\\nmodule application.vm.run_program\\nimport application.vm.run_program.ProcessorFactory\\nimport application.vm.run_program.<Presenter>\\nimport application.arcl.<ArchitectureLoader>\\nimport application.pl.<ProgramLoader>\\nimport application.sys.<System>\\nimport application.vm.run_program.<Request>\\nimport domain.smf.architecture.<Architecture>\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.processor.Processor\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport application.vm.run_program.Response\\nclass RunProgram\\nRunProgram(\\nProcessorFactory processorFactory,\\n<Presenter> presenter,\\n<ArchitectureLoader> architectureLoader,\\n<ProgramLoader> programLoader,\\n<System> system\\n)\\nthis.processorFactory = processorFactory\\nthis.presenter = presenter\\nthis.architectureLoader = architectureLoader\\nthis.programLoader = programLoader\\nthis.system = system\\nexec(<Request> request)\\n<Architecture> architecture = architectureLoader.load(request.getArchitectureName())\\n<Interpreter> interpreter = architecture.getInterpreter(this.system)\\nProcessor processor = processorFactory.create(interpreter)\\nProgram program = programLoader.load(request.getProgramReference())\\nExitStatus exitStatus = processor.run(program)\\npresenter.present(new Response(exitStatus))\\n```\\n```\\nmodule adapters.sm.run_program\\nimport application.vm.run_program.RunProgram\\nimport application.vm.run_program.<Request>\\nclass Controller\\nController(RunProgram service)\\nthis.service = service\\nrunProgram(<Request> request)\\nservice.run(request)\\n```\\n```\\nmodule adapters.sm.run_program\\nimport adapters.sm.run_program.<View>\\nimport application.vm.run_program.Response\\nimport adapters.sm.run_program.ViewModel\\nclass Presenter\\nPresenter(<View> view)\\nthis.view = view\\npresent(Response response)\\nViewModel viewModel = new ViewModel(response.getExitStatus())\\nview.render(viewModel)\\n```\\n","tokens":21,"id":3982,"Prediction":"## Decision\n\n**Decision:** To use a Microkernel Architecture for the playground project.\n\n**Context:** This project is meant to be a playground to experiment with different technologies and design solutions.\n\n**Problem:** The project requires flexibility to accommodate various technologies and design solutions, and the architecture should allow for easy experimentation and iteration.\n\n**Reasoning:** A Microkernel Architecture provides a lightweight and modular foundation for the project, allowing for the easy addition or removal of components as needed. This architecture enables us to decouple the core functionality from the specific implementations, making it easier to swap out technologies or design solutions as we experiment and learn.\n\n**Consequences:**\n\n* The project will have a more flexible and adaptable architecture, allowing for easier experimentation and iteration.\n* The microkernel will provide a clear separation of concerns, making it easier to understand and maintain the codebase.\n* The project will require more upfront planning and design to ensure that the microkernel is properly defined and implemented.\n\n**Alternatives Considered:** We considered using a Monolithic Architecture, but it would limit our flexibility and make it more difficult to experiment with different technologies and design solutions. We also considered using a Service-Oriented Architecture, but it would add complexity and overhead, which might not be necessary for a playground project.\n\n**Status:** This decision is approved and will be implemented in the project.\n\n**References:** None.","GenTime":"2024-08-10 19:31:38"}
{"File Name":"documents-api\/0002-use-base64-content-for-file-uploading.md","Context":"## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n","Decision":"We decided to use the first option (base 64 encoded uploads), for the following reason:\\n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.\\n","tokens":113,"id":3987,"Prediction":"## Decision\n\n**Decision:** We will use the S3 Presigned URL functionality to allow clients to upload files to S3.\n\n**Context:** As mentioned, there are several ways to allow clients to upload files to S3, including accepting Base 64 encoded files in a JSON POST request, accepting multipart form uploads, and using the S3 Presigned URL functionality.\n\n**Problem:** We need to decide which method to use for allowing clients to upload files to S3.\n\n**Reasoning:** We chose the S3 Presigned URL functionality for several reasons. Firstly, it is a secure and scalable solution, as it allows the client to act as the IAM which created the URL for a single operation, without having to store sensitive AWS credentials on the client-side. Secondly, it provides a more user-friendly experience, as the client can upload the file directly to S3 without having to wait for the API to process the file. Finally, it is a more efficient solution, as it reduces the amount of data that needs to be transferred between the client and the API.\n\n**Consequences:**\n\n* The API will generate a presigned URL for each file upload, which will be valid for a specified amount of time.\n* The client will use the presigned URL to upload the file directly to S3.\n* The API will not be responsible for storing or processing the file until it is uploaded to S3.\n* The client will be responsible for ensuring that the file is uploaded to the correct location and with the correct permissions.\n\n**Alternatives Considered:**\n\n* Accepting Base 64 encoded files in a JSON POST request: This method would require the API to store and process the file, which could lead to scalability issues.\n* Accepting multipart form uploads: This method would also require the API to store and process the file, and could lead to security issues if not implemented correctly.\n* Using a combination of the above methods: This method would require the API to handle multiple different file upload scenarios, which could lead to complexity and maintainability issues.\n\n**Decision History:**\n\n* The decision was made after considering the pros and cons of each method, and weighing the importance of security, scalability, and user experience.\n* The decision was reviewed and approved by the development team and stakeholders.\n\n**Future Development:**\n\n* The API will need to be modified to generate and manage presigned URLs for file uploads.\n* The API will need to be tested to ensure that it correctly generates and validates presigned URLs.\n* The API will need to be documented to ensure that clients understand how to use the presigned URL functionality.","GenTime":"2024-08-10 19:33:35"}
{"File Name":"decode-demo\/0003-remove-spectre-css-and-replace-with-bootstrap.md","Context":"## Context\\nThe deployed demo site must be designed to render nicely on mobile devices.\\nThis is so that participants in the pilot can pretend the demo site provides\\nthe same basic user experience as the wallet once this exists.\\nSpectre CSS at least in the configuration implemented in the original version\\nof this app doesn't render a UI that is usable from a mobile device,\\nparticularly when it comes to form inputs (buttons and fields were tiny and\\nhard to read).\\n","Decision":"We will replace [Spectre.CSS](https:\/\/picturepan2.github.io\/spectre\/) with\\n[Bootstrap](https:\/\/getbootstrap.com\/).\\n","tokens":98,"id":3999,"Prediction":"## Decision\n\nTo ensure the demo site is mobile-friendly and provides a similar user experience to the wallet, we will implement a responsive design using a CSS framework that is specifically designed for mobile-first development. Specifically, we will use Bootstrap's mobile-first approach, which allows us to write CSS that adapts to different screen sizes and devices.\n\n## Rationale\n\nUsing Bootstrap's mobile-first approach will allow us to:\n\n* Ensure that the demo site is easily readable and usable on mobile devices\n* Provide a consistent user experience across different devices and screen sizes\n* Reduce the need for separate mobile-specific code, making it easier to maintain and update the demo site\n* Take advantage of Bootstrap's pre-built responsive design components, such as navigation bars, forms, and grids, which are optimized for mobile devices\n\n## Consequences\n\n* We will need to update the existing CSS code to use Bootstrap's mobile-first approach, which may require significant changes to the existing codebase\n* We will need to test the demo site on different mobile devices and screen sizes to ensure that it renders correctly and is usable\n* We may need to make adjustments to the layout and design of the demo site to ensure that it is optimized for mobile devices\n\n## Status\n\nThis decision is currently in the \"proposed\" state. We will implement this decision and test the demo site on different mobile devices to ensure that it meets the requirements.","GenTime":"2024-08-10 19:37:51"}
{"File Name":"karma-sabarivka-reporter\/0001-documenting-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4002,"Prediction":"Here is a sample Architectural Decision Record (ADR) for the given context:\n\n## Decision\n\n**ADR-001: Database Schema Design for User Management**\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Problem\n\nThe project requires a robust user management system to handle user authentication, authorization, and data storage. The current implementation uses a simple JSON file for storing user data, which is not scalable and lacks data integrity.\n\n## Decision\n\nAfter evaluating various options, we have decided to use a relational database management system (RDBMS) with a schema designed specifically for user management. The chosen RDBMS is PostgreSQL, which provides robust support for SQL queries, transactions, and data integrity.\n\nThe database schema will consist of the following tables:\n\n* `users`: stores user information, including username, email, password, and role\n* `user_roles`: stores the many-to-many relationship between users and roles\n* `roles`: stores predefined roles, such as administrator, moderator, and user\n\nThe schema design will ensure data consistency, scalability, and ease of maintenance.\n\n## Rationale\n\nThe decision to use a RDBMS with a custom schema was based on the following factors:\n\n* Scalability: A RDBMS can handle large amounts of data and scale horizontally, making it suitable for a growing user base.\n* Data Integrity: The schema design ensures data consistency and integrity, reducing the risk of data corruption or loss.\n* Performance: The use of indexes, views, and stored procedures in the RDBMS will improve query performance and reduce the load on the application.\n* Maintainability: The schema design is modular and easy to maintain, making it easier to add new features or modify existing ones.\n\n## Status\n\nThis decision is currently in the \"implemented\" status, and the database schema has been designed and implemented.\n\n## Links\n\n* Database schema design document: [link]\n* PostgreSQL documentation: [link]\n\n## History\n\n* 2023-02-20: Initial decision made\n* 2023-02-22: Schema design finalized\n* 2023-03-01: Database schema implemented","GenTime":"2024-08-10 19:38:56"}
{"File Name":"apply-for-teacher-training\/0015-carrying-over-applications.md","Context":"## Context\\nThe current recruitment cycle ends on 18th September 2020. At that point there\\nwill be some candidates who could benefit from their application being carried\\nover to the next cycle. Carrying over an application means the candidate can\\napply to courses in the new recruitment cycle without having to fill in the\\nwhole application form again.\\n### Carrying over an application makes sense in the following states\\n#### Before the application reaches the provider\\nThese applications could be carried over because the provider has not seen them yet.\\n- Withdrawn\\n- Unsubmitted\\n- Ready to send to provider\\n#### After the application can\u2019t progress any further\\nThese applications could be carried over because they have reached an\\nunsuccessful end state. Enabling candidates to turn these into fresh applications\\nin the next cycle makes it as easy as possible for them to try again.\\n- Conditions not met\\n- Offer withdrawn\\n- Offer declined\\n- Application cancelled\\n- Rejected\\n### Carrying over an application does not make sense in the following states\\n#### While the application is already under consideration by the provider\\n- Awaiting provider decision\\n#### When the application already has an offer in flight\\n- Offer\\n- Meeting conditions (i.e. offer accepted)\\n- Recruited\\n### Copying the Apply again approach\\nThe current approach for moving applications into Apply again is to copy the\\nentire application (including references) and invite the user to add a new\\ncourse choice. This approach seems like it will work here too, with a couple of\\nextra things to take into account:\\n- applications that are carried over might be in Apply 1 or Apply again as the\\ncycle ends. All carried-over applications should start over as Apply 1\\napplications applications moving into Apply again all have complete\\nreferences because they\u2019ve already completed Apply 1, for which references\\nare required.\\n- Carried over applications might have no references, references in flight, or\\ncompleted references.\\nMoving the new application into the next cycle is a question of making sure its\\ncourse choices come from that cycle. As long as carrying over is only possible\\nonce the current cycle is closed, this should present no problems because the\\navailable courses will all come from the new cycle.\\n","Decision":"- We will only allow applications to be carried over once the current cycle is\\nover, and we\u2019ll only allow applications in the above states\\n- To carry over an application, we will adopt the Apply again pattern of\\ncloning the ApplicationForm and removing the courses\\n- We will copy references onto the carried-over application, but only if\\nthey\u2019re complete. Referees who had been contacted but had not responded\\nbefore the application was carried over will need to be cancelled.\\n- The applications that were carried over will remain in the database without\\nany further state change\\n- Applications which were not yet sent to the provider at end of cycle and also\\nnot carried over will still be in the database \u2014 we would like to mark these\\nwith a new state equivalent to \u201cincomplete at end of cycle\u201d. This state would\\nnever be visible to providers.\\n- It\u2019s up to the candidate whether to carry over their application, and we\u2019ll\\ngive them a button to do this\\n","tokens":467,"id":4010,"Prediction":"## Decision\n\nWe will implement a feature to carry over applications from the current recruitment cycle to the next cycle, allowing candidates to reuse their existing application information without having to fill out the entire form again. This feature will be implemented by copying the entire application, including references, and inviting the user to add a new course choice.\n\n## Context\n\nAs the current recruitment cycle ends on September 18th, 2020, there will be some candidates who could benefit from their application being carried over to the next cycle. Carrying over an application makes sense in certain states, such as before the application reaches the provider or after it has reached an unsuccessful end state. However, it does not make sense in states where the application is already under consideration by the provider or has an offer in flight.\n\n## Problem\n\nThe current approach for moving applications into Apply again is to copy the entire application and invite the user to add a new course choice. However, this approach has some limitations, such as not considering the state of the application and not handling the case where the application has no references, references in flight, or completed references.\n\n## Solution\n\nTo address these limitations, we will implement a feature that takes into account the state of the application and handles the different cases where the application has no references, references in flight, or completed references. The feature will also ensure that the course choices come from the new cycle, as long as carrying over is only possible once the current cycle is closed.\n\n## Rationale\n\nThe rationale for this decision is to provide a seamless experience for candidates who want to reuse their existing application information in the next recruitment cycle. By carrying over the application, we can reduce the effort required from the candidate and make it easier for them to try again in the next cycle.\n\n## Consequences\n\nThe consequences of this decision are:\n\n* Candidates will have a smoother experience when applying for courses in the next recruitment cycle.\n* The feature will require additional development and testing to ensure it works correctly in all scenarios.\n* The feature will need to be thoroughly tested to ensure it does not introduce any bugs or errors.\n\n## Alternatives\n\nThe alternative to this decision would be to not implement the feature to carry over applications, which would require candidates to start the application process from scratch in the next recruitment cycle. This would be a less user-friendly experience and may lead to a decrease in candidate engagement and conversion rates.","GenTime":"2024-08-10 19:41:21"}
{"File Name":"apply-for-teacher-training\/0010-cookie-sessions.md","Context":"## Context\\nThis application needs user sessions to allow users (candidates, providers, referees, support staff) to sign in. Rails offers a number of options for this. Each have a trade-off.\\n### Cookie sessions\\nAll session information is saved in a cookie. The cookie is encrypted to avoid the user changing or reading the data.\\nPro:\\n- It does not interact with other infrastructure\\n- It's the Rails default, well understood by developers\\nCon:\\n- When the user signs out, we do not invalidate the session. This means that if the user has made a copy of the cookie, they (or an attacker) can sign themselves back in.\\n- Users cannot sign out sessions on other devices (remote sign out)\\n### Storage based cookies\\nThis mechanism relies on a session ID being saved in a cookie. The session ID corresponds to a record either in a traditional database (PostgreSQL in our case) or in a caching service (Memcached, Redis).\\nPro:\\n- On sign out, the session is deleted and cannot be revived\\n- Sessions can be invalidated \"remotely\", to allow sign out of other devices\\nCon:\\n- Uses other infrastructure - slight performance overhead, risk of services being unavailable\\n- Sensitive data is stored in a database\\n","Decision":"Use session cookies.\\n","tokens":264,"id":4011,"Prediction":"## Decision\n\nAfter considering the trade-offs of each option, we have decided to use **Storage based cookies** for user sessions in our application.\n\n## Context\n\nAs described in the context, we need user sessions to allow users to sign in and sign out. We have considered two options: Cookie sessions and Storage based cookies. Each option has its pros and cons, and we have weighed these factors to make a decision.\n\n## Rationale\n\nWe have chosen Storage based cookies because they provide better security and flexibility compared to Cookie sessions. The main advantages of Storage based cookies are:\n\n* On sign out, the session is deleted and cannot be revived, which eliminates the risk of a user or an attacker signing themselves back in.\n* Sessions can be invalidated \"remotely\", allowing users to sign out of other devices, which is an important feature for our application.\n\nWhile Storage based cookies use other infrastructure, such as a database or caching service, this is a small price to pay for the increased security and flexibility they provide. We are willing to accept the slight performance overhead and risk of services being unavailable, as these are mitigated by the use of a reliable database and caching service.\n\nIn contrast, Cookie sessions do not provide the same level of security and flexibility, and are therefore not suitable for our application.\n\n## Consequences\n\nBy using Storage based cookies, we will need to ensure that our database and caching service are reliable and secure. We will also need to implement logic to invalidate sessions remotely, which will require additional development effort. However, these consequences are outweighed by the benefits of increased security and flexibility provided by Storage based cookies.\n\n## Status\n\nThis decision is approved and will be implemented in our application.","GenTime":"2024-08-10 19:41:40"}
{"File Name":"apply-for-teacher-training\/0003-initial-datamodel.md","Context":"## Context\\nIn the coming months we will be building this application. The first order of business is to come up with a sensible data model and application structure.\\n","Decision":"We discussed a database model that is based around a small number of tables with a lot of columns. In particular:\\n- `Candidate`, primarily used for authentication\\n- `ApplicationForm`, which belongs to a Candidate, and holds the\\ninformation that is common to all of the applications that the user\\nmakes (in the \"Apply 1\" phase, they can apply for up to three courses).\\n- `ApplicationChoice`, which holds the information that is course specific,\\nlike the status of the application and the personal statement. It holds a reference to the \"course choice\", which is the combination of course, provider and the training location.\\nThere will be some other models that hold references, work histories and qualifications.\\nThe eventual table structure will look something like this:\\n![data model](0003-initial-datamodel.png)\\n","tokens":33,"id":4021,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\nWe will use a Domain-Driven Design (DDD) approach to design the data model and application structure for our application.\n\n**Context**\n\nIn the coming months, we will be building this application. The first order of business is to come up with a sensible data model and application structure.\n\n**Problem**\n\nWe need to design a data model and application structure that accurately represents the business domain and is scalable, maintainable, and efficient.\n\n**Decision**\n\nAfter considering various approaches, we have decided to use Domain-Driven Design (DDD) to design the data model and application structure. This approach will allow us to create a clear and concise model of the business domain, which will be reflected in the application's architecture.\n\n**Rationale**\n\nUsing DDD will enable us to:\n\n* Create a clear and concise model of the business domain, which will help us to better understand the requirements and make more informed design decisions.\n* Design a data model that accurately represents the business domain and is scalable, maintainable, and efficient.\n* Create a flexible and adaptable application structure that can be easily extended and modified as the business requirements evolve.\n\n**Consequences**\n\n* We will need to spend some time upfront to learn and understand the DDD approach and its principles.\n* We will need to create a domain model that accurately represents the business domain, which may require some iteration and refinement.\n* We will need to ensure that the application structure is designed with scalability, maintainability, and efficiency in mind.\n\n**Status**\n\nApproved\n\n**Date**\n\n[Current Date]\n\n**Tags**\n\nDomain-Driven Design, DDD, Data Model, Application Structure","GenTime":"2024-08-10 19:43:55"}
{"File Name":"govuk-aws\/0009-environment-bootstrapping-process.md","Context":"## Context\\nWe need a process to bootstrap a new Amazon environment. This should be both heavily automated\\nand include set places where the operator can evaluate the status of the provisioning.\\nThis bootstrapping process assumes the following:\\n* The required repositories are available\\n* An Amazon Web Services admin account is available\\n* Backups of our data can be retrieved (for the restore process)\\n","Decision":"The chosen process has a few, early bootstrap steps, that differ from normal operations. These\\naim to quickly provide the usual level of self-service to allow teams to independently\\nrestore services without a dependency on a central operations team. An early draft of\\nthis process, which will be updated as changes are required, will be:\\n1. Clone all the relevant repositories\\n1. Build the S3 bucket for Terraform state\\n1. Provision the new DNS zones for this environment\\n1. Build the Puppet master\\n1. Deploy the puppet code and secrets\\n1. Build the deploy Jenkins\\n* Rebuild everything else in the usual deployment ways\\nThis document will be written so that one of the non-infrastructure members of the team\\nwill be able to provision a complete environment within our allowed recovery time.\\n","tokens":80,"id":4025,"Prediction":"## Decision\n\n**Decision:** Implement a Terraform-based automation process for bootstrapping a new Amazon environment, with interactive check points for the operator to evaluate the provisioning status.\n\n**Context:** As described, we need a process to bootstrap a new Amazon environment that is heavily automated and includes set places where the operator can evaluate the status of the provisioning. This process assumes the availability of required repositories, an Amazon Web Services admin account, and backups of our data for the restore process.\n\n**Problem:** Manually provisioning a new Amazon environment is time-consuming, prone to errors, and requires a high level of expertise. The process involves creating and configuring multiple resources, such as EC2 instances, RDS databases, and S3 buckets, which can be overwhelming for operators.\n\n**Rationale:** Terraform is a popular infrastructure as code (IaC) tool that allows us to define and manage our infrastructure in a declarative manner. By using Terraform, we can automate the provisioning process, ensure consistency across environments, and reduce the risk of human error. The interactive check points will provide the operator with a clear understanding of the provisioning status and allow them to intervene if necessary.\n\n**Decision Criteria:**\n\n1. Automation: Terraform allows us to automate the provisioning process, reducing the time and effort required to set up a new environment.\n2. Consistency: Terraform ensures consistency across environments by defining the infrastructure configuration in code, reducing the risk of human error.\n3. Interactivity: The interactive check points will provide the operator with a clear understanding of the provisioning status, allowing them to intervene if necessary.\n4. Reusability: The Terraform configuration can be reused across environments, reducing the time and effort required to set up new environments.\n\n**Consequences:**\n\n1. Increased automation: The Terraform-based automation process will reduce the time and effort required to set up a new environment.\n2. Improved consistency: The Terraform configuration will ensure consistency across environments, reducing the risk of human error.\n3. Enhanced transparency: The interactive check points will provide the operator with a clear understanding of the provisioning status.\n4. Increased reusability: The Terraform configuration can be reused across environments, reducing the time and effort required to set up new environments.\n\n**Implementation:**\n\n1. Define the Terraform configuration for the Amazon environment, including the required resources and dependencies.\n2. Implement the interactive check points using Terraform's built-in features, such as output variables and conditional statements.\n3. Test the Terraform configuration to ensure it works as expected.\n4. Deploy the Terraform configuration to the production environment.\n\n**Monitoring and Feedback:**\n\n1. Monitor the Terraform configuration for errors and exceptions.\n2. Provide feedback to the operator on the provisioning status, including any issues or errors that may have occurred.\n3. Continuously improve the Terraform configuration based on feedback and monitoring results.","GenTime":"2024-08-10 19:45:11"}
{"File Name":"govuk-aws\/0012-security-groups-in-terraform.md","Context":"## Context\\nThere are two methods of defining security groups for AWS in Terraform and they are distinguished by how you add rules: in-line and separate. Using in-line rules keeps the definition close to the resource but when ever a rule is changed Terraform will re-generate the entire resource. Using separate rules Terraform will only make the single rule change but there is greater boilerplate and separation between the group resource and the rule resource.\\n","Decision":"Security groups will be defined separate to their rules.\\nAdditionally each security group will be defined, in its entirety, in a single file.\\n","tokens":89,"id":4032,"Prediction":"## Decision\n\n**Decision:** We will use separate security group rules in Terraform for defining AWS security groups.\n\n**Context:** As described in the context, there are two methods of defining security groups for AWS in Terraform: in-line and separate. In-line rules keep the definition close to the resource, but whenever a rule is changed, Terraform will re-generate the entire resource. On the other hand, separate rules allow Terraform to make only the single rule change, but there is greater boilerplate and separation between the group resource and the rule resource.\n\n**Rationale:** We have chosen to use separate security group rules in Terraform because it provides greater flexibility and maintainability. By separating the rules from the group resource, we can make changes to individual rules without affecting the entire resource. This approach also allows us to reuse rules across multiple resources, reducing boilerplate and improving code organization.\n\n**Consequences:**\n\n* Pros:\n\t+ Improved maintainability and flexibility\n\t+ Reduced boilerplate\n\t+ Reusable rules\n* Cons:\n\t+ Greater separation between the group resource and the rule resource\n\t+ Requires more explicit configuration\n\n**Alternatives Considered:** We considered using in-line rules, but ultimately decided against it due to the limitations it imposes on our ability to make changes to individual rules without affecting the entire resource.\n\n**Status:** Implemented\n\n**Links:** [Terraform documentation on security group rules](https:\/\/www.terraform.io\/docs\/providers\/aws\/r\/security_group.html#rules)","GenTime":"2024-08-10 19:47:22"}
{"File Name":"govuk-aws\/0002-hosting-platforms.md","Context":"## Context\\nWe need to decide upon a platform to host the future GOV.UK infrastructure. Long term, this will be primarily the GOV.UK PaaS but in the interim, we need to converge with that plan and also upgrade and modernise the current infrastructure.\\nGDS policy for hosting of GDS internal services is PaaS first and AWS for anything that can not be run on the PaaS.\\n","Decision":"We are using Amazon Web Services as our hosting provider of choice. This conforms to the [GDS Tech Forum Hosting Guide](https:\/\/github.com\/alphagov\/gds-tech\/pull\/7).\\nWe will initially be using the `eu-west-1` region, Ireland. This region has 3 availability zones and also contains the GDS PaaS which will allow easier sharing and peering.\\n","tokens":86,"id":4041,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will host the GOV.UK infrastructure on AWS, utilizing a combination of managed services and custom solutions, while also laying the groundwork for the eventual transition to the GOV.UK PaaS.\n\n**Context**\n\nAs mentioned in the context, we need to decide on a platform to host the future GOV.UK infrastructure, which will primarily be the GOV.UK PaaS in the long term. In the interim, we need to converge with that plan and upgrade and modernize the current infrastructure. Additionally, GDS policy dictates that PaaS should be the first choice for hosting GDS internal services, with AWS being the next option for services that cannot be run on PaaS.\n\n**Problem**\n\nThe current infrastructure is outdated and requires modernization to meet the growing demands of GOV.UK services. Additionally, the interim solution needs to be scalable, secure, and cost-effective to support the transition to the GOV.UK PaaS.\n\n**Reasoning**\n\nAWS provides a robust and scalable infrastructure that can support the current and future needs of GOV.UK services. By utilizing managed services such as EC2, RDS, and S3, we can reduce the complexity and cost of maintaining our own infrastructure. Additionally, AWS provides a wide range of tools and services that can help us modernize and upgrade our infrastructure, such as AWS Lambda, API Gateway, and CloudFormation.\n\n**Decision Criteria**\n\n* Scalability: AWS provides a scalable infrastructure that can support the growing demands of GOV.UK services.\n* Security: AWS provides a secure infrastructure with built-in security features, such as VPCs, subnets, and IAM roles.\n* Cost-effectiveness: AWS provides a cost-effective solution that can help reduce the cost of maintaining our own infrastructure.\n* Modernization: AWS provides a wide range of tools and services that can help us modernize and upgrade our infrastructure.\n* Alignment with GDS policy: AWS aligns with GDS policy by providing a PaaS-like experience for services that cannot be run on the GOV.UK PaaS.\n\n**Consequences**\n\n* The decision to host the GOV.UK infrastructure on AWS will require significant changes to our current infrastructure and operations.\n* The transition to AWS will provide opportunities for modernization and upgrading our infrastructure, but will also require significant planning and resources.\n* The decision aligns with GDS policy and sets the stage for a seamless transition to the GOV.UK PaaS in the long term.\n\n**Reevaluation**\n\nThis decision will be reevaluated in 6 months to assess the effectiveness of the AWS solution and to determine if any changes are needed to support the transition to the GOV.UK PaaS.","GenTime":"2024-08-10 19:50:58"}
{"File Name":"govuk-aws\/0029-combine-api-redis-into-backend-redis.md","Context":"## Context\\nWe currently have 3 Redis instances that we run in Elasticache:\\n- backend-redis\\n- logs-redis\\n- api-redis\\nMost applications use backend-redis, the logging cluster uses logs-redis (which should\\nsoon be replaced), and Rummager uses api-redis.\\nThis was traditionally the case because Rummager lived in a different vDC to redis-1\/redis-2,\\nbut this is no longer a concern in AWS.\\n","Decision":"Update Rummager configuration so it uses backend-redis, and remove api-redis.\\n","tokens":101,"id":4044,"Prediction":"## Decision\n\n**Decision:** Migrate all Redis instances to a single, highly available, and scalable Redis cluster in Elasticache.\n\n**Context:** We currently have three separate Redis instances (backend-redis, logs-redis, and api-redis) running in Elasticache, each serving a specific application or use case. While this was previously necessary due to infrastructure constraints, we now have the ability to consolidate these instances into a single, highly available, and scalable Redis cluster.\n\n**Rationale:**\n\n* Simplifying our infrastructure reduces complexity and makes it easier to manage and maintain.\n* A single Redis cluster provides better availability and redundancy, ensuring that our applications can continue to function even in the event of an instance failure.\n* Scalability is improved, as we can easily add or remove capacity as needed, without having to worry about individual instance constraints.\n* Consistency is improved, as all applications will be using the same Redis instance, reducing the risk of inconsistencies and errors.\n\n**Consequences:**\n\n* We will need to carefully plan and execute the migration to minimize downtime and ensure that all applications continue to function correctly.\n* We will need to update our application configurations to point to the new, single Redis cluster.\n* We may need to implement additional monitoring and alerting to ensure that the new cluster is functioning as expected.\n\n**Alternatives Considered:**\n\n* Continuing to run separate Redis instances, which would maintain the current level of complexity and reduce the benefits of consolidation.\n* Implementing a load balancer to distribute traffic across multiple Redis instances, which would add additional complexity and may not provide the same level of high availability as a single, highly available cluster.\n\n**Status:** Approved\n\n**Implementation:** The migration will be carried out by [Name] in [Timeframe]. The team will work closely together to ensure a smooth transition and minimal downtime.","GenTime":"2024-08-10 19:52:00"}
{"File Name":"govuk-aws\/0038-mongo_replacement_by_documentdb.md","Context":"## Context\\nGOV.UK uses MongoDB as the backing database for a number of apps. In a generic\\nGOV.UK environment, there are 5 MongoDB clusters: one for Licensify (a legacy\\nservice), one for performance monitoring, one for router and another per cloud\\nprovider (i.e. AWS and Carrenza) for all other apps (e.g. assets, imminence,\\nmaslow, content, publisher and short url manager).\\nOne architectural strategy principle of TechOps is to make use as much as\\npossible AWS managed services in order to reduce the burden on GOV.UK teams to\\nmanage Commercial off-the-shelf (COTS) functionality\/services.\\n","Decision":"The approaches taken for migrating from MongoDB to AWS DocumentDB are:\\n1. new apps that are being migrated from legacy cloud providers (e.g UKCloud and\\nCarrenza) to AWS and use MongoDB should be evaluated for compatibility with\\nAWS DocumentDB during the migration phase. Based on this evaluation, a\\ndecision should be made based on cost benefit whether a particular migrating\\napp should be using DocumentDB or MongoDB.\\n2. there are some apps (e.g. content store and imminence) that have already been\\nmigrated to AWS and uses the MongoDB cluster there. These apps should be\\nevaluated for compatibility with AWS DocumentDB and prioritised accordingly\\nfor migration to AWS DocumentDB.\\n","tokens":145,"id":4047,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nAs the primary backing database for GOV.UK apps, we will utilize Amazon DocumentDB (with MongoDB compatibility) instead of traditional MongoDB, in order to leverage the benefits of an AWS managed service and reduce the operational burden on our teams.\n\n**Context**\n\nAs GOV.UK currently uses MongoDB for multiple applications, and TechOps emphasizes the use of AWS managed services to reduce the need for manual management of COTS functionality\/services.\n\n**Problem**\n\nThe current use of traditional MongoDB requires our teams to manage and maintain the database clusters, which can be time-consuming and resource-intensive. This can lead to a higher operational burden and potential downtime.\n\n**Decision Rationale**\n\nBy using Amazon DocumentDB, we can take advantage of the managed service provided by AWS, which includes features such as automated backups, patching, and monitoring. This will allow our teams to focus on higher-level tasks and reduce the risk of downtime or data loss. Additionally, Amazon DocumentDB provides MongoDB compatibility, ensuring a seamless transition for our existing applications.\n\n**Consequences**\n\n* Reduced operational burden on our teams\n* Improved reliability and availability of our database services\n* Simplified management and maintenance of our database clusters\n* Potential cost savings through reduced administrative overhead\n\n**Alternatives Considered**\n\n* Continue using traditional MongoDB, with our teams managing and maintaining the clusters\n* Evaluate and implement a different NoSQL database solution that is not managed by AWS\n\n**Status**\n\nApproved\n\n**Date**\n\n[Insert Date]\n\nNote: This decision record is a summary of the decision-making process and should be updated as necessary to reflect changes or new information.","GenTime":"2024-08-10 19:53:05"}
{"File Name":"govuk-aws\/0015-dns-infrastructure.md","Context":"## Context\\n- All our instances will need to be able to resolve internal infrastructure services, such\\nus Puppet, Graphite or Logstash\\n- Some services and application endpoints will need to be exposed to the Internet and\\nbe resolved by public DNS. For instance alerts.integration, deploy.integration, www-origin, etc\\n- We want to be able to create new pieces of infrastructure alongside the current piece of infrastructure\\nwith the ability to test direct access to each piece using DNS endpoints\\n- We want to control which stack is running the active version of a piece of infrastructure, and control\\nhow applications connect using DNS\\n- We want to ensure the site and all links works correctly when users browse using the\\npublishing (publishing.service.gov.uk) domain.\\n","Decision":"![DNS](.\/0015-govuk-aws-dns.jpg?raw=true \"DNS Infrastructure\")\\n#### Stack domains\\nEach stack has an internal and external DNS domain. All Terraform projects in that stack add records\\nto Route53 zones to expose the service internally and\/or externally.\\nFor instance, a 'green' stack has its own `green.<internalrootdomain>` and `green.<externalrootdomain>`\\ndomain. Puppet and Icinga services in this stack will add `puppet.green.<internalrootdomain>` and\\n`alerts.green.<externalrootdomain>` to Route53.\\nThis is for an infrastructure level view only. Applications will not work correctly across independent stacks,\\nand will only correctly work using the Publishing domain.\\n### Root domain service records\\nAll services will need an entry with the root domain that points to a stack record. This entry\\ncan be updated to select the active version of each service.\\nFor instance, machines are using the Puppet service `puppet.<internalrootdomain>` that is a CNAME\\nof `puppet.green.<internalrootdomain>`. At some point, a new Puppet stack 'blue' is provisioned to\\ntest a new version, and when it has been tested we switch the CNAME to the new stack, so\\n`puppet.<internalrootdomain>` resolves to `puppet.blue.<internalrootdomain>`.\\n#### External Route53 zones\\nThere is a public (external) Route53 zone to manage the external root domain. Each stack has also its own\\nexternal Route53 zone where we delegate the stack subdomain.\\nFor instance, if we are setting up a new environment with a public root domain `test.govuk.digital`,\\nand create a new stack 'green', we'll have a zone for `test.govuk.digital` and a zone for the stack\\nsubdomain `green.test.govuk.digital`, that stores the stack records.\\n#### Internal Route53 zones\\nFor internal domains we cannot replicate the external configuration because it's not possible to do DNS\\ndelegation with internal zones in the same VPC when the domains overlap.\\nFor instance, for the previous scenario we will have an internal zone for the root domain\\n`test.govuk-internal.digital`, but we cannot create and delegate a subdomain for a zone\\n`green.test.govuk-internal.digital` in the same VPC because it overlaps the root domain.\\nIn this case, for internal domains we can only have a single zone, but we'll still keep the same\\ndomain schema and each stack will generate records appending the stack subdomain.\\n","tokens":154,"id":4056,"Prediction":"## Decision\n\n**Service Discovery and DNS Management**\n\n## Context\n\nAs outlined in the context, our instances need to resolve internal infrastructure services, expose services and application endpoints to the Internet, create new infrastructure pieces, control the active version of infrastructure, and ensure site and link correctness when users browse using the publishing domain.\n\n## Decision\n\nWe will implement a service discovery and DNS management system that utilizes a combination of techniques to achieve the required functionality. Specifically, we will use:\n\n1. **Consul** as our service discovery tool to manage and register internal infrastructure services, such as Puppet, Graphite, and Logstash. Consul will provide a centralized registry of services, allowing instances to resolve and communicate with each other.\n2. **CoreDNS** as our DNS server to manage public DNS records for exposed services and application endpoints. CoreDNS will provide a flexible and extensible DNS solution that can be easily integrated with our existing infrastructure.\n3. **Ansible** to automate the creation and configuration of new infrastructure pieces, including DNS records and service registrations. Ansible will enable us to quickly and consistently deploy new infrastructure and ensure that it is properly configured.\n4. **Kubernetes** (or another container orchestration tool) to manage and deploy our infrastructure services. Kubernetes will provide a scalable and highly available platform for our infrastructure services, and will integrate seamlessly with Consul and CoreDNS.\n\n## Rationale\n\nThe chosen solution provides a scalable, flexible, and highly available service discovery and DNS management system that meets our requirements. Consul and CoreDNS provide a robust and reliable foundation for service discovery and DNS management, while Ansible and Kubernetes enable us to automate and orchestrate the creation and deployment of new infrastructure pieces.\n\n## Consequences\n\nThe implementation of this service discovery and DNS management system will:\n\n* Provide a centralized registry of internal infrastructure services, making it easier to manage and resolve services across the organization.\n* Allow for the creation of new infrastructure pieces alongside the existing infrastructure, with the ability to test direct access to each piece using DNS endpoints.\n* Enable control over which stack is running the active version of a piece of infrastructure, and control how applications connect using DNS.\n* Ensure the site and all links work correctly when users browse using the publishing domain.\n\nOverall, this solution will provide a robust and scalable infrastructure for our organization, enabling us to efficiently manage and deploy new infrastructure pieces, and ensure the reliability and availability of our services.","GenTime":"2024-08-10 19:56:25"}
{"File Name":"govuk-aws\/0032-transfer-artefact-binary.md","Context":"## Context\\nWe create binary artifacts for applications when we execute the **Deploy_App**\\nJenkins job. The binary file (artifact) gets stored in an AWS S3 bucket\\n(For example **govuk-integration-artefact**). We need the binary file to deploy\\nthe application to the next environment. For example, we will use the binary\\nfile from **Integration** to deploy the application in the **Staging**\\nenvironment.\\n","Decision":"We decided to use a mechanism that is located outside the environments involved\\nand pass the contents to the correct locations when the package becomes\\navailable.\\n![Transfer Mechanism](.\/0032-transfer-artefact-binary-img01.png?raw=true \"Transfer Mechanism\")\\n### Example\\nWe will use the **Integration** and **Staging** environments as examples.\\n1. We execute the **Deploy_Apps -> router** Jenkins job in the Integration environment.\\n2. A binary file is created in the **govuk-integration-artefact** AWS S3 bucket.\\n3. We have an AWS SNS Topic called **govuk-integration-artefact**.\\n4. We have an AWS S3 **govuk-integration-artefact** bucket event notification. This event sends a notification to the AWS SNS Topic when an **ObjectCreate** event is triggered.\\n5. The **Staging** environment has an AWS S3 bucket called **govuk-staging-artefact**.\\n6. We have an AWS SNS Subscription that subscribes to the **govuk-integration-artefact** AWS SNS Topic. This subscription will enable Staging to know when a new object gets created in the **govuk-integration-artefact** AWS S3 bucket.\\n7. We have an AWS Lambda function called **govuk-staging-artefact**. This function copies objects from the destination bucket to the target bucket. The trigger for the function is the AWS SNS Subscription.\\n","tokens":95,"id":4057,"Prediction":"## Decision\n\n**Store Binary Artifacts in a Dedicated S3 Bucket with a Standardized Naming Convention**\n\n## Context\n\nWe create binary artifacts for applications when we execute the **Deploy_App** Jenkins job. The binary file (artifact) gets stored in an AWS S3 bucket (For example **govuk-integration-artefact**). We need the binary file to deploy the application to the next environment. For example, we will use the binary file from **Integration** to deploy the application in the **Staging** environment.\n\n## Problem\n\nThe current approach of storing binary artifacts in a single S3 bucket (e.g., **govuk-integration-artefact**) makes it difficult to manage and track the artifacts across different environments and applications. This can lead to confusion and errors when trying to deploy the correct binary file to the next environment.\n\n## Decision\n\nTo address this issue, we will store binary artifacts in a dedicated S3 bucket with a standardized naming convention. This will allow us to easily identify and retrieve the correct binary file for each environment and application.\n\n### Solution\n\nWe will create a new S3 bucket (e.g., **govuk-binary-artefacts**) and store each binary artifact in a subdirectory with a format like `application-name\/environment-name\/artifact-version`. For example, `my-app\/staging\/v1.2.3.zip`.\n\n### Rationale\n\nThis decision provides several benefits:\n\n1. **Improved organization**: The dedicated S3 bucket and standardized naming convention make it easier to find and manage binary artifacts across different environments and applications.\n2. **Reduced errors**: The clear and consistent naming convention reduces the likelihood of errors when trying to deploy the correct binary file to the next environment.\n3. **Increased scalability**: The dedicated S3 bucket can be easily scaled to handle a large number of binary artifacts without affecting the performance of other applications.\n\n### Consequences\n\n1. **Additional infrastructure costs**: We will need to pay for the additional storage and bandwidth costs associated with the dedicated S3 bucket.\n2. **Changes to the Deploy_App Jenkins job**: We will need to update the Jenkins job to store the binary artifact in the new S3 bucket with the standardized naming convention.\n3. **Impact on existing applications**: We will need to update the existing applications to retrieve the binary artifact from the new S3 bucket and use the standardized naming convention.\n\n### Monitoring and Feedback\n\nWe will monitor the usage and performance of the dedicated S3 bucket and the standardized naming convention to ensure that it is meeting our requirements. We will also gather feedback from the development and operations teams to identify any issues or areas for improvement.","GenTime":"2024-08-10 19:56:53"}
{"File Name":"govuk-aws\/0008-postgres-on-puppetmaster.md","Context":"## Context\\nWe need to decide whether to run PostgreSQL on the Puppetmaster itself or use\\nRDS from the off.\\nCurrently our Puppetmaster uses a local PostgreSQL instance as its database. In moving to AWS we have the opportunity to replace this with an RDS instance.\\nThis may remove a reasonable amount of puppet code used to configure the database and associated tasks (e.g. backups). It would also require an amount of Terraform work to provision the RDS instance and some updates to the Puppetmaster code to use that instance.\\n","Decision":"The Puppetmaster will continue to use a local instance of PostgreSQL.\\n","tokens":110,"id":4061,"Prediction":"## Decision\n\n**Decision:** We will use RDS from the off as our PostgreSQL database for the Puppetmaster.\n\n## Context\n\nAs described in the context, we have the opportunity to replace our local PostgreSQL instance with an RDS instance in AWS. This would simplify our Puppet code and reduce the amount of code used to configure the database and associated tasks, such as backups.\n\n## Problem\n\nOur current local PostgreSQL instance requires manual configuration and maintenance, which can be time-consuming and error-prone. Additionally, our Puppet code is currently responsible for configuring the database, which can make it difficult to manage and maintain.\n\n## Solution\n\nUsing RDS would allow us to offload the responsibility of database configuration and maintenance to AWS, freeing up our Puppet code to focus on other tasks. This would also provide a more scalable and reliable solution, as RDS instances are designed to handle high traffic and provide automatic backups.\n\n## Rationale\n\nThe benefits of using RDS outweigh the additional complexity of provisioning and configuring the instance. The reduced Puppet code and simplified database management would make our infrastructure easier to maintain and update. Additionally, using RDS would provide a more scalable and reliable solution, which is important for our Puppetmaster.\n\n## Decision Drivers\n\n* Simplify Puppet code and reduce maintenance tasks\n* Improve scalability and reliability of our Puppetmaster\n* Take advantage of AWS's managed database service\n\n## Alternatives Considered\n\n* Continue using a local PostgreSQL instance\n* Use a combination of local PostgreSQL and RDS\n\n## Conclusion\n\nUsing RDS from the off as our PostgreSQL database for the Puppetmaster is the best decision for our infrastructure. It will simplify our Puppet code, improve scalability and reliability, and take advantage of AWS's managed database service.","GenTime":"2024-08-10 19:58:17"}
{"File Name":"ichatsdk-evolution\/0001-use-c-language.md","Context":"## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":96,"id":4062,"Prediction":"## Context and Problem Statement\nIn our e-commerce platform, we are facing a scalability issue with our product recommendation system, which is built using a complex algorithm that requires significant computational resources. This has resulted in slow response times and increased latency, negatively impacting the user experience. We need to decide on a solution to improve the scalability and performance of our product recommendation system.\n\n## Decision Drivers\n* User experience: We need to ensure that our product recommendation system responds quickly and efficiently to user requests, to maintain a high level of user satisfaction and engagement.\n* Scalability: We need to be able to handle a large volume of user requests and product data without experiencing significant performance degradation.\n* Development complexity: We need to consider the complexity and maintainability of the solution, to ensure that it can be easily integrated with our existing system and maintained over time.\n* Cost: We need to consider the cost of the solution, including the cost of hardware, software, and development resources.","GenTime":"2024-08-10 19:58:27"}
{"File Name":"planet4-docs\/adr-0001-use-gitbook-for-technical-documentation.md","Context":"### Context and Problem Statement\\nCurrently Technical Documentation lives in the Handbook, with an [entry point](https:\/\/planet4.greenpeace.org\/create\/tech\/) and everything else organized under the [Development tag](https:\/\/planet4.greenpeace.org\/tag\/development\/). On the other hand Gitbook is a platform specifically for maintaining documentation, based on Markdown. We created a [Planet 4 space](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/) in GP Gitbook account to test how a potential structure of our documentation would look.\\n### Decision Drivers\\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","Decision":"* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","tokens":492,"id":4087,"Prediction":"Here is the Architectural Decision Record (ADR) for the given context:\n\n**Decision:** Move to Gitbook all Technical documentation (Development & Infrastructure)\n\n**Context:** Currently, Technical Documentation lives in the Handbook, but we are considering moving it to Gitbook, a platform specifically designed for maintaining documentation based on Markdown.\n\n**Problem Statement:** The current Technical Documentation is scattered across the Handbook, making it difficult to organize and maintain. We need to decide whether to stay in the Handbook or move to Gitbook.\n\n**Decision Drivers:** The Community Management Team should be consulted, as this decision affects the internal development community.\n\n**Considered Options:** Move to Gitbook, Stay in the Handbook\n\n**Decision Outcome:** We have decided to move to Gitbook all Technical documentation (Development & Infrastructure).\n\n**Pros and Cons of the Options:**\n\n**Move to Gitbook:**\n\n* Good: Gitbook is a dedicated documentation platform with better UI\/UX, easy grouping and browsing, and connection to a public Github repo for easy contributions.\n* Good: It's the choice of other teams in the Org, making it easier for people to discover and use.\n\n**Stay in the Handbook:**\n\n* Good: Everything regarding P4 is in one place.\n* Bad: The Handbook is perceived as internal-only, which is not always the case for technical documentation.\n* Bad: Custom development is required for the Handbook, and manual styling customization is needed for technical documentation.\n\n**Links:**\n\n* P4 space on Gitbook: https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/\n* Git Repo: https:\/\/github.com\/greenpeace\/planet4-docs\/\n\nThis ADR provides a clear summary of the decision-making process, including the context, problem statement, decision drivers, considered options, and pros and cons of each option. It also includes a summary of the chosen option and relevant links for further information.","GenTime":"2024-08-10 20:04:28"}
{"File Name":"new-orbit\/0001-use-openshift-as-cloud-provider.md","Context":"## Context and Problem Statement\\nWe want to deploy our application in docker containers that can be easily updated\\n","Decision":"Chosen option: \"OpenShift\", because\\n* Built on Kubernetes.\\nThe bank has experience on it.\\nProvides a lot of added value tools for CI\/CD, automated builds.\\nIs supported by RedHat and we have a great support contract for it.\\n","tokens":21,"id":4096,"Prediction":"## Decision\n\n**Decision:** We will use a Docker Compose file to manage and deploy our application in a consistent and scalable manner.\n\n**Context and Problem Statement:** We want to deploy our application in Docker containers that can be easily updated.\n\n**Reasoning:**\n\n* Docker Compose provides a simple and intuitive way to define and run multi-container Docker applications.\n* It allows us to define the services, networks, and volumes required for our application, making it easy to manage and update our containers.\n* Docker Compose supports multiple platforms, including Linux, Windows, and macOS, ensuring that our application can be deployed consistently across different environments.\n* By using Docker Compose, we can decouple our application from the underlying infrastructure, making it easier to scale and manage our containers.\n\n**Consequences:**\n\n* We will need to create a Docker Compose file that defines the services and dependencies required for our application.\n* We will need to update the Docker Compose file whenever we make changes to our application or its dependencies.\n* We will need to use Docker Compose to start and stop our containers, as well as to manage the underlying infrastructure.\n\n**Alternatives Considered:**\n\n* We considered using Docker Swarm to manage our containers, but decided against it due to its complexity and limited support for Windows.\n* We also considered using Kubernetes, but decided against it due to its steep learning curve and the need for a more complex infrastructure setup.\n\n**Status:** This decision is approved and will be implemented in our application.","GenTime":"2024-08-10 20:06:56"}
{"File Name":"celestia-core\/adr-024-sign-bytes.md","Context":"## Context\\nCurrently, the messages exchanged between tendermint and a (potentially remote) signer\/validator,\\nnamely votes, proposals, and heartbeats, are encoded as a JSON string\\n(e.g., via `Vote.SignBytes(...)`) and then\\nsigned . JSON encoding is sub-optimal for both, hardware wallets\\nand for usage in ethereum smart contracts. Both is laid down in detail in [issue#1622].\\nAlso, there are currently no differences between sign-request and -replies. Also, there is no possibility\\nfor a remote signer to include an error code or message in case something went wrong.\\nThe messages exchanged between tendermint and a remote signer currently live in\\n[privval\/socket.go] and encapsulate the corresponding types in [types].\\n[privval\/socket.go]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/socket.go#L496-L502\\n[issue#1622]: https:\/\/github.com\/tendermint\/tendermint\/issues\/1622\\n[types]: https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/types\\n","Decision":"- restructure vote, proposal, and heartbeat such that their encoding is easily parseable by\\nhardware devices and smart contracts using a  binary encoding format ([amino] in this case)\\n- split up the messages exchanged between tendermint and remote signers into requests and\\nresponses (see details below)\\n- include an error type in responses\\n### Overview\\n```\\n+--------------+                      +----------------+\\n|              |     SignXRequest     |                |\\n|Remote signer |<---------------------+  tendermint    |\\n| (e.g. KMS)   |                      |                |\\n|              +--------------------->|                |\\n+--------------+    SignedXReply      +----------------+\\nSignXRequest {\\nx: X\\n}\\nSignedXReply {\\nx: X\\nsig: Signature \/\/ []byte\\nerr: Error{\\ncode: int\\ndesc: string\\n}\\n}\\n```\\nTODO: Alternatively, the type `X` might directly include the signature. A lot of places expect a vote with a\\nsignature and do not necessarily deal with \"Replies\".\\nStill exploring what would work best here.\\nThis would look like (exemplified using X = Vote):\\n```\\nVote {\\n\/\/ all fields besides signature\\n}\\nSignedVote {\\nVote Vote\\nSignature []byte\\n}\\nSignVoteRequest {\\nVote Vote\\n}\\nSignedVoteReply {\\nVote SignedVote\\nErr  Error\\n}\\n```\\n**Note:** There was a related discussion around including a fingerprint of, or, the whole public-key\\ninto each sign-request to tell the signer which corresponding private-key to\\nuse to sign the message. This is particularly relevant in the context of the KMS\\nbut is currently not considered in this ADR.\\n[amino]: https:\/\/github.com\/tendermint\/go-amino\/\\n### Vote\\nAs explained in [issue#1622] `Vote` will be changed to contain the following fields\\n(notation in protobuf-like syntax for easy readability):\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Vote {\\nVersion       fixed32\\nHeight        sfixed64\\nRound         sfixed32\\nVoteType      fixed32\\nTimestamp     Timestamp         \/\/ << using protobuf definition\\nBlockID       BlockID           \/\/ << as already defined\\nChainID       string            \/\/ at the end because length could vary a lot\\n}\\n\/\/ this is an amino registered type; like currently privval.SignVoteMsg:\\n\/\/ registered with \"tendermint\/socketpv\/SignVoteRequest\"\\nmessage SignVoteRequest {\\nVote vote\\n}\\n\/\/  amino registered type\\n\/\/ registered with \"tendermint\/socketpv\/SignedVoteReply\"\\nmessage SignedVoteReply {\\nVote      Vote\\nSignature Signature\\nErr       Error\\n}\\n\/\/ we will use this type everywhere below\\nmessage Error {\\nType        uint  \/\/ error code\\nDescription string  \/\/ optional description\\n}\\n```\\nThe `ChainID` gets moved into the vote message directly. Previously, it was injected\\nusing the [Signable] interface method `SignBytes(chainID string) []byte`. Also, the\\nsignature won't be included directly, only in the corresponding `SignedVoteReply` message.\\n[Signable]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/types\/signable.go#L9-L11\\n### Proposal\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Proposal {\\nHeight            sfixed64\\nRound             sfixed32\\nTimestamp         Timestamp         \/\/ << using protobuf definition\\nBlockPartsHeader  PartSetHeader     \/\/ as already defined\\nPOLRound          sfixed32\\nPOLBlockID        BlockID           \/\/ << as already defined\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalRequest\"\\nmessage SignProposalRequest {\\nProposal proposal\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalReply\"\\nmessage SignProposalReply {\\nProp   Proposal\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n### Heartbeat\\n**TODO**: clarify if heartbeat also needs a fixed offset and update the fields accordingly:\\n```proto\\nmessage Heartbeat {\\nValidatorAddress Address\\nValidatorIndex   int\\nHeight           int64\\nRound            int\\nSequence         int\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatRequest\"\\nmessage SignHeartbeatRequest {\\nHb Heartbeat\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatReply\"\\nmessage SignHeartbeatReply {\\nHb     Heartbeat\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n","tokens":249,"id":4097,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\n* **Use a binary encoding for messages exchanged between tendermint and remote signers\/validators**\n\n**Context**\n\nAs described in the provided context, the current JSON encoding of messages exchanged between tendermint and remote signers\/validators (votes, proposals, and heartbeats) is sub-optimal for both hardware wallets and usage in Ethereum smart contracts. Additionally, there are no differences between sign requests and replies, and there is no possibility for a remote signer to include an error code or message in case something goes wrong.\n\n**Rationale**\n\nUsing a binary encoding for messages exchanged between tendermint and remote signers\/validators will provide several benefits. Firstly, it will improve performance by reducing the overhead of JSON encoding and decoding. Secondly, it will provide a more secure and efficient way of transmitting data, which is critical for hardware wallets and smart contracts. Finally, it will enable remote signers to include error codes and messages in case something goes wrong, improving the overall reliability and fault tolerance of the system.\n\n**Consequences**\n\n* The `Vote.SignBytes()` method will need to be updated to generate a binary encoding of the vote instead of a JSON string.\n* The `privval\/socket.go` file will need to be updated to use the new binary encoding for messages exchanged between tendermint and remote signers\/validators.\n* The `types` package will need to be updated to include the new binary encoding format for messages.\n\n**Alternatives Considered**\n\n* Continuing to use JSON encoding for messages exchanged between tendermint and remote signers\/validators, which would maintain compatibility with existing code but would not address the performance and security concerns.\n* Using a different encoding format, such as Protocol Buffers or MessagePack, which would require additional infrastructure and testing but would provide similar benefits to the chosen solution.\n\n**Status**\n\nThis decision is approved and will be implemented in the next release of the tendermint software.","GenTime":"2024-08-10 20:07:17"}
{"File Name":"celestia-core\/adr-005-consensus-params.md","Context":"## Context\\nConsensus critical parameters controlling blockchain capacity have until now been hard coded, loaded from a local config, or neglected.\\nSince they may be need to be different in different networks, and potentially to evolve over time within\\nnetworks, we seek to initialize them in a genesis file, and expose them through the ABCI.\\nWhile we have some specific parameters now, like maximum block and transaction size, we expect to have more in the future,\\nsuch as a period over which evidence is valid, or the frequency of checkpoints.\\n","Decision":"### ConsensusParams\\nNo consensus critical parameters should ever be found in the `config.toml`.\\nA new `ConsensusParams` is optionally included in the `genesis.json` file,\\nand loaded into the `State`. Any items not included are set to their default value.\\nA value of 0 is undefined (see ABCI, below). A value of -1 is used to indicate the parameter does not apply.\\nThe parameters are used to determine the validity of a block (and tx) via the union of all relevant parameters.\\n```\\ntype ConsensusParams struct {\\nBlockSize\\nTxSize\\nBlockGossip\\n}\\ntype BlockSize struct {\\nMaxBytes int\\nMaxTxs int\\nMaxGas int\\n}\\ntype TxSize struct {\\nMaxBytes int\\nMaxGas int\\n}\\ntype BlockGossip struct {\\nBlockPartSizeBytes int\\n}\\n```\\nThe `ConsensusParams` can evolve over time by adding new structs that cover different aspects of the consensus rules.\\nThe `BlockPartSizeBytes` and the `BlockSize.MaxBytes` are enforced to be greater than 0.\\nThe former because we need a part size, the latter so that we always have at least some sanity check over the size of blocks.\\n### ABCI\\n#### InitChain\\nInitChain currently takes the initial validator set. It should be extended to also take parts of the ConsensusParams.\\nThere is some case to be made for it to take the entire Genesis, except there may be things in the genesis,\\nlike the BlockPartSize, that the app shouldn't really know about.\\n#### EndBlock\\nThe EndBlock response includes a `ConsensusParams`, which includes BlockSize and TxSize, but not BlockGossip.\\nOther param struct can be added to `ConsensusParams` in the future.\\nThe `0` value is used to denote no change.\\nAny other value will update that parameter in the `State.ConsensusParams`, to be applied for the next block.\\nTendermint should have hard-coded upper limits as sanity checks.\\n","tokens":111,"id":4099,"Prediction":"## Decision\n\n**Decision:** Initialize consensus critical parameters in a genesis file and expose them through the Application Blockchain Interface (ABCI).\n\n**Context:** Consensus critical parameters controlling blockchain capacity have until now been hard coded, loaded from a local config, or neglected. With the need to support different networks and potentially evolving parameters over time, we aim to initialize these parameters in a genesis file and expose them through the ABCI.\n\n**Rationale:**\n\n1. **Flexibility**: By storing parameters in a genesis file, we can easily modify or update them without requiring changes to the code.\n2. **Network-specific configuration**: This approach allows for network-specific configurations, enabling different networks to have unique parameter settings.\n3. **Future-proofing**: As new parameters are introduced, we can simply add them to the genesis file without affecting the underlying code.\n4. **Decoupling**: Exposing parameters through the ABCI provides a clear interface for other components to access and use these parameters, decoupling them from the underlying implementation.\n\n**Consequences:**\n\n1. **Additional complexity**: Introducing a genesis file and ABCI interface will add complexity to the system.\n2. **Genesis file management**: We will need to manage the genesis file, ensuring it is properly updated and validated.\n\n**Status:** Accepted\n\n**Implementation:** We will implement a genesis file that stores consensus critical parameters and expose them through the ABCI. We will also develop a mechanism for managing and validating the genesis file.","GenTime":"2024-08-10 20:07:33"}
{"File Name":"celestia-core\/adr-004-historical-validators.md","Context":"## Context\\nRight now, we can query the present validator set, but there is no history.\\nIf you were offline for a long time, there is no way to reconstruct past validators. This is needed for the light client and we agreed needs enhancement of the API.\\n","Decision":"For every block, store a new structure that contains either the latest validator set,\\nor the height of the last block for which the validator set changed. Note this is not\\nthe height of the block which returned the validator set change itself, but the next block,\\nie. the first block it comes into effect for.\\nStoring the validators will be handled by the `state` package.\\nAt some point in the future, we may consider more efficient storage in the case where the validators\\nare updated frequently - for instance by only saving the diffs, rather than the whole set.\\nAn alternative approach suggested keeping the validator set, or diffs of it, in a merkle IAVL tree.\\nWhile it might afford cheaper proofs that a validator set has not changed, it would be more complex,\\nand likely less efficient.\\n","tokens":56,"id":4101,"Prediction":"## Decision\n\nTo address the issue of not having a history of validator sets, we will implement a solution that stores the historical validator sets in a database. This will enable the light client to reconstruct past validators, even if the user was offline for a long time.\n\n## Context\n\nAs you mentioned, the current API only allows querying the present validator set, but there is no history of previous validator sets. This makes it impossible for the light client to reconstruct past validators, which is a critical feature for our application.\n\n## Problem\n\nThe lack of historical validator sets creates a significant problem for the light client, as it cannot accurately verify the state of the network without access to the historical data. This can lead to inconsistencies and errors, which can compromise the security and integrity of the network.\n\n## Rationale\n\nImplementing a solution to store historical validator sets will provide several benefits:\n\n1. Improved light client functionality: The light client will be able to reconstruct past validators, ensuring that it can accurately verify the state of the network even if the user was offline for a long time.\n2. Increased security: By storing historical validator sets, we can ensure that the network is more resilient to attacks and errors, as the light client will be able to detect and correct any inconsistencies.\n3. Better user experience: The light client will be able to provide a more accurate and reliable experience for users, even in situations where the user is offline for an extended period.\n\n## Decision Criteria\n\nWe considered the following criteria when making this decision:\n\n1. Impact on the light client: The solution must improve the functionality and accuracy of the light client.\n2. Security: The solution must ensure the security and integrity of the network.\n3. Complexity: The solution must be feasible to implement and maintain.\n4. User experience: The solution must provide a better experience for users.\n\n## Solution\n\nWe will implement a solution that stores historical validator sets in a database. This will involve:\n\n1. Designing a database schema to store the historical validator sets.\n2. Implementing a mechanism to store the historical validator sets in the database.\n3. Implementing a mechanism to retrieve the historical validator sets from the database.\n4. Updating the light client to use the historical validator sets to reconstruct past validators.\n\n## Conclusion\n\nImplementing a solution to store historical validator sets will improve the functionality and accuracy of the light client, increase the security and integrity of the network, and provide a better experience for users. We believe that this decision will have a positive impact on the overall performance and reliability of our application.","GenTime":"2024-08-10 20:08:24"}
{"File Name":"celestia-core\/adr-060-go-api-stability.md","Context":"## Context\\nWith the release of Tendermint 1.0 we will adopt [semantic versioning](https:\/\/semver.org). One major implication is a guarantee that we will not make backwards-incompatible changes until Tendermint 2.0 (except in pre-release versions). In order to provide this guarantee for our Go API, we must clearly define which of our APIs are public, and what changes are considered backwards-compatible.\\nCurrently, we list packages that we consider public in our [README](https:\/\/github.com\/tendermint\/tendermint#versioning), but since we are still at version 0.x we do not provide any backwards compatiblity guarantees at all.\\n### Glossary\\n* **External project:** a different Git\/VCS repository or code base.\\n* **External package:** a different Go package, can be a child or sibling package in the same project.\\n* **Internal code:** code not intended for use in external projects.\\n* **Internal directory:** code under `internal\/` which cannot be imported in external projects.\\n* **Exported:** a Go identifier starting with an uppercase letter, which can therefore be accessed by an external package.\\n* **Private:** a Go identifier starting with a lowercase letter, which therefore cannot be accessed by an external package unless via an exported field, variable, or function\/method return value.\\n* **Public API:** any Go identifier that can be imported or accessed by an external project, except test code in `_test.go` files.\\n* **Private API:** any Go identifier that is not accessible via a public API, including all code in the internal directory.\\n","Decision":"From Tendermint 1.0, all internal code (except private APIs) will be placed in a root-level [`internal` directory](https:\/\/golang.org\/cmd\/go\/#hdr-Internal_Directories), which the Go compiler will block for use by external projects. All exported items outside of the `internal` directory are considered a public API and subject to backwards compatibility guarantees, except files ending in `_test.go`.\\nThe `crypto` package may be split out to a separate module in a separate repo. This is the main general-purpose package used by external projects, and is the only Tendermint dependency in e.g. IAVL which can cause some problems for projects depending on both IAVL and Tendermint. This will be decided after further discussion.\\nThe `tm-db` package will remain a separate module in a separate repo. The `crypto` package may possibly be split out, pending further discussion, as this is the main general-purpose package used by other projects.\\n","tokens":336,"id":4112,"Prediction":"## Decision\n\nTo ensure backwards compatibility guarantees with the adoption of semantic versioning, we will clearly define which Go APIs are public and what changes are considered backwards-compatible.\n\n## Context\n\nAs we move to Tendermint 1.0 and adopt semantic versioning, we must provide guarantees that we will not make backwards-incompatible changes until Tendermint 2.0 (except in pre-release versions). To achieve this, we need to define which of our Go APIs are public and what changes are considered backwards-compatible.\n\n## Problem\n\nCurrently, we list packages that we consider public in our README, but since we are still at version 0.x, we do not provide any backwards compatibility guarantees at all.\n\n## Decision Criteria\n\n* We must clearly define which Go APIs are public to ensure that external projects can rely on their stability and functionality.\n* We must define what changes are considered backwards-compatible to ensure that we do not break existing code in external projects.\n* We must ensure that our definition of public APIs is consistent with the Go programming language's rules for exported and private identifiers.\n\n## Decision\n\nWe define a Go API as public if it can be imported or accessed by an external project, except test code in `_test.go` files. We consider the following to be public APIs:\n\n* Go identifiers starting with an uppercase letter (exported)\n* Go identifiers that can be accessed via an exported field, variable, or function\/method return value\n\nWe define a Go API as private if it is not accessible via a public API, including all code in the internal directory.\n\n## Consequences\n\n* We will ensure that all public APIs are documented and tested to ensure their stability and functionality.\n* We will ensure that all changes to public APIs are backwards-compatible, unless explicitly stated in the release notes.\n* We will provide clear guidance on what changes are considered backwards-compatible to external projects.\n\n## Rationale\n\nOur decision is based on the need to provide clear guarantees to external projects about the stability and functionality of our Go APIs. By defining what constitutes a public API, we can ensure that external projects can rely on our code and that we do not break existing code in external projects.","GenTime":"2024-08-10 20:11:33"}
{"File Name":"celestia-core\/adr-056-light-client-amnesia-attacks.md","Context":"## Context\\nWhilst most created evidence of malicious behavior is self evident such that any individual can verify them independently there are types of evidence, known collectively as global evidence, that require further collaboration from the network in order to accumulate enough information to create evidence that is individually verifiable and can therefore be processed through consensus. [Fork Accountability](https:\/\/github.com\/tendermint\/spec\/blob\/master\/spec\/consensus\/light-client\/accountability.md) has been coined to describe the entire process of detection, proving and punishing of malicious behavior. This ADR addresses specifically what a light client amnesia attack is and how it can be proven and the current decision around handling light client amnesia attacks. For information on evidence handling by the light client, it is recommended to read [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md).\\n### Amnesia Attack\\nThe schematic below explains a scenario where an amnesia attack can occur such that two sets of honest nodes, C1 and C2, commit different blocks.\\n![](..\/imgs\/tm-amnesia-attack.png)\\n1. C1 and F send PREVOTE messages for block A.\\n2. C1 sends PRECOMMIT for round 1 for block A.\\n3. A new round is started, C2 and F send PREVOTE messages for a different block B.\\n4. C2 and F then send PRECOMMIT messages for block B.\\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\\nThis forged block can then be used to fool light clients trying to verify it. It must be stressed that there are a few more hurdles or dimensions to the attack to consider.For a more detailed walkthrough refer to Appendix A.\\n","Decision":"The decision surrounding amnesia attacks has both a short term and long term component. In the long term, a more sturdy protocol will need to be fleshed out and implemented. There is already draft documents outlining what such a protocol would look like and the resources it would require (see references). Prior revisions however outlined a protocol which had been implemented (See Appendix B). It was agreed that it still required greater consideration and review given it's importance. It was therefore discussed, with the limited time frame set before 0.34, whether the protocol should be completely removed or if there should remain some logic in handling the aforementioned scenarios.\\nThe latter of the two options meant storing a record of all votes in any height with which there was more than one round. This information would then be accessible for applications if they wanted to perform some off-chain verification and punishment.\\nIn summary, this seemed like too much to ask of the application to implement only on a temporary basis, whilst not having the domain specific knowledge and considering such a difficult and unlikely attack. Therefore the short term decision is to identify when the attack has occurred and implement the detector algorithm highlighted in [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md) but to not implement any accountability protocol that would identify malicious validators and allow applications to punish them. This will hopefully change in the long term with the focus on eventually reaching a concrete and secure protocol with identifying and dealing with these attacks.\\n","tokens":379,"id":4113,"Prediction":"## Decision\n\n**Title:** Handling Light Client Amnesia Attacks\n\n**Status:** Approved\n\n**Context:** As described in the Fork Accountability documentation, light client amnesia attacks are a type of malicious behavior that can occur in our network, where honest nodes commit to different blocks, and a forged block is created to fool light clients. This ADR addresses the decision on how to handle such attacks.\n\n**Problem:** The current implementation of light client amnesia attacks is vulnerable to attacks that can create forged blocks, which can be used to deceive light clients. This can lead to inconsistencies in the network and compromise the integrity of the blockchain.\n\n**Decision:** To mitigate light client amnesia attacks, we will implement a mechanism to detect and prevent such attacks. Specifically, we will:\n\n1. Implement a new data structure to store the history of PREVOTE and PRECOMMIT messages.\n2. Use this data structure to detect inconsistencies in the PREVOTE and PRECOMMIT messages received from different nodes.\n3. When inconsistencies are detected, we will trigger a re-computation of the evidence to ensure that the evidence is valid and consistent.\n4. If the evidence is invalid or inconsistent, we will reject the block and notify the network of the potential attack.\n\n**Rationale:** By implementing this mechanism, we can detect and prevent light client amnesia attacks, ensuring the integrity and consistency of the blockchain. This decision is based on the need to protect the network from malicious behavior and to ensure the reliability of the evidence.\n\n**Consequences:**\n\n* The implementation of this mechanism will require additional computational resources and storage.\n* The detection of inconsistencies in PREVOTE and PRECOMMIT messages may lead to temporary delays in the verification of blocks.\n* The rejection of blocks due to inconsistencies may lead to temporary inconsistencies in the network.\n\n**Alternatives Considered:** We considered alternative approaches, such as implementing a more complex evidence verification mechanism or using a different data structure to store PREVOTE and PRECOMMIT messages. However, these alternatives were deemed less effective or more complex than the proposed solution.\n\n**Implementation:** The implementation of this decision will be done in the following phases:\n\n1. Design and implementation of the new data structure to store PREVOTE and PRECOMMIT messages.\n2. Integration of the detection mechanism to detect inconsistencies in PREVOTE and PRECOMMIT messages.\n3. Implementation of the re-computation of evidence mechanism.\n4. Testing and validation of the implementation.\n\n**Unresolved Questions:** None.","GenTime":"2024-08-10 20:12:00"}
{"File Name":"celestia-core\/adr-018-ABCI-Validators.md","Context":"## Context\\nADR 009 introduced major improvements to the ABCI around validators and the use\\nof Amino. Here we follow up with some additional changes to improve the naming\\nand expected use of Validator messages.\\n","Decision":"### Validator\\nCurrently a Validator contains `address` and `pub_key`, and one or the other is\\noptional\/not-sent depending on the use case. Instead, we should have a\\n`Validator` (with just the address, used for RequestBeginBlock)\\nand a `ValidatorUpdate` (with the pubkey, used for ResponseEndBlock):\\n```\\nmessage Validator {\\nbytes address\\nint64 power\\n}\\nmessage ValidatorUpdate {\\nPubKey pub_key\\nint64 power\\n}\\n```\\nAs noted in [ADR-009](adr-009-ABCI-design.md),\\nthe `Validator` does not contain a pubkey because quantum public keys are\\nquite large and it would be wasteful to send them all over ABCI with every block.\\nThus, applications that want to take advantage of the information in BeginBlock\\nare _required_ to store pubkeys in state (or use much less efficient lazy means\\nof verifying BeginBlock data).\\n### RequestBeginBlock\\nLastCommitInfo currently has an array of `SigningValidator` that contains\\ninformation for each validator in the entire validator set.\\nInstead, this should be called `VoteInfo`, since it is information about the\\nvalidator votes.\\nNote that all votes in a commit must be from the same round.\\n```\\nmessage LastCommitInfo {\\nint64 round\\nrepeated VoteInfo commit_votes\\n}\\nmessage VoteInfo {\\nValidator validator\\nbool signed_last_block\\n}\\n```\\n### ResponseEndBlock\\nUse ValidatorUpdates instead of Validators. Then it's clear we don't need an\\naddress, and we do need a pubkey.\\nWe could require the address here as well as a sanity check, but it doesn't seem\\nnecessary.\\n### InitChain\\nUse ValidatorUpdates for both Request and Response. InitChain\\nis about setting\/updating the initial validator set, unlike BeginBlock\\nwhich is just informational.\\n","tokens":44,"id":4119,"Prediction":"## Decision\n\n**ADR 010: Consistent Naming and Use of Validator Messages**\n\n## Context\n\nAs mentioned in ADR 009, the ABCI has undergone significant changes to improve the handling of validators and the use of Amino. To further enhance the maintainability and usability of the code, we have decided to standardize the naming and expected use of Validator messages.\n\n## Problem\n\nThe current implementation of Validator messages has led to confusion and inconsistencies in the naming and usage of these messages. This has resulted in difficulties in understanding the purpose of each message and has made it challenging to maintain and extend the codebase.\n\n## Decision\n\nTo address this issue, we will introduce a consistent naming convention for Validator messages and define their expected use. Specifically:\n\n1. **Naming Convention**: Validator messages will follow the pattern `Validate<Verb><Noun>`, where `<Verb>` is a verb indicating the action being performed (e.g., \"Create\", \"Update\", \"Delete\") and `<Noun>` is a noun indicating the entity being validated (e.g., \"Account\", \"Transaction\", \"Block\").\n2. **Expected Use**: Validator messages will be used to validate specific entities or actions within the ABCI. Each message will be designed to perform a specific validation task, such as checking the integrity of a block or verifying the authenticity of a transaction.\n\n## Rationale\n\nBy introducing a consistent naming convention and defining the expected use of Validator messages, we can:\n\n* Improve code readability and maintainability by reducing confusion and inconsistencies.\n* Enhance the usability of the codebase by providing clear documentation and guidelines for developers.\n* Facilitate the extension and modification of the codebase by providing a clear framework for adding new Validator messages.\n\n## Consequences\n\nThe implementation of this decision will require updates to the existing codebase to conform to the new naming convention and expected use of Validator messages. This may involve renaming existing messages and updating documentation and comments to reflect the new naming convention.\n\n## Status\n\nThis decision is currently in the implementation phase. The changes will be reviewed and tested before being merged into the main codebase.\n\n## Links\n\n* ADR 009: Improvements to the ABCI around validators and the use of Amino","GenTime":"2024-08-10 20:13:22"}
{"File Name":"celestia-core\/adr-026-general-merkle-proof.md","Context":"## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding\/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n","Decision":"### ProofOperator\\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\\n```go\\ntype ProofOperator interface {\\nRun([][]byte) ([][]byte, error)\\nGetKey() []byte\\nProofOp() ProofOp\\n}\\n```\\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\\n### ProofOp\\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\\n","tokens":99,"id":4130,"Prediction":"## Decision\n\n**Decision:** Implement a `ProofOperator` interface to handle merkle proofs in `abci.ResponseQuery`, allowing for layering, chaining, and extensibility.\n\n**Context:** As described in the context, using raw `[]byte` for merkle proofs in `abci.ResponseQuery` is inflexible and makes it difficult to handle multilayer merkle proofs and general cases.\n\n**Problem:** The current implementation of merkle proofs in `abci.ResponseQuery` is not scalable and does not provide a general way to chain proofs. It also makes it difficult to add new proof formats without modifying the existing code.\n\n**Solution:** Define a `ProofOperator` interface that provides a standardized way to operate on merkle proofs. This interface will allow users to define their own merkle proof formats and layer them easily. The `ProofOperator` interface will provide methods for encoding, decoding, and combining proofs.\n\n**Rationale:** By introducing the `ProofOperator` interface, we can decouple the proof logic from the specific proof format, making it easier to add new proof formats and layer proofs. This approach also provides a clear and standardized way to operate on merkle proofs, making it easier for users to work with and extend the system.\n\n**Pros:**\n\n* Provides a general way to chain proofs\n* Allows for layering merkle proofs without decoding\/reencoding\n* Makes the proof format extensible, allowing third-party proof types\n* Decouples the proof logic from the specific proof format\n\n**Cons:**\n\n* Introduces a new interface that needs to be implemented and maintained\n* Requires changes to the existing code to accommodate the new interface\n\n**Alternatives Considered:** One alternative would be to continue using raw `[]byte` for merkle proofs and add additional logic to handle layering and chaining. However, this approach would be inflexible and difficult to maintain.\n\n**Conclusion:** Implementing the `ProofOperator` interface is the best solution for handling merkle proofs in `abci.ResponseQuery`. It provides a scalable and extensible way to operate on merkle proofs, making it easier for users to work with and extend the system.","GenTime":"2024-08-10 20:16:01"}
{"File Name":"celestia-core\/adr-005-decouple-blockid-and-partsetheader.md","Context":"## Context\\nCelestia has multiple commits to the block data via the `DataHash` and the `PartSetHeader` in the `BlockID`. As stated in the [#184](https:\/\/github.com\/celestiaorg\/lazyledger-core\/issues\/184), we no longer need the `PartSetHeader` for this additional commitment to the block's data. However, we are still planning to use the `PartSetHeader` for block propagation during consensus in the short-medium term. This means that we will remove the `PartSetHeader` from as many places as possible, but keep it in the `Proposal` struct.\\n","Decision":"While we build other better designs to experiment with, we will continue to implement the design specified here as it is not orthogonal. https:\/\/github.com\/celestiaorg\/lazyledger-core\/pull\/434#issuecomment-869158788\\n","tokens":132,"id":4152,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will remove the `PartSetHeader` from the `BlockID` and `DataHash` commits, as it is no longer necessary for the additional commitment to the block's data, but we will keep it in the `Proposal` struct for block propagation during consensus in the short-medium term.\n\n**Context**\n\nAs described in issue #184, the `PartSetHeader` is no longer needed for the additional commitment to the block's data. However, it is still planned to use the `PartSetHeader` for block propagation during consensus in the short-medium term.\n\n**Rationale**\n\nRemoving the `PartSetHeader` from the `BlockID` and `DataHash` commits simplifies the code and reduces the complexity of the system. This decision also aligns with the original intention of removing the `PartSetHeader` from as many places as possible.\n\n**Consequences**\n\n* The code will be simplified and easier to maintain.\n* The system will be less complex and more efficient.\n* The `PartSetHeader` will still be used for block propagation during consensus in the short-medium term.\n\n**Decision Owner**\n\n[Your Name]\n\n**Status**\n\nApproved\n\n**References**\n\n[#184](https:\/\/github.com\/celestiaorg\/lazyledger-core\/issues\/184)","GenTime":"2024-08-10 20:24:22"}
{"File Name":"digital-paper-edit-client\/2019-05-01-security.md","Context":"## Context and Problem Statement\\nThese are notes based on the InfoSec meeting held on the 30th of May 2019.\\n## Decision Drivers\\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\n","Decision":"* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\nWe are going with Option 2 as this provides us with security, while\\nproviding a simpler way to set up access for journalists.\\nBen Thornton has agreed to this.\\nThe benefits of this approach means that we will be\\nusing existing BBC services to implement security, rather than implementing our\\nown servers. A single VPC is sufficient for **protected** levels of service, and\\nwe can apply SecurityGroups in between to have added security.\\nThe downside of this approach is that it is not using SSO but we can add that\\nlater on.\\nIn terms of data retention, we will go with the recommended 90 days.\\n![Architecture Diagram with Security](.\/dpe-transcript-EC2_security.png)\\n","tokens":67,"id":4166,"Prediction":"Here is an Architectural Decision Record (ADR) based on the provided context and problem statement:\n\n**ADR Title:** Authentication and Authorization System for Journalists\n\n**Context and Problem Statement:**\n\nAs discussed in the InfoSec meeting on May 30th, 2019, we need to design an authentication and authorization system for journalists that balances ease of use with robust security features. The system should provide multiple security levels (public, protect, or restrict) and data retention policies based on inactivity.\n\n**Decision:**\n\nTo achieve the desired balance between ease of use and security, we will implement a multi-factor authentication system using a combination of username and password, and a second factor (e.g., one-time password or biometric authentication) for high-security access. This approach will provide an additional layer of security without compromising the ease of use for journalists.\n\n**Decision Drivers:**\n\n* Ease of use for journalists when system authenticates them: By providing a simple username and password authentication process, we can ensure that journalists can easily access the system without being overwhelmed by complex security measures.\n* Security levels (public, protect, or restrict): The multi-factor authentication system will provide three security levels, allowing us to tailor the level of security to the specific needs of each journalist or article.\n* Data retention based on inactivity: We will implement a data retention policy that automatically deletes or archives articles and user data after a certain period of inactivity, ensuring that sensitive information is not retained indefinitely.\n* Complexity: The multi-factor authentication system will provide an additional layer of security without introducing unnecessary complexity, making it easier for journalists to use and maintain.\n\n**Consequences:**\n\n* Increased security: The multi-factor authentication system will provide an additional layer of security, reducing the risk of unauthorized access to sensitive information.\n* Improved ease of use: The simple username and password authentication process will make it easier for journalists to access the system, reducing the risk of errors or frustration.\n* Data retention: The data retention policy will ensure that sensitive information is not retained indefinitely, reducing the risk of data breaches or unauthorized access.\n\n**Rationale:**\n\nThe multi-factor authentication system provides a balance between ease of use and security, making it an effective solution for our authentication and authorization needs. By providing three security levels, we can tailor the level of security to the specific needs of each journalist or article, ensuring that sensitive information is protected while still allowing for easy access. The data retention policy will also ensure that sensitive information is not retained indefinitely, reducing the risk of data breaches or unauthorized access.","GenTime":"2024-08-10 20:28:50"}
{"File Name":"digital-paper-edit-client\/2019-04-29-SQL-database-schema.md","Context":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","tokens":248,"id":4167,"Prediction":"Here is the Architectural Decision Record:\n\n## Decision\n\nWe will use a modular and incremental schema design approach for the SQL database in PostgreSQL and SQLite, with a focus on ease of versioning and reuse across different milestones and environments.\n\n## Context and Problem Statement\n\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version. It should consider the project Milestones and be easy to version incrementally. For example, have 3 versions of the schema for the 3 main milestones: Feature: Transcript Editing, Feature: Transcript Annotations, and Feature: Paper Editing.\n\n## Decision Drivers\n\n* Easy to reason around\n* Easy to version incrementally\n* Modular and reusable schema design\n\n## Rationale\n\nThe decision to use a modular and incremental schema design approach is based on the need to easily version and reuse the schema across different milestones and environments. By breaking down the schema into smaller, independent modules, we can easily add or remove tables and columns as needed, without affecting the overall schema. This approach also allows us to easily switch between different database systems, such as PostgreSQL and SQLite, without having to rewrite the entire schema.\n\n## Consequences\n\n* The schema design will be more modular and easier to reason around.\n* The schema will be easier to version incrementally, allowing us to easily track changes and roll back to previous versions if needed.\n* The schema will be more reusable across different milestones and environments, reducing the need for duplicate work and increasing the overall efficiency of the development process.\n\n## Status\n\nThis decision is approved and will be implemented in the next schema design iteration.","GenTime":"2024-08-10 20:29:08"}
{"File Name":"digital-paper-edit-client\/2019-09-28-time-estimation-for-links.md","Context":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n","Decision":"* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","tokens":49,"id":4171,"Prediction":"## Decision\n\nWe will integrate a feature to track and include the time taken for voice overs in the rough cut estimation process.\n\n## Context\n\nAs requested by the users, we need to include the time taken for voice overs in the rough cut estimation process to provide a more accurate estimate of the final product's duration.\n\n## Decision Drivers\n\n* Ease of implementation: We will implement this feature by integrating with our existing project management system, which will allow us to easily track the time spent on voice overs.\n* Accuracy of time estimated: By including the time taken for voice overs, we will provide a more accurate estimate of the final product's duration, which will help our clients plan their project timelines more effectively.\n* Extensibility: This feature will be extensible to other types of tasks, such as editing and color correction, allowing us to further improve the accuracy of our rough cut estimation process.\n\n## Decision Outcome\n\nWe will implement a feature to track and include the time taken for voice overs in the rough cut estimation process. This feature will be integrated with our existing project management system and will provide a more accurate estimate of the final product's duration.\n\n## Pros and Cons\n\nPros:\n\n* Provides a more accurate estimate of the final product's duration\n* Helps clients plan their project timelines more effectively\n* Extensible to other types of tasks\n\nCons:\n\n* May require additional resources to implement and maintain\n* May require changes to our existing project management system\n\n## Rationale\n\nWe chose to implement this feature because it will provide a more accurate estimate of the final product's duration, which is important for our clients. It will also help us to better plan our project timelines and resources. While there may be some additional resources required to implement and maintain this feature, we believe the benefits will outweigh the costs.","GenTime":"2024-08-10 20:30:15"}
{"File Name":"rails-template\/0009-use-scripts-to-rule-them-all.md","Context":"## Context\\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\\nAll[1].\\nThis repository should include reference and document this decision.\\n[1]\\nhttps:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md\\n","Decision":"By default we will follow the Scripts To Rule Them All pattern for common tasks\\nin this template.\\n","tokens":67,"id":4178,"Prediction":"## Decision\n\n**Decision:** Adopt the \"Scripts To Rule Them All\" pattern for scripting and automation in our repository.\n\n**Context:** As approved by ndxw, we will be following the \"Scripts To Rule Them All\" pattern for scripting and automation in our repository. This pattern emphasizes the importance of using scripts to automate repetitive tasks and simplify our workflow.\n\n**Problem:** Our current workflow involves manual repetitive tasks that can be time-consuming and prone to errors. We need a more efficient and scalable way to automate these tasks.\n\n**Reasoning:** The \"Scripts To Rule Them All\" pattern provides a structured approach to scripting and automation, which will help us to:\n\n* Simplify our workflow by automating repetitive tasks\n* Increase efficiency by reducing manual labor\n* Improve accuracy by minimizing human error\n* Enhance scalability by allowing us to easily add new scripts as our needs evolve\n\n**Decision Criteria:**\n\n* Simplify our workflow and reduce manual labor\n* Increase efficiency and accuracy\n* Enhance scalability and flexibility\n* Align with the approved RFC and ndxw's guidelines\n\n**Consequences:**\n\n* We will need to create and maintain a set of scripts to automate our workflow\n* We will need to ensure that our scripts are well-documented and easily maintainable\n* We will need to integrate our scripts with our existing tools and workflows\n* We will need to continuously review and update our scripts to ensure they remain effective and efficient\n\n**Status:** Approved\n\n**Implementation:** We will implement the \"Scripts To Rule Them All\" pattern by creating a set of scripts that automate our workflow. We will document our scripts and ensure they are easily maintainable. We will also integrate our scripts with our existing tools and workflows.\n\n**Monitoring and Review:** We will regularly review and update our scripts to ensure they remain effective and efficient. We will also monitor the performance and impact of our scripts on our workflow and make adjustments as needed.\n\nBy adopting the \"Scripts To Rule Them All\" pattern, we will be able to simplify our workflow, increase efficiency, and enhance scalability, ultimately improving our overall productivity and quality of work.","GenTime":"2024-08-10 20:32:33"}
{"File Name":"rails-template\/0006-use-simplecov-to-monitor-code-test-coverage.md","Context":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n","Decision":"Use Simplecov with RSpec to monitor coverage changes on every test run\\n","tokens":32,"id":4179,"Prediction":"## Decision\n\n**Decision:** Implement automated testing using a testing framework and integrate it with our CI\/CD pipeline.\n\n**Context:** We want to keep our test coverage as high as possible without having to run manual checks as these take time and are easy to forget.\n\n**Problem:** Manual testing is time-consuming and prone to human error, which can lead to a decrease in test coverage and a higher risk of introducing bugs into our production code.\n\n**Reasoning:** By implementing automated testing, we can ensure that our code is thoroughly tested and that any changes made to the code do not introduce new bugs. This will increase our confidence in the quality of our code and reduce the likelihood of errors making it into production.\n\n**Consequences:**\n\n* We will need to invest time and resources into setting up and maintaining our testing framework.\n* Our development team will need to write tests for new code and ensure that existing code is properly tested.\n* We will need to integrate our testing framework with our CI\/CD pipeline to ensure that tests are run automatically on each code change.\n\n**Alternatives Considered:**\n\n* Manual testing: While this is the simplest approach, it is time-consuming and prone to human error.\n* Automated testing with a custom solution: While this approach would allow us to tailor our testing to our specific needs, it would require a significant amount of time and resources to develop and maintain.\n\n**Decision Outcome:** We will implement automated testing using a testing framework and integrate it with our CI\/CD pipeline. This will allow us to ensure that our code is thoroughly tested and that any changes made to the code do not introduce new bugs.","GenTime":"2024-08-10 20:32:51"}
{"File Name":"push-sdk-ios\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\n[this article.][1]\\n","tokens":16,"id":4196,"Prediction":"Here is an Architectural Decision Record (ADR) for the project:\n\n## Decision\n\n### Problem\n\nWe need to record the architectural decisions made on this project to ensure that all stakeholders are aware of the design choices and to facilitate future maintenance and evolution of the system.\n\n### Context\n\nAs the project grows in complexity, it becomes increasingly important to document and communicate the architectural decisions made throughout its development. This will help to ensure that the system is designed in a way that is scalable, maintainable, and meets the needs of its users.\n\n### Solution\n\nWe will use the Architectural Decision Record (ADR) template to document each architectural decision made on the project. This will include:\n\n* A unique identifier for the decision\n* A description of the decision and its context\n* The problem being solved or the goal being achieved\n* The alternatives considered\n* The decision made\n* The rationale behind the decision\n* The expected impact on the system\n* The responsible person or team for the decision\n\n### Consequences\n\nBy documenting our architectural decisions, we can:\n\n* Ensure that all stakeholders are aware of the design choices and can make informed decisions about the system\n* Facilitate future maintenance and evolution of the system by providing a clear understanding of the design principles and trade-offs\n* Reduce the risk of making duplicate or conflicting decisions\n* Improve communication among team members and stakeholders by providing a single source of truth for architectural decisions\n\n### Status\n\nThis decision is currently in the \"Proposed\" status. It will be reviewed and approved by the project stakeholders before being implemented.\n\n### Links\n\n* [Link to the ADR template](https:\/\/github.com\/adridiac\/architecture-decisions\/blob\/master\/adr-template.md)\n* [Link to the decision log](https:\/\/github.com\/adridiac\/architecture-decisions\/blob\/master\/decisions.md)\n\n### History\n\n* 2023-02-15: Proposed\n* 2023-02-20: Reviewed and approved by project stakeholders\n* 2023-02-25: Implemented\n\nNote: This is a sample ADR, you can adjust it according to your needs and project requirements.","GenTime":"2024-08-10 20:37:25"}
{"File Name":"verify-hub\/0001-record-architechture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4206,"Prediction":"## Decision\n\n**Implement a Microservices Architecture for the Project**\n\n## Context\n\nAs we need to record the architectural decisions made on this project, we have decided to adopt a Microservices Architecture (MSA) to ensure scalability, flexibility, and maintainability of the system.\n\n## Problem\n\nOur project requires integration with multiple third-party services, handling large amounts of data, and providing a scalable and fault-tolerant infrastructure. Traditional monolithic architecture would make it challenging to meet these requirements.\n\n## Decision\n\nWe will implement a Microservices Architecture, where the system is broken down into smaller, independent services that communicate with each other using APIs. Each service will be responsible for a specific business capability, allowing for:\n\n1. **Scalability**: Each service can be scaled independently, allowing us to allocate resources more efficiently.\n2. **Flexibility**: New services can be added or removed without affecting the entire system.\n3. **Maintainability**: Each service can be developed, tested, and deployed independently, reducing the complexity of the system.\n\n## Consequences\n\n1. **Increased complexity**: With more moving parts, the system will require more effort to manage and maintain.\n2. **Higher operational costs**: We will need to invest in additional infrastructure and personnel to manage the services.\n3. **Potential for increased latency**: Inter-service communication may introduce additional latency.\n\n## Rationale\n\nThe benefits of a Microservices Architecture outweigh the potential drawbacks. By breaking down the system into smaller, independent services, we can:\n\n1. **Improve scalability**: Each service can be scaled independently to meet changing demands.\n2. **Enhance flexibility**: New services can be added or removed as needed, allowing for greater adaptability.\n3. **Improve maintainability**: Each service can be developed, tested, and deployed independently, reducing the complexity of the system.\n\n## Status\n\nThis decision is approved and will be implemented in the next development cycle.\n\n## Links\n\n* [Microservices Architecture](https:\/\/microservices.io\/)\n* [Benefits of Microservices Architecture](https:\/\/www.infoq.com\/articles\/microservices-architecture-benefits)","GenTime":"2024-08-10 20:40:58"}
{"File Name":"react-library-template\/0006-use-jest.md","Context":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":4223,"Prediction":"## Decision\n\n**Decision:** We will use Jest as our test framework for our React application, leveraging its built-in support for React and TypeScript.\n\n## Context\n\nAs mentioned, Jest is the standard and recommended test framework for React applications. Its integration with React provides a seamless testing experience, allowing us to write and run tests for our React components with ease. Additionally, Jest's support for TypeScript ensures that we can take advantage of TypeScript's type checking and other features while writing our tests.\n\n## Rationale\n\n* Jest's extensive support for React, including its ability to automatically detect and render React components, makes it an ideal choice for our React-based application.\n* Jest's integration with TypeScript allows us to leverage TypeScript's type checking and other features while writing our tests, ensuring that our tests are robust and reliable.\n* Using Jest as our test framework aligns with the standard and recommended testing approach for React applications, making it easier for our team to onboard and collaborate on testing efforts.\n\n## Consequences\n\n* We will need to configure Jest in our project and set up our test environment to take advantage of its features.\n* We will need to write tests for our React components using Jest's API and syntax.\n* We will need to ensure that our tests are robust and reliable, using Jest's built-in features such as mocking and snapshot testing.\n\nBy choosing Jest as our test framework, we can ensure a seamless testing experience for our React application, leveraging its built-in support for React and TypeScript.","GenTime":"2024-08-10 20:46:16"}
{"File Name":"react-library-template\/0005-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":4224,"Prediction":"## Decision\n\nWe will use ESLint as our primary linter for our JavaScript and TypeScript codebase, and integrate it into our development workflow to ensure consistency and catch errors automatically.\n\n## Context\n\nAs you mentioned, linting is a good practice to enforce consistency in our code and catch errors automatically. ESLint is a widely-used and well-maintained linter that has good support for both JavaScript and TypeScript, making it a suitable choice for our codebase.\n\n## Rationale\n\nUsing ESLint will allow us to:\n\n* Enforce a consistent coding style throughout our codebase\n* Catch errors and warnings early in the development process, reducing the likelihood of introducing bugs\n* Improve code readability and maintainability\n* Provide a unified way of handling errors and warnings across our codebase\n\n## Consequences\n\n* We will need to configure ESLint to fit our specific coding style and requirements\n* We will need to integrate ESLint into our development workflow, including setting up automated checks and reporting\n* We may need to make changes to our code to conform to ESLint's rules and best practices\n\n## Alternatives\n\n* We could use a different linter, such as TSLint or JSLint, but ESLint is widely used and has good support for TypeScript\n* We could not use a linter at all, but this would likely lead to inconsistencies and errors in our codebase\n\n## Status\n\nThis decision is approved and will be implemented in our codebase.","GenTime":"2024-08-10 20:46:32"}
{"File Name":"react-library-template\/0007-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":4225,"Prediction":"## Decision\n\n**Title:** Implement Dependabot to manage dependency updates\n\n**Context:** We want to ensure all dependencies stay up to date. Dependabot offers a service where a bot opens PRs on GitHub when new versions of dependencies are released.\n\n**Decision:** We will implement Dependabot to manage dependency updates in our project.\n\n**Rationale:**\n\n* Dependabot is a widely-used and well-maintained service that can automatically detect and open PRs for dependency updates.\n* This will save our team time and effort in manually checking for updates and creating PRs.\n* Dependabot can also help us to ensure that our dependencies are up to date, which is important for security and stability.\n* We can customize Dependabot to fit our specific needs and workflow.\n\n**Consequences:**\n\n* We will need to configure Dependabot in our GitHub repository.\n* We will need to review and merge the PRs opened by Dependabot.\n* We may need to make changes to our build process to accommodate the updated dependencies.\n\n**Alternatives Considered:**\n\n* Manually checking for updates and creating PRs.\n* Using a different dependency management tool.\n* Not using a dependency management tool at all.\n\n**Status:** Implemented\n\n**Links:**\n\n* Dependabot documentation: [insert link]\n* Our GitHub repository configuration: [insert link]","GenTime":"2024-08-10 20:46:47"}
{"File Name":"react-library-template\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":4226,"Prediction":"## Decision\n\n**Decision:** We will adopt Prettier as our official code formatter to ensure consistency in code style across all projects.\n\n**Context:** As mentioned, we want to ensure that all projects use a single, familiar code style. Prettier is an opinionated code formatter that supports most languages in the JavaScript ecosystem and has become a standard in the industry, with over 1 million repositories using it on GitHub.\n\n**Rationale:**\n\n* Consistency: By adopting Prettier, we can ensure that all code in our projects follows a consistent style, making it easier for developers to read and maintain the codebase.\n* Efficiency: Prettier automates the formatting process, saving developers time and reducing the risk of human error.\n* Industry standard: Prettier is widely adopted and recognized as a standard in the industry, making it easier for new team members to adapt to our codebase.\n* Language support: Prettier supports most languages in the JavaScript ecosystem, making it a versatile choice for our projects.\n\n**Consequences:**\n\n* All new code will be formatted according to Prettier's guidelines.\n* Existing code will be formatted using Prettier as part of our continuous integration\/continuous deployment (CI\/CD) pipeline.\n* Developers will be required to use Prettier to format their code before committing changes.\n* We will provide guidance on how to configure Prettier for specific projects and languages.\n\n**Status:** Implemented\n\n**Owners:** [List the names of the team members or stakeholders responsible for implementing and maintaining this decision]\n\n**History:**\n\n* [Insert dates and descriptions of changes made to this decision]\n\nThis decision is subject to review and revision as needed.","GenTime":"2024-08-10 20:47:06"}
{"File Name":"react-library-template\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4229,"Prediction":"Here is an Architectural Decision Record (ADR) for your software project:\n\n**Decision**\n\nAs the software development team, we have decided to use a microservices architecture for our project, with each microservice responsible for a specific business capability.\n\n**Context**\n\nOur project involves developing a complex system that requires integrating multiple services, including user authentication, payment processing, and data storage. We have identified that a monolithic architecture would lead to a tightly coupled system that is difficult to maintain, scale, and evolve. A microservices architecture would allow us to break down the system into smaller, independent services that can be developed, tested, and deployed independently.\n\n**Problem**\n\nOur initial design proposed a monolithic architecture, which would lead to the following problems:\n\n* Tightly coupled components would make it difficult to modify or replace individual components without affecting the entire system.\n* A single point of failure would exist, making the system vulnerable to downtime or errors.\n* Scalability and performance would be limited by the monolithic architecture.\n\n**Reasoning**\n\nWe have chosen a microservices architecture because it:\n\n* Allows for independent development, testing, and deployment of individual services, reducing the complexity and risk of changes.\n* Enables scalability and performance improvements by allowing each service to be scaled independently.\n* Provides a more flexible and adaptable system that can evolve with changing business requirements.\n\n**Decision Drivers**\n\n* Complexity of the system and the need for independent development and testing of individual services.\n* Scalability and performance requirements.\n* Business requirements for flexibility and adaptability.\n\n**Consequences**\n\n* Increased complexity in terms of communication and integration between services.\n* Additional overhead in terms of service discovery, load balancing, and monitoring.\n* Potential for increased latency due to the need for communication between services.\n\n**Pros and Cons**\n\nPros:\n\n* Increased scalability and performance.\n* Improved flexibility and adaptability.\n* Easier maintenance and evolution of individual services.\n\nCons:\n\n* Increased complexity.\n* Additional overhead in terms of service discovery, load balancing, and monitoring.\n* Potential for increased latency.\n\n**Status**\n\nThis decision is currently in the \"implemented\" status, as we have started implementing the microservices architecture in our project.\n\n**References**\n\n* [1] \"Microservices Architecture\" by Martin Fowler\n* [2] \"Designing Distributed Systems\" by Brendan Burns\n\n**Links**\n\n* [1] https:\/\/martinfowler.com\/articles\/microservices.html\n* [2] https:\/\/www.oreilly.com\/library\/view\/designing-distributed-systems\/9781492047771\/","GenTime":"2024-08-10 20:48:07"}
{"File Name":"Horace\/0012-use-network-storage-for-large-datafiles.md","Context":"## Context\\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and \"expected\" results for unit and system testing.\\nThese data files are too large to store in GitHub along side the test code, but will not change frequently.\\nSimilar data files are also required for Euphonic testing.\\n","Decision":"The data will be stored in STFC hosted SAN (storage area network).\\nTests will read the data from this network storage location, either by copying the files locally or reading the remote file.\\n","tokens":78,"id":4234,"Prediction":"## Decision\n\nTo store the large `sqw` and `nxspe` data files, we will utilize a cloud-based object storage service, such as Amazon S3 or Google Cloud Storage, instead of storing them alongside the test code in GitHub.\n\n## Context\n\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and \"expected\" results for unit and system testing. These data files are too large to store in GitHub along side the test code, but will not change frequently. Similar data files are also required for Euphonic testing.\n\n## Problem\n\nStoring large data files in GitHub alongside the test code is not feasible due to size constraints and versioning issues. The data files are not expected to change frequently, making it unnecessary to store them in a version-controlled repository like GitHub.\n\n## Decision\n\nWe will use a cloud-based object storage service to store the large data files. This approach provides several benefits:\n\n* Scalability: Cloud storage services can handle large amounts of data and scale as needed.\n* Flexibility: We can access the data files from anywhere and use them in our testing environment.\n* Cost-effectiveness: Storing data files in the cloud is more cost-effective than storing them on local machines or in a version-controlled repository.\n\n## Consequences\n\n* We will need to configure our testing environment to access the data files stored in the cloud.\n* We will need to ensure that the data files are properly secured and access-controlled to prevent unauthorized access.\n* We will need to consider the costs associated with storing and retrieving data files from the cloud.\n\n## Rationale\n\nStoring large data files in a cloud-based object storage service is the most practical and cost-effective solution for our testing environment. It allows us to scale our storage needs as required, while keeping our GitHub repository lightweight and focused on code.","GenTime":"2024-08-10 20:49:51"}
{"File Name":"Horace\/0017-separate-absolute-and-relative-indexing-APIs-in-pixel-array.md","Context":"## Context\\nThe pixel array within an SQW object can be too large to fit into memory.\\nTo avoid running out of memory, the object holding the pixel array can be\\nfile-backed.\\nThis means that only a \"page\" of the pixel array is loaded into memory at any\\none time.\\nTherefore two possible ways to index into the pixel array exist:\\n1. **Absolute index**:\\n_The position of the pixel in the full, file-backed, pixel array_.\\n2. **Relative index**:\\n_The position of the pixel in the currently loaded page of pixel data._\\n","Decision":"There will be two separate APIs for accessing data,\\nthese APIs distinguish between the two types of indexing.\\n1. **Perform absolute indexing  using `get_` methods:**\\nObtaining a subset of pixels or pixel data by absolute index will be\\npossible using a `get_` method.\\nFor example, the following will retrieve pixels 100-200 by absolute index:\\n```matlab\\npixels.get_pixels(100:200)\\n```\\nSimilarly, to retrieve a range of data from particular pixel array fields:\\n```matlab\\npixels.get_data({'signal', 'variance'}, 100:200)\\n```\\nAt time of writing, there are no plans to implement similar `set_`\\nmethods with absolute indexing.\\nIf there becomes a requirement for setters with absolute indexing,\\nthese should follow a similar syntax.\\nFor example, the following should set signal and variance from absolute\\nindex 100 to 200, to zero:\\n```matlab\\nset_data({'signal', 'variance'}, 100:200, zeros(2, 101));\\n```\\n2. **Perform relative indexing using attribute:**\\nObtaining pixel data using an attribute will return just the data for the\\ncurrently cached page.\\nHence, indexing into these attributes will be relative.\\nFor example, the following will retrieve the signal values of pixels 10-20\\nin the currently cached page:\\n```matlab\\npixels.signal(10:20)\\n```\\nSetting of pixel data will be possible using attributes and relative\\nindexing.\\nFor example the following will set pixels 1 to 20 on the current page to\\nzero:\\n```matlab\\npixels.signal(1:20) = 0\\n```\\n","tokens":128,"id":4236,"Prediction":"## Decision\n\n**Decision:** We will use a hybrid indexing approach that combines both absolute and relative indexing for the pixel array within an SQW object.\n\n**Context:** As mentioned, the pixel array within an SQW object can be too large to fit into memory, and to avoid running out of memory, the object can be file-backed. This means that only a \"page\" of the pixel array is loaded into memory at any one time. Therefore, two possible ways to index into the pixel array exist: absolute index, which represents the position of the pixel in the full, file-backed, pixel array, and relative index, which represents the position of the pixel in the currently loaded page of pixel data.\n\n**Problem:** The main problem is that both absolute and relative indexing are useful in different scenarios, and using only one of them would limit the functionality of the SQW object. Absolute indexing is necessary when accessing pixels outside the current page, while relative indexing is necessary when accessing pixels within the current page.\n\n**Constraints:** The constraints are that we need to balance the need for efficient memory usage with the need for easy access to the pixel array.\n\n**Decision Criteria:**\n\n1. **Flexibility:** We need to be able to access pixels both within and outside the current page.\n2. **Efficiency:** We need to minimize memory usage and optimize page loading.\n3. **Ease of use:** We need to provide a simple and intuitive way for users to access the pixel array.\n\n**Decision:** We will use a hybrid indexing approach that combines both absolute and relative indexing. This approach will allow us to access pixels both within and outside the current page, while also minimizing memory usage and optimizing page loading.\n\n**Implementation:** We will implement the hybrid indexing approach by maintaining two separate indices: an absolute index and a relative index. The absolute index will be used to track the position of the pixel in the full, file-backed, pixel array, while the relative index will be used to track the position of the pixel in the currently loaded page of pixel data. When accessing a pixel, we will first check if it is within the current page using the relative index. If it is, we will use the relative index to access the pixel. If it is not, we will use the absolute index to access the pixel and load the necessary page into memory if necessary.\n\n**Rationale:** The hybrid indexing approach provides the flexibility to access pixels both within and outside the current page, while also minimizing memory usage and optimizing page loading. It also provides a simple and intuitive way for users to access the pixel array.\n\n**Alternatives:** The alternative to the hybrid indexing approach would be to use only one of the two indexing approaches. However, this would limit the functionality of the SQW object and make it less efficient.","GenTime":"2024-08-10 20:50:40"}
{"File Name":"Horace\/0007-use-herbert-as-library-dependency.md","Context":"## Context\\nThe Horace and Herbert projects are tightly coupled. Herbert build artifacts are required for integration and MATLAB testing of Horace source.\\nChanges made to Herbert may change or break dependent MATLAB or C++ code in Horace.\\n","Decision":"To make the depdencency explicit Herbert will be regarded as a library.\\nAs a consequence:\\n- Herbert builds will NOT trigger Horace builds\\n- Horace builds (both `PR` and `master`) will always use the latest `master` build of Herbert\\n- Build artifacts will will copied from the latest successful `master-<target-os>-<target-matlab>` build on the Herbert CI server.\\n","tokens":48,"id":4245,"Prediction":"## Decision\n\n**Tightly Coupled Projects: Horace and Herbert**\n\n## Context\n\nThe Horace and Herbert projects are tightly coupled, where Herbert build artifacts are required for integration and MATLAB testing of Horace source. Changes made to Herbert may change or break dependent MATLAB or C++ code in Horace.\n\n## Problem\n\nThe tight coupling between Horace and Herbert projects poses several challenges:\n\n1. Changes to Herbert may break Horace, leading to integration issues and delays.\n2. Herbert's changes may require corresponding changes to Horace, increasing the complexity of the development process.\n3. The coupling between the projects makes it difficult to maintain and evolve them independently.\n\n## Decision\n\nTo mitigate the risks associated with the tight coupling between Horace and Herbert, we will implement a service-based architecture. Specifically:\n\n1. We will introduce a service layer between Horace and Herbert, which will encapsulate the interaction between the two projects.\n2. The service layer will provide a standardized interface for Herbert to communicate with Horace, allowing for loose coupling and reducing the impact of changes to Herbert on Horace.\n3. We will use APIs and message queues to enable asynchronous communication between the service layer and Horace, reducing the need for tight coupling and improving the overall system's scalability and maintainability.\n\n## Consequences\n\n1. The introduction of a service layer will increase the complexity of the system, but it will also provide a clear separation of concerns and improve the maintainability of the codebase.\n2. The service layer will act as a buffer between Horace and Herbert, allowing for changes to Herbert to be made independently without affecting Horace.\n3. The use of APIs and message queues will enable asynchronous communication, improving the system's responsiveness and scalability.\n\n## Rationale\n\nThe service-based architecture will provide a flexible and scalable solution for the tight coupling between Horace and Herbert. By introducing a service layer, we will reduce the impact of changes to Herbert on Horace, improving the overall maintainability and evolvability of the system.","GenTime":"2024-08-10 20:52:51"}
{"File Name":"laundromat\/0003-choice-of-entities.md","Context":"## Context\\nThe choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:\\n* It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)\\n* It is a so-called \u201cspecial categories\u201d of information (e.g. medical information)\\n* It is present in the data in non-trivial quantities\\n","Decision":"We have chosen the following NER entities:\\n* ORG (Organisation)\\n* LOC (Location)\\n* PER (Person)\\n* FNR (Personal number)\\n* MONEY\\n* DATE_TIME (Dates, time of day, name of day, and name of month)\\n* MEDICAL_CONDITIONS\\nEntities that will be left purely to RegEx are:\\n* NAV_YTELSE and NAV_OFFICE\\n* AGE\\n* TLF (Telephone number)\\n* BACC (Bank account number)\\nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.\\n","tokens":134,"id":4261,"Prediction":"## Decision\n\n**Title:** Entity Selection Criteria for Anonymization\n\n**Context:** As mentioned in the provided context, the choice of entities to anonymize will significantly impact the performance of our model, and we are constrained by time and the number of entities that can be included.\n\n**Decision:** We will include entities that fulfill at least two of the following three categories:\n\n1. **Direct Identifiers or Close Proxies**: Entities that are directly identifying or serve as close proxies, such as names, phone numbers, and other personally identifiable information.\n2. **Special Categories of Information**: Entities that belong to special categories of information, such as medical information, that require additional protection and handling.\n3. **Entities Present in Non-Trivial Quantities**: Entities that appear in the data in non-trivial quantities, indicating their significance and potential impact on the model's performance.\n\n**Rationale:** By focusing on entities that meet at least two of these criteria, we can ensure that we are prioritizing the most critical and sensitive information for anonymization. This approach will help us strike a balance between performance and time constraints, while also maintaining the integrity and security of the data.\n\n**Consequences:**\n\n* The model's performance may be impacted by the reduced number of entities included.\n* The anonymization process may be more complex and time-consuming due to the increased specificity of the entity selection criteria.\n* The security and integrity of the data will be maintained by prioritizing the most sensitive information.\n\n**Alternatives Considered:** We considered alternative approaches, such as including all entities that meet any of the criteria or using a more general set of criteria. However, these alternatives were deemed less effective in balancing performance and security constraints.","GenTime":"2024-08-10 20:57:41"}
{"File Name":"backdrop\/adr-002-persistent-storage.md","Context":"# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","Decision":"We will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","tokens":403,"id":4266,"Prediction":"## Decision\n\nThe decision is to use MongoDB as the technology for providing persistence for the Performance Platform.\n\n## Context\n\nThe Performance Platform is expected to take in unstructured data from various sources and allow users to collect and analyze the data in a single place. The volume of data is currently small, and the platform will be built in a small scale before scaling up. The data will not be the canonical source, and therefore, ACID properties and CAP theorem are not a major concern. The team has experience with MongoDB in production and is comfortable operating it.\n\n## Status\n\nThe decision has been accepted.\n\n## Consequences\n\nThe consequence of this decision is to use MongoDB with an appropriate replica-set configuration to ensure high availability and data durability.","GenTime":"2024-08-10 20:58:44"}
{"File Name":"sexual-health-service-finder\/0002-use-express-web-framework.md","Context":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":41,"id":4271,"Prediction":"## Decision\n\n**Decision:** We will use the Express.js web framework for our project.\n\n**Context:** A web framework is needed in order to provide a level of abstraction over low-level language features. Effort spent solving non-domain specific problems should be kept to an absolute minimum.\n\n**Problem:** We need a web framework that allows us to focus on building our application's core logic without worrying about the underlying infrastructure.\n\n**Reasoning:** Express.js is a lightweight and flexible web framework that provides a simple and intuitive way to build web applications. It has a small footprint and is well-suited for building scalable and high-performance web applications. Additionally, Express.js has a large and active community, which means there are many resources available for learning and troubleshooting.\n\n**Consequences:**\n\n* We will be able to focus on building our application's core logic without worrying about the underlying infrastructure.\n* We will have access to a large and active community of developers who can provide support and resources.\n* We will be able to build a scalable and high-performance web application.\n\n**Alternatives Considered:** We considered using other web frameworks such as Koa.js or Hapi, but ultimately decided on Express.js due to its simplicity and flexibility.\n\n**Status:** This decision is final and has been implemented in our project.","GenTime":"2024-08-10 21:00:32"}
{"File Name":"sexual-health-service-finder\/0005-calculate-distance-between-origin-and-result-items-within-the-application.md","Context":"## Context\\nThe move to Azure search has introduced the need to calculate the distance\\nbetween the search point and each result item. Previously, when using\\nElasticsearch, the distance was returned within the query response. Azure\\nsearch does not have this capability, it is currently a\\n[feature request](https:\/\/feedback.azure.com\/forums\/263029-azure-search\/suggestions\/17760211-support-geo-distance-in-select-result).\\n","Decision":"The decision is to calculate the distance between the search point and each\\nresult item within the consuming application i.e. the web app. The calculation\\nfor\\n[great-circle distance](https:\/\/en.wikipedia.org\/wiki\/Great-circle_distance)\\nis well known and available in numerous languages.\\n","tokens":90,"id":4274,"Prediction":"## Decision\n\n**Decision:** Implement a separate calculation of distance between the search point and each result item in the Azure Search service.\n\n**Context:** The move to Azure Search has introduced the need to calculate the distance between the search point and each result item. Previously, when using Elasticsearch, the distance was returned within the query response. Azure Search does not have this capability, it is currently a feature request.\n\n**Rationale:**\n\n* Azure Search does not currently support returning distance as part of the query response, making it necessary to implement a separate calculation.\n* Calculating distance between the search point and each result item is a common use case in many applications, especially those that require geospatial search.\n* Implementing a separate calculation will allow us to maintain flexibility and scalability in our search functionality, even in the absence of native support for distance calculation in Azure Search.\n\n**Consequences:**\n\n* Additional complexity in our search implementation, as we will need to handle the calculation of distance separately from the query response.\n* Potential performance impact, as we will need to make additional requests to the Azure Search service to retrieve the necessary data for distance calculation.\n* Increased latency in our search results, as the distance calculation will be performed after the initial query response.\n\n**Alternatives Considered:**\n\n* Waiting for Azure Search to implement native support for distance calculation, which may take an unknown amount of time.\n* Using a third-party service to handle geospatial search and distance calculation, which may add additional complexity and cost to our application.\n\n**Status:** Accepted\n\n**Implementors:** [List the names of the developers or teams responsible for implementing this decision]\n\n**Links:** [Link to the Azure Search feature request for distance calculation]","GenTime":"2024-08-10 21:01:27"}
{"File Name":"sexual-health-service-finder\/0006-use-in-house-postcodes-search-rather-than-postcodes-io.md","Context":"## Context\\nThe application has been using https:\/\/postcodes.io\/. Postcodes.io is available\\nfor free and does not require any relationship between consumer and supplier.\\nOne of the drawbacks to this is that although support is available when using\\nthe free tier, there is no provision for any specific support relationship.\\nMore formal relationships are available but this is through the paid product\\nhttps:\/\/ideal-postcodes.co.uk\/.  Given the number of requests and the fact the\\nfree tier was supplying the information required, it had previously been\\ndecided there was no need to migrate to a paid product. However, the strategic\\ndirection has been set such that high value APIs, usable across many products\\nshould be supplied in-house. To that end, there is now an in-house postcode\\nsearch available.\\n","Decision":"The decision is to replace the use of postcodes.io with the in-house postcode\\nlookup, as per the strategic direction.\\n","tokens":165,"id":4276,"Prediction":"## Decision\n\n**Decision:** Migrate the postcode lookup functionality from Postcodes.io to the in-house postcode search solution.\n\n## Context\n\nThe application has been using Postcodes.io, a free API that provides postcode information. While the free tier has been sufficient for the application's needs, the lack of a formal support relationship and the potential for downtime or changes to the API have raised concerns. The strategic direction has shifted towards developing in-house high-value APIs that can be used across multiple products, and an in-house postcode search solution is now available.\n\n## Problem\n\nThe current reliance on Postcodes.io's free tier poses the following problems:\n\n1. Lack of formal support relationship: While support is available, there is no dedicated support channel or priority for issues specific to our application.\n2. Potential downtime or changes to the API: As a free service, Postcodes.io may change or discontinue its API at any time, affecting our application's functionality.\n3. Limited scalability: As the application's usage increases, the free tier may not be able to handle the load, leading to performance issues or errors.\n\n## Decision Rationale\n\nThe decision to migrate to the in-house postcode search solution is driven by the following factors:\n\n1. Control and reliability: By developing the API in-house, we can ensure that it is designed and implemented to meet our specific requirements, providing a more reliable and stable service.\n2. Customization: The in-house solution can be tailored to our specific needs, allowing for more precise and accurate results.\n3. Scalability: The in-house solution can be designed to handle increased traffic and usage, ensuring that the application remains performant and available.\n4. Strategic alignment: Developing in-house high-value APIs aligns with the company's strategic direction, enabling the creation of a robust and scalable technology foundation.\n\n## Consequences\n\nThe migration to the in-house postcode search solution will require:\n\n1. Development effort: The in-house solution will require development and testing, which will involve additional resources and time.\n2. Integration: The new API will need to be integrated into the application, which may require changes to existing code and infrastructure.\n3. Potential disruption: The migration may cause temporary disruption to the application's functionality, requiring careful planning and testing to minimize the impact.\n\nHowever, the benefits of having a reliable, scalable, and customizable in-house API will outweigh the costs and challenges, ensuring a more robust and sustainable technology foundation for the application.","GenTime":"2024-08-10 21:02:10"}
{"File Name":"early-careers-framework\/0003-separate-ecf-and-npq-calculation-engines-and-hashes-as-interfaces.md","Context":"## Context\\nThere are currently two different training schemes in scope for the track-and-pay project. There are rumours of a possible third in the future.\\nThere are some similarities and many differences in the inputs, outputs and maths for payments for these training schemes. For example:\\n1. Both have fixed payments at 40%\\n2. Both allow pulling some of the fixed payment into earlier \"setup\" payments (for cashflow).\\n3. The output payments are very different in detail.\\n4. They have different banding systems.\\n### People\\nTim Abell and Pavel Lisovin (track and pay developers) discussed the issue amongst ourselves and came to this as a decision for the time being.\\n","Decision":"1. Build two payment engines that do not share code.\\n2. Have similar input\/output interfaces (ruby hash structures) that can later be easily converted to JSON.\\n3. Use similar patterns for both engines of Gherkin BDD driven unit tests plus normal rspec unit tests.\\n","tokens":146,"id":4277,"Prediction":"## Decision\n\n**Decoupling of Payment Logic from Training Scheme**\n\n## Context\n\nThere are currently two different training schemes in scope for the track-and-pay project, with rumors of a possible third in the future. While there are similarities between the schemes, there are also many differences in the inputs, outputs, and math for payments. This complexity makes it challenging to maintain and extend the payment logic.\n\n## Problem\n\nThe current approach of having a single, monolithic payment logic module is prone to errors and difficult to maintain. As new training schemes are introduced, the payment logic will become increasingly complex and harder to understand.\n\n## Decision\n\nTo address this issue, we will decouple the payment logic from the training scheme. We will create a separate module for each training scheme, with each module responsible for its own payment logic. This will allow us to:\n\n* Keep each payment logic module simple and focused on its specific requirements\n* Easily maintain and extend each module independently\n* Avoid errors and complexity that arise from mixing different payment logic together\n\n## Consequences\n\n* Each training scheme will have its own payment logic module, which will be easier to understand and maintain\n* The payment logic modules will be more modular and reusable, reducing code duplication and increasing overall maintainability\n* As new training schemes are introduced, we can simply add a new payment logic module, without affecting the existing modules\n\n## Rationale\n\nTim Abell and Pavel Lisovin, the track-and-pay developers, discussed the issue and came to this decision as a temporary solution. Decoupling the payment logic from the training scheme will allow us to better manage the complexity and maintainability of the codebase.\n\n## Status\n\nThis decision is currently in progress, and we will continue to monitor and evaluate its effectiveness as the project evolves.","GenTime":"2024-08-10 21:02:29"}
{"File Name":"open-apparel-registry\/adr-002-decide-how-to-display-more-facilities.md","Context":"## Context\\nThe Open Apparel Registry currently includes more than 18,000 facilities. For\\nperformance reasons, we have paginated the facilities data API endpoint data so\\nthat it will [return a maximum of 500 results][pagination-pr] for any single\\nrequest. In turn this means that the frontend client will only ever display a\\nmaximum of 500 facilities at a time, rendered as clustered Leaflet markers via\\nReact-Leaflet. Facilities API requests are currently filtered using Django\\nquerysets whose inputs are querystring parameters included in the API requests.\\nTo enable users to view all of the OAR's facilities on the map simultaneously,\\nwe'll need to update how the API returns facilities for display and how the\\nclient renders them on the map. At present this means updating the application\\nso that it can display 18,000+ facilities simultaneously. Following upcoming MSI\\nintegration work, we anticipate that the number of OAR facilities will increase\\nto around 100,000 -- which the application should be able to map. In addition,\\nwe also want users to be able to filter these vector tiles by query parameters\\nlike contributor, facility name, and country, along with the map bounding box.\\nTo accomplish this we have decided to use vector tiles generated, ultimately,\\nby PostGIS's [`ST_AsMVT`][st-asmvt] function, rendering them in the frontend\\nwith [Leaflet Vector Grid][leaflet-vector-grid] (possibly via\\n[react-leaflet-vector-grid][react-leaflet-vector-grid]). We've decided to have\\nthe vector tiles cluster facilities by zoom level, which would limit the number\\nof actual points the frontend needs to display at any given time.\\nThis ADR documents a subsequent decision between setting up a dedicated\\n`ST_AsMVT`-based vector tile server, like [Martin][martin] or adding a new\\nvector tile endpoint to the existing Django web application which would make\\nthe `ST_AsMVT` query.\\n","Decision":"We have decided to add a vector tile endpoint to the existing Django app.\\nWhile Martin, in particular, seemed like a compelling solution, we had enough\\nopen questions about it to discourage us from taking on the complexity of\\nusing it here.\\nOur main apprehension about adding a tile endpoint to the existing web app is\\nthat it'll mingle tile requests with other requests in a way that could cause\\nperformance problems. However, given the size of the OAR's traffic and the\\npossibility of addressing traffic increases by scaling the number of app\\ninstances, this seemed like an acceptable tradeoff.\\n","tokens":412,"id":4281,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to add a new vector tile endpoint to the existing Django web application, rather than setting up a dedicated `ST_AsMVT`-based vector tile server.\n\n## Context\n\nAs mentioned in the context, we need to update the application to display over 18,000 facilities simultaneously, with the potential for this number to increase to around 100,000 after upcoming MSI integration work. We also want to enable users to filter these facilities by query parameters like contributor, facility name, and country, along with the map bounding box. To accomplish this, we have decided to use vector tiles generated by PostGIS's `ST_AsMVT` function, rendering them in the frontend with Leaflet Vector Grid.\n\n## Problem\n\nThe main problem we are trying to solve is how to efficiently handle the large number of facilities and provide a good user experience. We need to find a way to limit the number of actual points the frontend needs to display at any given time, while still allowing users to filter and view all facilities on the map.\n\n## Decision Rationale\n\nWe have decided to add a new vector tile endpoint to the existing Django web application for several reasons:\n\n* It allows us to reuse the existing infrastructure and avoid setting up a new dedicated server.\n* It provides a more seamless integration with the existing application, reducing the complexity of the solution.\n* It allows us to take advantage of the existing query filtering capabilities in the Django web application, making it easier to implement the filtering functionality.\n\n## Consequences\n\nBy adding a new vector tile endpoint to the existing Django web application, we will need to implement the `ST_AsMVT` query in the backend and handle the generation and caching of vector tiles. This will require additional development effort, but it will provide a more efficient and scalable solution for handling the large number of facilities.","GenTime":"2024-08-10 21:03:33"}
{"File Name":"bob\/0004-introduce-temporal-db.md","Context":"## Context\\nCurrently we are using a traditional RDBMS, PostgreSQL as the storage of all the CI\/CD state.\\nThe DB is the one and only state of the whole CI\/CD cluster, storing all of the pipeline, resource, artifact definitions, runs, logs etc. For all of these, tracking historical changes is of utmost importance.\\nUsing a traditional CRUD workflow with PostgeSQL raises the following issues:\\n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster down\\n- Central locking is a severe impediment to scale\\n- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needs\\n- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place system\\n- The tabular structure comes in our way with rigidity and is not simple to introduce schema changes\\n- Need of elaborate migration strategies which is further complicated with distributed state\\n","Decision":"Based on the above facts the following is decided:\\n- Use [XTDB](https:\/\/xtdb.com) as the temporal, document store for the following reasons:\\n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production use\\n- Has a quite unbundled design and uses a variety of storage backends and is transactor free\\n- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time\\n- Has an HTTP interface for non JVM clients\\n- Use it with JDBC\/PostgreSQL backend which is quite readily available and managed in all popular cloud providers\\n- Remove the CRUD way of doing things, expose the DB too via API for more powerful, direct analytical querying\\n","tokens":209,"id":4285,"Prediction":"## Decision\n\n**Decision:** Migrate from a traditional RDBMS (PostgreSQL) to a distributed, schema-agnostic, and event-sourced storage solution, using Apache Cassandra and Apache Kafka, to improve scalability, analytics, and auditing capabilities.\n\n## Context\n\nAs described in the context, the current traditional RDBMS approach using PostgreSQL has several limitations, including:\n\n* Difficulty in analytics due to coupled storage and querying\n* Central locking, which impedes scalability\n* Difficulty in tracking changes across time for audit and rollback needs\n* Limited historical logging for debugging failures\n* Rigidity in schema changes\n* Need for elaborate migration strategies due to distributed state\n\n## Rationale\n\nTo address these limitations, we will adopt a distributed, schema-agnostic, and event-sourced storage solution. Apache Cassandra will provide a scalable and fault-tolerant storage layer, while Apache Kafka will enable event-driven architecture and provide a unified event stream for auditing, analytics, and debugging.\n\nBy using Apache Cassandra, we will:\n\n* Decouple storage and querying, enabling faster and more efficient analytics\n* Improve scalability by distributing data across multiple nodes\n* Simplify schema changes and reduce the need for elaborate migration strategies\n\nBy using Apache Kafka, we will:\n\n* Provide a unified event stream for auditing, analytics, and debugging\n* Enable event-driven architecture, allowing for more efficient and scalable processing of events\n* Simplify tracking changes across time for audit and rollback needs\n\n## Consequences\n\n* The migration will require significant changes to the existing architecture and codebase\n* The team will need to learn and adapt to new technologies and concepts\n* The solution will require careful planning and testing to ensure data consistency and integrity\n* The solution will provide improved scalability, analytics, and auditing capabilities, enabling more efficient and effective management of the CI\/CD cluster.\n\n## Alternatives Considered\n\n* Other NoSQL databases, such as MongoDB or Redis, were considered but did not meet the requirements for scalability and event-sourced architecture.\n* A hybrid approach, using a combination of traditional RDBMS and NoSQL databases, was also considered but was deemed too complex and difficult to maintain.\n\n## Decision Date\n\n2023-02-15","GenTime":"2024-08-10 21:05:01"}
{"File Name":"molgenis-js-auth\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":4291,"Prediction":"Here is an example of an Architectural Decision Record (ADR):\n\n**Decision**\n\nTitle: Use of Microservices Architecture for the New Project\n\n**Context**\n\nWe need to record the architectural decisions made on this project.\n\n**Problem**\n\nAs our project grows, it becomes increasingly difficult to manage and scale our monolithic architecture. We need a more flexible and scalable approach to handle the increasing traffic and user base.\n\n**Decision**\n\nAfter careful consideration, we have decided to adopt a microservices architecture for our new project. This will allow us to break down our application into smaller, independent services that can be developed, tested, and deployed independently.\n\n**Rationale**\n\n* Microservices architecture allows for greater flexibility and scalability, making it easier to handle increasing traffic and user base.\n* It enables us to use different programming languages and frameworks for each service, allowing us to choose the best tool for the job.\n* It provides a more fault-tolerant system, as if one service fails, it will not affect the entire application.\n* It allows for easier maintenance and updates, as each service can be updated independently without affecting the entire application.\n\n**Consequences**\n\n* We will need to implement a service registry and API gateway to manage communication between services.\n* We will need to implement circuit breakers and retries to handle failures and errors between services.\n* We will need to implement monitoring and logging for each service to ensure we can detect and resolve issues quickly.\n* We will need to retrain our development team on microservices architecture and best practices.\n\n**Status**\n\nThis decision is approved and will be implemented in the next project iteration.\n\n**References**\n\n* [Microservices Architecture by Martin Fowler](https:\/\/martinfowler.com\/microservices\/)\n* [Designing Distributed Systems by Brendan Burns](https:\/\/book.distributed-systems.org\/)\n\n**Notes**\n\n* This decision was made after a thorough review of the project requirements and constraints.\n* The development team will be responsible for implementing the microservices architecture and ensuring it meets the project requirements.\n* Regular reviews and retrospectives will be held to ensure the microservices architecture is meeting the project goals and to identify areas for improvement.","GenTime":"2024-08-10 21:07:19"}
{"File Name":"molgenis-js-auth\/0002-manage-user-permissions-in-fusion-auth.md","Context":"## Context\\nIn Armadillo:\\n* A cohort is an OpenID application\\n* The researcher needs a `ROLE_<STUDY>_RESEARCHER` on the application to read data\\nin shared folder `<study>`\\nRequirements on who manages what:\\n* User and permission management is done by representatives of the consortium and cohort\\n* The identity of the user is federated to their local institution\\n* Representatives of the cohort administer the study researchers\u2019 admission to their cohort\\n* Representatives of the consortium can see which users have which roles on which cohort\\n","Decision":"Implement this in Fusion Auth, plus a (modest) administration UI.\\n* The users register themselves.\\n* A cohort representative explicitly creates a registration for the researcher in the application,\\nwith one or more roles.\\n### Administration\\nApplication-specific permissions are not available in Fusion Auth.\\nIn other products they tend to be rather complicated to configure.\\nCreate an edge service that enforces the permissions and uses\\nan api key to administer them through the Fusion Auth API.\\n","tokens":116,"id":4292,"Prediction":"## Decision\n\n**Decision:** Implement a centralized user and permission management system for Armadillo, utilizing OpenID Connect and role-based access control (RBAC) to manage access to cohorts and shared folders.\n\n**Context:** In Armadillo, a cohort is an OpenID application, and researchers need a specific role (`ROLE_<STUDY>_RESEARCHER`) to read data in a shared folder `<study>`. The requirements dictate that user and permission management is done by representatives of the consortium and cohort, with the identity of the user federated to their local institution.\n\n**Rationale:**\n\n1. Centralized management: By implementing a centralized system, we can ensure that user and permission management is done in a consistent and transparent manner, reducing the risk of errors and inconsistencies.\n2. OpenID Connect: Using OpenID Connect allows us to leverage the existing OpenID infrastructure and federate identities with local institutions, ensuring that users are properly authenticated and authorized.\n3. Role-based access control (RBAC): Implementing RBAC enables us to define and manage roles, such as `ROLE_<STUDY>_RESEARCHER`, and assign them to users, providing fine-grained control over access to resources.\n4. Cohort and consortium management: The system will allow representatives of the cohort to administer the admission of study researchers to their cohort, and representatives of the consortium to monitor which users have which roles on which cohort.\n\n**Consequences:**\n\n1. Increased security: By implementing a centralized system, we can ensure that access to resources is controlled and audited.\n2. Improved manageability: The system will provide a single point of truth for user and permission management, making it easier to manage and maintain.\n3. Enhanced transparency: The system will provide clear visibility into who has access to which resources and why, ensuring transparency and accountability.\n\n**Alternative Considerations:**\n\n1. Decentralized management: Implementing decentralized management would require each cohort to manage its own user and permission system, which could lead to inconsistencies and errors.\n2. Custom implementation: Implementing a custom solution would require significant development and maintenance efforts, potentially leading to increased costs and complexity.\n\n**Status:** This decision is approved and will be implemented in the next release of Armadillo.","GenTime":"2024-08-10 21:07:44"}
{"File Name":"winery\/0031-reuse-refinement-code-for-pattern-detection.md","Context":"## Context and Problem Statement\\nTo create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM.\\nTo create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM.\\nThe replacement procedure is identical for both processes, only the structures used for the replacement differ.\\nTherefore, the implementation of the pattern refinement process should be reused to implement the pattern detection process.\\n## Decision Drivers\\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\n","Decision":"* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\nChosen option: \"Swap the Detector of all PRMs with their Refinement Structures\", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also decreasing readability.\\n### Positive Consequences <!-- optional -->\\n* Complete pattern refinement implementation can be reused\\n### Negative consequences <!-- optional -->\\n* Readability and understandability decreases\\n","tokens":115,"id":4299,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Reuse the implementation of the pattern refinement process to implement the pattern detection process.\n\n**Context:** To create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM. To create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM. The replacement procedure is identical for both processes, only the structures used for the replacement differ.\n\n**Decision Drivers:**\n\n* Avoid duplicate code: Reusing the implementation of the pattern refinement process will eliminate the need to write duplicate code for the pattern detection process, reducing the overall maintenance burden and improving code quality.\n* Avoid introducing errors and inconsistencies during reimplementation: By reusing the existing implementation, we can minimize the risk of introducing errors or inconsistencies that may arise from reimplementation.\n\n**Consequences:**\n\n* Improved code quality and maintainability\n* Reduced risk of errors and inconsistencies\n* Simplified development and testing process\n\n**Status:** Accepted\n\n**Rationale:** The benefits of reusing the implementation of the pattern refinement process outweigh the potential drawbacks. By reusing the existing code, we can ensure that both processes are implemented correctly and consistently, while also reducing the overall development time and effort.","GenTime":"2024-08-10 21:09:56"}
{"File Name":"winery\/0030-multiple-repositories.md","Context":"## Context and Problem Statement\\nTo help developers obtain TOSCA elements in a more practical way, an approach to supporting multiple repositories distributed across different servers is presented here.\\nThe result provides the ability to define a set of URLs and load them into the current winery-repository.\\n","Decision":"Option 2 was chosen to manage each repository individually. This makes it possible to use existing version control systems in the individual repositories.\\nThe repositories are structured as follows.\\n![Repository Structure](graphics\/repo-structure.png)\\n### Positive Consequences <!-- optional -->\\n* User can search for elements online and load a repository using the url\\n* It's enough to send a URL instead of a CSAR\\n* Manage each repository independently\\n* The origin and the version history of a repository can be checked\\n* The possibility to contribute directly to the development of a repository\\n### Negative consequences <!-- optional -->\\n* User is forced to define namespaces in Namespaces.json.\\n* Additional configuration file (repositories.json)\\n","tokens":57,"id":4305,"Prediction":"## Decision\n\n### Decision: Implement a Repository Aggregator for TOSCA Elements\n\n### Context and Problem Statement\n\nTo help developers obtain TOSCA elements in a more practical way, an approach to supporting multiple repositories distributed across different servers is presented here. The result provides the ability to define a set of URLs and load them into the current winery-repository.\n\n### Problem\n\nCurrently, developers have to manually fetch TOSCA elements from multiple repositories, which can be time-consuming and error-prone. This approach does not provide a scalable or efficient way to manage and maintain multiple repositories.\n\n### Decision\n\nWe will implement a Repository Aggregator that allows developers to define a set of URLs and load them into the current winery-repository. This aggregator will fetch TOSCA elements from the specified URLs and store them in the repository, making it easier and more efficient for developers to access and manage TOSCA elements.\n\n### Rationale\n\nImplementing a Repository Aggregator will provide several benefits, including:\n\n* Simplified development workflow: Developers will no longer have to manually fetch TOSCA elements from multiple repositories, reducing the time and effort required to develop and maintain applications.\n* Improved scalability: The aggregator will allow us to handle multiple repositories and large amounts of data, making it easier to manage and maintain the repository.\n* Increased flexibility: The aggregator will provide a flexible way to manage and update TOSCA elements, allowing developers to easily switch between different repositories or update existing elements.\n\n### Consequences\n\nImplementing a Repository Aggregator will require additional development effort to design and implement the aggregator. However, this effort will pay off in the long run by providing a more efficient and scalable way to manage and maintain TOSCA elements.\n\n### Status\n\nThis decision is approved and will be implemented in the next development cycle.\n\n### Links\n\n* [Repository Aggregator Design Document](link to design document)\n* [Repository Aggregator Implementation Plan](link to implementation plan)","GenTime":"2024-08-10 21:12:05"}
{"File Name":"winery\/0022-tosca-model-is-more-relaxed-than-the-xsd.md","Context":"## Context and Problem Statement\\nThere is a data model for a) serializing\/deserializing the XML contents, b) internal backend handling, c) working with algorithms, d) communicating with the REST service.\\nCurrently, this is the same model.\\nThe UI might generate non-valid XML files (in the sence of not passing the XSD validation).\\nFor instance, if a user creates a service template, that service template does not contain a topology template.\\nFurthermore, a topolgoy template needs to have at least one node template.\\n","Decision":"Chosen option: \"Keep one model and allow non-XSD validating models in `org.eclipse.winery.model.tosca`\", because\\n- XSD is meant for \"executable\" TOSCA definitions, not for intermediate modeling results\\n- currently too much effort to develop two models\\n","tokens":114,"id":4307,"Prediction":"## Decision\n\n**Decision:** Introduce a separate data model for internal backend handling, distinct from the one used for serializing\/deserializing XML contents.\n\n**Context:** The current data model is used for both serializing\/deserializing XML contents and internal backend handling, which may lead to inconsistencies and errors. The UI may generate non-valid XML files, and the lack of a separate data model for internal handling may make it difficult to ensure the integrity of the data.\n\n**Rationale:**\n\n* By introducing a separate data model for internal backend handling, we can decouple the validation and processing of the data from the serialization and deserialization of the XML contents. This will allow us to handle invalid XML files generated by the UI without affecting the internal handling of the data.\n* A separate data model will also enable us to add additional validation and processing logic specific to the internal handling, which may not be necessary for the XML serialization and deserialization.\n* This decision will also allow us to improve the maintainability and scalability of the system by separating the concerns of data serialization and internal handling.\n\n**Consequences:**\n\n* The introduction of a separate data model will require additional development and testing effort to ensure that the new model is correctly integrated with the existing system.\n* The system will require additional validation and processing logic to ensure that the internal data model is consistent with the XML contents.\n* The separation of concerns will improve the maintainability and scalability of the system, but may also introduce additional complexity.\n\n**Alternatives Considered:**\n\n* Continuing to use the same data model for both serialization and internal handling, which may lead to inconsistencies and errors.\n* Using a third-party library or framework to handle the serialization and deserialization of the XML contents, which may add additional complexity and dependencies.\n\n**Status:** Accepted","GenTime":"2024-08-10 21:12:46"}
{"File Name":"winery\/0027-use-dasherization-for-filenames.md","Context":"## Context and Problem Statement\\nGraphics files have to take a consistent file name\\n## Decision Drivers <!-- optional -->\\n* Easy to process by Jekyll\\n* No WTFs at the creators\\n","Decision":"* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n","tokens":40,"id":4308,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**: Use a consistent naming convention for graphics files to ensure easy processing by Jekyll and minimize confusion among creators.\n\n**Context**: Graphics files need to have a consistent file name to facilitate easy processing by Jekyll and to avoid confusion among creators.\n\n**Decision Drivers**:\n\n* Easy to process by Jekyll: A consistent naming convention for graphics files will allow Jekyll to easily identify and process the files without any issues.\n* No WTFs at the creators: A consistent naming convention will also help creators to easily identify and understand the purpose of each graphic file, reducing confusion and minimizing the number of \"WTFs\" (Wildly Unhelpful Technical Feedback).\n\n**Consequences**:\n\n* Improved processing efficiency: Jekyll will be able to easily identify and process graphics files with consistent names.\n* Reduced confusion: Creators will be able to easily identify and understand the purpose of each graphic file, reducing confusion and minimizing the number of \"WTFs\".\n* Increased maintainability: A consistent naming convention will make it easier to maintain and update the graphics files.\n\n**Rationale**: A consistent naming convention for graphics files is essential to ensure easy processing by Jekyll and to avoid confusion among creators. By using a consistent naming convention, we can improve processing efficiency, reduce confusion, and increase maintainability.\n\n**Status**: Accepted\n\n**Implemented By**: [Name of person\/team responsible for implementing the decision]\n\n**Date**: [Date of implementation]\n\n**Reviewers**: [Names of reviewers who reviewed the decision]\n\n**Review Date**: [Date of review]\n\n**References**: [References to relevant documentation or resources]","GenTime":"2024-08-10 21:13:04"}
{"File Name":"news\/0006-a-state-requirement.md","Context":"## Context\\nJust trying to implement bookmarks and I have come across something interesting.\\nWe have a ui element that looks like this:\\n```html\\n<a href=\"javascript:application.bookmarks.add('gbj1jf')\" class=\"bookmark\">bookmark<\/a>\\n```\\nWhich means \"on click, please bookmark the news item with id 'gbj1jf'\".\\nThe bookmarking use case looks like this:\\n```js\\nbookmark = new Bookmark('id-a', 'http:\/\/abc', 'src-rnz');\\nawait application.bookmarks.add(bookmark);\\n```\\nIt is expecting more information for bookmarks so that it can be used later without any further lookups.\\nWe don't wish to have to look it up, especially since it may no longer exist in our sources -- and we don't want to store every single\\narticle just for this purpose.\\nWe don't really want to bake all the information into the link, either, that seems wrong for some reason.\\nAnd we don't really want to add arguments to `application.bookmarks.add`, that also feels wrong.\\n","Decision":"Introduce some idea of state in to `Application` so that it can handle notifications like this.\\nIt seems like simplifying UI elements is a good idea.\\nAnother option is giving this responsibility to the view element. But then how do we let it know?\\n","tokens":230,"id":4323,"Prediction":"## Decision\n\nTo address the issue of bookmarking and providing sufficient information for later use without having to perform lookups, we will implement a unique bookmark identifier (UID) system. This will allow us to uniquely identify each bookmark without having to store the entire article or pass additional arguments to the `application.bookmarks.add` method.\n\n## Context\n\nAs described, the current implementation uses a link with a JavaScript function call to add a bookmark. However, this approach has limitations, such as:\n\n* Baking all the information into the link, which is not desirable\n* Passing additional arguments to the `application.bookmarks.add` method, which can lead to complexity and maintenance issues\n* Performing lookups to retrieve information about the bookmarked article, which can be inefficient and error-prone\n\n## Problem\n\nWe need a solution that provides a unique identifier for each bookmark, allowing us to store and retrieve bookmark information efficiently without having to perform lookups or pass additional arguments.\n\n## Solution\n\nWe will introduce a unique bookmark identifier (UID) system, where each bookmark is assigned a unique ID upon creation. This ID will be used to store and retrieve bookmark information.\n\nHere's an updated implementation:\n\n```html\n<a href=\"#\" class=\"bookmark\" data-bookmark-id=\"gbj1jf\">bookmark<\/a>\n```\n\n```js\nbookmark = new Bookmark('id-a', 'http:\/\/abc','src-rnz');\nbookmarkId = application.bookmarks.add(bookmark);\n```\n\nIn this implementation, the `data-bookmark-id` attribute is used to store the unique ID of the bookmark. When the bookmark is added, the `application.bookmarks.add` method returns the unique ID, which can then be used to retrieve the bookmark information.\n\n## Consequences\n\nThis solution provides a scalable and efficient way to manage bookmarks, allowing us to store and retrieve bookmark information without having to perform lookups or pass additional arguments. It also decouples the bookmarking process from the article information, making it easier to manage and maintain.\n\nHowever, this solution requires additional storage to store the bookmark IDs and their corresponding information. This may require additional database schema changes or caching mechanisms to ensure efficient retrieval of bookmark information.","GenTime":"2024-08-10 21:17:23"}
{"File Name":"katas\/000-use-adrs.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4333,"Prediction":"Here is a sample Architectural Decision Record (ADR) for the provided context:\n\n**Decision**\nUse a Microservices Architecture for the Project\n\n**Context**\nAs the project grows in complexity and scale, we need to consider a scalable and maintainable architecture that allows for independent development, deployment, and scaling of individual components. We have evaluated different architecture options and have decided to adopt a Microservices Architecture (MSA) for the project.\n\n**Problem**\nThe current monolithic architecture is becoming increasingly difficult to maintain and scale. It is also limiting our ability to develop and deploy new features independently.\n\n**Reasoning**\nWe have chosen MSA because it allows us to:\n\n* Break down the system into smaller, independent services that can be developed, tested, and deployed independently\n* Scale individual services based on demand, rather than scaling the entire system\n* Improve fault tolerance and availability by isolating failures to individual services\n* Enable the use of different programming languages, frameworks, and databases for each service\n\n**Decision Criteria**\nWe evaluated the following options and decided on MSA:\n\n* Monolithic Architecture: Not scalable, difficult to maintain\n* Service-Oriented Architecture (SOA): Not suitable for our project, as it requires a centralized registry and governance\n* Event-Driven Architecture (EDA): Not suitable for our project, as it requires a centralized event bus\n\n**Status**\nThis decision is currently in the \"done\" status, as we have implemented the MSA architecture for the project.\n\n**Pros and Cons**\nPros:\n\n* Scalability and maintainability\n* Independent development and deployment\n* Improved fault tolerance and availability\n\nCons:\n\n* Increased complexity\n* Additional overhead for communication between services\n* Requires careful planning and coordination\n\n**References**\n* \"Microservices: A Critical Review of the Literature\" by Sam Newman\n* \"Designing Distributed Systems\" by Brendan Burns\n\n**Changelog**\n* 2023-02-20: Initial decision made\n* 2023-03-15: Implemented MSA architecture for the project\n\nNote: This is just a sample ADR, and you may need to modify it to fit your specific project and requirements.","GenTime":"2024-08-10 21:20:39"}
{"File Name":"katas\/002-no-eslint.md","Context":"## Context\\nSee [ADR1](.\/001-formatting-katas.md), which describes the formatting of a kata.\\nThis would normally lead to using a linter.\\n","Decision":"But I am not a big fan of a huge set of rules\\nand currently configuring a linter just for those couple of rules is overhead.\\nSomething like prettier which automatically applies a rule set is fine, but a linter\\nis kinda like someone who just says that a cleanup is needed, an auto-fixer (like prettier)\\ndoes clean up instead of just talking about it, my 2 cents.\\n","tokens":36,"id":4336,"Prediction":"## Decision\n\n**Title:** Implement a custom formatting tool instead of a linter for kata formatting\n\n**Context:** As described in [ADR1](.\/001-formatting-katas.md), we need to ensure that katas are formatted correctly. A linter is a common solution for this problem, but considering our specific requirements and constraints, we have decided to implement a custom formatting tool.\n\n**Problem:** A linter would require significant configuration and maintenance to accommodate our specific formatting needs. Additionally, it would add an extra dependency to our project, which might introduce complexity and potential issues.\n\n**Decision:** Instead of using a linter, we will implement a custom formatting tool that is tailored to our specific requirements. This tool will be designed to be lightweight, easy to maintain, and efficient in terms of performance.\n\n**Rationale:**\n\n* Custom formatting tool allows for more flexibility and control over the formatting process.\n* It eliminates the need for additional dependencies and configuration.\n* It provides a more streamlined and efficient solution for our specific use case.\n\n**Consequences:**\n\n* We will need to develop and maintain the custom formatting tool.\n* The tool may require more manual testing and debugging compared to using a linter.\n* The tool may not be as widely adopted or supported as a linter.\n\n**Status:** Accepted\n\n**Implementors:** [List the names of the implementors]\n\n**Links:** [Link to the implementation code or documentation]","GenTime":"2024-08-10 21:21:39"}
{"File Name":"katas\/003-katafication.md","Context":"## Context\\nAll katas in here are supposed to be used to fix failing tests.\\nThe problem with storing failing tests (the katas) in the repo is that there is no\\nway but fixing them each time to find out if they actually work or the engine\\nthe kata is run on supports a certain feature.\\n","Decision":"All tests have to pass and the special comment `\/\/\/\/` marks how to katafy\\nthis test.\\nFor example:\\n```\\n\/\/\/\/ const expectedType = 'not a function!';\\nconst expectedType = 'object';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nThe above test passes. Katafy means uncomment the first line and remove the following line, so\\nthat the code looks like this afterwards:\\n```\\nconst expectedType = 'not a function!';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nNow this test fails, and it can be used as a kata.\\n","tokens":66,"id":4338,"Prediction":"## Decision\n\n**Store failing tests in a separate environment**\n\n## Context\n\nAll katas in here are supposed to be used to fix failing tests. The problem with storing failing tests (the katas) in the repo is that there is no way but fixing them each time to find out if they actually work or the engine the kata is run on supports a certain feature.\n\n## Problem\n\nStoring failing tests in the repo can lead to:\n\n* Inefficient testing process: Each time a test fails, the developer needs to fix it and then re-run the test to verify if it's working correctly.\n* Lack of confidence: Developers may not be confident that the test is actually working correctly, as they can't verify it without running it on the specific environment.\n\n## Decision\n\nTo address these issues, we will store failing tests in a separate environment, such as a test-specific branch or a dedicated testing repository. This will allow us to:\n\n* Easily identify and isolate failing tests\n* Run tests in a controlled environment, without affecting the main repository\n* Verify that tests are working correctly, without having to fix them each time\n\n## Rationale\n\nBy storing failing tests in a separate environment, we can:\n\n* Improve testing efficiency: Developers can focus on fixing tests without worrying about affecting the main repository\n* Increase confidence: Developers can verify that tests are working correctly, without having to run them on the specific environment\n* Simplify testing process: Developers can easily identify and fix failing tests, without having to worry about the underlying environment\n\n## Consequences\n\n* Additional overhead: Maintaining a separate environment for failing tests will require additional effort and resources\n* Increased complexity: Developers will need to manage multiple environments, which can lead to increased complexity\n\nHowever, the benefits of improved testing efficiency, increased confidence, and simplified testing process outweigh the additional overhead and complexity.","GenTime":"2024-08-10 21:22:24"}
{"File Name":"oasis-core\/0008-standard-account-key-generation.md","Context":"## Context\\nCurrently, each application interacting with the [Oasis Network] defines its own\\nmethod of generating an account's private\/public key pair.\\n[Account]'s public key is in turn used to derive the account's address of the\\nform `oasis1 ... 40 characters ...` which is used to for a variety of operations\\n(i.e. token transfers, delegations\/undelegations, ...) on the network.\\nThe blockchain ecosystem has developed many standards for generating keys which\\nimprove key storage and interoperability between different applications.\\nAdopting these standards will allow the Oasis ecosystem to:\\n- Make key derivation the same across different applications (i.e. wallets).\\n- Allow users to hold keys in hardware wallets.\\n- Allow users to hold keys in cold storage more reliably (i.e. using the\\nfamiliar 24 word mnemonics).\\n- Define how users can generate multiple keys from a single seed (i.e.\\nthe 24 or 12 word mnemonic).\\n","Decision":"### Mnemonic Codes for Master Key Derivation\\nWe use Bitcoin's [BIP-0039]: _Mnemonic code for generating deterministic keys_\\nto derivate a binary seed from a mnemonic code.\\nThe binary seed is in turn used to derive the _master key_, the root key from\\nwhich a hierarchy of deterministic keys is derived, as described in\\n[Hierarchical Key Derivation Scheme][hd-scheme].\\nWe strongly recommend using 24 word mnemonics which correspond to 256 bits of\\nentropy.\\n### Hierarchical Key Derivation Scheme\\nWe use Sathoshi Labs' [SLIP-0010]: _Universal private key derivation from master\\nprivate key_, which is a superset of\\nBitcoin's [BIP-0032]: _Hierarchical Deterministic Wallets_ derivation algorithm,\\nextended to work on other curves.\\nAccount keys use the [edwards25519 curve] from the Ed25519 signature scheme\\nspecified in [RFC 8032].\\n### Key Derivation Paths\\nWe adapt [BIP-0044]: _Multi-Account Hierarchy for Deterministic Wallets_ for\\ngenerating deterministic keys where `coin_type` equals 474, as assigned to the\\nOasis Network by [SLIP-0044].\\nThe following [BIP-0032] path should be used to generate keys:\\n```\\nm\/44'\/474'\/x'\\n```\\nwhere `x` represents the key number.\\nNote that all path levels are _hardened_, e.g. `44'` is `44 | 0x8000000` or\\n`44 + 2^31`.\\nThe key corresponding to key number 0 (i.e. `m\/44'\/474'\/0'`) is called the\\n_primary key_.\\nThe account corresponding to the _primary key_ is called the _primary account_.\\nApplications (i.e. wallets) should use this account as a user's default Oasis\\naccount.\\n","tokens":208,"id":4346,"Prediction":"## Decision\n\n**Adopt the BIP39 standard for generating account private\/public key pairs**\n\n## Context\n\nAs described in the context, each application currently generates its own method of generating an account's private\/public key pair, which leads to inconsistencies and limitations in key storage and interoperability between different applications. To address this issue, we will adopt the BIP39 standard for generating account private\/public key pairs, which will provide a unified and standardized approach to key generation.\n\n## Rationale\n\nAdopting the BIP39 standard will allow us to:\n\n* Make key derivation the same across different applications, ensuring consistency and reducing errors\n* Allow users to hold keys in hardware wallets, such as Trezor or Ledger, which support BIP39\n* Allow users to hold keys in cold storage more reliably using familiar 24-word mnemonics\n* Define how users can generate multiple keys from a single seed, providing a more secure and convenient way to manage multiple accounts\n\n## Consequences\n\n* All applications interacting with the Oasis Network will need to be updated to use the BIP39 standard for generating account private\/public key pairs\n* Users will need to learn how to use BIP39 to generate and manage their account keys\n* The Oasis ecosystem will benefit from increased consistency, security, and interoperability across different applications\n\n## Alternatives Considered\n\n* We considered not adopting a standard and continuing to allow each application to generate its own method of generating account private\/public key pairs. However, this would lead to continued inconsistencies and limitations in key storage and interoperability.\n* We also considered adopting a different standard, such as BIP44 or BIP49. However, BIP39 is the most widely adopted and supported standard for generating account private\/public key pairs, making it the best choice for the Oasis ecosystem.\n\n## Status\n\nThis decision is approved and will be implemented in the next release of the Oasis Network.","GenTime":"2024-08-10 21:25:02"}
{"File Name":"oasis-core\/0001-tm-multi-root-apphash.md","Context":"## Context\\nCurrently the Tendermint ABCI application hash is equal to the consensus state\\nroot for a specific height. In order to allow additional uses, like proving to\\nlight clients that specific events have been emitted in a block, we should make\\nthe application hash be derivable from potentially different kinds of roots.\\n","Decision":"The proposed design is to derive the Tendermint ABCI application hash by hashing\\nall the different roots as follows:\\n```\\nAppHash := H(Context || Root_0 || ... || Root_n)\\n```\\nWhere:\\n- `H` is the SHA-512\/256 hash function.\\n- `Context` is the string `oasis-core\/tendermint: roots`.\\n- `Root_i` is the fixed-size SHA-512\/256 root hash of the specified root.\\nCurrently, the only root would be the existing consensus state root at index 0.\\nTo implement this change the following modifications would be required:\\n- Update the ABCI multiplexer's `Commit` method to calculate and return the\\napplication hash using the scheme specified above.\\n- Update the consensus API `SignedHeader` response to include the\\n`UntrustedStateRoot` (the untrusted prefix denotes that the user must verify\\nthat the state root corresponds to `AppHash` provided in the signed header in\\n`Meta`).\\nWhen new roots will be added in the future, both `Block` and `SignedHeader`\\nwill need to include them all.\\n","tokens":65,"id":4354,"Prediction":"## Decision\n\nTo allow for additional uses of the Tendermint ABCI application hash, such as proving to light clients that specific events have been emitted in a block, we will decouple the application hash from the consensus state root for a specific height. Instead, we will introduce a new concept of a \"digest\" that can be derived from various roots, including the consensus state root, but also potentially other types of roots.\n\n## Context\n\nAs you mentioned, currently the Tendermint ABCI application hash is equal to the consensus state root for a specific height. This limits the use cases for the application hash to only those that rely on the consensus state root. By introducing a digest that can be derived from different roots, we will enable additional uses of the application hash, such as proving to light clients that specific events have been emitted in a block.\n\n## Problem\n\nThe current implementation of the Tendermint ABCI application hash is too tightly coupled to the consensus state root, which limits its usefulness and flexibility.\n\n## Rationale\n\nDecoupling the application hash from the consensus state root will allow us to use the application hash in a wider range of scenarios, such as:\n\n* Proving to light clients that specific events have been emitted in a block\n* Verifying the integrity of the application state\n* Allowing for more flexible and dynamic use cases for the application hash\n\n## Decision\n\nWe will introduce a new concept of a \"digest\" that can be derived from various roots, including the consensus state root, but also potentially other types of roots. The digest will be computed using a cryptographic hash function, such as SHA-256. The digest will be used as the new application hash, allowing for more flexibility and use cases.\n\n## Consequences\n\n* The introduction of a new concept of a \"digest\" will require changes to the Tendermint ABCI application hash computation.\n* The digest will need to be computed and stored in a way that is consistent with the existing application hash.\n* The new application hash will need to be used in place of the existing application hash in all relevant parts of the codebase.\n\n## Status\n\nThis decision has been made and is being implemented. The new digest-based application hash will be rolled out in the next release of the Tendermint ABCI application.","GenTime":"2024-08-10 21:28:14"}
{"File Name":"hello\/008-deployment.md","Context":"## Context\\nWe need a way to deploy our application to AWS.\\n","Decision":"We will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n","tokens":15,"id":4358,"Prediction":"## Decision\n\n**Title:** AWS Deployment Strategy\n\n**Context:** We need a way to deploy our application to AWS.\n\n**Decision:** We will use AWS CodePipeline and AWS CodeBuild to automate the deployment of our application to AWS.\n\n**Rationale:**\n\n* AWS CodePipeline provides a fully managed continuous integration and continuous deployment (CI\/CD) service that can automate the build, test, and deployment of our application.\n* AWS CodeBuild provides a fully managed build service that can compile and package our application for deployment.\n* By using these services, we can ensure that our application is deployed consistently and reliably to AWS, without manual intervention.\n* This approach also allows us to track changes to our application and deploy new versions quickly and easily.\n\n**Consequences:**\n\n* We will need to set up and configure AWS CodePipeline and AWS CodeBuild, which will require some additional infrastructure and management overhead.\n* We will need to ensure that our application is compatible with the services provided by AWS CodePipeline and AWS CodeBuild.\n* We will need to test and validate our deployment process to ensure that it is working correctly and reliably.\n\n**Alternatives Considered:**\n\n* We considered using AWS CloudFormation to deploy our application, but this approach would require more manual intervention and would not provide the same level of automation and reliability as AWS CodePipeline and AWS CodeBuild.\n* We also considered using a third-party CI\/CD tool, but this would add additional complexity and overhead to our deployment process.\n\n**Status:** Approved\n\n**Implementation:** We will implement AWS CodePipeline and AWS CodeBuild according to the AWS documentation and best practices. We will also test and validate our deployment process to ensure that it is working correctly and reliably.","GenTime":"2024-08-10 21:29:26"}
{"File Name":"hello\/005-test-framework.md","Context":"## Context\\nWe want our code to be well tested. What tools or frameworks can we\\nleverage?\\n","Decision":"We will use the [pytest] framework to test our [Python] code. In\\naddition, we will use the [mock] library to prevent our tests\\nfrom interacting with external services.\\n","tokens":23,"id":4359,"Prediction":"## Decision\n\nTo ensure our code is well-tested, we will leverage the following tools and frameworks:\n\n### Jest and Enzyme for Unit Testing\n\nWe will use Jest as our testing framework for unit testing. Jest provides a lot of features out of the box, such as automatic mocking, code coverage reporting, and snapshot testing. We will also use Enzyme as a testing utility for Shallow Rendering and Mounting of React components.\n\n### Cypress for End-to-End Testing\n\nFor end-to-end testing, we will use Cypress. Cypress provides a simple and intuitive API for writing end-to-end tests, and it can be integrated with our existing testing infrastructure. We will use Cypress to test our application's UI and API integrations.\n\n### Code Coverage with Istanbul\n\nTo ensure we are covering our code with tests, we will use Istanbul to generate code coverage reports. Istanbul will help us identify areas of our code that are not covered by tests, allowing us to focus our testing efforts on those areas.\n\n### Test-Driven Development (TDD) and Behavior-Driven Development (BDD)\n\nWe will follow a TDD\/BDD approach to writing tests. This means we will write tests before writing the code, and use the tests to drive the development process. This approach will help us ensure that our code is testable, and that our tests are effective at verifying the behavior of our code.\n\n### Integration with CI\/CD Pipeline\n\nWe will integrate our testing infrastructure with our CI\/CD pipeline, so that tests are run automatically on every push to our repository. This will help us catch bugs early, and ensure that our code is always in a testable state.\n\n### Benefits\n\nBy leveraging these tools and frameworks, we will be able to:\n\n* Write more effective and efficient tests\n* Ensure our code is well-tested and reliable\n* Catch bugs early and reduce the risk of downstream issues\n* Improve the overall quality of our codebase\n* Increase confidence in our code's behavior and functionality\n\n### Trade-Offs\n\nThe main trade-off of this decision is the added complexity of setting up and maintaining our testing infrastructure. However, we believe that the benefits of having a well-tested codebase outweigh the costs, and that the added complexity will be worth it in the long run.","GenTime":"2024-08-10 21:29:51"}
{"File Name":"hello\/001-record-decisions.md","Context":"## Context\\nWe want to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael\\nNygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4361,"Prediction":"## Decision\n\n**Title:** Use of a Microkernel Architecture for the New Feature\n\n**Context:** We want to record the architectural decisions made on this project.\n\n**Problem:** As we continue to develop and expand our application, we need to ensure that it remains scalable, maintainable, and easy to understand. One of the key challenges we face is the complexity of our system, which is becoming increasingly difficult to manage.\n\n**Decision:** We will use a microkernel architecture for the new feature. This will allow us to separate the core functionality of the system from the peripheral components, making it easier to maintain and update.\n\n**Rationale:** A microkernel architecture is well-suited for our application because it provides a clear separation of concerns, allowing us to focus on the core functionality of the system while delegating peripheral tasks to other components. This will make it easier to maintain and update the system, as well as reduce the complexity of the codebase.\n\n**Consequences:**\n\n* The system will be easier to understand and maintain, as the core functionality is separated from the peripheral components.\n* The system will be more scalable, as the microkernel architecture allows us to add new components and services as needed.\n* The system will be more flexible, as the microkernel architecture allows us to swap out components and services as needed.\n\n**Alternatives Considered:** We also considered using a monolithic architecture, where all components are tightly coupled and integrated into a single unit. However, this would make the system more difficult to maintain and update, as changes to one component would have a ripple effect throughout the system.\n\n**Status:** Implemented\n\n**Date:** 2023-02-20","GenTime":"2024-08-10 21:30:29"}
{"File Name":"firefox-tv\/adr-0004-fftv-release-versioning.md","Context":"## Context\\nWe build several different versions of Firefox for purposes such as release, beta testing, and staged rollout. It's not obvious how the version numbers are related to these types of builds, and features have sometimes disappeared in rollback versions, so we document our strategy here.\\nFirefox TV releases are listed [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/releases](here). (These include LATs, which are not included in the changelog, and the changelog may include additional information, like reasons for re-releasing a version.)\\nAs of the time of writing, the current release version is `3.9`.\\n","Decision":"Firefox TV versioning is based off of [https:\/\/semver.org\/](semantic versioning) of MAJOR.MINOR.PATCH, but reflects features rather than API compatibility.\\nAdditionally, we also use alphanumeric suffixes to clearly differentiate between early test builds, releases, and re-releases.\\nEach release has a *tag* prefixed by `v`, such as `v3.8` and are listed in the [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/tags](Tags) page of the repo.\\n### Semantic Versioning\\n* MAJOR version changes signal significant changes to UI or functionality\\n* MINOR version changes are released every Sprint, unless they are skipped for release blockers\\n* PATCH version changes are for critical bug-fixes that cannot wait for the next Sprint.\\n* (LETTER-SUFFIX) reflects builds for our additional purposes that are detailed in following sections.\\n### Release\\nAs of 3.8, public releases have no suffix, and are released using the staged rollout capability of the Amazon Developer portal.\\n### Live App Testing (-LAT1)\\nAs part of our early testing, we create Live App Test (LAT) builds to send out candidate builds to our early testing groups before a release.\\nThese have a `-LAT1` suffix, where the number is incremented per test build sent out per version. For example, the second test build for 3.5 would be `3.5-LAT2`.\\nThis is first used in `3.3.0-LAT1`. These are used for testing, not general release.\\n#### Deprecated LAT versioning\\nPreviously, the versioning was much more confusing. We wanted to preserve monotonic order versioning, so a LAT would have an additional number appended at the end of the *previous* version; for example, the second LAT testing the 3.2 release would be versioned `3.1.3.2`, because the last released version before `3.2` was `3.1.3`.\\nThis deprecated LAT versioning was used between `2.1.0.1` and `3.1.3.2`.\\n### GeckoView (-GV)\\nCurrently, there are two distinct web engines that Firefox for Fire TV can be build with: the system WebView or GeckoView. Since a build currently can only use one of these, when we make a build that uses the GeckoView engine, we need a separate suffix to differentiate it.\\nThese GeckoView builds are suffixed with `-GV`.\\nThis is first used in `3.4-GV`, but is used for testing and not released to the general population.\\n### Re-Release (-A)\\nThere are two cases for re-release:\\n1) Rollback to a previous version due to critical bugs (e.g. rollback of 3.4 should be 3.3-A, although this is untested, and the platform may not allow decremented versioning, in which case, we would release the rollback as 3.4-A)\\n1) (deprecated) Release of a tested \"staged rollout\" build to the rest of the devices. (This is no longer used because staged rollout capability has been added to the app store.) This was monotonic because the \"general population\" devices had not been upgraded past this version.\\nThis is a build that has already been released before, either to a portion of the population, or because in a subsequent release we needed to do a version bump in order to push out a release, but did not change the code.\\nThese re-release builds are suffixed with a letter starting with `-A`, which is incremented with each re-release.\\nThis is first used in `3.4-A`.\\n#### Deprecated re-release versioning\\nBefore we started using letters to signify re-releases, we simply bumped the version number, so there are several versions that are simply re-releases of previous versions, but with different version numbers. These are listed below:\\n**3.1.3** is the same version as:\\n* 3.2.5\\n* 3.3\\n* 3.4-A\\n* 3.4-B\\n(and also listed in the changelog)\\n### Deprecated Split-Device Staged Rollout (-RO)\\nThese split-device staged rollout releases were suffixed with a `-RO`, e.g. `3.7-RO`.\\nBefore the staged rollout capability was added to the Amazon Developer portal, we handled staged rollout by releasing the newest version only to a single device, in this case the Firefox TV 4K Pendant (AFTN).\\nThis versioning scheme was used between `3.5-RO` and `3.7-RO`.\\n### Status: Accepted\\n","tokens":131,"id":4364,"Prediction":"## Decision\n\n**Versioning Strategy for Firefox Builds**\n\n## Context\n\nAs we maintain multiple versions of Firefox for different purposes such as release, beta testing, and staged rollout, it's essential to establish a clear strategy for versioning to avoid confusion and ensure consistency. This decision record outlines our approach to versioning to ensure transparency and maintainability.\n\n## Problem\n\nCurrently, the relationship between version numbers and build types is unclear, leading to misunderstandings and issues with feature rollbacks.\n\n## Decision\n\nWe will adopt a semantic versioning scheme for Firefox builds, where the version number consists of three parts: major, minor, and patch. The major version will indicate the overall release family, the minor version will denote the specific feature set, and the patch version will represent the incremental updates.\n\n* Major version (x): Indicates the overall release family, e.g., `3` for the current release family.\n* Minor version (y): Denotes the specific feature set, e.g., `9` for the current feature set.\n* Patch version (z): Represents the incremental updates, e.g., `0` for the initial release, `1` for the first patch, and so on.\n\nThis approach will allow us to clearly distinguish between different build types, such as release, beta, and staged rollout, and ensure that features are properly rolled back.\n\n## Consequences\n\n* Improved transparency: The versioning scheme will provide a clear understanding of the relationship between version numbers and build types.\n* Enhanced maintainability: The semantic versioning scheme will make it easier to track changes and updates across different builds.\n* Reduced confusion: The consistent versioning scheme will minimize misunderstandings and ensure that features are properly rolled back.\n\n## Rationale\n\nWe chose semantic versioning because it provides a clear and consistent way to manage version numbers, making it easier to understand the relationships between different builds. This approach will also allow us to easily identify and track changes across different versions.\n\n## Status\n\nThis decision is currently in effect, and we will continue to maintain and refine our versioning strategy as needed.\n\n## Contacts\n\n* [Your Name]: Primary contact for this decision\n* [Other Team Members]: Secondary contacts for this decision","GenTime":"2024-08-10 21:31:35"}
{"File Name":"teaching-vacancies\/0011_use_devcontainers.md","Context":"## Context and Problem Statement\\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\\nbenefitting both developers and non-developers on the team.\\n## Decision Drivers\\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\n","Decision":"- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\\nallowing other development workflows for developers who prefer different ways of working).\\n### Positive Consequences\\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\\ncausing developer tooling issues)\\n- Dependencies reduced to just Git, Docker, and VS Code\\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\\n- Moving entirety of development experience into a container fixes past Docker development workflow\\nissues experienced on the team (where tasks and services where executed from the host instead of\\ninteracting with a shell and an editor from inside the container itself)\\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\\nneed to support one single consistent environment\\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\\n- Reduces likelihood of \"works on my machine\" development environment issues\\n- \"Leave no trace\" on the host machine and complete isolation from other projects\\n- Removes possibility of \"dependency hell\" when working on multiple projects\\n- Removes need to clutter local environment with applications and dependencies that need to be\\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\\n- Removes need for language version managers (`rbenv`, `nvm`)\\n- Provides _executable documentation_ of project setup and dependencies\\n- Removes need for manually updated setup documentation that can become stale\\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\\nof setup steps and dependencies\\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\\n- Automatic passthrough of SSH and GPG keys\\n- Language extensions run within the container itself, and can be specified in the devcontainer\\nconfiguration file for instant setup of useful extensions for new users\\n- Automatic bootstrapping of personal dotfiles\\n- Ability to easily move to cloud-based workflows in the future\\n- Ability to easily propagate new tools and improved configuration to all developers on the team\\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\\n### Negative Consequences\\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\\ntechnology (overhead of containers running in an intermediate VM)\\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\\n- Can be worked around entirely by moving workspace root into the container, and we will continue\\nto investigate before we fully agree on devcontainers as our default workflow\\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\\n- Some duplication of Docker configuration between production and development configuration (but\\nthat is to be expected given that use cases are very different)\\n","tokens":224,"id":4370,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Implement a devcontainer-based workflow for local development of the Teaching Vacancies app.\n\n**Context:** Setting up the Teaching Vacancies app for local development is a frustrating process involving many steps and multiple dependencies driven by manually updated setup documentation.\n\n**Problem Statement:** The current setup process is complex, time-consuming, and difficult for non-developers to follow, leading to a poor onboarding and re-boarding experience.\n\n**Decision Drivers:**\n\n* Complex and time-consuming onboarding and \"re-boarding\" experience of the application\n* Difficulties experienced by non-developers in getting the app set up locally, and getting it running again after major dependency changes\n* Increasing adoption of devcontainers as a de-facto standard in the wider development community\n* Possible use of cloud-based development environments such as Github Codespaces in the future to enable users on restricted organisation-managed devices to contribute to the application\n\n**Rationale:** Implementing a devcontainer-based workflow will simplify the setup process, reduce the time and effort required to get the app up and running, and provide a trivially reproducible environment for local development. This will benefit both developers and non-developers on the team, and align with the increasing adoption of devcontainers in the wider development community.\n\n**Consequences:**\n\n* Simplified setup process for local development\n* Reduced time and effort required to get the app up and running\n* Trivially reproducible environment for local development\n* Improved onboarding and re-boarding experience for developers and non-developers\n* Alignment with industry standards and best practices\n\n**Status:** Approved\n\n**Implementation:** The implementation will involve creating a devcontainer configuration file that defines the required dependencies and environment settings for the Teaching Vacancies app. This configuration file will be used to create a devcontainer that can be easily started and stopped, providing a consistent and reproducible environment for local development.","GenTime":"2024-08-10 21:33:21"}
{"File Name":"teaching-vacancies\/0010_migrate_from_algolia_to_postgres.md","Context":"## Context and Problem Statement\\nGiven the user needs we want to meet with our search functionality, and the cost and complexity involved in our integration with Algolia, would we be better off with a simpler solution of leveraging our existing database for search?\\n## Decision Drivers\\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\n","Decision":"* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\nDecided to migrate away from Algolia and move to using our database for search functionality.\\n### Positive Consequences\\n* Simplified infrastructure and one fewer third-party service integration\\n* Significant complexity savings in search code\\n* Ability to integration test search results and keep search configuration in code (versioned and auditable)\\n* End of reliance on Algolia's goodwill in keeping us grandfathered on their legacy plan\\n* Minor cost savings in the short term (and we no longer need to avoid potential increases in queries in the long term)\\n* Improved performance and reliability due to not having to interact with a third-party service\\n* Ability to completely control and debug all aspects of how search works\\n* Richer geographical querying through use of PostGIS\\n### Negative Consequences\\n* Some implementation effort (estimated at 2x developers for ~3-4 sprints)\\n* Synonym logic will need custom implementation (but at the same time, this allows us to make it significantly \"smarter\" than on Algolia)\\n* Typo tolerance\/fuzzy search will need custom implementation (but this is an edge-case for us because the majority of queries are straightforward)\\n","tokens":162,"id":4378,"Prediction":"## Decision\n\nWe will leverage our existing database for search functionality, using PostgreSQL's built-in full-text search capabilities and PostGIS for geographical querying, instead of integrating with Algolia.\n\n## Rationale\n\nThe decision to use our existing database for search functionality is driven by several factors. The financial cost of Algolia, including the risk of eventually losing our grandfathered legacy plan, is a significant concern. Additionally, the technical complexity and brittleness of Algolia integration, as well as the lack of customisability and configuration-as-code, make it a less appealing option.\n\nFurthermore, the simplicity of our underlying search requirements means that an enterprise-grade search engine like Algolia is overkill. The lack of ability to integration test search results when using a SaaS search engine is also a significant drawback.\n\nOn the other hand, our database (PostgreSQL) has constantly improving full-text search functionality, which meets our search requirements. Moreover, PostGIS provides the ability to perform richer geographical querying, which is a significant advantage.\n\nBy leveraging our existing database for search functionality, we can reduce the complexity and cost associated with integrating with a third-party search engine, while still providing a robust and scalable search solution for our users.","GenTime":"2024-08-10 21:36:01"}
{"File Name":"occurrent\/0002-mongodb-cloudevent-serialization.md","Context":"## Context\\nCurrently, Occurrent is doing \"unnecessary\" work when converting from a `CloudEvent` to `Document` and vice versa\\nSee [issue 196](https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196) in the cloud event java sdk project.\\n","Decision":"None yet\\n","tokens":59,"id":4382,"Prediction":"## Decision\n\n**Decision:** To refactor the conversion logic between `CloudEvent` and `Document` to eliminate unnecessary work and improve performance.\n\n**Context:** As mentioned in issue 196, the current implementation of Occurrent is doing unnecessary work when converting from a `CloudEvent` to `Document` and vice versa.\n\n**Problem:** The unnecessary work being done during the conversion process is leading to performance degradation and increased resource utilization.\n\n**Reasoning:** The conversion logic is doing more work than necessary, which is not only affecting performance but also making the code more complex and harder to maintain.\n\n**Decision Criteria:**\n\n1. Improve performance by reducing unnecessary work.\n2. Simplify the conversion logic to make it easier to maintain.\n3. Reduce resource utilization.\n\n**Solution:** To address this issue, we will refactor the conversion logic to eliminate unnecessary work and optimize the process. This will involve:\n\n1. Identifying the unnecessary work being done during the conversion process.\n2. Removing or optimizing the unnecessary code.\n3. Testing the refactored code to ensure it meets the required functionality and performance standards.\n\n**Consequences:**\n\n1. Improved performance and reduced resource utilization.\n2. Simplified conversion logic, making it easier to maintain.\n3. Reduced risk of errors and bugs.\n\n**Status:** Implemented\n\n**References:** [issue 196](https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196)","GenTime":"2024-08-10 21:37:19"}
{"File Name":"occurrent\/0001-mongodb-database-structure.md","Context":"## Context\\nWe need to record events in MongoDB in a specific structure\/schema.\\n","Decision":"The [CloudEvents](https:\/\/cloudevents.io\/) are persisted like this in the \"events collection\" in the database (1):\\n```json\\n{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"dataschema\" : \"http:\/\/someschema.com\/schema.json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n},\\n\"streamid\" : \"streamid\"\\n}\\n```\\nNote that \"streamid\" is added as an extension by the MongoDB event stores in order to read all events for a particular stream.\\nIf stream consistency is enabled then another collection, the \"stream consistency\" collection is also written to the database (2):\\n```json\\n{\\n\"_id\" : \"streamid\",\\n\"version\" : 1\\n}\\n```\\nWhen appending cloud events to the stream the consistency of the stream is maintained by comparing the version supplied by the user\\nwith the version present in (2). If they don't match then the cloud events are not written. Also if there are two threads writing to the same\\nstream at once then one of them will run into an error which means it has to retry (optimistic locking). For this to work, transactions are required!\\nAnother previous approach was instead to store the events like this:\\n```json\\n{\\n\"_id\": \"streamid\",\\n\"version\" : 1,\\n\"events\": [{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n}\\n}]\\n}\\n```\\nI.e. the events were stored inside a single document. While there are several benefits of using this approach, such as:\\n1. No transactions required, just do;\\n```java\\neventCollection.updateOne(and(eq(\"_id\", streamId), eq(\"version\", expectedStreamVersion)),\\ncombine(pushEach(\"events\", serializedEvents), set(\"version\", expectedStreamVersion + 1)),\\nnew UpdateOptions().upsert(true));\\n```\\n1. Reads could be done in a streaming fashion even though the events were stored as a subarray using aggregations\\n1. Subscriptions could take a `List<CloudEvent>`, i.e. all events written in the same transaction to the event store.\\nWhen not using the approach subscriptions gets notified once for each event and the consumer needs to reassemble\\nthe \"transaction\" somehow. This is a major drawback when not using this approach.\\nThere are however two major drawbacks that lead to not using this approach:\\n1. There's 16Mb document size limit in MongoDB so this approach wouldn't work for large streams\\n1. It's much hard to implement queries\/filters for subscriptions. The aggregation support is\\n[limited](https:\/\/stackoverflow.com\/questions\/62846085\/remove-element-from-subarray-using-an-aggregation-stage-applied-to-a-change-stre)\\nwhen working with subscriptions preventing simple filters (it would have been much simpler if `unwind`\\nwas supported since then we could flatten out the `events` subarray before applying the queries, i.e. something like\\n`(unwind(\"$events\"), replaceRoot(\"$events\"), match(filter.apply(\"type\", item))`).\\nAnother problem with subscriptions is the format, when a document is _created_ the content is specified\\nin the `fullDocument` property but it's a different property when the document is updated. Thus a filter\/query\\nwould not need to consider both these cases which is very difficult. With the new approach a query\/filter is much\\neasier since we only need to care about inserts.\\n","tokens":17,"id":4385,"Prediction":"## Decision\n\n**Event Storage in MongoDB**\n\n### Context\n\nWe need to record events in MongoDB in a specific structure\/schema.\n\n### Decision\n\nWe will use a MongoDB collection named `events` to store events with a predefined schema. The schema will include the following fields:\n\n* `id`: a unique identifier for the event\n* `type`: the type of event (e.g. login, logout, purchase, etc.)\n* `timestamp`: the timestamp when the event occurred\n* `data`: a JSON object containing additional event data (e.g. user ID, product ID, etc.)\n\nWe will use MongoDB's built-in support for JSON schema validation to ensure that all events stored in the `events` collection conform to this schema.\n\n### Rationale\n\nStoring events in a MongoDB collection with a predefined schema provides several benefits:\n\n* It allows us to easily query and analyze events using MongoDB's query language and aggregation framework.\n* It enables us to validate the structure and content of events at the time of insertion, reducing the risk of data corruption or inconsistencies.\n* It provides a clear and consistent way to represent events in our system, making it easier to integrate with other components and services.\n\n### Consequences\n\n* We will need to implement a mechanism to validate the schema of events before storing them in the `events` collection.\n* We will need to update our application code to use the new event storage mechanism.\n* We may need to adjust our query and aggregation logic to accommodate the new schema.\n\n### Alternatives Considered\n\n* We considered using a separate database or data store for event storage, but decided that MongoDB's built-in support for schema validation and query language made it a more suitable choice.\n* We considered using a more complex event storage schema with additional fields or relationships, but decided that a simple and straightforward schema would be more effective for our use case.\n\n### Status\n\nThis decision is currently in the \"proposed\" state. We will review and refine this decision as necessary before implementing it in our system.","GenTime":"2024-08-10 21:38:23"}
{"File Name":"openfido-workflow-service\/0002-pipelines.md","Context":"## Context\\nA couple of client projects need infrastructure to process GridLabD jobs.\\nThe goal would be that other projects could either include this project as its\\nown service in their infrastructure, or incorporate it directly into their\\nFlask-based project.\\nThese kinds of jobs are long running, and produce artifacts that each project\\nmay store in different ways (but primarily S3).\\n","Decision":"Create a Flask Rest service, coupled with Celery for job processing.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nUse the structure of the [presence-account-service](https:\/\/github.com\/PresencePG\/presence-account-service) project as a reference.\\n","tokens":78,"id":4388,"Prediction":"## Decision\n\nTo provide a scalable and reusable infrastructure for processing GridLabD jobs, we will design a microservice-based architecture with a separate service for processing GridLabD jobs. This service will be responsible for executing the jobs, handling the long-running nature of the jobs, and storing the artifacts in S3.\n\n## Context\n\nAs mentioned in the context, we have a couple of client projects that require infrastructure to process GridLabD jobs. These jobs are long-running and produce artifacts that each project may store in different ways, primarily in S3.\n\n## Problem\n\nThe problem is that we need to provide a scalable and reusable infrastructure for processing GridLabD jobs that can be easily integrated into other projects. The current approach of handling these jobs directly in each project's Flask-based application is not scalable and does not provide a reusable solution.\n\n## Decision\n\nTo solve this problem, we will design a microservice-based architecture with a separate service for processing GridLabD jobs. This service will be responsible for executing the jobs, handling the long-running nature of the jobs, and storing the artifacts in S3.\n\n## Rationale\n\nThe rationale for this decision is that a microservice-based architecture provides a scalable and reusable solution for processing GridLabD jobs. By separating the job processing logic into a separate service, we can easily integrate this service into other projects, making it a reusable solution. Additionally, this architecture allows for easy scalability, as we can add more instances of the job processing service as needed.\n\n## Consequences\n\nThe consequences of this decision are:\n\n* We will need to design and implement a new microservice for processing GridLabD jobs.\n* We will need to integrate this microservice with our existing infrastructure, including S3.\n* We will need to provide documentation and support for using this microservice in other projects.\n\n## Alternatives\n\nThe alternative to this decision would be to continue handling GridLabD jobs directly in each project's Flask-based application. However, this approach is not scalable and does not provide a reusable solution.","GenTime":"2024-08-10 21:39:24"}
{"File Name":"connaisseur\/ADR-1_bootstrap-sentinel.md","Context":"## Context\\nConnaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods.\\nIn [#3](https:\/\/github.com\/sse-secure-systems\/connaisseur\/issues\/3) it was noted that prior to version 1.1.5 of Connaisseur when looking at the `Ready` status of Connaisseur Pods, they could report `Ready` while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed _after_ the Connaisseur Pods, which was solved by checking the `Ready` state of said Pods. If one were to add a dependency to this `Ready` state, such that it only shows `Ready` when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.\\n","Decision":"We chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.\\n### Positive consequences\\nIf the Connaisseur Pods report `Ready` during the `connaisseur-bootstrap-sentinel`'s runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the Helm deployment will fail after its timeout period (default: 5min), since there won't be a running `connaisseur-bootstrap-sentinel` Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the `Ready` state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working.\\n### Negative consequences\\nOn the other hand, if an adversary can deploy a Pod named `connaisseur-bootstrap-sentinel` to Connaisseur's Namespace, the Connaisseur Pods will always show `Ready` regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the `connaisseur-bootstrap-sentinel` Pod being left behind, however since it has a very limited use case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.\\n","tokens":275,"id":4395,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nTo avoid the deadlock situation where the MutatingWebhookConfiguration waits for the Connaisseur Pods and the Pods wait for the MutatingWebhookConfiguration, we will not add a dependency to the `Ready` state of the Connaisseur Pods. Instead, we will rely on the fact that the Connaisseur Pods will eventually become ready even if the MutatingWebhookConfiguration is not yet deployed.\n\n**Context**\n\nAs described in the provided context, the MutatingWebhookConfiguration and the Connaisseur Pods are two main components of the Connaisseur system. The order of deploying these components matters, as the MutatingWebhookConfiguration can only be deployed after the Connaisseur Pods. Prior to version 1.1.5, the Connaisseur Pods could report `Ready` while being non-functional due to the missing MutatingWebhookConfiguration. However, this issue was solved by checking the `Ready` state of the Connaisseur Pods.\n\n**Problem**\n\nThe problem is that adding a dependency to the `Ready` state of the Connaisseur Pods would create a deadlock situation, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.\n\n**Decision Rationale**\n\nThe decision is to not add a dependency to the `Ready` state of the Connaisseur Pods because it would create a circular dependency that would lead to a deadlock. Instead, we will rely on the fact that the Connaisseur Pods will eventually become ready even if the MutatingWebhookConfiguration is not yet deployed. This approach ensures that the system will eventually become functional, even if there is a delay in deploying the MutatingWebhookConfiguration.\n\n**Consequences**\n\nThe consequence of this decision is that the system may experience a delay in becoming functional if the MutatingWebhookConfiguration is not deployed immediately after the Connaisseur Pods. However, this delay is acceptable because it does not affect the overall functionality of the system.\n\n**Status**\n\nThis decision is approved and will be implemented in the Connaisseur system.\n\n**References**\n\n[#3](https:\/\/github.com\/sse-secure-systems\/connaisseur\/issues\/3) - Issue describing the problem with the `Ready` state of the Connaisseur Pods.","GenTime":"2024-08-10 21:41:11"}
{"File Name":"connaisseur\/ADR-2_release-management.md","Context":"## Context\\nDuring its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed\/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart.\\nA single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility.\\n","Decision":"For choice 1, we decided to go for two branches. On the one hand, `master` being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a `develop` branch that hosts the current state of development and will be merged to `master` whenever we want to create a new release.\\nThis way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release.\\nIn the process of automating most of the release process, we will run an integration test with locally built images for pull requests to `master`. Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the `master` branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the `master` branch referencing the new release version.\\nAfter the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working.\\nWe decided for this option as it does not expose credentials to GitHub Actions, which we wanted to avoid especially in light of the [recent GitHub Actions injection attacks](https:\/\/bugs.chromium.org\/p\/project-zero\/issues\/detail?id=2070) and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys.\\n### Positive consequences\\n- We can develop without having to ship changes immediatly.\\n- Release process does not expose credentials to GitHub Actions.\\n- Code gets Git tags.\\n### Negative consequences\\n- Process from code to release for a single change is more cumbersome than right now.\\n- Release still requires human intervention.\\n","tokens":131,"id":4397,"Prediction":"## Decision\n\nTo address the issue of a single person being responsible for building, signing, and pushing Connaisseur images whenever a new pull request is accepted, we will implement a Continuous Integration\/Continuous Deployment (CI\/CD) pipeline using Jenkins and Docker.\n\n## Context\n\nAs mentioned in the context, Connaisseur was initially developed and maintained by a single person, and the process of building and pushing images was not a significant concern. However, with the growth of the team and the increasing number of contributions, it has become impractical for a single person to handle this task. Moreover, with the publication of Connaisseur, it is essential that the uploaded image corresponds to the most recent version referenced in the Helm chart.\n\n## Problem\n\nThe current process of building, signing, and pushing images is manual, time-consuming, and prone to errors. It also creates a bottleneck for the development team, as they have to wait for the images to be built and pushed before they can proceed with their work.\n\n## Decision Drivers\n\n1. Scalability: With the growth of the team, we need a solution that can scale with the increasing number of contributions and releases.\n2. Agility: We need a solution that can automate the process of building, signing, and pushing images, allowing the development team to focus on their work.\n3. Reliability: We need a solution that can ensure the reliability and consistency of the image builds and deployments.\n4. Security: We need a solution that can ensure the security and integrity of the images being built and deployed.\n\n## Solution\n\nWe will implement a CI\/CD pipeline using Jenkins and Docker. The pipeline will automate the process of building, signing, and pushing images whenever a new pull request is accepted. The pipeline will also ensure that the uploaded image corresponds to the most recent version referenced in the Helm chart.\n\n## Consequences\n\n1. Improved scalability: The CI\/CD pipeline will allow us to scale with the increasing number of contributions and releases.\n2. Increased agility: The automated process will allow the development team to focus on their work, reducing the time and effort required for building and pushing images.\n3. Improved reliability: The pipeline will ensure the reliability and consistency of the image builds and deployments.\n4. Enhanced security: The pipeline will ensure the security and integrity of the images being built and deployed.\n\n## Rationale\n\nThe CI\/CD pipeline is the best solution for addressing the issue of building, signing, and pushing images. It will automate the process, ensuring that it is scalable, agile, reliable, and secure.","GenTime":"2024-08-10 21:41:54"}
{"File Name":"gp-redirect\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4401,"Prediction":"Here is an Architectural Decision Record (ADR) for your software:\n\n**Decision**\n\n**Title**: Implement a Microservices Architecture\n\n**Status**: Accepted\n\n**Context**: As our project grows, we need to ensure scalability, maintainability, and flexibility. We have identified that our monolithic architecture is becoming a bottleneck and is limiting our ability to develop and deploy new features efficiently.\n\n**Problem**: Our monolithic architecture makes it difficult to:\n\n* Scale individual components independently\n* Develop and deploy new features without affecting the entire application\n* Maintain and update individual components without affecting the entire application\n* Use different programming languages and frameworks for different components\n\n**Decision**: We will implement a microservices architecture, where each component of our application is a separate service that communicates with other services using APIs.\n\n**Rationale**: A microservices architecture will allow us to:\n\n* Scale individual components independently, improving overall system scalability\n* Develop and deploy new features without affecting the entire application, improving development speed and efficiency\n* Maintain and update individual components without affecting the entire application, improving maintainability\n* Use different programming languages and frameworks for different components, improving flexibility\n\n**Consequences**:\n\n* Increased complexity due to the need to manage multiple services\n* Potential for increased network latency due to the need for services to communicate with each other\n* Need for additional infrastructure and monitoring tools to support multiple services\n\n**References**:\n\n* [1] \"Microservices Architecture\" by Martin Fowler\n* [2] \"Designing Distributed Systems\" by Brendan Burns\n\n**Implementation**:\n\n* We will use a service registry to manage the discovery and communication between services\n* We will use APIs to communicate between services\n* We will use a message broker to handle asynchronous communication between services\n* We will implement circuit breakers and fallbacks to handle failures and timeouts between services\n\n**Review**:\n\nThis decision will be reviewed in 3 months to assess its effectiveness and identify any areas for improvement.\n\nI hope this helps! Let me know if you have any questions or need further assistance.","GenTime":"2024-08-10 21:43:09"}
{"File Name":"rtk-companion\/0003-mvp-functionality-in-add-card-window.md","Context":"## Context and Problem Statement\\nAfter experimenting with the previous MVP goal defined by [ADR-0002](0002-define-goal-for-an-mvp.md), I've decided it's not a good approach.\\nMy main issue with is that in a way, it is re-creating Anki's \"Add Card\" screen, albeit a custom one which adds RTK cards.\\nHere's a screenshot of the UI created and filled with placeholder text.\\n![](..\/assets\/rtk-browser-screenshot.png)\\nWhile I still think it'd be cool to have an RTK browser in Anki and working with QT is ....interesting, I thought\\na simpler approach should be sought.\\nSo the problem stands, what should be done as an MVP and what should it contain?\\n","Decision":"After some experimenting and research, I've decided an MVP should augment the `Add Card`.\\nHere's the vision of how a user interacts with the MVP.\\n1. User opens the `Add Card` window which has a \"RTK\" button, and a \"Stories\" button.\\n1. User types in their desired Heisig Kanji number is the \"Number\" field.\\n1. User clicks the RTK button and the character, and stroke order get auto filled.\\n1. User clicks \"Stories\" button, which opens an embedded browser window to user stories.\\n### Positive Consequences <!-- optional -->\\n- The approach is simpler.\\n- We keep Building UI elements with `QT` to a minimum under this approach.\\n### Negative Consequences <!-- optional -->\\n- We leave out Additional keyword meanings and the radicals for the Kanji\\n","tokens":161,"id":4402,"Prediction":"## Decision\n\n**ADR-0003: Simplifying the MVP Approach for the RTK Browser**\n\n## Context\n\nThe previous MVP approach, defined in ADR-0002, has been deemed unsuitable due to its complexity and similarity to Anki's \"Add Card\" screen. The goal is to create a simplified MVP that effectively solves the problem at hand.\n\n## Decision\n\nTo address the problem, we will adopt a more straightforward approach for the MVP. Instead of recreating the \"Add Card\" screen, we will focus on creating a minimal viable product that allows users to import RTK cards into Anki. This will involve the following features:\n\n1. A simple import wizard that guides users through the process of importing RTK cards.\n2. Basic card formatting and styling to ensure the imported cards are visually appealing and easy to read.\n3. Integration with Anki's existing card management system to ensure seamless importation.\n\n## Rationale\n\nBy focusing on the core functionality of importing RTK cards, we can create a more focused and efficient MVP that meets the needs of our users. This approach will also allow us to test and validate the concept without getting bogged down in unnecessary complexity.\n\n## Consequences\n\nThis decision means that we will not be creating a custom RTK browser within Anki, but rather a simple import wizard that integrates with Anki's existing functionality. This may impact the overall user experience, but it will allow us to deliver a more focused and effective MVP.\n\n## Status\n\nThis decision is approved and will be implemented in the next iteration.","GenTime":"2024-08-10 21:43:26"}
{"File Name":"Conduit\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4408,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\n**Title**: Implement a Microservices Architecture for the Project\n\n**Context**: We need to record the architectural decisions made on this project.\n\n**Problem**: Our current monolithic architecture is causing scalability and maintainability issues. As the project grows, it's becoming increasingly difficult to make changes and deploy new features.\n\n**Decision**: We will implement a microservices architecture for the project, breaking down the monolithic application into smaller, independent services that communicate with each other using APIs.\n\n**Rationale**:\n\n* Microservices architecture allows for greater scalability and flexibility, as each service can be developed, deployed, and scaled independently.\n* It enables us to use different programming languages and frameworks for each service, allowing us to leverage the best tools for the job.\n* Microservices architecture makes it easier to maintain and update individual services without affecting the entire application.\n* It provides a more fault-tolerant system, as if one service goes down, the others can continue to function.\n\n**Consequences**:\n\n* We will need to implement a service registry and API gateway to manage communication between services.\n* We will need to implement circuit breakers and retries to handle service failures.\n* We will need to implement monitoring and logging for each service.\n* We will need to rearchitect our database schema to accommodate the new microservices architecture.\n\n**Status**: Approved\n\n**Date**: 2023-02-20\n\n**Links**: [Link to related documentation or code]\n\n**Implementors**: [List of team members responsible for implementing this decision]\n\nBy recording this decision, we can ensure that all stakeholders are aware of the architectural direction of the project and can make informed decisions about future development.","GenTime":"2024-08-10 21:45:32"}
{"File Name":"buildit-all\/0006-use-okta-as-identity-provider.md","Context":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n","Decision":"* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","tokens":279,"id":4414,"Prediction":"## Decision\n\n**Use Okta as the Identity as a Service provider for authentication and authorization in our application**\n\n## Context\n\nAs discussed, Okta provides a forever free developer account with support for up to 7000 active users per month, making it a suitable choice for our application's anticipated usage. Okta also offers a variety of authentication providers, including social providers like Microsoft OpenConnect ID, which we plan to use. Additionally, Okta has integrations with Spring Boot 1.x and Spring Security 5.x, making it easy to implement authentication and authorization in our application.\n\n## Problem\n\nWe need to implement authentication and authorization in our application, and we require a scalable and reliable solution that can handle a large number of users.\n\n## Decision\n\nAfter evaluating the options, we have decided to use Okta as our Identity as a Service provider for authentication and authorization in our application. Okta's forever free developer account and support for up to 7000 active users per month make it a suitable choice for our application's anticipated usage. Additionally, Okta's integrations with Spring Boot 1.x and Spring Security 5.x make it easy to implement authentication and authorization in our application.\n\n## Consequences\n\n* We will use Okta's Microsoft OpenConnect ID provider to authenticate users and map them to Okta users.\n* We will use Okta's React client SDK to handle client-side authentication and authorization.\n* We will leverage Okta's admin screens and dashboards to manage roles and groups, providing a more uniform and standards-adhering API.\n* We will have the option to add additional social logins and support for AD directly if needed in the future.\n\n## Rationale\n\nWe chose Okta because it provides a scalable and reliable solution for authentication and authorization, and its integrations with Spring Boot 1.x and Spring Security 5.x make it easy to implement in our application. Additionally, Okta's forever free developer account and support for up to 7000 active users per month make it a suitable choice for our application's anticipated usage.","GenTime":"2024-08-10 21:47:37"}
{"File Name":"buildit-all\/0008-database.md","Context":"## Context\\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\\n","Decision":"* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\\n* Use MariaDB JDBC driver - has native Aurora support for failover\\n","tokens":29,"id":4415,"Prediction":"## Decision\n\nTo persist the locations, bookables, and bookings, we will use a relational database management system (RDBMS) with an ORM (Object-Relational Mapping) tool.\n\n## Context\n\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\n\n## Problem\n\nBookit requires a data storage solution that can handle the persistence of its core entities: locations, bookables, and bookings. The data needs to be stored in a way that ensures consistency, scalability, and fault tolerance. Additionally, the data storage solution should be able to handle the high traffic and large amounts of data that Bookit is expected to handle.\n\n## Decision\n\nWe will use a relational database management system (RDBMS) such as PostgreSQL or MySQL to store the data. This choice is based on the following reasons:\n\n* Relational databases are well-suited for storing structured data and can handle large amounts of data.\n* They provide strong consistency and isolation guarantees, which is essential for a system that requires high availability and fault tolerance.\n* They support transactions, which allows us to ensure data consistency and integrity.\n* They have a wide range of tools and libraries available for development and maintenance.\n\nTo interact with the database, we will use an ORM (Object-Relational Mapping) tool such as Hibernate or TypeORM. This will allow us to map our domain objects to the relational database schema, and provide a layer of abstraction between our application code and the database.\n\n## Consequences\n\nUsing a relational database management system with an ORM tool will provide the following benefits:\n\n* Improved data consistency and integrity\n* Improved scalability and fault tolerance\n* Improved development and maintenance efficiency\n* Better support for transactions and concurrency\n\nHowever, it also means that we will need to design and implement a database schema, which can be a complex and time-consuming task. Additionally, we will need to ensure that the database is properly configured and optimized for performance.\n\n## Rationale\n\nWe considered other options such as NoSQL databases, but they do not provide the same level of consistency and isolation guarantees as relational databases. We also considered using a cloud-based database service, but we decided to use a self-managed database to have more control over the database and to avoid vendor lock-in.\n\nOverall, using a relational database management system with an ORM tool is the best choice for Bookit's data storage needs, as it provides the necessary consistency, scalability, and fault tolerance while also allowing for efficient development and maintenance.","GenTime":"2024-08-10 21:48:04"}
{"File Name":"buildit-all\/0005-use-id-token-from-microsoft-as-bearer-token.md","Context":"## Context\\nIn the interest of time and getting something to work, we are going to break up the steps further\\n","Decision":"* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\\n* Proper validation of the id_token will still occur\\n","tokens":24,"id":4416,"Prediction":"## Decision\n\nTo break up the steps further to prioritize time and getting the software functional, we will implement a modular architecture that allows for incremental development and testing of individual components.\n\n## Context\n\nIn the interest of time and getting something to work, we are going to break up the steps further to prioritize time and getting the software functional.\n\n## Problem\n\nThe complexity of the software development process is overwhelming, and we are facing a tight deadline. We need to find a way to break down the development process into manageable chunks to ensure timely completion.\n\n## Reasoning\n\nBreaking up the steps further will allow us to focus on one component at a time, reducing the complexity and increasing the chances of success. This approach will also enable us to test and validate individual components before moving on to the next one, reducing the risk of errors and rework.\n\n## Decision Criteria\n\n* Prioritize time and getting the software functional\n* Break down the development process into manageable chunks\n* Focus on one component at a time\n* Test and validate individual components before moving on to the next one\n* Reduce the risk of errors and rework\n\n## Status\n\nApproved\n\n## Implemented By\n\n[Your Name]\n\n## Date\n\n[Date]\n\n## References\n\n* [Link to relevant documentation or resources]\n\nNote: This is a simple example, you can add more details and sections as per your requirement.","GenTime":"2024-08-10 21:48:19"}
{"File Name":"buildit-all\/0007-use-pac4j-to-validate-tokens.md","Context":"## Context\\nhttp:\/\/www.pac4j.org\\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it's not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\\n","Decision":"* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n","tokens":144,"id":4422,"Prediction":"Here is the Architectural Decision Record (ADR):\n\n**Decision**\n\nWe will use pac4j as the alternative to our custom filter and token validator for Azure AD authentication.\n\n**Context**\n\nWhile exploring alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box. The driver for this alternative is to have a library where we can point it to the discovery URL (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\" Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint. While it's not hard, OpenID Connect libraries need to be adjusted to handle this. Pac4j does this for free.\n\n**Problem**\n\nOur current custom filter and token validator for Azure AD authentication are not scalable and require manual adjustments to handle the unique requirements of the Microsoft Azure AD Issuer URL.\n\n**Solution**\n\nUsing pac4j, we can leverage its built-in support for Azure AD and avoid the need for custom filtering and token validation. This will simplify our authentication process and reduce the risk of errors.\n\n**Rationale**\n\nPac4j's support for Azure AD out of the box and its ability to handle the unique requirements of the Microsoft Azure AD Issuer URL make it a more scalable and maintainable solution than our current custom implementation.\n\n**Consequences**\n\n* We will need to integrate pac4j into our application and configure it to use the Azure AD discovery URL.\n* We will no longer need to maintain our custom filter and token validator for Azure AD authentication.\n* Our application will be more scalable and maintainable due to the use of a widely-used and well-maintained library.\n\n**Status**\n\nApproved\n\n**Date**\n\n[Insert date]\n\n**References**\n\n* pac4j documentation: http:\/\/www.pac4j.org\n* Microsoft Azure AD documentation: https:\/\/docs.microsoft.com\/en-us\/azure\/active-directory\/develop\/active-directory-v2-protocols-oidc","GenTime":"2024-08-10 21:50:29"}
{"File Name":"buildit-all\/0002-version-api-via-uri-path.md","Context":"## Context\\nVersioning APIs can be controversial.  Ideally, APIs don't need to change, or be additive so as not to break backwards compatibility.  Occasionally, some changes require bigger changes.\\nWe will do our best to be open w\/ what we accept and strict w\/ what we return\\nThere are lots of ways an API can change:\\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\\n* Breaking changes at a resource level (\/v1\/booking) needs to change\\n* Breaking changes in behavior\\nThis decision currently applies to the 1st bullet.\\n","Decision":"All API endpoints will start with the version of the API (e.g. \/v1\/resource).  We don't anticipate this to change.\\n","tokens":127,"id":4424,"Prediction":"## Decision\n\n**API Versioning Strategy for Breaking Changes Across the Entire API**\n\n## Context\n\nAs discussed, versioning APIs can be controversial, and ideally, APIs should not need to change or be additive to maintain backwards compatibility. However, occasionally, some changes require bigger changes, such as a complete shift from REST to GraphQL. This decision applies to the first bullet point, where the entire API needs to change.\n\n## Problem\n\nThe current API is facing a significant change, requiring a complete overhaul of the architecture. This change will break the existing API, and we need to decide on a strategy to handle this.\n\n## Decision\n\nWe will implement a semantic versioning strategy for the API, where the major version number will be incremented when making breaking changes across the entire API. This means that when we make significant changes, such as switching from REST to GraphQL, we will increment the major version number (e.g., from v1 to v2).\n\n## Rationale\n\n* This approach allows us to clearly communicate to users that the API has changed significantly and requires updates on their end.\n* It provides a clear separation between backward-compatible changes and breaking changes, making it easier for users to understand what changes they need to make.\n* It gives us flexibility to make changes without worrying about backward compatibility, as users will be aware of the changes and can adapt accordingly.\n\n## Consequences\n\n* Users will need to update their code to accommodate the new API version.\n* We will need to provide clear documentation and communication about the changes and the new API version.\n* This approach may lead to a temporary disruption in service while users update their code.\n\n## Status\n\nThis decision is currently in effect and will be reviewed as needed.\n\n## Links\n\n* [Semantic Versioning](https:\/\/semver.org\/)\n* [API Versioning Strategies](https:\/\/www.martinfowler.com\/articles\/api-design-versioning.html)","GenTime":"2024-08-10 21:51:07"}
{"File Name":"buildit-all\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4428,"Prediction":"Here is an Architectural Decision Record (ADR) for the software:\n\n## Decision: Implementing a Microservices Architecture\n\n### Context\n\nAs our project grows in complexity and size, we need to consider a scalable and maintainable architecture that allows for independent development, deployment, and scaling of individual components. We have evaluated several architecture options, including Monolithic, Service-Oriented Architecture (SOA), and Microservices Architecture. After careful consideration, we have decided to implement a Microservices Architecture for our project.\n\n### Decision\n\nWe will break down our monolithic application into smaller, independent services that communicate with each other using lightweight protocols and APIs. Each service will be responsible for a specific business capability and will be developed, tested, and deployed independently. This will allow us to:\n\n* Improve scalability and fault tolerance by allowing each service to be scaled independently\n* Enhance maintainability by allowing developers to work on individual services without affecting the entire application\n* Increase flexibility by allowing us to choose the best technology stack for each service\n* Improve collaboration by allowing multiple teams to work on different services simultaneously\n\n### Status\n\nThis decision has been reviewed and approved by the project stakeholders.\n\n### Proponents\n\n* [Your Name]\n\n### Opponents\n\n* None\n\n### Rationale\n\nWe chose Microservices Architecture because it aligns with our project goals and values. It provides a scalable and maintainable architecture that allows for independent development and deployment of individual components. It also provides flexibility and collaboration benefits that will help us deliver the project on time and within budget.\n\n### Alternatives Considered\n\n* Monolithic Architecture: We considered a monolithic architecture, but it would not provide the same level of scalability, maintainability, and flexibility as a Microservices Architecture.\n* Service-Oriented Architecture (SOA): We considered SOA, but it would require a centralized registry and governance, which would add complexity to our system.\n\n### Links\n\n* [Link to relevant documentation or design decisions]\n\n### Implementation\n\nThe implementation of this decision will involve:\n\n* Breaking down the monolithic application into smaller services\n* Designing and implementing APIs for communication between services\n* Developing and testing individual services\n* Deploying and scaling individual services\n\n### Rollback Plan\n\nIn case the Microservices Architecture does not meet our expectations, we will consider rolling back to a monolithic architecture. However, we believe that the benefits of a Microservices Architecture outweigh the risks, and we are committed to making it work.\n\n### Changelog\n\n* [Date] - Initial decision recorded\n* [Date] - Decision reviewed and approved by project stakeholders","GenTime":"2024-08-10 21:52:36"}
{"File Name":"verify-service-provider\/0023-we-will-report-the-version-in-a-saml-extension.md","Context":"## Context\\nThe verify-service-provider will be deployed to a number of relying parties.\\nWhich relying party is using which version could potentially become difficult\\nto keep track of.\\nTo make it as easy as possible for us to manage this we'd like the\\nverify-service-provider to report its version in some way.\\nBecause the verify-service-provider is not intended to be accessible to the\\ninternet we can't simply expose an endpoint that reports the version number.\\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\\nheader.\\nThere's also a concern about the security implications of reporting a version\\nnumber in cleartext.\\nWe considered a couple of options:\\n- Requesting metadata from Verify with a custom user-agent string\\n- Sending the version in an unencrypted saml extension\\n- Sending the version in an encrypted saml extension\\n","Decision":"We decided to send the version number in the SAML AuthnRequests as an encrypted\\nSAML extension. The XML will look roughly like this:\\n```\\n<saml:AuthnRequest>\\n<saml:Issuer>...<\/saml:Issuer>\\n<saml:Signature>...<\/saml:Signature>\\n<saml:Extensions>\\n<saml:EncryptedAttribute>...<\/saml:EncryptedAttribute>\\n<\/saml:Extensions>\\n<\/saml:AuthnRequest>\\n```\\nOnce decrypted, the Attribute in the Extensions will look like:\\n```\\n<saml:Attribute Name=\"Versions\">\\n<saml:AttributeValue xsi:type=\"metrics:VersionsType\">\\n<metrics:ApplicationVersion>3.4.1<\/metrics:ApplicationVersion>\\n<\/saml:AttributeValue>\\n<\/saml:Attribute>\\n```\\n","tokens":182,"id":4436,"Prediction":"## Decision\n\nAfter considering the options, we have decided to implement the third option: Sending the version in an encrypted SAML extension.\n\n## Context\n\nAs mentioned in the context, the verify-service-provider will be deployed to multiple relying parties, and it's essential to keep track of which relying party is using which version. However, since the verify-service-provider is not accessible to the internet, we cannot expose an endpoint to report the version number. Additionally, due to security concerns, we cannot report the version number in cleartext.\n\n## Problem\n\nWe need to find a way to report the version number of the verify-service-provider to the relying parties without compromising security and without exposing an endpoint.\n\n## Decision\n\nWe have decided to send the version number in an encrypted SAML extension. This approach meets our requirements as it:\n\n* Allows us to report the version number to the relying parties\n* Keeps the version number secure by encrypting it\n* Does not require exposing an endpoint or using a custom HTTP header\n\n## Consequences\n\nBy sending the version number in an encrypted SAML extension, we ensure that the version number remains secure and confidential. This approach also simplifies the process of managing and tracking the versions used by each relying party.\n\n## Rationale\n\nWe considered the first option, requesting metadata from Verify with a custom user-agent string, but it may not be feasible or reliable. The second option, sending the version in an unencrypted SAML extension, is not secure and compromises the confidentiality of the version number. The third option, sending the version in an encrypted SAML extension, is the most secure and reliable approach.","GenTime":"2024-08-10 21:54:57"}
{"File Name":"verify-service-provider\/0013-we-will-write-acceptance-tests-against-compliance-tool.md","Context":"## Context\\nVerify provide a \"compliance tool\" which allows relying parties to check that their\\nimplementations are compatible with the SAML profile of Verify.\\nCurrently relying parties must test their service against the compliance tool as part\\nof the onboarding process.\\nBy writing tests against the compliance tool we can:\\n* make sure our own implementation is compliant\\n* demonstrate to relying parties how to write tests against the compliance tool\\n* learn more about the user experience of using the compliance tool\\n","Decision":"We'll write acceptance tests against the compliance tool for the verify service provider\\nand the stub relying parties.\\nTests for the service provider will give us direct feedback on whether it's compliant.\\nTests for the stub relying parties will give us confidence they work end-to-end and should\\nprovide a template for real relying parties to write their own tests.\\n","tokens":101,"id":4442,"Prediction":"## Decision\n\n### Decision: Implement a Test-Driven Development (TDD) approach for the compliance tool\n\n### Context\n\nAs described in the provided context, the compliance tool is used to verify that relying parties' implementations are compatible with the SAML profile of Verify. Currently, relying parties must test their service against the compliance tool as part of the onboarding process.\n\n### Problem\n\nThe current approach of relying parties testing their service against the compliance tool is manual and time-consuming. This can lead to errors, delays, and increased costs. Moreover, the compliance tool is not being used to its full potential, as it is not being used to drive the development of the tool itself.\n\n### Solution\n\nTo address the problem, we will implement a Test-Driven Development (TDD) approach for the compliance tool. This will ensure that the tool is thoroughly tested and validated before being released to relying parties. By writing tests against the compliance tool, we can:\n\n* Make sure our own implementation is compliant with the SAML profile of Verify\n* Demonstrate to relying parties how to write tests against the compliance tool, reducing the complexity of the onboarding process\n* Learn more about the user experience of using the compliance tool, allowing us to improve its usability and effectiveness\n\n### Rationale\n\nImplementing a TDD approach for the compliance tool will ensure that it is thoroughly tested and validated, reducing the risk of errors and improving its overall quality. Additionally, it will allow us to demonstrate the value of the compliance tool to relying parties, making it easier for them to adopt and use it. By driving the development of the tool with tests, we can ensure that it meets the requirements of relying parties and is effective in verifying their implementations.\n\n### Consequences\n\nImplementing a TDD approach for the compliance tool will require additional time and resources upfront. However, it will pay off in the long run by reducing the risk of errors, improving the quality of the tool, and making it easier for relying parties to use.","GenTime":"2024-08-10 21:56:59"}
{"File Name":"verify-service-provider\/0006-we-will-build-a-js-client.md","Context":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":4443,"Prediction":"## Decision\n\nTo provide a frictionless integration for users currently using Node.js and Passport, we will implement a RESTful API that utilizes Passport.js for authentication and authorization. This will allow users to easily integrate our service with their existing applications.\n\n## Context\n\nAs mentioned, at least one user is currently using Node.js and Passport, and we want to provide a seamless integration experience for them. Additionally, other users may be using different languages and frameworks, so we need to design our API to be flexible and adaptable.\n\n## Problem\n\nIf we don't provide a RESTful API that uses Passport.js, users who are currently using Node.js and Passport will need to implement their own authentication and authorization mechanisms, which could be time-consuming and error-prone. This could lead to a poor user experience and potentially even cause issues with our service.\n\n## Decision\n\nTo solve this problem, we will implement a RESTful API that uses Passport.js for authentication and authorization. This will allow users to easily integrate our service with their existing applications, regardless of the language or framework they are using.\n\n## Consequences\n\nBy implementing a RESTful API that uses Passport.js, we will:\n\n* Provide a seamless integration experience for users currently using Node.js and Passport\n* Allow other users to easily integrate our service with their applications, regardless of the language or framework they are using\n* Reduce the complexity and effort required for users to integrate our service with their applications\n* Improve the overall user experience and satisfaction with our service\n\n## Rationale\n\nWe chose to implement a RESTful API that uses Passport.js because it is a widely-used and well-established authentication and authorization mechanism. By using Passport.js, we can provide a consistent and reliable authentication and authorization experience for all users, regardless of the language or framework they are using. Additionally, Passport.js provides a high degree of flexibility and customization, which will allow us to adapt to the needs of our users and provide a tailored experience for each user.","GenTime":"2024-08-10 21:57:20"}
{"File Name":"verify-service-provider\/0025-we-will-only-release-one-configuration-file.md","Context":"## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n","Decision":"We will remove the verify-service-provider-env.yml file from src\/main\/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path\/to\/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n","tokens":228,"id":4448,"Prediction":"## Decision\n\nTo simplify the configuration of the Verify Service Provider, we will remove the ability to use a YAML file and only allow configuration through environment variables.\n\n## Context\n\nHistorically, we have had two ways of configuring Verify Service Provider: using environment variables and using a YAML file. The use of two configuration methods was due to restrictions with the Java buildpack used by CloudFoundry, as well as a philosophical point that 12 factor applications should be configured through their environment.\n\n## Problem\n\nThe current configuration system is complex and difficult to manage, with two separate methods of configuration. This complexity is due to the restrictions of the Java buildpack, which does not allow for command line arguments, and the uncertainty about how CloudFoundry manages static files.\n\n## Decision\n\nTo simplify the configuration of the Verify Service Provider, we will remove the ability to use a YAML file and only allow configuration through environment variables. This will simplify the configuration process and make it easier for users to manage their configuration.\n\n## Consequences\n\n* The ability to use a YAML file will be removed, and users will only be able to configure the Verify Service Provider through environment variables.\n* The complexity of the configuration system will be reduced, making it easier for users to manage their configuration.\n* The philosophical point about 12 factor applications being configured through their environment will be upheld.\n* The Java buildpack restrictions will no longer be a concern, as we will only be using environment variables for configuration.\n\n## Rationale\n\nThe decision to remove the ability to use a YAML file is based on the complexity and difficulty of managing the current configuration system. By simplifying the configuration process, we can make it easier for users to manage their configuration and reduce the complexity of the system. Additionally, the philosophical point about 12 factor applications being configured through their environment will be upheld, which aligns with the principles of the 12 factor app methodology.","GenTime":"2024-08-10 21:59:34"}
{"File Name":"verify-service-provider\/0008-provide-an-end-to-end-stub.md","Context":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":4454,"Prediction":"## Decision\n\n**Decision:** Implement a mock Service Provider interface to simulate a typical Verify journey for user research and testing.\n\n**Context:** We wish to run regular user research and testing against the prototype Service Provider. To support user research, we need to provide a user journey that resembles a typical Verify journey. The area we are most interested in is the interface between the Service and the Hub.\n\n**Problem:** The Service Provider is still under development, and we cannot yet provide a realistic user journey for testing and research. The current implementation is not stable or scalable enough to support user testing.\n\n**Reasoning:** Implementing a mock Service Provider interface will allow us to simulate a typical Verify journey, providing a realistic user experience for testing and research. This will enable us to gather valuable feedback from users and identify areas for improvement in the Service Provider's interface with the Hub.\n\n**Consequences:**\n\n* The mock Service Provider interface will need to be maintained and updated regularly to ensure it remains relevant and accurate.\n* The interface will need to be designed to mimic the behavior of the real Service Provider, including any potential errors or edge cases.\n* The mock interface will allow us to test and research the Service Provider's interface with the Hub independently of the Service Provider's development schedule.\n\n**Alternatives Considered:**\n\n* Delaying user research and testing until the Service Provider is fully developed. This would mean missing valuable opportunities for feedback and iteration.\n* Using a simplified or abstracted representation of the Service Provider's interface. This would not provide a realistic user experience and may not capture the complexities of the real interface.\n\n**Decision Owner:** [Name]\n\n**Implementation Status:** In progress\n\n**Review Date:** [Date]","GenTime":"2024-08-10 22:01:43"}
{"File Name":"event-routing-backends\/0005-PII-leakage-prevention.rst","Context":"Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","Decision":"--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","tokens":307,"id":4477,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\n### Event-routing-backends: Accessor Method for PII-Sensitive Data\n\n#### Context\n\nEvent-routing-backends transforms and emits edx events that may contain PII (Personally Identifiable Information) which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in Event-routing-backends and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\n\n#### Decision\n\n1. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\n2. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\n3. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\n4. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\n\n#### Benefits\n\n1. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\n\n**Reference**\n\n_method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139","GenTime":"2024-08-10 22:06:55"}
{"File Name":"event-routing-backends\/0006-versioning-of-event-transformers.rst","Context":"Context\\n-------\\nEvent transformers may undergo modification in future in response to consumer request, change in specification, bug fixes etc.\\nDecision\\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","Decision":"--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","tokens":398,"id":4483,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\n\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the semantic version of the event-routing-backends package used to generate the event.\n\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\n\n#. Transformer version number will be the semantic version of the event-routing-backends package.\n\n#. The event-routing-backends major version will be incremented when:\n    * Transformer is changed due to update in original specification (xAPI or Caliper).\n    * A key is removed from or renamed in the existing transformer.\n    * Value of a key is updated in the existing transformer.\n\n#. The event-routing-backbacks minor version will be incremented when:\n    * A key is added to an existing transformer.\n    * A new event is added for transformation.\n\n#. Minor version (Z) will be incremented when:\n    * A bug is fixed.\n\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\n\n**Context**\n\nEvent transformers may undergo modification in future in response to consumer request, change in specification, bug fixes etc.","GenTime":"2024-08-10 22:08:29"}
{"File Name":"platform\/2020-08-14-implement-individual-sorting.md","Context":"## Context\\nShop owners should be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should be possible to define a system default sorting option for product listings.\\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\\n","Decision":"From now on, it is possible to define custom sortings via the administration.\\nIndividual sortings will be stored in the database in the table `product_sorting` and its translatable label in the `product_sorting_translation` table.\\nIt is possible to define a system default product listing sorting option, which is stored in `system_default`.`core.listing.defaultSorting`.\\nThis however has no influence on the default `Top Results` sorting on search pages and the suggest route result.\\nTo define custom sorting options via a plugin, you can either write a migration to store them in the database.\\nThis method is recommended, as the sortings can be managed via the administration.\\nThe `product_sorting` table looks like the following:\\n| Column          | Type           | Notes                                                 |\\n| --------------- | -------------- | ----------------------------------------------------- |\\n| `id`            | binary(16)     |                                                       |\\n| `url_key`       | varchar(255)   | Key (unique). Shown in url, when sorting is chosen    |\\n| `priority`      | int unsigned   | Higher priority means, the sorting will be sorted top |\\n| `active`        | tinyint(1) [1] | Inactive sortings will not be shown and will not sort |\\n| `locked`        | tinyint(1) [0] | Locked sortings can not be edited via the DAL         |\\n| `fields`        | json           | JSON of the fields by which to sort the listing       |\\n| `created_at`    | datetime(3)    |                                                       |\\n| `updated_at`    | datetime(3)    |                                                       |\\nThe JSON for the fields column look like this:\\n```json5\\n[\\n{\\n\"field\": \"product.name\",        \/\/ property to sort by (mandatory)\\n\"order\": \"desc\",                \/\/ \"asc\" or \"desc\" (mandatory)\\n\"priority\": 0,                  \/\/ in which order the sorting is to applied (higher priority comes first) (mandatory)\\n\"naturalSorting\": 0\\n},\\n{\\n\"field\": \"product.cheapestPrice\",\\n\"order\": \"asc\",\\n\"priority\": 100,\\n\"naturalSorting\": 0\\n},\\n\/\/ ...\\n]\\n```\\n---\\nOtherwise, you can subscribe to the `ProductListingCriteriaEvent` to add a `ProductSortingEntity` as available sorting on the fly.\\n```php\\n<?php\\nnamespace Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\Example;\\nuse Shopware\\Core\\Content\\Product\\Events\\ProductListingCriteriaEvent;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingCollection;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingEntity;\\nuse Symfony\\Component\\EventDispatcher\\EventSubscriberInterface;\\nclass ExampleListingSubscriber implements EventSubscriberInterface {\\npublic static function getSubscribedEvents(): array\\n{\\nreturn [\\nProductListingCriteriaEvent::class => ['addMyCustomSortingToStorefront', 500],\\n];\\n}\\npublic function addMyCustomSortingToStorefront(ProductListingCriteriaEvent $event): void\\n{\\n\/** @var ProductSortingCollection $availableSortings *\/\\n$availableSortings = $event->getCriteria()->getExtension('sortings') ?? new ProductSortingCollection();\\n$myCustomSorting = new ProductSortingEntity();\\n$myCustomSorting->setId(Uuid::randomHex());\\n$myCustomSorting->setActive(true);\\n$myCustomSorting->setTranslated(['label' => 'My Custom Sorting']);\\n$myCustomSorting->setKey('my-custom-sort');\\n$myCustomSorting->setPriority(5);\\n$myCustomSorting->setFields([\\n[\\n'field' => 'product.name',\\n'order' => 'desc',\\n'priority' => 1,\\n'naturalSorting' => 0,\\n],\\n]);\\n$availableSortings->add($myCustomSorting);\\n$event->getCriteria()->addExtension('sortings', $availableSortings);\\n}\\n}\\n```\\n","tokens":117,"id":4493,"Prediction":"## Decision\n\n**Title:** Implement Custom Sorting Options for Product Listings and Search Result Pages via Administration\n\n**Context:** Shop owners should be able to define custom sorting options for product listings and search result pages out of the administration. It should be possible to define a system default sorting option for product listings. `Top Results` will be the default on search pages and suggest route, which sorts products by `_score`. Currently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`. This is somewhat tedious and makes it impossible to define individual sortings via the administration.\n\n**Decision:** We will implement a new feature in the administration panel that allows shop owners to define custom sorting options for product listings and search result pages without requiring technical expertise. This feature will include a user-friendly interface for creating and managing custom sorting options, as well as the ability to set a system default sorting option for product listings.\n\n**Rationale:**\n\n* Providing a user-friendly interface for defining custom sorting options will simplify the process for shop owners and reduce the technical burden on developers.\n* Allowing shop owners to define custom sorting options via the administration panel will increase flexibility and customization options for product listings and search result pages.\n* Defining a system default sorting option for product listings will ensure consistency across the platform and provide a standard way of sorting products.\n\n**Consequences:**\n\n* The implementation of this feature will require changes to the administration panel and the underlying code.\n* The feature will need to be thoroughly tested to ensure it is stable and functional.\n* The feature may require additional documentation to help shop owners understand how to use and manage custom sorting options.\n\n**Status:** Proposed","GenTime":"2024-08-10 22:11:45"}
{"File Name":"platform\/2021-01-21-deprecation-strategy.md","Context":"## Context\\nDefine a strategy for deprecations.\\n","Decision":"### Dogma\\n* Don't do changes without feature-flags (only exception are bugfixes)\\n* Don't break things without an alternative\\n* Don't break things in a minor release\\n* Annotate upcoming breaks as soon as possible\\n* Test all new implementations and changes\\n* Be expressive and very verbose on instructions in your inline feature flag comments\\n* There is a world outside with developers that use our public code\\n### Synopsys\\nAs we decided to work in the trunk based development from now on, there are different kinds of cases we need to consider while implementing changes to not cause any breaks while developing for future features.\\nThe main difference we have to take in account, is if we break currently behaviour with our changes or not.\\nFor this difference we have 4 different cases:\\n* Minor Changes which don't cause any breaks or deprecations\\n* Minor Changes which cause deprecations\\n* Minor Changes as part of a major feature which don't cause any breaks\\n* Major changes which cause breaks\\nFor a quick overview this is how you have to deal with the different cases.\\nConcrete Examples and further explanation follow below.\\n#### Only Minor Changes (no breaks)\\nFeature and changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etc\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Only Minor Changes (with deprecating code)\\nFeature and Changes tend to be released in a minor release and are developed in a backward compatible manner, but deprecate old code. For example a class is replaced by a new one.\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Major Changes (Breaks)\\nParts of a major feature or refactoring which breaks current behaviour. Removal of classes, methods or properties, change of signatures, business logic changes...\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* only difference between the case above is, that you have to take care about the fact, that the whole old behaviour needs to be fully functional until the next major.\\n* Write specific tests for the major flag which tests the new behaviour.\\n* Detailed description here [Detailed Rules](DetailedRules)\\n","tokens":12,"id":4497,"Prediction":"## Decision\n\n**Title:** Deprecation Strategy\n\n**Context:** As our software evolves, we need to define a strategy for deprecating features, APIs, and other components to ensure a smooth transition for users and maintain a healthy codebase.\n\n**Decision:** We will adopt a gradual deprecation approach, which involves the following steps:\n\n1. **Announce deprecation**: We will announce the deprecation of a feature, API, or component at least 6 months before removing it. This will give users and stakeholders time to adapt and plan for the change.\n2. **Provide alternatives**: We will provide alternatives or suggest workarounds for deprecated features, APIs, or components. This will help users to continue using the software without disruption.\n3. **Phased removal**: We will remove deprecated features, APIs, or components in phases, starting with the least used or most critical ones. This will allow us to monitor the impact and make adjustments as needed.\n4. **Error handling**: We will implement error handling mechanisms to catch and handle attempts to use deprecated features, APIs, or components. This will provide a gentle transition for users and help us identify any issues.\n5. **Documentation updates**: We will update our documentation to reflect deprecations, including removals, and provide clear guidance on the alternatives and workarounds.\n6. **Communication**: We will communicate the deprecation plan to our users, stakeholders, and team members through regular updates, blog posts, and release notes.\n\n**Rationale:** Our goal is to minimize disruption to users while ensuring the long-term maintainability and sustainability of our software. By announcing deprecations in advance, providing alternatives, and phasing out removals, we can ensure a smooth transition and reduce the risk of breaking changes.\n\n**Consequences:**\n\n* Users will have time to adapt to changes and plan for the future.\n* Our codebase will be cleaner and more maintainable.\n* We will reduce the risk of breaking changes and minimize the impact on users.\n* We will improve our communication and documentation to ensure transparency and clarity.\n\n**Alternatives Considered:**\n\n* Abrupt removal of deprecated features, APIs, or components, which could cause disruption and breakages.\n* Not announcing deprecations, which could lead to unexpected changes and user frustration.\n\n**Status:** Approved\n\n**Implementation:** The deprecation strategy will be implemented in the next software release, with the first set of deprecated features, APIs, or components announced and removed in the subsequent release.","GenTime":"2024-08-10 22:13:14"}
{"File Name":"platform\/2021-09-22-refactor-theme-inheritance.md","Context":"## Context\\nCurrently, the themes can only inherit config fields from the default Storefront theme.\\nAlso, this inheritance is only a snapshot by activation time of the theme - The configs are copied to the new theme and changes to the default theme config will not appear in the new theme without a re-activation.\\nThe different possibilities to inherit different parts of a theme, like scripts, templates and config, can also cause problems on later updates.\\n","Decision":"To take this points into account, we have decided to add a new inheritance key for the `configFields` in the `theme.json` which allow a theme to inherit its config from other themes in a given order:\\n```json\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n```\\n<details>\\n<summary>Complete theme.json with part inheritances<\/summary>\\n```json\\n{\\n\"name\": \"MyDevelopmentTheme\",\\n\"author\": \"Shopware AG\",\\n\"views\": [\\n\"@Storefront\",\\n\"@Plugins\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"style\": [\\n\"app\/storefront\/src\/scss\/overrides.scss\",\\n\"@Storefront\",\\n\"app\/storefront\/src\/scss\/base.scss\"\\n],\\n\"script\": [\\n\"@Storefront\",\\n\"app\/storefront\/dist\/storefront\/js\/my-development-theme.js\"\\n],\\n\"asset\": [\\n\"@Storefront\",\\n\"app\/storefront\/src\/assets\"\\n],\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"config\": {\\n\"blocks\": {\\n\"exampleBlock\": {\\n\"label\": {\\n\"en-GB\": \"Example block\",\\n\"de-DE\": \"Beispiel Block\"\\n}\\n}\\n},\\n\"sections\": {\\n\"exampleSection\": {\\n\"label\": {\\n\"en-GB\": \"Example section\",\\n\"de-DE\": \"Beispiel Sektion\"\\n}\\n}\\n},\\n\"fields\": {\\n\"my-single-test-select-field\": {\\n\"editable\": false\\n},\\n\"my-single-select-field\": {\\n\"label\": {\\n\"en-GB\": \"Select a font size\",\\n\"de-DE\": \"W\u00e4hle ein Schriftgr\u00f6\u00dfe\"\\n},\\n\"type\": \"text\",\\n\"value\": \"24\",\\n\"custom\": {\\n\"componentName\": \"sw-single-select\",\\n\"options\": [\\n{\\n\"value\": \"16\",\\n\"label\": {\\n\"en-GB\": \"16px\",\\n\"de-DE\": \"16px\"\\n}\\n},\\n{\\n\"value\": \"20\",\\n\"label\": {\\n\"en-GB\": \"20px\",\\n\"de-DE\": \"20px\"\\n}\\n},\\n{\\n\"value\": \"24\",\\n\"label\": {\\n\"en-GB\": \"24px\",\\n\"de-DE\": \"24px\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n},\\n\"usps-positions\": {\\n\"label\":\\n{\\n\"en-GB\": \"Position\",\\n\"de-DE\": \"Position\"\\n},\\n\"scss\": true,\\n\"type\": \"text\",\\n\"value\": [\\n\"top\",\\n\"bottom\"\\n],\\n\"custom\": {\\n\"componentName\": \"sw-multi-select\",\\n\"options\": [\\n{\\n\"value\": \"bottom\",\\n\"label\":\\n{\\n\"en-GB\": \"bottom\",\\n\"de-DE\": \"unten\"\\n}\\n},\\n{\\n\"value\": \"top\",\\n\"label\":\\n{\\n\"en-GB\": \"top\",\\n\"de-DE\": \"oben\"\\n}\\n},\\n{\\n\"value\": \"middle\",\\n\"label\":\\n{\\n\"en-GB\": \"middle\",\\n\"de-DE\": \"mittel\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"tab\": \"usps\",\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n}\\n}\\n}\\n}\\n```\\n<\/details>\\n","tokens":90,"id":4498,"Prediction":"## Decision\n\n**Decision:** Introduce a more robust and dynamic theme inheritance mechanism that allows themes to inherit config fields from any parent theme, and also allows for real-time updates to inherited config fields.\n\n**Context:** As described in the context, the current theme inheritance mechanism has limitations, including only inheriting from the default Storefront theme, and not allowing for real-time updates to inherited config fields.\n\n**Problem:** The current mechanism is inflexible and can lead to problems when updating themes or config fields. It also limits the ability to create a theme hierarchy with multiple levels of inheritance.\n\n**Reasoning:** By introducing a more robust theme inheritance mechanism, we can:\n\n* Allow themes to inherit config fields from any parent theme, not just the default Storefront theme.\n* Enable real-time updates to inherited config fields, so that changes to the parent theme config are reflected in the child theme without requiring re-activation.\n* Simplify the process of creating a theme hierarchy with multiple levels of inheritance.\n\n**Decision Drivers:**\n\n* Flexibility and scalability: The new mechanism should allow for a more flexible and scalable theme hierarchy.\n* Real-time updates: The new mechanism should enable real-time updates to inherited config fields, to avoid the need for re-activation.\n* Maintainability: The new mechanism should be easy to maintain and update, with minimal impact on existing themes.\n\n**Consequences:**\n\n* Increased complexity: The new mechanism will introduce additional complexity to the theme inheritance system.\n* Potential for conflicts: The new mechanism may introduce conflicts between different themes or config fields.\n\n**Alternatives Considered:**\n\n* Continue with the current mechanism and live with its limitations.\n* Introduce a separate inheritance mechanism for each type of theme asset (e.g. scripts, templates, config).\n* Use a third-party library or framework to handle theme inheritance.\n\n**Next Steps:**\n\n* Design and implement the new theme inheritance mechanism.\n* Test and validate the new mechanism to ensure it meets the requirements and does not introduce conflicts.\n* Document the new mechanism and provide guidance on how to use it effectively.","GenTime":"2024-08-10 22:13:36"}
{"File Name":"platform\/2021-09-06-make-core-mail-templates-independent-from-storefront-urls.md","Context":"## Context\\nSome mail templates of the core component (Newsletter, Registration, Password Recovery, Order Status mails) depend on storefront Urls to be included in the mails.\\nThose Urls are not available when shopware is used in \"headless\" mode, without the storefront bundle being installed.\\nFor some mails (Newsletter subscription, Double Opt-In, Password recovery), the Url was made configurable over the system config and over the settings inside the administration.\\nThe default values for those Urls are the ones that the storefront bundle would use.\\nThis option does not really scale well as each Url that should be used, needs to be configurable in the administration and this can grow quickly out of hand.\\nAdditionally, it is not clear when and where those configs should be used to generate the absolute Urls, as with the BusinessEvent system and the upcoming FlowBuilder, the sending of mails is not necessarily triggered by the same entry point all the times, but different trigger can lead to sending the same mails.\\n","Decision":"There shouldn't be any links generated on PHP-side as that can be hard to override per sales-channel and can not easily be changed by apps, and links should be generated inside the mailTemplates with string concatenation instead of `raw_url`-twig functions, so the links can still be generated even if the route is not registered in the system.\\nTo make generation of urls inside the mail templated easier, we will add a `{{ domain }}` variable to the twig context, that contains the domain of the current salesChannelContext, of the order in question etc.\\nThe URLs we use in the core mail templates become part of the public API, and custom frontends should adhere to theme and provide routes under the same path, or create redirects so that the default URLs work for their frontend implementation.\\nThe default urls are:\\n```\\n\/account\/order\/{deepLinkCode} -> opens the order details of the given order\\n\/account\/recover\/password?hash={recoverHash} -> start password recovery process\\n\/newsletter-subscribe?em={emailHash}&hash={subscribeHash} -> Subscribe email with given hash to the newsletter (for douple-opt in)\\n\/registration\/confirm?em={emailHash}&hash={subscribeHash} -> Confirm registration for user eith the given mail hash (for douple-opt in)\\n```\\nIf the custom frontends can't or don't want to use our default URLs they can use the possibility to override the existing mail templates to generate custom URLs.\\nWe will deprecate the usage of the system-config configuration values and the events thrown when the links are generated on PHP-side and remove those in the next major version.\\nTo be forward compatible we will already pass the necessary data needed for generating the links into the templates, so the urls can be already generated inside the mail templates.\\nThird party clients (like the PWA) should either adhere to our default URLs or add additional mail templates, that generate the correct urls for their client.\\nIn addition to that the third party client could extend the core mail template, rather than providing a new one, and then deciding in an `IF\/ELSE` what url needs to be generated based on the salesChannel or domain.\\n","tokens":204,"id":4500,"Prediction":"## Decision\n\nTo address the issue of mail templates depending on storefront URLs in a scalable and maintainable way, we will introduce a new mechanism for generating absolute URLs in the core component. This mechanism will be based on a new interface `UrlGenerator` that will be responsible for generating absolute URLs from a given relative URL.\n\nThe `UrlGenerator` interface will have a method `generateAbsoluteUrl` that takes a relative URL as an argument and returns an absolute URL. This method will be responsible for determining the correct absolute URL based on the current context, such as whether the shop is running in \"headless\" mode or not.\n\nWe will then inject the `UrlGenerator` interface into the mail templates and use it to generate the absolute URLs instead of relying on hardcoded storefront URLs. This will allow us to decouple the mail templates from the storefront URLs and make them more flexible and scalable.\n\nTo make it easy to configure the `UrlGenerator` instance, we will provide a default implementation that uses the storefront URLs when available and falls back to a configurable system setting when not. This will allow administrators to configure the URLs for specific mail templates without having to create separate configurations for each URL.\n\nThe `UrlGenerator` interface will be implemented by the `UrlGeneratorFactory` class, which will be responsible for creating and configuring the `UrlGenerator` instances. The `UrlGeneratorFactory` will use the system settings and the current context to determine which implementation to use.\n\nBy introducing the `UrlGenerator` interface and the `UrlGeneratorFactory` class, we will be able to decouple the mail templates from the storefront URLs and make them more flexible and scalable. This will also make it easier to add new mail templates and configurations without having to modify the existing code.","GenTime":"2024-08-10 22:14:14"}
{"File Name":"platform\/2021-12-07-admin-extension-api-standards.md","Context":"## Context\\nWe need to provide ways for extension developers to add custom components and views to different places in the administration. Multiple solutions where discussed and tested, this ADR contains a summary of the final solution.\\n","Decision":"### Word definitions\\nFor a better understanding of the following text it is good to have a definition for specific words:\\n#### Location\\nExtensions can render custom views with the Admin-Extension-API via iFrames. To support multiple views in different places every \"location\" of the iFrame gets a unique ID. These can be defined by the app\/plugin developer itself.\\n*Example:*\\nAn app wants to render a custom iFrame in a card on the dashboard. The \"location\" of the iFrame has then a specific \"locationID\" like `sw-dashboard-example-app-dashboard-card`. The app can also render another iFrames which also get \"locationIDs\". In our example it is a iFrame in a custom modal: `example-app-example-modal-content`.\\nThe app want to render different views depending on the \"location\" of the iFrame. So the app developer can render the correct view depending on the \"locationID\":\\n```js\\nif (sw.location.is('sw-dashboard-example-app-dashboard-card')) {\\nrenderDashboardCard();\\n}\\nif (sw.location.is('example-app-example-modal-content')) {\\nrenderModalContent();\\n}\\n```\\n#### PositionID (PositionIdentifier)\\nDevelopers can extend existing areas or create new areas in the administration with the Admin-Extension-API. To identify the positions which the developer want to extend we need a unique ID for every position. We call these IDs \"positionID\".\\n*Example:*\\nAn app wants to add a new tab item to a tab-bar. In the administration are many tab-bars available. So the developer needs to choose the correct \"positionID\" to determine which tab-bar should be extended. In this example the developer adds a new tab item to the tab-bar in the product detail page.\\n```js\\nsw.ui.tabs('sw-product-detail').addTabItem({ ... })\\n```\\n### Solution:\\nWe use the concept of component sections for providing injection points for extension components.\\n#### Component Sections\\nIn most cases developers will directly use the extension capabilities of the UI components (e.g. adding tab items, adding button to grid, ...). This will cover most needs of many extensions.\\nTo give them more flexibility we introduce a feature named \"Component Sections\". These are sections where any extension developer can inject components. These components are prebuilt and they can also contain custom render views with iFrames. The developer needs to use the feature and choose the matching positionID for the component position.\\n```js\\n\/\/ Adding a card before the manufacturer card with custom fields entries.\\nsw.ui.componentSection('sw-manufacturer-card-custom-fields__before').add({\\n\/\/ The Extension-API provides different components out of the box\\ncomponent: 'card',\\n\/\/ Props are depending on the type of component\\nprops: {\\ntitle: 'This is the title',\\nsubtitle: 'I am the subtitle',\\n\/\/ Some components can render a custom view. In this case the extension can render custom content in the card.\\nlocationId: 'example-app-card-before-manufactuer-custom-fields-card'\\n}\\n})\\n```\\n#### Vue Devtools Plugin for finding the PositionIDs\\nIt is impossible to create a list of all potential position IDs. And they would be hard to manage. To solve this problem we are writing a custom plugin for the Vue Devtools. This plugin will be available for Vue Devtools 6+. It makes identifying the position IDs very easy.\\nJust open the plugin in the Devtools (It is available directly when you open the Administration). Then you can see all positions at the current administration view which are available for extending. If you click at one position ID you get more information about it. Like the property in the Meteor-Extension-SDK so that you directly know what functionality this position has.\\nIn summary: the Devtool plugin provides a visual way to see which parts can be extended and what are the positionIDs for the extension position.\\n","tokens":43,"id":4507,"Prediction":"## Decision\n\n### Decision: Implement a Plugin Architecture with a Custom Plugin Manager\n\n### Context\n\nWe need to provide ways for extension developers to add custom components and views to different places in the administration. Multiple solutions where discussed and tested, this ADR contains a summary of the final solution.\n\n### Problem\n\nThe current system does not provide a mechanism for extension developers to easily add custom components and views to the administration interface. This limits the flexibility and extensibility of the system, making it difficult to adapt to changing requirements and user needs.\n\n### Decision Criteria\n\n* Flexibility: The solution should allow extension developers to easily add custom components and views to different places in the administration.\n* Scalability: The solution should be able to handle a large number of custom components and views without affecting the performance of the system.\n* Maintainability: The solution should be easy to maintain and update, with minimal impact on the rest of the system.\n* Security: The solution should provide a secure way to load and execute custom code.\n\n### Solution\n\nWe will implement a plugin architecture with a custom plugin manager. The plugin manager will provide a standardized way for extension developers to register their custom components and views, and will handle the loading and execution of these components and views.\n\nThe plugin architecture will consist of the following components:\n\n* Plugins: These are the custom components and views developed by extension developers. Each plugin will be a separate module that can be loaded and executed by the plugin manager.\n* Plugin Manager: This is the component that manages the loading and execution of plugins. It will provide a standardized API for plugins to register themselves and for the system to load and execute plugins.\n* Plugin Registry: This is a database or configuration file that stores information about the available plugins, including their name, description, and version.\n\nThe plugin manager will handle the following tasks:\n\n* Loading plugins: The plugin manager will load plugins from the plugin registry and initialize them.\n* Executing plugins: The plugin manager will execute plugins and provide them with access to the system's APIs and services.\n* Managing plugin dependencies: The plugin manager will manage the dependencies between plugins and ensure that they are loaded and executed in the correct order.\n* Providing a standardized API: The plugin manager will provide a standardized API for plugins to register themselves and for the system to load and execute plugins.\n\n### Rationale\n\nThe plugin architecture provides a flexible and scalable solution for adding custom components and views to the administration interface. It allows extension developers to easily add new functionality to the system without affecting the core codebase. The custom plugin manager provides a standardized way for plugins to register themselves and for the system to load and execute plugins, making it easy to maintain and update the system.\n\n### Consequences\n\n* The plugin architecture will provide a flexible and scalable way for extension developers to add custom components and views to the administration interface.\n* The custom plugin manager will provide a standardized way for plugins to register themselves and for the system to load and execute plugins.\n* The system will be more maintainable and easier to update, with minimal impact on the rest of the system.\n* The system will be more secure, with a standardized way to load and execute custom code.\n\n### Next Steps\n\n* Implement the plugin architecture and custom plugin manager.\n* Develop a set of APIs and services that plugins can use to interact with the system.\n* Test the plugin architecture and custom plugin manager to ensure that they are working correctly.\n* Provide documentation and support for extension developers to help them develop and deploy plugins.","GenTime":"2024-08-10 22:17:10"}
{"File Name":"platform\/2022-01-05-add-feature-flag-support-for-storefront-scss.md","Context":"## Context\\n* There is no possibility to check for feature flags inside the Storefront SCSS.\\n* Altering the SCSS depending on a feature flag will require workarounds like e.g. \"making up\" and additional class in the template and use the feature toggle in twig instead.\\n* The SCSS of a selector which is hidden behind a feature flag will still be in the compiled CSS.\\n* It is not easily possible to make breaking changes inside SCSS functions, mixins or variables backward-compatible with the use of feature flags.\\n","Decision":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.\\n* The feature configuration from `Feature::getAll()` is converted to a SCSS map inside `\\Shopware\\Storefront\\Theme\\ThemeCompiler::getFeatureConfigScssMap`.\\n* This SCSS map is always added to the SCSS string which gets processed by `\\Shopware\\Storefront\\Theme\\ThemeCompiler::compileTheme`.\\n* For webpack hot-proxy the `var\/config_js_features.json` is used instead.\\n* The SCSS map looks like this: `$sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);`\\n* See https:\/\/sass-lang.com\/documentation\/values\/maps\\n* A globally available function `feature()` is used to read inside the SCSS map if a desired feature is active.\\nExample:\\n```scss\\nbody {\\n@if feature('FEATURE_NEXT_1') {\\nbackground-color: #ff0000;\\n} @else {\\nbackground-color: #ffcc00;\\n}\\n}\\n```\\n","tokens":112,"id":4508,"Prediction":"## Decision\n\n**Feature Flag Implementation in SCSS**\n\n**Context**: There is no possibility to check for feature flags inside the Storefront SCSS, and altering the SCSS depending on a feature flag requires workarounds, such as creating an additional class in the template and using the feature toggle in Twig instead.\n\n**Decision**: We will not implement feature flags directly in SCSS. Instead, we will use Twig to conditionally apply CSS classes based on feature flags. This approach will allow us to keep the SCSS code clean and avoid the issues mentioned above, such as compiled CSS still being generated for hidden feature flag selectors and the difficulty in making breaking changes backward-compatible with feature flags.\n\n**Rationale**: By separating the feature flag logic from the SCSS code, we can maintain a clean and modular architecture for our frontend code. This approach also simplifies the process of implementing and managing feature flags, as we can focus on the Twig templates and avoid the complexities of SCSS.\n\n**Consequences**:\n\n* The SCSS code will remain unaffected by feature flags, making it easier to maintain and debug.\n* The Twig templates will need to be updated to include conditional logic for feature flags, which may require additional development time.\n* The compiled CSS will no longer include selectors for hidden feature flag selectors, reducing the overall size of the CSS file.\n\n**Status**: Approved","GenTime":"2024-08-10 22:17:25"}
{"File Name":"platform\/2020-08-28-import-acl-privileges-from-other-roles.md","Context":"## Context\\nSome modules have components which require many acl privileges. Examples\\nare the rule builder or the media manager. Therefore, you need all privileges\\nin each module which have these components. Also you do not want to add the\\nmodule to the dependency section because then the user has full access to module\\nin the administration.\\n","Decision":"To avoid duplication of these privileges we use a helper function. These\\nfunction returns all privileges from the other module dynamically. You can\\nuse it directly in the privileges:\\n```js\\nShopware.Service('privileges')\\n.addPrivilegeMappingEntry({\\ncategory: 'permissions',\\nparent: null,\\nkey: 'promotion',\\nroles: {\\nviewer: {\\nprivileges: ['promotion:read',],\\ndependencies: []\\n},\\neditor: {\\nprivileges: [\\n'promotion:update',\\nShopware.Service('privileges').getPrivileges('rule.creator')\\n],\\ndependencies: [\\n'promotion.viewer'\\n]\\n}\\n}\\n});\\n```\\n","tokens":68,"id":4525,"Prediction":"## Decision\n\nTo address the issue of modules requiring many ACL privileges, we will implement a separate permission system for components that require elevated privileges. This will allow us to grant specific permissions to users without granting them full access to the module.\n\n## Context\n\nAs described, some modules have components that require many ACL privileges, such as the rule builder or media manager. This means that if a user needs to access these components, they would need to have all the privileges in each module that contains these components. Additionally, adding the module to the dependency section would grant the user full access to the module, which is not desirable.\n\n## Decision Rationale\n\nImplementing a separate permission system for components with elevated privileges will provide a more fine-grained control over what users can do. This will allow us to:\n\n* Grant specific permissions to users without granting them full access to the module\n* Reduce the number of privileges required for users to access certain components\n* Improve security by limiting the amount of sensitive data that can be accessed\n\n## Pros and Cons\n\nPros:\n\n* Improved security by limiting access to sensitive data\n* Reduced number of privileges required for users to access certain components\n* More fine-grained control over what users can do\n\nCons:\n\n* Additional complexity in the permission system\n* Potential for increased maintenance and debugging efforts\n\n## Alternatives Considered\n\n* Adding the module to the dependency section and granting full access to the module\n* Creating a separate module for components with elevated privileges\n* Using a different permission system altogether\n\n## Conclusion\n\nImplementing a separate permission system for components with elevated privileges is the best solution to address the issue of modules requiring many ACL privileges. This will provide a more secure and fine-grained control over what users can do, while also reducing the number of privileges required for users to access certain components.","GenTime":"2024-08-10 22:22:47"}
{"File Name":"tove\/adr-03.md","Context":"## Context\\nWhen a transcription is approved, a set of flat files containing the transcription data will be saved to Azure. Users will have the option to download a zip file containing their requested subject, group, workflow, or project. Depending on the speed at which we are able to zip the necessary files, we will either trigger a direct download, or provide a link to the location of the zip file to the user.\\nThe goal is to investigate Azure\u2019s storage options (specifically Blob Storage and File Services) and decide which tool is best suited for our needs.\\n### Factors to consider:\\n* How easy is it to share a file to the end user? What is the process for this?\\n* Ease of use, how complicated is it to set up, maintain, edit\\n* access permission features\\n* Speed of accessing and iterating through files (e.g. getting all files in a given directory)\\n### Terminology:\\n**Blob:** acronym for \u201cBinary Large Object\u201d\\n**Container:** synonym for \u201dS3 Bucket\u201d\\n**Shared Access Signature:** similar functionality as \u201cS3 Presigned URLs\u201d\\n","Decision":"We don't appear to have any need for most of the additional functionality that comes with File Service, which makes me reluctant to want to use it. In addition, the number of articles and resources available on communicating with Blob Storage to set up file zipping is much greater than what's available for File Service. My initial understanding of Blob Storage led me to believe that permissions could only be set at the container level, but this turned out to be wrong. With the ability to set blob-specific permissions, we will be able to use a single container to store the transcription-specific files, and the user-requested zip files.\\nUltimately, my choice is to go with Blob Storage: the more basic, simple storage tool that gives us what we need and nothing more. That being said, I'd still like to keep the option of using Azure File Service on the table, in case it turns out that we *would* benefit from the additional functionality that it offers.\\nAs for what type of blob we will use, my choice would be to store each data file in its own block blob. If we were to choose to store multiple files within a single blob (and have each file be associated with a block ID on that blob), we would lose the ability to name each individual file. Hypothetically, it would be possible to create a database table with columns \u201cblock ID\u201d and \u201cname\u201d, to emulate a naming functionality, but this seems far more complicated than its worth. In addition, the [azure-storage-blob](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob) gem gives us a simple interface for working with block blobs and saves us the trouble of having to write HTTP requests ourselves.\\nFinal questions:\\n1. Q: Blob Storage doesn't have any concrete hierarchy beyond Storage Account\/Blob Container - within a container, directories are virtual, demarcated by prefixes in the file name. Will this end up being problematic for us? Will it complicate file retrieval?\\nA: Retrieving files from a file system with virtual directories shouldn't be any different than retrieving files from a normal file system. As long as blob prefixes are constructed in a way that reflects the organizational system used within the application\/database, there should be no trouble. File retrieval may be helped by append blobs - final decision on blob type is still TBD.\\n2. Q: Would there be any benefit to caching files on on-premises file servers? If this sounds like something we'd like to employ, it would be worth reconsidering Azure File Service.\\nA: This doesn't appear to be something we will need.\\n### Links and Articles:\\n1. [Microsoft: Deciding when to use Azure Blobs, Azure Files, or Azure Disks](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-decide-blobs-files-disks)\\n2. [Azure Files FAQ](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/files\/storage-files-faq) (see \u2018Why would I use an Azure file share versus Azure Blob Storage for my data?\u2019)\\n3. [Stack Overflow: Blob Storage vs File Service](https:\/\/stackoverflow.com\/questions\/24880430\/azure-blob-storage-vs-file-service)\\n4. [Microsoft: Introducing Azure File Service](https:\/\/blogs.msdn.microsoft.com\/windowsazurestorage\/2014\/05\/12\/introducing-microsoft-azure-file-service\/) (scroll to When to use Azure Files vs Azure Blobs vs Azure Disks)\\n5. [Microsoft: Azure Storage scalability and performance targets for storage accounts](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-scalability-targets)\\n6. [Azure Blob Overview](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-overview)\\n7. [Azure Blob Introduction](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-introduction)\\n8. [How to mount Blob storage as a file system with blobfuse](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-how-to-mount-container-linux)\\n9. [Block blobs, append blobs, and page blobs](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/understanding-block-blobs--append-blobs--and-page-blobs)\\n10. [Azure Blob Storage gem](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob)\\n","tokens":231,"id":4533,"Prediction":"## Decision\n\nAfter considering the factors and terminology provided, I recommend using Azure Blob Storage to store and share the transcription data files.\n\n## Rationale\n\nAzure Blob Storage is well-suited for storing and sharing large amounts of binary data, such as the transcription data files. Here are some key benefits:\n\n* **Easy sharing**: Azure Blob Storage provides a simple way to share files with users. We can generate a shared access signature (SAS) for each file, which allows users to access the file without having to log in to the Azure portal. This is similar to S3 Presigned URLs.\n* **Ease of use**: Azure Blob Storage is easy to set up and maintain. We can create a container to store the files and use the Azure portal or Azure CLI to upload and manage the files.\n* **Access permission features**: Azure Blob Storage provides robust access permission features, including support for shared access signatures, containers, and blobs. We can control access to files at the container, blob, or file level.\n* **Speed of accessing and iterating through files**: Azure Blob Storage provides fast and efficient access to files. We can use the Azure Blob Storage API or Azure Data Lake Storage Gen2 to iterate through files in a container and retrieve specific files or directories.\n\nIn contrast, Azure File Services is designed for file shares and may not be the best fit for storing and sharing large amounts of binary data. While it provides some similar features to Azure Blob Storage, it is designed for file shares and may not provide the same level of scalability and performance.\n\nOverall, Azure Blob Storage provides a simple, scalable, and secure way to store and share transcription data files, making it the best choice for our needs.","GenTime":"2024-08-10 22:25:37"}
{"File Name":"ockam\/0007-rust-error-handling.md","Context":"## Context\\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\\nThere are multiple requirements to error handling:\\n- agile enough to be used in different cases\\n- portable to work in different environments with different constraints\\n- convenient for both library developer and library user\\n","Decision":"In search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\\n","tokens":91,"id":4536,"Prediction":"## Decision\n\n**Error Handling Strategy for Ockam Rust Library**\n\n## Context\n\nAs mentioned, error handling is a crucial aspect of the Ockam Rust library, and its third-party crates, such as transport and vault implementations. The requirements for error handling are:\n\n* Agile enough to be used in different cases\n* Portable to work in different environments with different constraints\n* Convenient for both library developers and users\n\n## Decision\n\nAfter careful consideration, we have decided to adopt a combination of the `Result` type and the `?` operator from the `std::fmt` module to handle errors in the Ockam Rust library. This approach provides a flexible and convenient way to handle errors, allowing for easy propagation and handling of errors throughout the codebase.\n\n**Rationale**\n\n1. **Agility**: The `Result` type and `?` operator provide a flexible way to handle errors, allowing for easy adaptation to different error handling scenarios.\n2. **Portability**: The `Result` type and `?` operator are part of the Rust standard library, ensuring that the error handling strategy is portable across different environments and constraints.\n3. **Convenience**: The `Result` type and `?` operator simplify error handling, making it easier for both library developers and users to handle errors in a consistent and readable way.\n\n**Implementation**\n\nWe will use the `Result` type to represent the outcome of an operation, where `Ok` represents a successful operation and `Err` represents an error. The `?` operator will be used to propagate errors up the call stack, allowing for easy handling of errors at the top level.\n\n**Alternatives Considered**\n\n1. **Error types**: We considered defining custom error types for each error scenario, but this approach would lead to a proliferation of error types, making the code more complex and harder to maintain.\n2. **Error handling frameworks**: We considered using error handling frameworks such as `error-chain` or `failure`, but these frameworks add additional dependencies and complexity to the codebase.\n\n**Conclusion**\n\nThe combination of the `Result` type and the `?` operator provides a flexible, portable, and convenient way to handle errors in the Ockam Rust library, meeting the requirements outlined in the context. This decision ensures that error handling is consistent and easy to use throughout the codebase.","GenTime":"2024-08-10 22:26:25"}
{"File Name":"ockam\/0005-routing-data-formats.md","Context":"## Context\\nWe need a standard format for messages to be exchanged by the routing protocol.\\nThis format would be used by routers on different implementations.\\n","Decision":"We use the following formats:\\nFor a message:\\n```\\n{\\nonward_route: Route,\\nreturn_route: Route,\\npayload: Any\\n}\\n```\\nWhere\\n`Route` - an ordered list of addresses.\\nFor an address:\\n```\\n{\\ntype: Integer,\\ndata: Any\\n}\\n```\\n","tokens":31,"id":4538,"Prediction":"## Decision\n\n**Standard Message Format for Routing Protocol**\n\n## Context\n\nAs we need a standard format for messages to be exchanged by the routing protocol, allowing routers on different implementations to communicate effectively.\n\n## Problem\n\nWithout a standardized message format, routers from different implementations may use different formats, leading to compatibility issues and difficulties in integrating different systems.\n\n## Decision\n\nWe will adopt the Open Routing Protocol Message Format (ORP-MF) as the standard format for messages exchanged by the routing protocol. This format is widely accepted and used in the industry, ensuring compatibility and ease of integration with other systems.\n\n## Rationale\n\n1. **Interoperability**: ORP-MF is a widely accepted and standardized format, ensuring that routers from different implementations can communicate effectively.\n2. **Simplification**: Using a standardized format reduces the complexity of implementing and maintaining routing protocols, as developers do not need to worry about format compatibility.\n3. **Flexibility**: ORP-MF is designed to be flexible, allowing for easy extension and modification to accommodate new features and requirements.\n\n## Consequences\n\n1. **Implementation complexity**: Developing routing protocol implementations that conform to ORP-MF may require additional effort and resources.\n2. **Limited flexibility**: While ORP-MF is designed to be flexible, it may not accommodate all specific requirements or customizations.\n\n## Alternatives Considered\n\n1. **Custom message format**: Developing a custom message format for our specific use case. However, this would require significant development and maintenance efforts, and may lead to compatibility issues with other systems.\n2. **Proprietary message format**: Using a proprietary message format specific to our implementation. However, this would limit interoperability with other systems and may require additional development and maintenance efforts.\n\n## Decision History\n\nThis decision was made after considering the requirements and constraints of the routing protocol, as well as the need for interoperability and simplicity. The decision was reviewed and approved by the architecture team.","GenTime":"2024-08-10 22:27:03"}
{"File Name":"ockam\/0001-record-architectural-decisions.md","Context":"## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that\\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\\ntechniques.\\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\\n","tokens":21,"id":4539,"Prediction":"Here is an Architectural Decision Record (ADR) for the context you provided:\n\n**Decision**\n\nWe will use a documentation-driven approach to record and track architectural decisions made during the development of Ockam.\n\n**Context**\n\nAs we develop Ockam, we need to record the architectural decisions that we make to ensure that they are well-documented, easily accessible, and can be reviewed and revisited as needed. This will help us to maintain a consistent and well-designed architecture, reduce the risk of duplicated effort, and provide a clear understanding of the reasoning behind our design choices.\n\n**Problem**\n\nWithout a systematic approach to recording architectural decisions, we risk losing track of important design choices and decisions, which can lead to confusion, duplicated effort, and a lack of consistency in our architecture.\n\n**Decision**\n\nTo address this problem, we will use a documentation-driven approach to record and track architectural decisions made during the development of Ockam. This will involve:\n\n* Creating a separate document or repository for recording architectural decisions\n* Using a standard template to document each decision, including the context, problem, decision, and rationale\n* Reviewing and revising decisions as needed to ensure they remain relevant and accurate\n* Making decisions publicly available and easily accessible to all team members\n\n**Consequences**\n\nBy using a documentation-driven approach to record and track architectural decisions, we will:\n\n* Ensure that important design choices and decisions are well-documented and easily accessible\n* Reduce the risk of duplicated effort and confusion\n* Provide a clear understanding of the reasoning behind our design choices\n* Improve communication and collaboration among team members\n* Facilitate the review and revision of decisions as needed\n\n**References**\n\n* [1] \"Architectural Decision Records\" by Michael Nygard\n* [2] \"Documenting Architectural Decisions\" by Martin Fowler","GenTime":"2024-08-10 22:27:22"}
{"File Name":"unfinished-design-system\/001-components-documentation.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Storybook](https:\/\/storybook.js.org\/). This tool helps us document our components and develop them since we can make a simple canvas to interact with it.\\nSome major companies with mature design systems use this tool, which could be a precious asset.\\n","tokens":53,"id":4544,"Prediction":"Here is the Architectural Decision Record:\n\n### Context\nThe user has provided the following context:\n\nWe are building a new e-commerce platform and need to decide on the architecture for handling user authentication and authorization. We want to ensure that the system is scalable, secure, and easy to maintain.\n\n### Decision\n## Implement a token-based authentication system using JSON Web Tokens (JWT)\n\nWe will use JSON Web Tokens to authenticate and authorize users. JWTs are a widely accepted and well-documented standard for token-based authentication. They provide a secure and efficient way to verify the identity of users and authorize access to protected resources.\n\n### Status\nAccepted\n\n### Consequences\n* Pros:\n\t+ Improved security: JWTs are digitally signed and can be verified to ensure the authenticity of the user.\n\t+ Scalability: Token-based authentication allows for easy horizontal scaling, as the authentication logic is decoupled from the application logic.\n\t+ Flexibility: JWTs can be used with various programming languages and frameworks.\n* Cons:\n\t+ Additional complexity: Implementing JWTs requires a good understanding of cryptography and token validation.\n\t+ Limited support for stateful sessions: JWTs are designed for stateless authentication, which may require additional logic for handling stateful sessions.\n\n### More reading\nFor more information on JSON Web Tokens, please refer to the official specification: [RFC 7519](https:\/\/tools.ietf.org\/html\/rfc7519).\n\n### Updates\n* 2023-02-20: Added a note on the limitations of JWTs for stateful sessions.\n* 2023-03-15: Updated the status to \"Accepted\" after a thorough review of the architecture.\n\nPlease let me know if this meets your requirements!","GenTime":"2024-08-10 22:28:37"}
{"File Name":"unfinished-design-system\/002-unit-testing-tools.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use Jest with Enzyme.\\nJest is an incredible unit testing tool for Javascript. It is easy to use, has a great syntax, and our team is familiar with it.\\nEnzyme, on the other hand, is a unit testing tool for React components. It has some libraries that help us debugging and testing those resources.\\nWe've decided on this stack because our team has already used it in the past, and it can improve our development speed.\\n","tokens":53,"id":4547,"Prediction":"Here is an Architectural Decision Record:\n\n### Context\nAs our application grows in complexity, we are experiencing issues with scalability and maintainability. Our current database schema is becoming increasingly difficult to manage, and we are seeing performance issues when handling large amounts of data. We need to make a decision on how to refactor our database schema to improve scalability and maintainability.\n\n### Decision\nWe will implement a micro-ORM (Object-Relational Mapping) system to abstract away the underlying database and provide a more maintainable and scalable architecture. Specifically, we will use Dapper, a popular.NET micro-ORM, to simplify our database interactions and reduce the complexity of our database schema.\n\n### Status\nApproved\n\n### Consequences\n* Pros:\n\t+ Improved scalability: By using a micro-ORM, we can reduce the overhead of database interactions and improve the performance of our application.\n\t+ Improved maintainability: The abstraction provided by the micro-ORM will make it easier to switch between different databases or make changes to our database schema without affecting the rest of the application.\n* Cons:\n\t+ Increased complexity: Introducing a new technology may add complexity to our application, particularly for developers who are not familiar with the micro-ORM.\n\t+ Additional dependencies: Our application will now depend on the micro-ORM, which may add to the overall size and complexity of our project.\n\n### More reading\n* Dapper documentation: <https:\/\/github.com\/StackExchange\/dapper-dot-net>\n* Micro-ORMs: A comparison of popular.NET micro-ORMs: <https:\/\/www.c-sharpcorner.com\/article\/micro-orms-a-comparison-of-popular-net-micro-orms\/>\n\n### Updates\n* 2023-02-20: Implemented Dapper in the application and started testing.\n* 2023-03-01: Completed testing and validated the improvements in scalability and maintainability.\n\nLet me know if you want me to add anything else!","GenTime":"2024-08-10 22:29:33"}
{"File Name":"unfinished-design-system\/006-yarn-workspaces.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Yarn Workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.\\n","tokens":63,"id":4549,"Prediction":"Here is an Architectural Decision Record:\n\n### Context\nThe company is planning to develop a new e-commerce platform, which will be a complex system handling thousands of transactions per day. The development team has been tasked with designing a scalable and maintainable architecture for the platform.\n\n### Decision\nAfter considering various options, the development team has decided to use a microservices architecture for the e-commerce platform. This decision was made based on the following factors:\n\n* Scalability: Microservices allow for easier scalability, as each service can be scaled independently.\n* Maintainability: With microservices, each service can be developed, tested, and deployed independently, making it easier to maintain and update the platform.\n* Flexibility: Microservices provide more flexibility, as each service can be developed in a different programming language and framework.\n\n### Status\nThe decision has been made and is currently being implemented.\n\n### Consequences\nThe main consequences of this decision are:\n\n* Increased complexity: Microservices add complexity to the system, as each service needs to be managed and maintained separately.\n* Higher costs: Developing and deploying multiple services can be more expensive than developing a monolithic application.\n* Potential for integration issues: With multiple services, there is a higher risk of integration issues and conflicts.\n\n### Experience Report\nThe development team has gained experience with microservices architecture in previous projects and is confident in its ability to implement and maintain the e-commerce platform.\n\n### More reading\nFor more information on microservices architecture, see [1] and [2].\n\n### Updates\nThe decision to use microservices architecture has been updated to include the use of a service registry and API gateway to manage service discovery and communication.\n\nPlease let me know if you want me to add anything else.","GenTime":"2024-08-10 22:30:08"}
{"File Name":"unfinished-design-system\/004-tokenization-and-static-assets.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Style Dictionary](https:\/\/amzn.github.io\/style-dictionary\/#\/) as our framework to handle tokens. It is easy to use since we can define our tickets in standard JSON but empowering them with string interpolation, variables, and other features.\\nFor our static assets, we're going to host them at [AWS S3](https:\/\/aws.amazon.com\/s3\/), defining the proper CORS rules, and refer the location of those as tokens for our applications to use.\\nInstead of increasing the size of our Javascript bundle with static assets, we prefer to keep it simple and light by hosting those in an S3 bucket and asking for the application to download it.\\n","tokens":53,"id":4550,"Prediction":"Here is an Architectural Decision Record for a software:\n\n### Context\nThe company's e-commerce platform is experiencing a significant increase in traffic and sales, leading to performance issues and slow page loads. The development team has identified that the current database schema and query optimization are major contributors to these issues. The team needs to decide on a new database schema and query optimization strategy to improve the performance of the e-commerce platform.\n\n### Decision\nAfter conducting a thorough analysis and weighing the pros and cons, the development team has decided to adopt a hybrid database schema that combines relational and NoSQL databases. The relational database will be used for storing structured data such as product information, customer information, and order history, while the NoSQL database will be used for storing semi-structured data such as product reviews, ratings, and customer feedback. Additionally, the team will implement query optimization techniques such as indexing, caching, and query parallelization to improve query performance.\n\n### Status\nThe decision has been made and implemented in the development environment. The team is currently testing and refining the new database schema and query optimization strategy to ensure that it meets the performance requirements of the e-commerce platform.\n\n### Consequences\nThe consequences of this decision are:\n\n* Improved performance: The hybrid database schema and query optimization strategy will improve the performance of the e-commerce platform, reducing page load times and improving the overall user experience.\n* Increased scalability: The NoSQL database will provide increased scalability and flexibility, allowing the platform to handle a large volume of data and traffic.\n* Simplified data modeling: The hybrid database schema will simplify data modeling and reduce the complexity of data relationships, making it easier to maintain and update the database.\n\n### More reading\nFor more information on hybrid database schema and query optimization techniques, please refer to the following resources:\n\n* \"Hybrid Database Schema: A Guide to Combining Relational and NoSQL Databases\" by [Author]\n* \"Query Optimization Techniques for Improving Database Performance\" by [Author]\n\n### Updates\nThis decision record will be updated as necessary to reflect changes to the decision, status, or consequences.","GenTime":"2024-08-10 22:30:31"}
{"File Name":"scholarsphere\/0003-no-derivatives.md","Context":"## Context\\nWhen viewing different file types over the web, large video files or other non-standard files, may not display well in\\nthe browser, so the common solution is to create derivatives for a better user experience. These also include thumbnails\\nfor a sample picture of what the resource looks like.\\nManaging and creating derivatives can be hard and requires careful architectural considerations.\\nHowever, the modern web has adapted to larger files, and generally handles a wider array of file types than it did in\\nthe past. Also, thumbnails only work for image and video formats. Generic icon-based images for any file type can be\\nmore informative.\\n","Decision":"Scholarsphere will not create any derivative files. Icons will be used to represent file types, and the original\\nuploaded file can be rendered when applicable.\\n","tokens":129,"id":4553,"Prediction":"## Decision\n\n**Title:** Handling File Derivatives and Thumbnails for Improved User Experience\n\n**Context:** When serving files of various types over the web, including large video files and non-standard file formats, creating derivatives and thumbnails can enhance the user experience. However, managing and creating these derivatives requires careful architectural consideration.\n\n**Problem:** Traditional approaches to handling file derivatives and thumbnails can be complex and resource-intensive, especially when dealing with large files and a wide range of file types.\n\n**Decision:** Given the advancements in modern web technology and the limitations of thumbnails, we will adopt a hybrid approach to handling file derivatives and thumbnails. For image and video formats, we will continue to generate thumbnails to provide a visual representation of the file. For other file types, we will use generic icon-based images to provide a more informative representation.\n\n**Rationale:**\n\n* Modern web technology has improved handling of larger files and a wider array of file types, reducing the need for complex derivative creation.\n* Thumbnails are limited to image and video formats, and may not be effective for other file types.\n* Generic icon-based images can provide a more informative representation of file types that do not have a visual representation.\n\n**Consequences:**\n\n* Improved user experience through enhanced file representation.\n* Reduced complexity in managing and creating derivatives.\n* Better support for a wide range of file types.\n\n**Alternatives Considered:**\n\n* Continuing to use traditional approaches to handling file derivatives and thumbnails, which may be complex and resource-intensive.\n* Abandoning the use of thumbnails and relying solely on generic icon-based images, which may not provide the same level of user experience.\n\n**Implementation:**\n\n* Implement a hybrid approach to handling file derivatives and thumbnails, using thumbnails for image and video formats and generic icon-based images for other file types.\n* Utilize modern web technology to improve handling of larger files and a wider array of file types.\n* Ensure that generic icon-based images are informative and provide a clear representation of the file type.","GenTime":"2024-08-10 22:31:30"}
{"File Name":"scholarsphere\/0009-acl-actor-permissions.md","Context":"## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n","Decision":"Access controls and depositor or proxy depositor rights are independent from one another.\\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\\n","tokens":79,"id":4554,"Prediction":"## Decision\n\nTo ensure accurate and efficient permission evaluation for works and collections, we will implement a hybrid permission system that combines both author-based and access control-based permissions.\n\n## Context\n\nAs described, permissions on works and collections can originate from two sources: the author of the resource (e.g., depositor or proxy depositor) and access controls (ACLs) that grant permissions based on user or group identity. When determining access to a given resource, both sources may need to be consulted.\n\n## Problem\n\nThe existing system lacks a clear and scalable approach to handling permissions from multiple sources. This can lead to inconsistent permission evaluation, errors, and performance issues.\n\n## Decision Criteria\n\n1. **Accuracy**: Ensure that permission evaluation accurately reflects the intended access controls.\n2. **Efficiency**: Minimize the performance impact of permission evaluation.\n3. **Scalability**: Support a large number of users, resources, and permission combinations.\n4. **Flexibility**: Allow for easy modification or addition of new permission sources or rules.\n\n## Decision\n\nTo address the problem, we will implement a hybrid permission system that:\n\n1. Stores author-based permissions in a separate database or data structure.\n2. Stores access control-based permissions in a separate database or data structure.\n3. Develops a permission evaluation algorithm that consults both sources and resolves any conflicts or inconsistencies.\n4. Implements caching mechanisms to improve performance and reduce the load on the permission evaluation algorithm.\n5. Provides a flexible framework for adding new permission sources or rules.\n\n## Consequences\n\n1. Improved accuracy and consistency in permission evaluation.\n2. Enhanced scalability and performance.\n3. Increased flexibility in managing permissions.\n4. Simplified maintenance and updates.\n\n## Rationale\n\nThe hybrid permission system allows us to leverage the strengths of both author-based and access control-based permissions while minimizing their weaknesses. By storing each type of permission in a separate database or data structure, we can efficiently retrieve and evaluate permissions. The permission evaluation algorithm will ensure that both sources are consulted and any conflicts or inconsistencies are resolved. This approach will provide a robust and scalable permission system that meets the needs of our users and stakeholders.","GenTime":"2024-08-10 22:31:53"}
{"File Name":"scholarsphere\/0002-define-use-of-decorators-and-presenters.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nWe need to distinguish between decorator and presenter objects in order to clarify which would be used in a given\\nsituation.\\n","Decision":"Decorators extend SimpleDelegator and will always delegate undefined methods to the delegated object.\\nPresenters take the form of \"plain ol' Ruby objects\" (POROs) and would generally not delegate methods to an object.\\nTheir usage is designed to be more flexible when the rendering of content isn't tied specifically to one object.\\n","tokens":46,"id":4556,"Prediction":"## Decision\n\n**Distinguish Between Decorator and Presenter Objects Using a Single Interface**\n\n## Context\n\nAs mentioned, we need to distinguish between decorator and presenter objects in order to clarify which would be used in a given situation. Both decorators and presenters are used to encapsulate business logic and provide a layer of abstraction, but they serve different purposes and have different responsibilities.\n\n## Problem\n\nWithout a clear distinction between decorator and presenter objects, it can be confusing to determine which object to use in a specific scenario, leading to potential misuse and errors.\n\n## Decision\n\nTo address this issue, we will introduce a single interface, `BusinessLogicComponent`, that both decorator and presenter objects will implement. This interface will define the common methods and properties that both types of objects share, such as `execute()` and `getOutput()`.\n\nBy implementing the `BusinessLogicComponent` interface, both decorator and presenter objects will be forced to provide a consistent API, making it easier to distinguish between them. This will also enable us to write generic code that can work with either type of object, without having to worry about the specific implementation details.\n\n## Consequences\n\n* Easier to distinguish between decorator and presenter objects\n* Consistent API for both types of objects\n* Ability to write generic code that can work with either type of object\n* Potential for increased complexity due to the introduction of an additional interface\n\n## Rationale\n\nBy using a single interface, we can create a clear and consistent way to distinguish between decorator and presenter objects, making it easier to understand and use them in our codebase. This decision also allows us to write more flexible and reusable code, which is essential for maintaining a scalable and maintainable software architecture.","GenTime":"2024-08-10 22:32:39"}
{"File Name":"scholarsphere\/0008-asset-pipeline.md","Context":"## Context\\nWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to the\\nfact that it was present as both a gem and an npm package.\\n","Decision":"We removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuery\\nvia yarn.\\n","tokens":41,"id":4557,"Prediction":"## Decision\n\n**Decision:** To separate the package management of Bootstrap and jQuery into separate gems and npm packages, respectively.\n\n**Context:** As mentioned, we were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap, which was present as both a gem and an npm package.\n\n**Problem:** The dual presence of Bootstrap as both a gem and an npm package made it difficult to update one without affecting the other, leading to a security vulnerability in jQuery.\n\n**Reasoning:** By separating the package management of Bootstrap and jQuery, we can update each package independently, ensuring that we can quickly address security vulnerabilities and keep our dependencies up-to-date.\n\n**Decision Drivers:**\n\n* Security: The ability to quickly address security vulnerabilities is critical to maintaining the integrity of our application.\n* Flexibility: Separating package management allows us to update each package independently, giving us more flexibility in our development process.\n* Maintainability: By separating the package management, we can more easily manage and maintain our dependencies, reducing the complexity of our build process.\n\n**Consequences:**\n\n* Increased complexity in our build process due to the need to manage multiple package managers.\n* Potential for version conflicts between Bootstrap and jQuery if not managed carefully.\n* Improved security and maintainability of our application.\n\n**Status:** Implemented\n\n**Links:** [Link to relevant code changes or documentation]","GenTime":"2024-08-10 22:32:53"}
{"File Name":"elm-spec\/003_loading_harness.md","Context":"## Context\\nRight now, in order to run an elm-spec program, we need to do a few things in a definite order.\\nAt some point, compile the Elm code using the `Compiler` from elm-spec-core. This will\\nwrap the compiled code so that the parts of the program that interact with the outside world\\ncan be easily faked out.\\nThen, to get the program running: First, create an `ElmContext` object. This creates\\nall the fake objects on the `window`\\nobject that the compiled elm code will attempt to reference. Second, evaluate the compiled\\nelm code. It doesn't matter when we compile the elm code, of course, just that it is\\nevaluated in the browser environment *after* we have created a new `ElmContext` in that\\nenvironment.\\nSo, it's a little wild, I guess, that simply instantiating an `ElmContext` modifies the `window`\\nobject and so on.\\nPart of the need for this comes from the fact that the compiled Elm code is wrapped in an IFFE.\\nBut there's no reason why we actually have to do that ...\\nWe've been able to deal with this problem so far because the only things that need to go\\nthrough this process are elm-spec-runner and karma-elm-spec-framework. But with the harness,\\nwe are now asking a test writer to follow this process as well. For that reason, we need to\\nsimplify it so it's not a source of errors.\\n","Decision":"We should change this flow so that we don't need to create an `ElmContext` and evaluate the\\ncompiled Elm code in a particular order.\\nFirst, we will wrap the compiled Elm code in a function that takes an `ElmContext`. Evaluating\\nthis code will still attach the `Elm` object to the window (since we're providing it with a\\nproxy object). But by using a normal function here, we have more control over when the `Elm`\\nobject is loaded.\\nThen, we need to have `ElmContext` store references to all the fakes inside itself -- there's no\\nreal need to store these on the `window` object. So the only things we need to store on the `window`\\nare (1) the function to load Elm -- because this is how the compiled Elm code provides the function;\\nI don't think we can reference it any other way. (2) Data on what the compiler actually\\ntried to do, like what files it tried to compile, the path and all that; we store this on the window\\nso it's available as soon as the compiled code is evaluated -- there may be better ways to do this.\\nAnd (3) the function for loading files, which has to be a function accessible on the window\\nanyway (since that's how Playwright and JSDOM allow us to register a function to be executed in Node).\\nOnce we do this, then it turns out that the test writer doesn't need to create an `ElmContext`\\nat all ... we can have `HarnessController` do that when `prepareHarness` is called. In fact,\\nwe don't even need to bundle anything extra. This will be included in the bundle of tests that\\nget executed in the browser.\\nWe could also change SuiteRunner to create it's own `ElmContext` as well, but it's not necessary.\\n","tokens":318,"id":4592,"Prediction":"## Decision\n\nTo simplify the process of running an Elm-spec program and reduce the likelihood of errors, we will encapsulate the creation of the `ElmContext` and compilation of the Elm code into a single, self-contained step. This will eliminate the need for the test writer to manually create the `ElmContext` and compile the code separately.\n\n## Context\n\nAs described, the current process of running an Elm-spec program is complex and prone to errors. It involves compiling the Elm code using the `Compiler` from elm-spec-core, creating an `ElmContext` object, and then evaluating the compiled code in the browser environment. This process is not only cumbersome but also requires a deep understanding of the underlying mechanics.\n\n## Problem\n\nThe current process is error-prone and difficult to maintain. The test writer is required to manually create the `ElmContext` and compile the code separately, which can lead to mistakes and confusion.\n\n## Decision\n\nTo address this problem, we will create a new class, `ElmRunner`, that encapsulates the creation of the `ElmContext` and compilation of the Elm code. This class will take care of all the necessary steps, including creating the `ElmContext`, compiling the code, and evaluating it in the browser environment.\n\n## Consequences\n\n* Simplifies the process of running an Elm-spec program, making it easier for test writers to use.\n* Reduces the likelihood of errors, as the complex process is now encapsulated in a single class.\n* Improves maintainability, as the code is now more organized and easier to understand.\n\n## Rationale\n\nThe decision to create a new class, `ElmRunner`, is based on the need to simplify the process of running an Elm-spec program and reduce the likelihood of errors. By encapsulating the creation of the `ElmContext` and compilation of the Elm code, we can make the process more straightforward and easier to use. This will benefit both the test writers and the maintainers of the code.","GenTime":"2024-08-10 22:43:46"}
{"File Name":"octagon\/0002-components-shall-be-stateless-by-default.md","Context":"## Context\\nOctagon strives to be a view layer for consumers.  Applications manage their state via their own independent mechanisms.  State embedded within Octagon components may reduce the declaritive ability of said components & violate the top-down, _\"config via props\"_ React mantra.  Consequently, stateful components generally deviate from the pure view layer goals we wish to provide.  Embedded state reduces the ablitity for apps to \"declare\" Octagon components into view state.\\nExample, consider if a `<OctoNavItem \/>` maintains state on whether or not that navigation page is active or not.  If a user reloaded the page, an active item may suddenly show as not active, because that component's state is lost on reload.\\nInstead, the API should be stateless, and be used like: `<OctoNavItem active \/>`.\\nNow, the user of the view layer is in control.\\n","Decision":"Make things stateless.  In rare cases, some emphemeral state is OK to maintain internally to components.  These should be highlighted, drawn out, and discussed before merge.  Ideally, these issues should be raised during design, pre-merge if a feature branch is longer running.\\n","tokens":189,"id":4596,"Prediction":"## Decision\n\nTo maintain the declarative ability of Octagon components and adhere to the \"config via props\" mantra, we will design Octagon components to be stateless and only accept props to configure their behavior. This decision is made to ensure that the view layer remains decoupled from the application's state management and allows users to control the behavior of the components.\n\n## Context\n\nAs mentioned in the context, embedded state within Octagon components can reduce their declarative ability and make it difficult for applications to \"declare\" them into view state. Additionally, stateful components can lead to unexpected behavior, such as losing state on page reload, which can be frustrating for users.\n\n## Rationale\n\nBy making Octagon components stateless, we can ensure that they remain pure view layers and do not manage their own state. This allows applications to manage their own state independently and decouples the view layer from the application's state management. This approach also enables users to control the behavior of the components by passing props, which is in line with the \"config via props\" mantra.\n\n## Consequences\n\n* Octagon components will be designed to be stateless and only accept props to configure their behavior.\n* Applications will be responsible for managing their own state and passing props to Octagon components to configure their behavior.\n* The view layer will remain decoupled from the application's state management, making it easier to maintain and extend.\n\n## Alternatives Considered\n\n* Designing Octagon components to manage their own state, which would lead to a tightly coupled view layer and application state management.\n* Using a hybrid approach, where Octagon components manage some state, but also accept props to configure their behavior. This approach would still lead to a tightly coupled view layer and application state management.\n\n## Decision History\n\nThis decision was made after careful consideration of the trade-offs between stateful and stateless components. The team decided that a stateless approach would be more in line with the \"config via props\" mantra and would allow for a more decoupled view layer and application state management.","GenTime":"2024-08-10 22:44:29"}
{"File Name":"octagon\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4597,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\n**Title:** Use a Microservices Architecture for the Project\n\n**Status:** Accepted\n\n**Context:** As our project is growing in complexity and scale, we need to ensure that our architecture can support the increasing demands of our users. We have considered various options, including a monolithic architecture, a service-oriented architecture, and a microservices architecture.\n\n**Decision:** We have decided to adopt a microservices architecture for our project. This decision is based on the following factors:\n\n* Scalability: Microservices allow us to scale individual services independently, which means we can scale the services that need it most, without affecting the entire application.\n* Flexibility: With microservices, we can develop and deploy services independently, which gives us the flexibility to use different programming languages, frameworks, and databases for each service.\n* Resilience: If one service experiences issues, it will not bring down the entire application, as each service is decoupled from the others.\n* Maintainability: Microservices make it easier to maintain and update individual services, without affecting the entire application.\n\n**Consequences:**\n\n* Increased complexity: With multiple services, our architecture will become more complex, and we will need to manage multiple services, each with its own configuration, deployment, and monitoring.\n* Higher operational costs: We will need to invest in additional infrastructure, personnel, and tools to support the microservices architecture.\n* Improved scalability and flexibility: Our architecture will be more scalable and flexible, allowing us to adapt to changing business needs and user demands.\n\n**Rationale:** We believe that the benefits of a microservices architecture outweigh the increased complexity and operational costs. By adopting this architecture, we will be able to build a more scalable, flexible, and resilient system that can support the growing demands of our users.\n\n**References:**\n\n* [1] \"Microservices Architecture\" by Martin Fowler\n* [2] \"Designing Distributed Systems\" by Brendan Burns\n\n**Implementing this decision:**\n\n* We will start by identifying the services that will make up our microservices architecture.\n* We will design and implement each service, using the required technologies and frameworks.\n* We will develop a strategy for deploying and managing our microservices.\n* We will monitor and analyze the performance of our microservices, and make adjustments as needed.","GenTime":"2024-08-10 22:44:55"}
{"File Name":"adr\/ADR-6-git-style-guide.md","Context":"## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n","Decision":"We agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client\/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\\n### Branches\\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\\n```\\nfix\/wrong_host\\n^  ^^------------^\\n|  ||\\n|  |+----> Summary in present tense.\\n|  +-----> Slash\\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\\n```\\nOther examples are:\\n```\\ndocs\/update_readme\\nrefactor\/new_welcome_message\\n```\\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+`\\n#### Exceptions:\\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\\n","tokens":28,"id":4600,"Prediction":"## Decision\n\n**Title:** Consistent Git History and Commit Message Format\n\n**Context:** As our team continues to grow and collaborate on multiple projects, it has become increasingly important to maintain a clear and consistent Git history and commit message format. This decision aims to establish a standard for our team to follow, ensuring that our Git histories are easy to navigate, understand, and maintain.\n\n**Decision:** We will adopt the following guidelines for our Git histories and commit messages:\n\n* **Git History:**\n\t+ We will use the conventional commit message format, which includes a brief summary of the changes made, followed by a detailed description of the changes.\n\t+ We will use the following format for our commit messages: `type: subject`, where `type` is one of the following: `feat`, `fix`, `docs`, `style`, `refactor`, `perf`, `test`, or `build`.\n\t+ We will include a `BREAKING CHANGE` section in the commit message if the change is breaking and requires attention from other team members.\n* **Commit Message Format:**\n\t+ We will use the following format for our commit messages: `type: subject`, where `type` is one of the following: `feat`, `fix`, `docs`, `style`, `refactor`, `perf`, `test`, or `build`.\n\t+ We will include a brief summary of the changes made in the subject line.\n\t+ We will include a detailed description of the changes made in the body of the commit message.\n\t+ We will use proper grammar and spelling in our commit messages.\n\n**Rationale:** By adopting a consistent format for our Git histories and commit messages, we can improve the readability and maintainability of our codebase. This will make it easier for new team members to understand the history of our project and for existing team members to quickly identify the changes made.\n\n**Consequences:**\n\n* We will need to update our existing commit messages to conform to the new format.\n* We will need to educate new team members on the importance of following the commit message format.\n* We will need to regularly review our commit messages to ensure they are following the format and are clear and concise.\n\n**Monitoring and Review:** We will regularly review our commit messages to ensure they are following the format and are clear and concise. We will also monitor the readability and maintainability of our codebase to ensure that the new format is effective in improving these aspects.","GenTime":"2024-08-10 22:46:07"}
{"File Name":"adr\/ADR-8-dao-content-servers-and-local-content-servers.md","Context":"## Context and Problem Statement\\n* Today Catalyst servers are split in two groups listed and unlisted in the DAO\\nListed servers sync with eachother by a pull mechanism (between each other, listed in the DAO).\\nUnlisted servers, sync again those who are listed but. Listed servers never pull changes from unlisted servers.\\n* Big scenes are constrained by server configurations or proxy limitations of the deployed nodes (i.e. body post size). Nowadays, those limitations are workarounded by selecting another server with a different configuration (Kyllian's).\\n### Use cases\\n* I want to push my scene to my own server and I expect it to replicate to other servers.\\n* Create a \"local-staging\" environment in which land owners or creators can synchronize the whole world from a DAO node and test locally.\\n* Broadcast of changes should be optional.\\n","Decision":"* Enable custom Catalyst servers from Explorer\\n* Define action plan for broadcasting\/commiting local deployments to the DAO servers. [issue](https:\/\/github.com\/decentraland\/catalyst\/issues\/195)\\n","tokens":178,"id":4601,"Prediction":"## Decision\n\n**Decentralized Server-to-Server Synchronization with Optional Broadcasting**\n\n## Context and Problem Statement\n\nAs described, the current architecture has listed and unlisted servers in the DAO, with listed servers pulling changes from each other, but not from unlisted servers. This approach has limitations, such as workarounds for big scenes and constraints due to server configurations or proxy limitations.\n\n## Decision Drivers\n\n* Decentralized synchronization to allow for more flexibility and scalability\n* Optional broadcasting to enable testing and debugging locally\n* Improved reliability and fault tolerance by reducing dependencies on specific servers\n\n## Decision Outcome\n\nWe will implement a decentralized server-to-server synchronization mechanism, where all servers (listed and unlisted) can push and pull changes to\/from each other. This will allow for more flexibility and scalability, as well as improved reliability and fault tolerance.\n\nTo achieve this, we will:\n\n1. Implement a peer-to-peer (P2P) synchronization mechanism, allowing servers to communicate directly with each other.\n2. Introduce a broadcasting mechanism, which will enable servers to opt-in to broadcasting changes to other servers.\n3. Update the DAO to manage server relationships and synchronization settings.\n\n## Rationale\n\nThis decision is driven by the need for a more decentralized and scalable architecture. By allowing all servers to communicate with each other, we can reduce dependencies on specific servers and improve fault tolerance. The optional broadcasting mechanism will enable testing and debugging locally, while also allowing for more flexibility in the synchronization process.\n\n## Pros and Cons\n\nPros:\n\n* More decentralized and scalable architecture\n* Improved reliability and fault tolerance\n* Optional broadcasting for testing and debugging\n\nCons:\n\n* Increased complexity in implementing the P2P synchronization mechanism\n* Potential for increased network traffic and resource usage\n\n## Implications\n\nThis decision will have implications for the development and maintenance of the Catalyst servers, as well as the overall architecture and scalability of the system. It is essential to carefully consider the trade-offs and implement the changes in a way that balances complexity with benefits.","GenTime":"2024-08-10 22:46:29"}
{"File Name":"adr\/ADR-36-kernel-repository-separation.md","Context":"## Context and Problem Statement\\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\\n","Decision":"We choose to create a separated repository to not block the releases and development of explorer during this migration stage.\\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\\n","tokens":135,"id":4609,"Prediction":"## Decision\n\n**Decision:** Implement a centralized package management system to manage the publishing cycles and processes of the decoupled packages.\n\n**Context:** As the Decentraland ecosystem is undergoing a significant transformation by decoupling its components into separate repositories, it is crucial to optimize the publishing cycles and processes to ensure that development is not blocked and hotfixes can still be released without disrupting the overall system.\n\n**Problem Statement:** The current approach of managing the publishing cycles and processes for each package individually may lead to inefficiencies, delays, and potential breakages in the system.\n\n**Reasoning:** A centralized package management system will enable the following benefits:\n\n1. **Simplified Publishing Process:** A single system will manage the publishing process for all packages, reducing the complexity and overhead of managing multiple publishing processes.\n2. **Improved Collaboration:** A centralized system will facilitate collaboration among developers, allowing them to work on different packages without worrying about conflicting publishing schedules.\n3. **Faster Hotfix Deployment:** With a centralized system, hotfixes can be deployed quickly and efficiently, minimizing the impact on the overall system.\n4. **Better Visibility and Control:** The centralized system will provide a single source of truth for package versions, dependencies, and publishing schedules, giving developers and maintainers better visibility and control over the system.\n\n**Consequences:**\n\n1. **Initial Complexity:** Implementing a centralized package management system will require an initial investment of time and resources.\n2. **Dependence on a Single System:** The system will introduce a single point of failure, requiring careful design and implementation to ensure high availability and reliability.\n\n**Alternatives Considered:**\n\n1. **Continue with Individual Publishing Processes:** This approach would maintain the current complexity and overhead of managing multiple publishing processes, potentially leading to delays and breakages.\n2. **Implement a Distributed Package Management System:** This approach would require significant changes to the existing infrastructure and may introduce additional complexity and dependencies.\n\n**Decision Criteria:**\n\n1. **Simplicity:** A centralized system will simplify the publishing process and reduce complexity.\n2. **Collaboration:** A centralized system will facilitate collaboration among developers.\n3. **Faster Hotfix Deployment:** A centralized system will enable faster hotfix deployment.\n4. **Visibility and Control:** A centralized system will provide better visibility and control over the system.\n\n**Status:** This decision is approved and will be implemented in the next release cycle.","GenTime":"2024-08-10 22:49:09"}
{"File Name":"adr\/ADR-7-standards-repository.md","Context":"# Context\\n* Is the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository still useful?\\n* Does it make sense to have ADR and standards?\\n* How does it play with [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals)?\\n# Options\\n* Archive both repositories, leaving only [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n* \u2705 Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards), leaving [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\n* Only use [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n# Decision\\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","Decision":"* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","tokens":338,"id":4610,"Prediction":"## Decision\n\nArchive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\n\n## Context\n\nIs the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository still useful? Does it make sense to have ADR and standards? How does it play with [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals)?\n\n## Options\n\n* Archive both repositories, leaving only [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards), leaving [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\n* Only use [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\n\n## Decision\n\nArchive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\nKeep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\n\n### Rationale\n\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can be leveraged by anyone.\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintaining both repositories.\n\nDate: 2020-10-16","GenTime":"2024-08-10 22:49:28"}
{"File Name":"adr\/ADR-40-ui-dependencies-upgrades.md","Context":"## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https:\/\/github.com\/Semantic-Org\/Semantic-UI-React\/pull\/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https:\/\/github.com\/decentraland\/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy\/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n","Decision":"The option to keep the an exact copy of the `Responsive` component (from the old `react-semantic-ui` lib version) was chosen in order to have a frictionless upgrade of the library.\\nThe procedure in which we'll be handling the upgrade is the following:\\n1. A non breaking change upgrade will be provided to our [UI library](https:\/\/github.com\/decentraland\/ui), keeping the `Responsive` component as a deprecated component and an alternative (describe below) will be provided to replace it.\\n2. A breaking change upgrade will be applied to our [UI library](https:\/\/github.com\/decentraland\/ui), whenever all of our dependencies are updated, removing the `Responsive` component.\\nWe\u2019ll be providing, alongside the `Responsive` component a set of components and hooks to replace it, using the `react-responsive`, library. This library was chosen in favor of the recommended `@artsy\/fresnel` mainly because of its versatility. The need of having to set a provider at the application's root level, (coupling the users of this dependency to `@artsy\/fresnel`) to have better SSR support that we don't currently need, made us decide not to go with it.\\nThe components built with the `react-responsive` and exposed to the consumers of our [UI library](https:\/\/github.com\/decentraland\/ui) will be the following:\\n- **Desktop** (for devices with `min width: 992`)\\n- **Tablet** (for devices with `min width: 768 and max width: 991`)\\n- **TabletAndBelow** (for devices with `max width: 991`, that is taking into consideration tablets and mobile devices)\\n- **Mobile** (for devices with `max width: 767`)\\n- **NotMobile** (for devices that don't comply with the requirements specified in Mobile)\\nThese components describe a conditional rendering based on the media the page in being rendered.\\nWhere we had:\\n```tsx\\n<Responsive\\nas={Menu}\\nsecondary\\nstackable\\nminWidth={Responsive.onlyTablet.minWidth}\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Responsive>\\n<Responsive\\n{...Responsive.onlyMobile}\\nclassName=\"dcl navbar-mobile-menu\"\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/Responsive>\\n```\\nWe now have:\\n```tsx\\n<NotMobile>\\n<Menu secondary stackable>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Menu>\\n<\/NotMobile>\\n<Mobile>\\n<div className=\"dcl navbar-mobile-menu\">\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/div>\\n<\/Mobile>\\n```\\nAnd, alongside these components, as explained before, we're exposing the following set of hooks:\\n- **useDesktopMediaQuery**\\n- **useTabletMediaQuery**\\n- **useTabletAndBelowMediaQuery**\\n- **useMobileMediaQuery**\\n- **useNotMobileMediaQuery**\\nWhich return true if the device is the one defined as the name of the hook.\\nThese types of hooks will provide us with newer functionality, being able to customize small portions of our code instead of forking our components into two.\\nAs an example, we can apply certain styles by simply:\\n```tsx\\nconst isMobile = useMobileMediaQuery()\\nconst classes = isMobile ? \"dcl mobile\" : \"dcl\"\\n<div className={classes}>\\n...\\n<\/div>\\n```\\n","tokens":379,"id":4613,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\nWe will use `react-semantic-ui` as the alternative library to the `Responsive` component, despite its poor Server-Side Rendering (SSR) support, due to its advantages in terms of non-coupling dependency and hooks support.\n\n**Context**\n\nThe organization has multiple UI apps and libraries with different React versions, causing issues when trying to consume them. To address this, we need to update all UI apps and libraries to React 17, which implies updating `react-semantic-ui` to its latest version. However, this update removes the widely used `Responsive` component, which is a breaking change for our current UI library.\n\n**Problem Statement**\n\nWe need to provide an alternative to the `Responsive` component, which allows rendering components based on device sizes. We have two alternatives: `@artsy\/fresnel` and `react-semantic-ui`. `@artsy\/fresnel` provides a ContextProvider component that wraps the whole application, but it lacks hooks support. `react-semantic-ui`, on the other hand, provides hooks and component solutions for rendering components with different media queries, but it has poor SSR support.\n\n**Decision Rationale**\n\nWe chose `react-semantic-ui` despite its poor SSR support because its non-coupling dependency and hooks support outweigh its disadvantages. By using `react-semantic-ui`, we can render different components or parts of components based on media queries, which is a versatile solution for our use case. Additionally, hooks support allows us to integrate `react-semantic-ui` with our existing React components.\n\n**Consequences**\n\n* We will need to find a solution to improve SSR support for our UI library, potentially using a different library or implementing a custom solution.\n* We will need to update our existing React components to use the hooks provided by `react-semantic-ui`.\n* We will need to ensure that our UI library is compatible with the new `react-semantic-ui` version and its breaking changes.\n\n**Status**\n\nThis decision is approved and will be implemented in the next release of our UI library.","GenTime":"2024-08-10 22:50:47"}
{"File Name":"adr\/ADR-24-decouple-kernel-and-unity-apis.md","Context":"## Context and Problem Statement\\nAs of today, kernel and unity have huge dependencies in business logic: sagas in TS, requests are made in TS, and everything is used from Unity.\\nThat generates coupling between technologies and teams, generating blockers and planning problems.\\n","Decision":"* Go for this proposal \u2705\\n### Actionables\\n- When starting development on new features, aim for this approach from the get-go\\n- Features that are already advanced in development or finished will be refactored gradually\\n- We will start by exposing signed requests and migrating the quests feature saga\\nNew features for the ECS\/SDK will have the common product process (RFC, roadmap, backlog, refinement). Alvaro will gather together the team when necessary to discuss implementation of new changes to the SDK.\\nCLI should be handled in the same way as the kernel and SDK developments.\\n","tokens":54,"id":4614,"Prediction":"## Decision\n\nTo reduce the coupling between technologies and teams, and to improve maintainability and scalability, we will adopt a microkernel architecture, where the kernel and unity will be separated into distinct layers with well-defined interfaces.\n\n## Context\n\nAs of today, kernel and unity have huge dependencies in business logic, with sagas written in TypeScript, requests made in TypeScript, and everything being used from Unity. This generates coupling between technologies and teams, leading to blockers and planning problems.\n\n## Problem\n\nThe current architecture is tightly coupled, making it difficult to maintain, test, and scale the system. The dependencies between kernel and unity create a single point of failure, and changes to one layer can have unintended consequences on the other.\n\n## Decision Criteria\n\n1. Separation of Concerns: We want to separate the concerns of the kernel and unity into distinct layers, making it easier to maintain, test, and scale each layer independently.\n2. Loosely Coupled Architecture: We want to reduce the coupling between the kernel and unity, making it easier to make changes to one layer without affecting the other.\n3. Scalability: We want to design the architecture to be scalable, allowing us to add or remove layers as needed.\n4. Testability: We want to design the architecture to be testable, making it easier to write unit tests and integration tests for each layer.\n\n## Decision\n\nWe will adopt a microkernel architecture, where the kernel and unity will be separated into distinct layers with well-defined interfaces. The kernel will be responsible for handling the business logic, while the unity will be responsible for handling the presentation layer. We will use APIs and message queues to communicate between the layers.\n\n## Rationale\n\nThe microkernel architecture will allow us to separate the concerns of the kernel and unity, reducing the coupling between the layers. This will make it easier to maintain, test, and scale each layer independently. The APIs and message queues will provide a clear interface between the layers, making it easier to understand and test the interactions between the layers.\n\n## Consequences\n\n1. Increased complexity: The microkernel architecture will add an additional layer of complexity to the system, requiring more planning and coordination between teams.\n2. Improved maintainability: The separation of concerns will make it easier to maintain and update each layer independently, reducing the risk of unintended consequences.\n3. Improved scalability: The microkernel architecture will provide a scalable architecture, allowing us to add or remove layers as needed.\n4. Improved testability: The APIs and message queues will provide a clear interface between the layers, making it easier to write unit tests and integration tests for each layer.\n\n## Next Steps\n\n1. Define the interfaces and APIs between the kernel and unity.\n2. Implement the APIs and message queues.\n3. Refactor the business logic to be contained within the kernel.\n4. Refactor the presentation layer to be contained within the unity.\n5. Test and validate the new architecture.","GenTime":"2024-08-10 22:51:19"}
{"File Name":"adr\/0002-SYS-rdbms.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4616,"Prediction":"## Decision\n\n**Use a Microservices Architecture for the New Project**\n\n## Context\n\nThe issue motivating this decision is the need to develop a new project that requires scalability, flexibility, and fault tolerance. The project involves multiple independent services that need to communicate with each other, and the existing monolithic architecture is not suitable for this requirement.\n\nThe context that influences or constrains this decision is the following:\n\n* The project requires a high degree of scalability, as it needs to handle a large volume of requests and data.\n* The project involves multiple independent services that need to communicate with each other, and the existing monolithic architecture is not designed to handle this complexity.\n* The project requires a high degree of flexibility, as the requirements are likely to change over time.\n* The project requires a high degree of fault tolerance, as any single point of failure can cause the entire system to fail.\n* The development team is experienced in microservices architecture and has successfully implemented it in previous projects.\n* The organization has a cloud-based infrastructure that supports microservices architecture.\n\n## Rationale\n\nThe decision to use a microservices architecture for the new project is based on the following rationale:\n\n* Microservices architecture allows for scalability, as each service can be scaled independently.\n* Microservices architecture allows for flexibility, as each service can be developed and deployed independently.\n* Microservices architecture allows for fault tolerance, as each service can be designed to be fault-tolerant and can be recovered independently.\n* Microservices architecture allows for loose coupling between services, which makes it easier to maintain and update the system over time.\n\n## Consequences\n\nThe consequences of this decision are:\n\n* The development team will need to design and implement multiple services that communicate with each other.\n* The system will require a service registry and a load balancer to manage the communication between services.\n* The system will require a database that can handle the data requirements of multiple services.\n* The system will require a monitoring and logging system to monitor the performance and health of each service.\n\nOverall, the decision to use a microservices architecture for the new project is based on the need for scalability, flexibility, and fault tolerance, and is expected to provide a robust and maintainable system that meets the requirements of the project.","GenTime":"2024-08-10 22:51:42"}
{"File Name":"adr\/ADR-9-DecentralandInterface-evolution-plan.md","Context":"## Context and Problem Statement\\nHow should we proceed to maintain compatibility between the current SDK and the next-gen SDK currently being developed?\\nTaking into consideration:\\n* All the scenes currently deployed must work as they work today.\\n* `DecentralandInterface` should work indefinitely to maintain backwards compatibility.\\n* Maintainance costs should be taken into account\\n","Decision":"We decided to develop a new and clean interface for the next-gen SDK, then create an adapter for the legacy `DecentralandInterface` (option 2) because:\\n* We should not be limited by the decisions of the past SDK\\n* The new SDK must eventually prove itself by reaching a feature parity with the current interface\\n* Avoid sunk cost fallacy with the old SDK\\n","tokens":72,"id":4618,"Prediction":"## Decision\n\nTo maintain compatibility between the current SDK and the next-gen SDK, we will adopt a gradual migration approach, ensuring that all scenes currently deployed continue to work as they do today, while also allowing for the development of new features and improvements in the next-gen SDK.\n\n## Context\n\nAs we develop the next-gen SDK, it is crucial to maintain compatibility with the current SDK to avoid disrupting the existing scenes and user experience. We must ensure that the `DecentralandInterface` continues to work indefinitely, providing backwards compatibility for existing scenes and applications. Additionally, we must consider the maintenance costs associated with supporting multiple SDK versions.\n\n## Solution\n\nTo achieve this, we will implement the following strategies:\n\n1. **Dual-SDK Support**: We will maintain two separate SDK versions: the current SDK and the next-gen SDK. This will allow us to continue supporting the existing scenes and applications while also developing new features and improvements in the next-gen SDK.\n2. **Gradual Migration**: We will gradually migrate existing scenes and applications to the next-gen SDK, ensuring that they continue to work as they do today. This will be done in a controlled and staged manner, allowing us to test and validate each migration before moving on to the next one.\n3. **Backwards Compatibility Layer**: We will develop a backwards compatibility layer within the next-gen SDK, which will allow it to seamlessly integrate with the current SDK and maintain compatibility with existing scenes and applications.\n4. **Testing and Validation**: We will conduct thorough testing and validation of each migration and update to ensure that existing scenes and applications continue to work as expected.\n5. **Documentation and Support**: We will provide detailed documentation and support for both the current SDK and the next-gen SDK, ensuring that developers and users have the necessary resources to adapt to the changes.\n\n## Rationale\n\nBy adopting a gradual migration approach, we can minimize disruption to existing scenes and applications, while also allowing us to develop and improve the next-gen SDK. The dual-SDK support strategy will enable us to maintain compatibility with existing scenes and applications, while the backwards compatibility layer will ensure that the next-gen SDK can seamlessly integrate with the current SDK. This approach will also allow us to reduce maintenance costs by minimizing the number of SDK versions we need to support.\n\n## Consequences\n\n* Increased development time and resources required for dual-SDK support and backwards compatibility layer\n* Potential for increased complexity in the codebase\n* Reduced maintenance costs in the long run by minimizing the number of SDK versions\n* Improved compatibility and support for existing scenes and applications\n* Ability to develop and improve the next-gen SDK without disrupting the existing user experience","GenTime":"2024-08-10 22:52:11"}
{"File Name":"adr\/ADR-5-how-to-organize-adr-files.md","Context":"## Context and Problem Statement\\n- How do we call the folder for assets?\\n- What is the pattern of the filename where an ADR is stored?\\n- How do we write titles in the ADR documents?\\n","Decision":"### Asset folder\\nChosen option: `resources`\\n### Filename\\nChosen option: `docs\/ADR-NNNN-title-using-dashes.md` and `docs\/resources\/ADR-NNNN\/{filename}`, because\\n- Adding `arch` or `adr` folders is redundant.\\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\\n- Having no spaces in filenames eases working in the command line.\\n- Prefixing with `docs\/ADR-` enables future new kinds of documents.\\n- Calling a folder `docs\/resources\/ADR-NNNN\/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\\n### Title (markdown)\\nChosen option: `# Plain title` because:\\n- At the moment of writing the ADR the writer may not know the final ADR number.\\n- It can be embeded by rendering tools\\n- It would duplicate data from the filename\\n","tokens":46,"id":4619,"Prediction":"## Decision\n\nTo standardize the naming convention and organization of Architectural Decision Records (ADRs), we will follow the guidelines outlined below:\n\n### Context\n\nAs we create more ADRs, it is essential to establish a consistent naming convention and folder structure to ensure easy retrieval and organization of these documents. This decision aims to provide a clear and consistent approach to naming and organizing ADRs.\n\n### Decision\n\n1. **Folder Structure:** We will create a top-level folder named \"adrs\" in the project's root directory. Within this folder, we will create subfolders organized by year and month (e.g., \"adrs\/2023\/03\").\n2. **Filename Pattern:** ADR filenames will follow the pattern \"adr-YYYYMMDD-<title>.md\", where:\n\t* \"adr\" is a prefix indicating that the file contains an ADR.\n\t* \"YYYYMMDD\" represents the date in the format year-month-day.\n\t* \"<title>\" is a concise title summarizing the contents of the ADR.\n\t* \".md\" indicates that the file is written in Markdown format.\n3. **Title Writing:** Titles in ADR documents will be written in title case, with the first word capitalized and the rest of the words in lowercase. For example, \"Introduce New Authentication Mechanism\".\n4. **Additional Guidelines:**\n\t* Use a descriptive title that accurately reflects the contents of the ADR.\n\t* Keep the title concise, ideally no more than 10-12 words.\n\t* Avoid using special characters, numbers, or punctuation in the title, except for hyphens (-) and underscores (_).\n\n### Rationale\n\nThis decision aims to provide a consistent and organized approach to storing and retrieving ADRs. By following this structure, we can easily find and access specific ADRs, making it easier to track and manage architectural decisions throughout the project's lifecycle.\n\n### Consequences\n\n* Improved organization and retrieval of ADRs.\n* Consistent naming convention and folder structure.\n* Easier maintenance and updates of ADRs.\n\n### Alternatives Considered\n\n* Using a different folder structure or naming convention.\n* Not using a consistent naming convention or folder structure.\n* Using a different file format (e.g., PDF instead of Markdown).\n\n### Implementation Status\n\nThis decision has been implemented, and the guidelines outlined above will be followed for all future ADRs. Existing ADRs will be updated to conform to this new structure and naming convention.","GenTime":"2024-08-10 22:52:37"}
{"File Name":"adr\/ADR-1-documenting-architecture-decisions.md","Context":"## Context\\nArchitecture for agile projects has to be described and defined differently. Not all decisions will be made at once, nor will all of them be done when the project begins.\\nAgile methods are not opposed to documentation, only to valueless documentation. Documents that assist the team itself can have value, but only if they are kept up to date. Large documents are never kept up to date. Small, modular documents have at least a chance at being updated.\\nNobody ever reads large documents, either. Most developers have been on at least one project where the specification document was larger (in bytes) than the total source code size. Those documents are too large to open, read, or update. Bite sized pieces are easier for for all stakeholders to consume.\\nOne of the hardest things to track during the life of a project is the motivation behind certain decisions. A new person coming on to a project may be perplexed, baffled, delighted, or infuriated by some past decision. Without understanding the rationale or consequences, this person has only two choices:\\n1. **Blindly accept the decision.**\\nThis response may be OK, if the decision is still valid. It may not be good, however, if the context has changed and the decision should really be revisited. If the project accumulates too many decisions accepted without understanding, then the development team becomes afraid to change anything and the project collapses under its own weight.\\n2. **Blindly change it.**\\nAgain, this may be OK if the decision needs to be reversed. On the other hand, changing the decision without understanding its motivation or consequences could mean damaging the project's overall value without realizing it. (E.g., the decision supported a non-functional requirement that hasn't been tested yet.)\\nIt's better to avoid either blind acceptance or blind reversal.\\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.\\nAn architecture decision record is a short text file in a format similar to an Alexandrian pattern. (Though the decisions themselves are not necessarily patterns, they share the characteristic balancing of forces.) Each record describes a set of forces and a single decision in response to those forces. Note that the decision is the central piece here, so specific forces may appear in multiple ADRs.\\nWe will keep ADRs in the project repository under `docs\/ADR-####-title.md`\\nWe should use a lightweight text formatting language like Markdown or Textile.\\nADRs will be numbered sequentially and monotonically. Numbers will not be reused.\\nIf a decision is reversed, we will keep the old one around, but mark it as superseded. (It's still relevant to know that it _was_ the decision, but is _no longer_ the decision.)\\nWe will use a format with just a few parts, so each document is easy to digest. The format has just a few parts.\\n**Title** These documents have names that are short noun phrases. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\n**Context** This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n**Decision** This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will \u2026\"\\n**Status** A decision may be \"proposed\" if the project stakeholders haven't agreed with it yet, or \"accepted\" once it is agreed. If a later ADR changes or reverses a decision, it may be marked as \"deprecated\" or \"superseded\" with a reference to its replacement.\\n**Consequences** This section describes the resulting context, after applying the decision. All consequences should be listed here, not just the \"positive\" ones. A particular decision may have positive, negative, and neutral consequences, but all of them affect the team and project in the future.\\nThe whole document should be one or two pages long. We will write each ADR as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments. (Bullets kill people, even PowerPoint bullets.)\\n# Status\\nAccepted.\\n# Consequences\\nOne ADR describes one significant decision for a specific project. It should be something that has an effect on how the rest of the project will run.\\nThe consequences of one ADR are very likely to become the context for subsequent ADRs. This is also similar to Alexander's idea of a pattern language: the large-scale responses create spaces for the smaller scale to fit into.\\nDevelopers and project stakeholders can see the ADRs, even as the team composition changes over time.\\nThe motivation behind previous decisions is visible for everyone, present and future. Nobody is left scratching their heads to understand, \"What were they thinking?\" and the time to change old decisions will be clear from changes in the project's context.\\n---\\nOriginal document: [Documenting Architecture Decisions](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) by Michael Nygard\\nMore reading:\\n1. [Importance of architecture decisions](http:\/\/www.computer.org\/portal\/web\/csdl\/doi\/10.1109\/MS.2009.52)\\n2. [Documenting software architectures](http:\/\/www.sei.cmu.edu\/library\/abstracts\/books\/0321552687.cfm)\\n","tokens":380,"id":4620,"Prediction":"## Decision\n\nAs part of our architectural decision record, we will adopt a documentation strategy that is tailored to agile projects. We will focus on creating small, modular documents that are easy to consume and update, rather than large, comprehensive documents that are difficult to maintain.\n\n### Context\n\nAs agile projects involve iterative development and continuous improvement, our documentation strategy must be flexible and adaptable to changing requirements. We will prioritize documents that provide value to the development team and stakeholders, such as design decisions, technical rationales, and project history.\n\n### Problem\n\nLarge, comprehensive documents can become outdated quickly, and are often ignored or forgotten. This can lead to confusion and misunderstandings among team members and stakeholders, particularly when new team members join the project. Blindly accepting or reversing past decisions without understanding the motivation or consequences can also lead to negative outcomes.\n\n### Decision\n\nWe will adopt a documentation strategy that includes:\n\n1. Small, modular documents that focus on specific topics or decisions.\n2. Regular updates to ensure that documents remain relevant and accurate.\n3. A clear and concise writing style that is easy to understand.\n4. A centralized location for storing and accessing documents, such as a wiki or documentation repository.\n5. A process for tracking and documenting design decisions, including the motivation and consequences of each decision.\n\n### Rationale\n\nBy adopting a documentation strategy that is tailored to agile projects, we can ensure that our documentation is relevant, accurate, and easy to consume. This will help to reduce confusion and misunderstandings, and improve communication among team members and stakeholders. By tracking and documenting design decisions, we can also ensure that the motivation and consequences of each decision are understood, and that future changes can be made with a clear understanding of the project's history and context.\n\n### Consequences\n\nBy adopting this documentation strategy, we can:\n\n* Improve communication and collaboration among team members and stakeholders.\n* Reduce confusion and misunderstandings.\n* Increase transparency and accountability.\n* Facilitate knowledge sharing and learning.\n* Improve the overall quality and maintainability of our software.","GenTime":"2024-08-10 22:53:00"}
{"File Name":"adr\/ADR-28-smart-contract-wallets-and-meta-transactions.md","Context":"## Context and Problem Statement\\nCurrently we support meta-transactions on Polygon from several of our dApps (Marketplace, Builder, Account) and the Explorer.\\nMeta-transactions require the user to sign a function signature that they want us to call on their behalf (so we can subsidize the gas).\\nThe smart contracts that support meta-transactions (like the Marketplace contract) recover the account from the signature on chain, and uses that as the message sender.\\nSmart contract wallets are smart contracts that can hold assets, and have a set of signers that can command the wallet to perform actions on those assets by providing signed messages authorizing given actions.\\nWhen a user of a contract wallet signs a meta transaction, and the public key is recovered on chain by the smart contract that supports meta-transactions, the public key is the one from the signer, and not the contract address of the smart contract wallet (which is the actual owner of the assets). This causes the meta-transactions to fail on chain.\\n","Decision":"We choose option 2 since we can never be sure that the smart contract wallet has a matching deployment on Polygon, and is not the case for the most popular smart contract wallets (like Argent or Authereum)\\n","tokens":205,"id":4623,"Prediction":"## Decision\n\nTo resolve the issue with meta-transactions failing on chain due to the public key being recovered as the signer's public key instead of the smart contract wallet's address, we will implement a custom solution that allows the smart contract wallet to sign the meta-transaction with its own address, rather than the signer's address.\n\n## Context\n\nThe current implementation of meta-transactions on Polygon requires the user to sign a function signature that they want us to call on their behalf. The smart contracts that support meta-transactions recover the account from the signature on chain and use that as the message sender. However, when a user of a contract wallet signs a meta-transaction, the public key recovered on chain is the one from the signer, not the contract address of the smart contract wallet, which is the actual owner of the assets. This causes the meta-transactions to fail on chain.\n\n## Rationale\n\nImplementing a custom solution to sign the meta-transaction with the smart contract wallet's address will allow us to recover the correct account on chain, ensuring that the meta-transactions are executed successfully. This approach will also provide more flexibility and control over the meta-transaction process, as we can customize the signing mechanism to meet the specific requirements of our dApps.\n\n## Trade-offs\n\n* Implementing a custom solution will require additional development effort and may introduce new complexity to the system.\n* However, this approach will provide a more robust and reliable solution for meta-transactions, ensuring that they are executed correctly and efficiently.\n\n## Consequences\n\n* The implementation of a custom solution will require additional testing and validation to ensure that it works correctly and is secure.\n* The solution will need to be integrated with the existing meta-transaction functionality in the smart contracts that support meta-transactions.\n* The solution will also need to be documented and communicated to the development team and stakeholders to ensure that everyone understands the new implementation and its implications.","GenTime":"2024-08-10 22:54:08"}
{"File Name":"adr\/ADR-10-profile-deployment-debouncing.md","Context":"## Context and Problem Statement\\nHow do we debouncing profile\/scene deployments to avoid DDoS and wasted bandwidth?\\n- As of today we only deploy profiles when changing the avatar\\n- Soon, a new deployment will happen after mute\/unmute\\n","Decision":"We decided to do Client side debouncing (both options 2 and 3). We may measure deployments in the future to revisit the server conversation.\\n","tokens":51,"id":4626,"Prediction":"## Decision\n\nTo debounce profile\/scene deployments and avoid DDoS and wasted bandwidth, we will implement a caching mechanism to store the deployed profiles and scenes, and only update the cache when necessary.\n\n## Context\n\nAs of today, we only deploy profiles when changing the avatar, which can lead to a high volume of deployments and potential DDoS attacks. With the upcoming deployment after mute\/unmute, this issue will only worsen. We need to find a solution to debounce these deployments and minimize the impact on our infrastructure and bandwidth.\n\n## Decision Rationale\n\n1. **Caching**: By caching the deployed profiles and scenes, we can reduce the number of requests to our servers and minimize the amount of data transferred. This will help alleviate the load on our infrastructure and reduce the risk of DDoS attacks.\n2. **Conditional updates**: We will update the cache only when necessary, i.e., when the avatar or mute\/unmute state changes. This will ensure that the cache is always up-to-date and that we don't waste bandwidth by deploying unnecessary changes.\n3. **TTL (Time-To-Live) mechanism**: To further optimize the caching mechanism, we will implement a TTL mechanism that will automatically expire the cache after a certain period of time. This will ensure that the cache is periodically refreshed and that we don't store outdated data.\n\n## Consequences\n\n1. **Reduced load on infrastructure**: By caching and debouncing deployments, we will reduce the load on our servers and infrastructure, making them more resilient to DDoS attacks and high traffic volumes.\n2. **Improved performance**: The caching mechanism will improve the overall performance of our application, as it will reduce the number of requests to our servers and minimize the amount of data transferred.\n3. **Increased security**: By implementing a caching mechanism and a TTL mechanism, we will reduce the risk of DDoS attacks and minimize the impact of potential security breaches.\n\n## Alternatives Considered\n\n1. **Implementing a queueing system**: We could implement a queueing system to manage the deployment requests and ensure that they are processed in a controlled manner. However, this approach would require significant changes to our architecture and would likely introduce additional complexity.\n2. **Using a content delivery network (CDN)**: We could use a CDN to cache and distribute our profiles and scenes. However, this approach would require significant changes to our infrastructure and would likely introduce additional costs.\n\n## Decision History\n\nThis decision was made after considering the current state of our infrastructure and the potential impact of DDoS attacks and high traffic volumes. The caching mechanism was chosen as the most effective solution to debounce deployments and minimize the load on our infrastructure.","GenTime":"2024-08-10 22:55:20"}
{"File Name":"monocle\/0009-changes-extended-with-task-data.md","Context":"## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n","Decision":"* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\nChosen option: \"Monocle API provides an interface for external task data crawlers\", because it will ease integration between Monocle and the\\nvarious task trackers available in the market.\\nA Monocle operator will need to write the crawler for its\\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\\nFrom the Monocle side, we provide a clear API for a task tracker\\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\\n\"severity\", \"change_url\", \"title\", ... The monocle API is then able to\\nfind corresponding Changes in the database that match the \"change_url\" field.\\nEach changes get a new attribute called \"task_data\" that is a list of\\nrelated task data records. Indeed multiple tasks might be related to a\\nsingle change.\\nAlso, Monocle keeps track of task data that do not match any Changes in the\\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\\n","tokens":79,"id":4636,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will implement a Changes metrics feature in the task tracker that allows users to get insights into the ratio of changes related to Feature Requests vs Bug fixing, using a simple and generic approach that does not make assumptions about the task tracker.\n\n**Context**\n\nAs a user, I want to get Changes metrics related to tasks defined in a task tracker. A simple usecase example is to get insight of the ratio of changes related to Feature Requests vs Bug fixing.\n\n**Decision Drivers**\n\n* Simple implementation: We will prioritize a simple implementation that is easy to understand and maintain, to minimize the risk of introducing complexity and errors.\n* No assumption about the task tracker: We will design the solution to be agnostic to the task tracker, to ensure that it can be easily integrated with different task trackers and to minimize the risk of vendor lock-in.\n* Support of a set of generic fields related to a task: We will define a set of generic fields that are commonly used to describe tasks, such as \"task type\" (e.g. Feature Request, Bug Fixing), \"task status\", and \"task priority\", to provide a flexible and reusable solution.\n\n**Consequences**\n\n* The solution will be easy to implement and maintain, with minimal complexity and risk of errors.\n* The solution will be flexible and reusable, allowing it to be easily integrated with different task trackers and to support a wide range of tasks and fields.\n* The solution will provide valuable insights into the ratio of changes related to Feature Requests vs Bug fixing, allowing users to make data-driven decisions and improve their workflow.\n\n**Status**\n\nThis decision is approved and will be implemented as part of the task tracker's changes metrics feature.\n\n**History**\n\n* 2023-02-20: Decision made by [Your Name]\n* 2023-02-22: Implementation started\n* 2023-03-15: Implementation completed\n\nPlease note that this is just a sample Architectural Decision Record, and you should adapt it to your specific use case and requirements.","GenTime":"2024-08-10 22:59:27"}
{"File Name":"monocle\/0011-search-query-language.md","Context":"## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n","Decision":"Chosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n","tokens":44,"id":4637,"Prediction":"## Decision\n\n### Decision: Introduce a Custom Query Language for Dashboard Filtering\n\n### Context and Problem Statement\n\nTo build custom dashboards, we need to define queries that are too complex for the existing filter box form. We would like to use flexible search expressions based on a proper query language.\n\n### Problem\n\nThe existing filter box form is limited in its ability to handle complex queries, making it difficult for users to create custom dashboards that meet their specific needs. This limitation hinders our ability to provide users with a flexible and powerful way to filter and analyze data.\n\n### Rationale\n\nIntroducing a custom query language for dashboard filtering will provide several benefits:\n\n1. **Increased flexibility**: Users will be able to create complex queries that are not possible with the existing filter box form.\n2. **Improved usability**: A query language will provide a more intuitive and user-friendly way for users to create custom filters.\n3. **Better data analysis**: With the ability to create complex queries, users will be able to analyze data in a more nuanced and detailed manner.\n\n### Decision Criteria\n\nWe considered the following factors when making this decision:\n\n1. **Complexity**: The query language should be able to handle complex queries, including filtering, grouping, and aggregating data.\n2. **Usability**: The query language should be easy to learn and use, with a minimal learning curve.\n3. **Scalability**: The query language should be able to handle large datasets and perform queries efficiently.\n\n### Trade-offs\n\n1. **Additional complexity**: Introducing a custom query language will add complexity to our system, which may require additional development and testing time.\n2. **Learning curve**: Users may need to learn a new query language, which may take some time and effort.\n\n### Conclusion\n\nWe will introduce a custom query language for dashboard filtering to provide users with a more flexible and powerful way to analyze data. This decision will enable us to create complex queries, improve usability, and provide better data analysis capabilities.\n\n### Next Steps\n\n1. Research existing query languages and select the most suitable one for our use case.\n2. Design and implement the query language in our system.\n3. Develop documentation and tutorials to help users learn the query language.\n4. Test and refine the query language to ensure it meets our requirements and is user-friendly.","GenTime":"2024-08-10 22:59:52"}
{"File Name":"monocle\/0007-production-ready-in-all-in-one-setup.md","Context":"## Context and Problem Statement\\nHow do we serve the API using a production ready server?\\n## Decision Drivers\\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\n","Decision":"* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\nChosen option: \"[option 1]\", because it is well known in the\\nindustry. It is not the most performant but it has good performances\\nto serve WSGI app like Flask thanks to the uwsgi protocol used between\\nNginx\/Apache.\\n### Negative Consequences\\n* complexify the architecture.\\n","tokens":56,"id":4642,"Prediction":"## Decision\n\nWe will use Gunicorn as the production-ready server to serve our API, due to its excellent integration with reverse proxy servers like Nginx and Apache, and its simplicity and performance when used with Flask.\n\n## Context and Problem Statement\n\nHow do we serve the API using a production-ready server?\n\n## Decision Drivers\n\n* Use a production-ready server.\n* Good integration with reverse proxy servers like Nginx and Apache.\n* Support Flask in a simple and performant way.\n\n## Status\n\nAccepted\n\n## Description\n\nGunicorn is a popular and well-maintained WSGI server that can be used to run our Flask application in production. It provides a simple and efficient way to deploy our API, and its support for multiple worker processes and multiple worker types (e.g., sync, async, and gevent) allows us to scale our application as needed.\n\nGunicorn also provides excellent integration with reverse proxy servers like Nginx and Apache, which allows us to take advantage of their caching and load balancing capabilities to improve the performance and reliability of our API.\n\n## Rationale\n\nWe chose Gunicorn over other options like uWSGI and mod_wsgi because of its ease of use and flexibility. Gunicorn is also widely used and well-documented, which makes it easier for us to find resources and support when we need it.\n\n## Consequences\n\n* Our API will be able to handle a large number of requests and scale as needed.\n* We will be able to take advantage of caching and load balancing capabilities provided by reverse proxy servers like Nginx and Apache.\n* Our application will be easy to deploy and manage in production.\n\n## Pros and Cons\n\nPros:\n\n* Easy to use and configure\n* Supports multiple worker processes and multiple worker types\n* Excellent integration with reverse proxy servers\n* Well-maintained and widely used\n\nCons:\n\n* May require additional configuration to optimize performance\n* May require additional resources (e.g., RAM, CPU) to handle high traffic volumes","GenTime":"2024-08-10 23:01:41"}
{"File Name":"monocle\/0010-choice-of-protobuf.md","Context":"## Context and Problem Statement\\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs\/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\\n* How do we define and maintain the interfaces?\\n","Decision":"Chosen option: \"Protobuf\", because it comes out best (see below).\\n### Positive Consequences\\n- We need to start defining interfaces outside of their implementations.\\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\\nFor example, use `POST \/get_projects BODY { \"index\": \"name\" }` instead of `GET \/projects?index=name`.\\n### Negative Consequences\\n- The team needs to learn a new language.\\n- Follow-up decisions required for the actual implementation.\\n","tokens":100,"id":4644,"Prediction":"## Decision\n\nTo define and maintain the interfaces between the various components of the service, we will use a formal interface description language (IDL) to specify the inputs and outputs of each API. We will choose OpenAPI (Swagger) as our IDL of choice due to its widespread adoption, ease of use, and extensive tooling support.\n\n## Context\n\nThe current API implementations are scattered across multiple modules, leading to inconsistencies and technical difficulties in maintaining and evolving the service. We want to define and maintain the interfaces between the web API, web clients, workers, and databases using an IDL to ensure consistency, clarity, and maintainability.\n\n## Decision Rationale\n\n* OpenAPI (Swagger) is a widely adopted and well-established IDL that provides a simple and intuitive way to define and document APIs.\n* OpenAPI allows us to define the structure and behavior of the API, including request and response bodies, query parameters, headers, and authentication mechanisms.\n* OpenAPI provides extensive tooling support, including code generation, API documentation, and testing frameworks, making it easier to implement and maintain the API.\n* OpenAPI is a standard language, making it easier to share and reuse the interface definitions across different components and teams.\n\n## Consequences\n\n* We will need to refactor the existing API implementations to conform to the OpenAPI definition.\n* We will need to maintain the OpenAPI definition in a centralized location, such as a version control system, to ensure consistency across the service.\n* We will need to update the API documentation and testing frameworks to reflect the new OpenAPI definition.\n* We will need to train the development team on the use of OpenAPI and its associated tooling.\n\n## Status\n\nThis decision is approved and will be implemented as part of the next major release of the service.","GenTime":"2024-08-10 23:02:21"}
{"File Name":"monocle\/0008-unify-idents-across-code-review-systems.md","Context":"## Context and Problem Statement\\nHow do we merge contributor identities across code review systems ?\\n## Decision Drivers\\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\n","Decision":"* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\nChosen option: \"[option 2]\", Simpler implementation, better query performance (no complexification of queries) and no potential metrics errorneous results returned compared\\nto \"[option 1]\".\\nHere is a configuration example.\\n```\\nidents:\\n- ident: John Doe\\naliases:\\n- github.com\/john-doe\\n- review.opendev.org\/John Doe\/12345\\n```\\n### Negative Consequences\\n* A tool need to crawl the whole index to discover objects to update. However this\\noperation should not happen often.\\n","tokens":47,"id":4645,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Merging contributor identities across code review systems will be achieved by utilizing a third-party identity resolution service, rather than implementing a custom solution.\n\n**Context and Problem Statement:** How do we merge contributor identities across code review systems?\n\n**Decision Drivers:**\n\n* Do not complexify the EL query system: Implementing a custom solution would require significant changes to the existing EL query system, which could lead to increased complexity and maintenance costs.\n* Identities can be configured via the Monocle configuration file: A third-party identity resolution service can be easily integrated with the Monocle configuration file, allowing for seamless configuration and management of contributor identities.\n\n**Rationale:** By utilizing a third-party identity resolution service, we can avoid the complexity of implementing a custom solution and instead leverage the expertise and scalability of a dedicated identity resolution provider. This approach also allows for easier configuration and management of contributor identities through the Monocle configuration file.\n\n**Consequences:**\n\n* Increased scalability and reliability: The third-party identity resolution service is designed to handle large volumes of data and can provide a more reliable solution for merging contributor identities.\n* Simplified implementation: The integration with the Monocle configuration file will be straightforward, reducing the complexity of the implementation.\n* Potential for improved accuracy: The third-party identity resolution service can utilize advanced algorithms and data sources to improve the accuracy of contributor identity merging.\n\n**Alternatives Considered:**\n\n* Implementing a custom solution using EL queries: This approach would require significant changes to the existing EL query system and could lead to increased complexity and maintenance costs.\n* Using a homegrown identity resolution service: This approach would require significant development effort and could lead to a solution that is not scalable or reliable.\n\n**Status:** Accepted","GenTime":"2024-08-10 23:02:40"}
{"File Name":"ibc-rs\/adr-005-relayer-v0-implementation.md","Context":"## Context\\nThis ADR documents the implementation of the `v0.1` [relayer lib crate]\\n[ibc-relayer].\\nThis library is instantiated in the [Hermes][hermes] binary of the\\n[ibc-relayer-cli crate][ibc-relayer-cli] (which is not the focus of this discussion).\\nAs a main design goal, `v0.1` is meant to lay a foundation upon which we can\\nadd more features and enhancements incrementally with later relayer versions.\\nThis is to say that `v0.1` may be deficient in terms of features or\\nrobustness, and rather aims to be simple, adaptable, and extensible.\\nFor this reason, we primarily discuss aspects of concurrency and architecture.\\n### Relayer versioning scheme\\nOn the mid-term, the relayer architecture is set out to evolve across three\\nversions.\\nThe first of these, `v0.1`, makes several simplifying assumptions\\nabout the environment of the relayer and its features. These assumptions\\nare important towards limiting the scope that `v0.1` aims to\\ncover, and allowing a focus on the architecture and concurrency model to\\nprovide for growth in the future.\\nThese assumptions are documented below in the [decision](#decision) section.\\n","Decision":"### Configuration\\nFor the most part, the relayer configuration will be\\nstatic: the configuration for chains and their respective objects (clients,\\nconnections, or channels) will be fully specified in the relayer\\nconfiguration file and will not change throughout execution.\\nLight clients are also statically defined in the config file, and cannot be\\nswitched dynamically at runtime.\\nRecent changes to the ICS protocol specifies identifier\\nselection for clients, connections, and channels to be [deterministic][ids].\\nFor this reason, we will not need to specify any identifiers in the\\nconfiguration file.\\nWe only specify which pairs of chains should communicate with one\\nanother, and the port identifier to use for that purpose.\\nThis pair of chains plus their corresponding port identifiers is called a\\n__relaying path__.\\nAny relaying path is unidirectional.\\nAn example with the relevant section of the configuration file follows.\\n```toml\\n[[connections]]\\na_chain = 'ibc-0'\\nb_chain = 'ibc-1'\\n[[connections.paths]]\\na_port = 'transfer'\\nb_port = 'transfer'\\n```\\nHere there are two chains, ith one connection between them, and a path for\\nrelaying on the port called `transfer` on both chains, from chain `ibc-0`\\nto `ibc-1`.\\n### Links\\nA [link][link] is a relayer-level protocol that implements packet relay across\\none relaying path.\\nThe relayer at `v0.1` will focus on a single link.\\nThis limitation will be lifted in subsequent versions.\\n### Chain State\\nEach chain is assumed to start with an empty IBC state.\\nThis means that the relayer will take care of creating the client,\\nconnection, and channel objects respectively on each side of a link.\\n### Proof Verification\\nThe `v0.1` relayer will _not_ do proof verification.\\n### Feature set\\nThe [complete list of features is documented elsewhere][features] in detail.\\n","tokens":275,"id":4650,"Prediction":"Here is the Architectural Decision Record (ADR) corresponding to the provided context:\n\n## Decision\n\n**Title:** Simplicity and Extensibility in `v0.1` Relayer Lib Crate\n\n**Context:** The `v0.1` relayer lib crate is designed to lay a foundation for incremental feature additions and enhancements in future relayer versions. As such, it prioritizes simplicity, adaptability, and extensibility over comprehensive feature sets and robustness.\n\n**Decision:** The `v0.1` relayer lib crate will make the following simplifying assumptions about its environment and features:\n\n1. **Simplified concurrency model**: The `v0.1` relayer lib crate will use a single-threaded model, focusing on synchronization and locking mechanisms to ensure thread-safety.\n2. **Limited feature set**: The `v0.1` relayer lib crate will only implement the most essential features, deferring more advanced or complex features to future versions.\n3. **Assumptions about the environment**: The `v0.1` relayer lib crate will assume a specific environment, such as a single-instance deployment or a specific configuration, to simplify its architecture and concurrency model.\n\n**Rationale:** By making these simplifying assumptions, the `v0.1` relayer lib crate can focus on establishing a solid foundation for future development, rather than trying to cover every possible scenario or feature. This approach allows for a more incremental and iterative development process, enabling the addition of new features and enhancements in future versions.\n\n**Status:** This decision is currently in effect for the `v0.1` relayer lib crate. Future versions may revisit and refine this decision based on feedback and changing requirements.\n\n**Changelog:** This decision will be tracked in the changelog for future reference.","GenTime":"2024-08-10 23:04:20"}
{"File Name":"ibc-rs\/adr-003-handler-implementation.md","Context":"## Context\\nIn this ADR, we provide recommendations for implementing the IBC\\nhandlers within the `ibc` (modules) crate.\\n","Decision":"Concepts are introduced in the order given by a topological sort of their dependencies on each other.\\n### Events\\nIBC handlers must be able to emit events which will then be broadcasted via the node's pub\/sub mechanism,\\nand eventually picked up by the IBC relayer.\\nAn event has an arbitrary structure, depending on the handler that produces it.\\nHere is the [list of all IBC-related events][events], as seen by the relayer.\\nNote that the consumer of these events in production would not be the relayer directly\\n(instead the consumer is the node\/SDK where the IBC module executes),\\nbut nevertheless handlers will reuse these event definitions.\\n[events]: https:\/\/github.com\/informalsystems\/hermes\/blob\/bf84a73ef7b3d5e9a434c9af96165997382dcc9d\/modules\/src\/events.rs#L15-L43\\n```rust\\npub enum IBCEvent {\\nNewBlock(NewBlock),\\nCreateClient(ClientEvents::CreateClient),\\nUpdateClient(ClientEvents::UpdateClient),\\nClientMisbehavior(ClientEvents::ClientMisbehavior),\\nOpenInitConnection(ConnectionEvents::OpenInit),\\nOpenTryConnection(ConnectionEvents::OpenTry),\\n\/\/     ...\\n}\\n```\\n### Logging\\nIBC handlers must be able to log information for introspectability and ease of debugging.\\nA handler can output multiple log records, which are expressed as a pair of a status and a\\nlog line. The interface for emitting log records is described in the next section.\\n```rust\\npub enum LogStatus {\\nSuccess,\\nInfo,\\nWarning,\\nError,\\n}\\npub struct Log {\\nstatus: LogStatus,\\nbody: String,\\n}\\nimpl Log {\\nfn success(msg: impl Display) -> Self;\\nfn info(msg: impl Display) -> Self;\\nfn warning(msg: impl Display) -> Self;\\nfn error(msg: impl Display) -> Self;\\n}\\n```\\n### Handler output\\nIBC handlers must be able to return arbitrary data, together with events and log records, as described above.\\nAs a handler may fail, it is necessary to keep track of errors.\\nTo this end, we introduce a type for the return value of a handler:\\n```rust\\npub type HandlerResult<T, E> = Result<HandlerOutput<T>, E>;\\npub struct HandlerOutput<T> {\\npub result: T,\\npub log: Vec<Log>,\\npub events: Vec<Event>,\\n}\\n```\\nWe introduce a builder interface to be used within the handler implementation to incrementally build a `HandlerOutput` value.\\n```rust\\nimpl<T> HandlerOutput<T> {\\npub fn builder() -> HandlerOutputBuilder<T> {\\nHandlerOutputBuilder::new()\\n}\\n}\\npub struct HandlerOutputBuilder<T> {\\nlog: Vec<String>,\\nevents: Vec<Event>,\\nmarker: PhantomData<T>,\\n}\\nimpl<T> HandlerOutputBuilder<T> {\\npub fn log(&mut self, log: impl Into<Log>);\\npub fn emit(&mut self, event: impl Into<Event>);\\npub fn with_result(self, result: T) -> HandlerOutput<T>;\\n}\\n```\\nWe provide below an example usage of the builder API:\\n```rust\\nfn some_ibc_handler() -> HandlerResult<u64, Error> {\\nlet mut output = HandlerOutput::builder();\\n\/\/ ...\\noutput.log(Log::info(\"did something\"))\\n\/\/ ...\\noutput.log(Log::success(\"all good\"));\\noutput.emit(SomeEvent::AllGood);\\nOk(output.with_result(42));\\n}\\n```\\n### IBC Submodule\\nThe various IBC messages and their processing logic, as described in the IBC specification,\\nare split into a collection of submodules, each pertaining to a specific aspect of\\nthe IBC protocol, eg. client lifecycle management, connection lifecycle management,\\npacket relay, etc.\\nIn this section we propose a general approach to implement the handlers for a submodule.\\nAs a running example we will use a dummy submodule that deals with connections, which should not\\nbe mistaken for the actual ICS 003 Connection submodule.\\n#### Reader\\nA typical handler will need to read data from the chain state at the current height,\\nvia the private and provable stores.\\nTo avoid coupling between the handler interface and the store API, we introduce an interface\\nfor accessing this data. This interface, called a `Reader`, is shared between all handlers\\nin a submodule, as those typically access the same data.\\nHaving a high-level interface for this purpose helps avoiding coupling which makes\\nwriting unit tests for the handlers easier, as one does not need to provide a concrete\\nstore, or to mock one.\\n```rust\\npub trait ConnectionReader\\n{\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd>;\\n}\\n```\\nA production implementation of this `Reader` would hold references to both the private and provable\\nstore at the current height where the handler executes, but we omit the actual implementation as\\nthe store interfaces are yet to be defined, as is the general IBC top-level module machinery.\\nA mock implementation of the `ConnectionReader` trait could looks as follows:\\n```rust\\nstruct MockConnectionReader {\\nconnection_id: ConnectionId,\\nconnection_end: Option<ConnectionEnd>,\\nclient_reader: MockClientReader,\\n}\\nimpl ConnectionReader for MockConnectionReader {\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd> {\\nif connection_id == &self.connection_id {\\nself.connection_end.clone()\\n} else {\\nNone\\n}\\n}\\n}\\n```\\n#### Keeper\\nOnce a handler executes successfully, some data will typically need to be persisted in the chain state\\nvia the private\/provable store interfaces. In the same vein as for the reader defined in the previous section,\\na submodule should define a trait which provides operations to persist such data.\\nThe same considerations w.r.t. to coupling and unit-testing apply here as well.\\n```rust\\npub trait ConnectionKeeper {\\nfn store_connection(\\n&mut self,\\nclient_id: ConnectionId,\\nclient_type: ConnectionType,\\n) -> Result<(), Error>;\\nfn add_connection_to_client(\\n&mut self,\\nclient_id: ClientId,\\nconnection_id: ConnectionId,\\n) -> Result<(), Error>;\\n}\\n```\\n#### Submodule implementation\\nWe now come to the actual definition of a handler for a submodule.\\nWe recommend each handler to be defined within its own Rust module, named\\nafter the handler itself. For example, the \"Create Client\" handler of ICS 002 would\\nbe defined in `modules::ics02_client::handler::create_client`.\\n##### Message type\\nEach handler must define a datatype which represent the message it can process.\\n```rust\\npub struct MsgConnectionOpenInit {\\nconnection_id: ConnectionId,\\nclient_id: ClientId,\\ncounterparty: Counterparty,\\n}\\n```\\n##### Handler implementation\\nIn this section we provide guidelines for implementing an actual handler.\\nWe divide the handler in two parts: processing and persistence.\\n###### Processing\\nThe actual logic of the handler is expressed as a pure function, typically named\\n`process`, which takes as arguments a `Reader` and the corresponding message, and returns\\na `HandlerOutput<T, E>`, where `T` is a concrete datatype and `E` is an error type which defines\\nall potential errors yielded by the handlers of the current submodule.\\n```rust\\npub struct ConnectionMsgProcessingResult {\\nconnection_id: ConnectionId,\\nconnection_end: ConnectionEnd,\\n}\\n```\\nThe `process` function will typically read data via the `Reader`, perform checks and validation, construct new\\ndatatypes, emit log records and events, and eventually return some data together with objects to be persisted.\\nTo this end, this `process` function will create and manipulate a `HandlerOutput` value like described in\\nthe corresponding section.\\n```rust\\npub fn process(\\nreader: &dyn ConnectionReader,\\nmsg: MsgConnectionOpenInit,\\n) -> HandlerResult<ConnectionMsgProcessingResult, Error>\\n{\\nlet mut output = HandlerOutput::builder();\\nlet MsgConnectionOpenInit { connection_id, client_id, counterparty, } = msg;\\nif reader.connection_end(&connection_id).is_some() {\\nreturn Err(Kind::ConnectionAlreadyExists(connection_id).into());\\n}\\noutput.log(\"success: no connection state found\");\\nif reader.client_reader.client_state(&client_id).is_none() {\\nreturn Err(Kind::ClientForConnectionMissing(client_id).into());\\n}\\noutput.log(\"success: client found\");\\noutput.emit(IBCEvent::ConnectionOpenInit(connection_id.clone()));\\nOk(output.with_result(ConnectionMsgProcessingResult {\\nconnection_id,\\nclient_id,\\ncounterparty,\\n}))\\n}\\n```\\n###### Persistence\\nIf the `process` function specified above succeeds, the result value it yielded is then\\npassed to a function named `keep`, which is responsible for persisting the objects constructed\\nby the processing function. This `keep` function takes the submodule's `Keeper` and the result\\ntype defined above, and performs side-effecting calls to the keeper's methods to persist the result.\\nBelow is given an implementation of the `keep` function for the \"Create Connection\" handlers:\\n```rust\\npub fn keep(\\nkeeper: &mut dyn ConnectionKeeper,\\nresult: ConnectionMsgProcessingResult,\\n) -> Result<(), Error>\\n{\\nkeeper.store_connection(result.connection_id.clone(), result.connection_end)?;\\nkeeper.add_connection_to_client(result.client_id, result.connection_id)?;\\nOk(())\\n}\\n```\\n##### Submodule dispatcher\\n> This section is very much a work in progress, as further investigation into what\\n> a production-ready implementation of the `ctx` parameter of the top-level dispatcher\\n> is required. As such, implementers should feel free to disregard the recommendations\\n> below, and are encouraged to come up with amendments to this ADR to better capture\\n> the actual requirements.\\nEach submodule is responsible for dispatching the messages it is given to the appropriate\\nmessage processing function and, if successful, pass the resulting data to the persistence\\nfunction defined in the previous section.\\nTo this end, the submodule should define an enumeration of all messages, in order\\nfor the top-level submodule dispatcher to forward them to the appropriate processor.\\nSuch a definition for the ICS 003 Connection submodule is given below.\\n```rust\\npub enum ConnectionMsg {\\nConnectionOpenInit(MsgConnectionOpenInit),\\nConnectionOpenTry(MsgConnectionOpenTry),\\n...\\n}\\n```\\nThe actual implementation of a submodule dispatcher is quite straightforward and unlikely to vary\\nmuch in substance between submodules. We give an implementation for the ICS 003 Connection module below.\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: Msg) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ConnectionReader + ConnectionKeeper,\\n{\\nmatch msg {\\nMsg::ConnectionOpenInit(msg) => {\\nlet HandlerOutput {\\nresult,\\nlog,\\nevents,\\n} = connection_open_init::process(ctx, msg)?;\\nconnection::keep(ctx, result)?;\\nOk(HandlerOutput::builder()\\n.with_log(log)\\n.with_events(events)\\n.with_result(()))\\n}\\nMsg::ConnectionOpenTry(msg) => \/\/ omitted\\n}\\n}\\n```\\nIn essence, a top-level dispatcher is a function of a message wrapped in the enumeration introduced above,\\nand a \"context\" which implements both the `Reader` and `Keeper` interfaces.\\n### Dealing with chain-specific datatypes\\nThe ICS 002 Client submodule stands out from the other submodules as it needs\\nto deal with chain-specific datatypes, such as `Header`, `ClientState`, and\\n`ConsensusState`.\\nTo abstract over chain-specific datatypes, we introduce a trait which specifies\\nboth which types we need to abstract over, and their interface.\\nFor the ICS 002 Client submodule, this trait looks as follow:\\n```rust\\npub trait ClientDef {\\ntype Header: Header;\\ntype ClientState: ClientState;\\ntype ConsensusState: ConsensusState;\\n}\\n```\\nThe `ClientDef` trait specifies three datatypes, and their corresponding interface, which is provided\\nvia a trait defined in the same submodule.\\nA production implementation of this interface would instantiate these types with the concrete\\ntypes used by the chain, eg. Tendermint datatypes. Each concrete datatype must be provided\\nwith a `From` instance to lift it into its corresponding `Any...` enumeration.\\nFor the purpose of unit-testing, a mock implementation of the `ClientDef` trait could look as follows:\\n```rust\\nstruct MockHeader(u32);\\nimpl Header for MockHeader {\\n\/\/ omitted\\n}\\nimpl From<MockHeader> for AnyHeader {\\nfn from(mh: MockHeader) -> Self {\\nSelf::Mock(mh)\\n}\\n}\\nstruct MockClientState(u32);\\nimpl ClientState for MockClientState {\\n\/\/ omitted\\n}\\nimpl From<MockClientState> for AnyClientState {\\nfn from(mcs: MockClientState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockConsensusState(u32);\\nimpl ConsensusState for MockConsensusState {\\n\/\/ omitted\\n}\\nimpl From<MockConsensusState> for AnyConsensusState {\\nfn from(mcs: MockConsensusState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockClient;\\nimpl ClientDef for MockClient {\\ntype Header = MockHeader;\\ntype ClientState = MockClientState;\\ntype ConsensusState = MockConsensusState;\\n}\\n```\\nSince the actual type of client can only be determined at runtime, we cannot encode\\nthe type of client within the message itself.\\nBecause of some limitations of the Rust type system, namely the lack of proper support\\nfor existential types, it is currently impossible to define `Reader` and `Keeper` traits\\nwhich are agnostic to the actual type of client being used.\\nWe could alternatively model all chain-specific datatypes as boxed trait objects (`Box<dyn Trait>`),\\nbut this approach runs into a lot of limitations of trait objects, such as the inability to easily\\nrequire such trait objects to be Clonable, or Serializable, or to define an equality relation on them.\\nSome support for such functionality can be found in third-party libraries, but the overall experience\\nfor the developer is too subpar.\\nWe thus settle on a different strategy: lifting chain-specific data into an `enum` over all\\npossible chain types.\\nFor example, to model a chain-specific `Header` type, we would define an enumeration in the following\\nway:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] \/\/ TODO: Add Eq\\npub enum AnyHeader {\\nMock(mocks::MockHeader),\\nTendermint(tendermint::header::Header),\\n}\\nimpl Header for AnyHeader {\\nfn height(&self) -> Height {\\nmatch self {\\nSelf::Mock(header) => header.height(),\\nSelf::Tendermint(header) => header.height(),\\n}\\n}\\nfn client_type(&self) -> ClientType {\\nmatch self {\\nSelf::Mock(header) => header.client_type(),\\nSelf::Tendermint(header) => header.client_type(),\\n}\\n}\\n}\\n```\\nThis enumeration dispatches method calls to the underlying datatype at runtime, while\\nhiding the latter, and is thus akin to a proper existential type without running\\ninto any limitations of the Rust type system (`impl Header` bounds not being allowed\\neverywhere, `Header` not being able to be treated as a trait objects because of `Clone`,\\n`PartialEq` and `Serialize`, `Deserialize` bounds, etc.)\\nOther chain-specific datatypes, such as `ClientState` and `ConsensusState` require their own\\nenumeration over all possible implementations.\\nOn top of that, we also need to lift the specific client definitions (`ClientDef` instances),\\ninto their own enumeration, as follows:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Eq)]\\npub enum AnyClient {\\nMock(mocks::MockClient),\\nTendermint(tendermint::TendermintClient),\\n}\\nimpl ClientDef for AnyClient {\\ntype Header = AnyHeader;\\ntype ClientState = AnyClientState;\\ntype ConsensusState = AnyConsensusState;\\n}\\n```\\nMessages can now be defined generically over the `ClientDef` instance:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]\\npub struct MsgCreateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub client_type: ClientType,\\npub consensus_state: CD::ConsensusState,\\n}\\npub struct MsgUpdateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub header: CD::Header,\\n}\\n```\\nThe `Keeper` and `Reader` traits are defined for any client:\\n```rust\\npub trait ClientReader {\\nfn client_type(&self, client_id: &ClientId) -> Option<ClientType>;\\nfn client_state(&self, client_id: &ClientId) -> Option<AnyClientState>;\\nfn consensus_state(&self, client_id: &ClientId, height: Height) -> Option<AnyConsensusState>;\\n}\\npub trait ClientKeeper {\\nfn store_client_type(\\n&mut self,\\nclient_id: ClientId,\\nclient_type: ClientType,\\n) -> Result<(), Error>;\\nfn store_client_state(\\n&mut self,\\nclient_id: ClientId,\\nclient_state: AnyClientState,\\n) -> Result<(), Error>;\\nfn store_consensus_state(\\n&mut self,\\nclient_id: ClientId,\\nconsensus_state: AnyConsensusState,\\n) -> Result<(), Error>;\\n}\\n```\\nThis way, only one implementation of the `ClientReader` and `ClientKeeper` trait is required,\\nas it can delegate eg. the serialization of the underlying datatypes to the `Serialize` bound\\nof the `Any...` wrapper.\\nBoth the `process` and `keep` function are defined to take a message generic over\\nthe actual client type:\\n```rust\\npub fn process(\\nctx: &dyn ClientReader,\\nmsg: MsgCreateClient<AnyClient>,\\n) -> HandlerResult<CreateClientResult<AnyClient>, Error>;\\npub fn keep(\\nkeeper: &mut dyn ClientKeeper,\\nresult: CreateClientResult<AnyClient>,\\n) -> Result<(), Error>;\\n```\\nSame for the top-level dispatcher:\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: ClientMsg<AnyClient>) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ClientReader + ClientKeeper;\\n```\\nWith this boilerplate out of way, one can write tests using a mock client, and associated mock datatypes\\nin a fairly straightforward way, taking advantage of the `From` instance to lift concerete mock datatypes\\ninto the `Any...` enumeration:\\n```rust\\n#[test]\\nfn test_create_client_ok() {\\nlet client_id: ClientId = \"mockclient\".parse().unwrap();\\nlet reader = MockClientReader {\\nclient_id: client_id.clone(),\\nclient_type: None,\\nclient_state: None,\\nconsensus_state: None,\\n};\\nlet msg = MsgCreateClient {\\nclient_id,\\nclient_type: ClientType::Tendermint,\\nconsensus_state: MockConsensusState(42).into(), \/\/ lift into `AnyConsensusState`\\n};\\nlet output = process(&reader, msg.clone());\\nmatch output {\\nOk(HandlerOutput {\\nresult,\\nevents,\\nlog,\\n}) => {\\n\/\/ snip\\n}\\nErr(err) => {\\npanic!(\"unexpected error: {}\", err);\\n}\\n}\\n}\\n```\\n","tokens":30,"id":4656,"Prediction":"## Decision\n\n**Title:** Handling IBC Messages in the `ibc` Crate\n\n**Context:** In this ADR, we provide recommendations for implementing the IBC handlers within the `ibc` (modules) crate.\n\n**Problem:** The `ibc` crate needs to handle incoming IBC messages from other chains, and process them accordingly.\n\n**Decision:** We will implement the IBC handlers as separate modules within the `ibc` crate, each responsible for handling a specific type of IBC message. These modules will be designed to be modular, testable, and extensible.\n\n**Rationale:**\n\n* Separating the IBC handlers into separate modules will allow for easier maintenance and updates, as each module can be modified independently.\n* This approach will also enable us to add new IBC handlers in the future without affecting the existing codebase.\n* By designing the modules to be modular, testable, and extensible, we can ensure that the code is maintainable and scalable.\n\n**Consequences:**\n\n* The `ibc` crate will have a more modular architecture, making it easier to maintain and update.\n* The addition of new IBC handlers will require minimal changes to the existing codebase.\n* The code will be more testable and scalable, reducing the risk of errors and bugs.\n\n**Status:** Accepted\n\n**Implementors:** [List the team members or individuals responsible for implementing this decision]\n\n**References:**\n\n* [Link to relevant documentation or code snippets]\n\nNote: This is a sample ADR, you should adapt it to your specific use case and project requirements.","GenTime":"2024-08-10 23:06:33"}
{"File Name":"docs\/0012-winery-topology-modeler.md","Context":"## Context and Problem Statement\\nWe want to use a component to graphically model nodes and edges in a web application.\\n## Decision Drivers <!-- optional -->\\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\n","Decision":"* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\nChosen option: Choosen a Combination of ngx-graph and plain d3.js, because ngx-graph might miss some usefull features but has good usability which is worth to have a look at\\n### Positive Consequences <!-- optional -->\\n* easy to use graph modeler\\n* only a few dependencies\\n* full control over features and behavior\\n### Negative consequences <!-- optional -->\\n* needs to be implemented\\n","tokens":59,"id":4689,"Prediction":"## Decision\n\nAfter careful consideration of the requirements and constraints, we have decided to use the `ngx-graph` library to graphically model nodes and edges in our web application.\n\n## Context\n\nWe want to use a component to graphically model nodes and edges in a web application.\n\n## Decision Drivers\n\n* Usable with Angular: `ngx-graph` is specifically designed for use with Angular and provides a simple and intuitive API for creating graph components.\n* Number of dependencies: `ngx-graph` has a minimal number of dependencies, which reduces the risk of conflicts and makes it easier to maintain.\n* Features: `ngx-graph` provides the features we need, including the ability to create nodes and connections, and offers a range of customization options.\n* Usability: The library has a user-friendly API and provides a range of examples and demos to help with implementation.\n\n## Rationale\n\nWe considered other options, such as using a lower-level library like `d3.js` or a more high-level library like `vis.js`. However, `ngx-graph` offers the best balance of features, usability, and maintainability. Its specific design for use with Angular and its minimal dependencies make it the most suitable choice for our project.\n\n## Consequences\n\n* We will use `ngx-graph` to create the graph component in our web application.\n* We will need to learn the basics of `ngx-graph` and its API to effectively use it in our project.\n* We will need to consider the limitations of `ngx-graph` and plan for any additional features or customizations that may be required.\n\nBy using `ngx-graph`, we can quickly and easily create a graph component that meets our requirements and provides a good user experience.","GenTime":"2024-08-10 23:17:22"}
{"File Name":"docs\/0022-function-to-component-mapping.md","Context":"## Context and Problem Statement\\nTo implement the EAI patterns we use a combination of a generic component\\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\\n## Decision Drivers <!-- optional -->\\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\n","Decision":"* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\\n","tokens":231,"id":4694,"Prediction":"## Decision\n\nWe will use a Service Registry pattern to wire instances of the generic component with the FaaS functions. This decision is driven by the need for a solution that is:\n\n* Supported by the language\/technology used to implement the generic component\n* Easy to integrate into MICO\n* A well-known and proven solution\n\nA Service Registry is a centralized repository that stores information about available services, including their addresses and interfaces. In our case, the Service Registry will store information about the FaaS functions, including their URLs and function names. The generic component can then query the Service Registry to retrieve the necessary information to call the FaaS functions.\n\nThe Service Registry pattern is well-suited for our use case because it provides a flexible and scalable way to manage the relationships between the generic component and the FaaS functions. It also allows for easy discovery and integration of new FaaS functions, making it easy to add new EAI patterns to the system.\n\nSome popular Service Registry implementations include Apache ZooKeeper, etcd, and Consul. We will evaluate these options and choose the one that best fits our requirements.\n\n## Status\n\nProposed\n\n## Affected Components\n\n* Generic component\n* FaaS functions\n* Service Registry\n\n## Changelog\n\n* [Date] - Initial proposal\n* [Date] - Final decision and implementation plan","GenTime":"2024-08-10 23:19:01"}
{"File Name":"docs\/0018-requirements-application-composition.md","Context":"## Context and Problem Statement\\nWe want to have clear and simple requirements when it comes to the way applications can be created in the user interface.\\n## Decision Drivers\\n* MUST be compatible with Lombok\\n","Decision":"* MUST be compatible with Lombok\\nChosen option: the first option, since we want a simple solution, in order to have the system running as soon as possible.\\n### Positive Consequences\\n* Lombok can be used.\\n* Better code quality.\\n### Negative consequences\\n* Applications cannot be created using other existing applications.\\n","tokens":42,"id":4695,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context and problem statement:\n\n**Decision:** Use a builder pattern to create applications in the user interface.\n\n**Context:** We want to have clear and simple requirements when it comes to the way applications can be created in the user interface.\n\n**Problem Statement:** We need to ensure that the application creation process is straightforward and easy to understand, while also being compatible with Lombok.\n\n**Decision Drivers:**\n\n* MUST be compatible with Lombok\n\n**Decision:** We have decided to use a builder pattern to create applications in the user interface. This pattern provides a clear and simple way to create objects step-by-step, which aligns with our goal of having clear and simple requirements. Additionally, the builder pattern is compatible with Lombok, which is a crucial requirement for our project.\n\n**Rationale:** The builder pattern allows us to create objects by step-by-step construction, which makes the process more intuitive and easier to understand. This pattern also provides a clear separation of concerns, which makes it easier to maintain and extend the code. Furthermore, the builder pattern is widely used and well-understood, which makes it easier for developers to learn and use.\n\n**Consequences:**\n\n* The user interface will be more intuitive and easier to use, as the application creation process will be more straightforward.\n* The code will be more maintainable and easier to extend, as the builder pattern provides a clear separation of concerns.\n* The compatibility with Lombok ensures that we can use Lombok's features to simplify our code and reduce boilerplate.\n\n**References:**\n\n* [Builder Pattern](https:\/\/en.wikipedia.org\/wiki\/Builder_pattern)\n* [Lombok](https:\/\/projectlombok.org\/)\n\n**Status:** Approved","GenTime":"2024-08-10 23:19:20"}
{"File Name":"openjdk-api-v3\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4699,"Prediction":"Here is a sample Architectural Decision Record (ADR) for the provided context:\n\n**Decision**\n\nUse a Microservices Architecture for the Project\n\n**Context**\n\nAs the project grows in complexity and size, we need to consider a scalable and maintainable architecture to ensure the system can adapt to changing requirements and user needs. We have evaluated different architecture options, including Monolithic, Service-Oriented Architecture (SOA), and Microservices. After careful consideration, we have decided to adopt a Microservices Architecture for the project.\n\n**Problem**\n\nThe current monolithic architecture is becoming increasingly difficult to maintain and scale. As the system grows, it is becoming harder to identify and isolate issues, leading to decreased performance and increased risk of system failure. Additionally, the monolithic architecture limits our ability to develop and deploy new features independently, which can lead to delays and decreased customer satisfaction.\n\n**Reasoning**\n\nA Microservices Architecture provides several benefits that address the problems mentioned above. By breaking down the system into smaller, independent services, we can:\n\n* Improve scalability and fault tolerance by allowing each service to be scaled and deployed independently\n* Increase maintainability by allowing developers to work on individual services without affecting the entire system\n* Enhance flexibility by enabling the development and deployment of new features independently\n* Reduce the risk of system failure by allowing each service to be isolated and restarted independently\n\n**Decision Criteria**\n\nWe considered the following criteria when making this decision:\n\n* Scalability: Microservices Architecture allows for easier scaling of individual services, which can improve overall system performance and responsiveness.\n* Maintainability: Breaking down the system into smaller services makes it easier to identify and fix issues, which can reduce downtime and improve overall system reliability.\n* Flexibility: Microservices Architecture enables the development and deployment of new features independently, which can improve time-to-market and customer satisfaction.\n* Risk: Microservices Architecture reduces the risk of system failure by allowing each service to be isolated and restarted independently.\n\n**Consequences**\n\nThe consequences of this decision include:\n\n* Increased complexity: Microservices Architecture can add complexity to the system, which can make it harder to develop and maintain.\n* Increased communication overhead: Microservices Architecture requires more communication between services, which can add overhead and complexity to the system.\n* Potential for service coupling: Microservices Architecture requires careful design to ensure that services are loosely coupled and can operate independently, which can be challenging.\n\n**Pros and Cons**\n\nPros:\n\n* Improved scalability and fault tolerance\n* Increased maintainability and flexibility\n* Reduced risk of system failure\n\nCons:\n\n* Increased complexity\n* Increased communication overhead\n* Potential for service coupling\n\n**Future Development**\n\nAs we move forward with the Microservices Architecture, we will need to:\n\n* Develop a clear service boundary definition to ensure that services are loosely coupled and can operate independently\n* Implement a communication mechanism between services, such as RESTful APIs or message queues\n* Develop a strategy for service discovery and registration\n* Monitor and analyze the system's performance and scalability to identify areas for improvement\n\nBy adopting a Microservices Architecture, we can build a scalable, maintainable, and flexible system that meets the needs of our users and stakeholders.","GenTime":"2024-08-10 23:20:43"}
{"File Name":"cloud-on-k8s\/adr-template.md","Context":"## Context and Problem Statement\\n[Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best].\\n### Positive Consequences <!-- optional -->\\n* [For example, improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [For example, compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":96,"id":4701,"Prediction":"## Context and Problem Statement\n\nIn our e-commerce platform, we are facing a scalability issue with our payment processing system. The current system is not able to handle a large number of concurrent payment requests, resulting in delayed order processing and a negative user experience. We need to decide on a solution to improve the scalability of our payment processing system.\n\n## Decision Drivers\n\n* Scalability: The system needs to be able to handle a large number of concurrent payment requests without a significant decrease in performance.\n* Security: The payment processing system requires high levels of security to protect sensitive customer data.\n* Cost: The chosen solution should be cost-effective and not add significant overhead to the system.\n* Integration: The solution should be easily integratable with our existing e-commerce platform.","GenTime":"2024-08-10 23:21:14"}
{"File Name":"cloud-on-k8s\/0006-sidecar-health.md","Context":"## Context and Problem Statement\\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\\nAs for now an Elasticsearch pod is composed of 2 containers:\\n- a main container for Elasticsearch\\n- a sidecar container for running the keystore-updater\\nWhat is the keystore-updater in the sidecar doing?\\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\\nTo connect to ES it depends on:\\n- an environment variable for the username\\n- secrets mounted as readonly files for the password and the CA certificate\\n- the Elasticsearch readiness\\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\\ncan impact the Elasticsearch availability by design.\\nHowever Go binaries that do simple things are very fast to start and very reliable.\\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\\ncompared to have an Elasticsearch failure.\\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\\ndo not report errors during this period. Then how to detect that ES has never started?\\nThe ES readiness probe will fail if ES never becomes ready.\\n## Decision Drivers\\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\n","Decision":"* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\\n### Positive Consequences\\n* Collecting the sidecar health from the operator side gives us more options to react to failures\\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\\n* Does not depend on the Kubernetes probes or the Kubelet\\n* Minimize external dependencies\\n### Negative Consequences\\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\\n* Add complexity and responsibility to the operator\\n","tokens":455,"id":4705,"Prediction":"## Decision\n\nAfter careful consideration of the context and problem statement, I propose the following decision:\n\n**Decision:** Implement a sidecar health check that periodically calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to verify the sidecar's ability to connect to Elasticsearch and successfully decrypt and re-read the keystore used by the snapshotter job. This health check will be implemented as a separate Go binary that runs in the sidecar container and reports its status to the cluster monitoring system.\n\n**Rationale:**\n\n1. **Error distinction:** By implementing a separate health check for the sidecar, we can easily identify sidecar failures from Elasticsearch failures.\n2. **Error side effect:** Since the sidecar is designed to be fast to start and reliable, the probability of sidecar failure is low. Moreover, we can introduce a start-up delay to account for expected sidecar errors when Elasticsearch is not ready yet.\n3. **Promote reliability and simplicity:** By implementing a dedicated health check, we promote reliability and simplicity by reducing the complexity of the main Elasticsearch container and making it easier to monitor and alert on sidecar failures.\n\n**Implementation:**\n\n1. Create a new Go binary that runs in the sidecar container and implements the health check.\n2. The health check will periodically call the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to verify the sidecar's ability to connect to Elasticsearch and successfully decrypt and re-read the keystore used by the snapshotter job.\n3. The health check will report its status to the cluster monitoring system, allowing for easy identification of sidecar failures.\n4. Introduce a start-up delay to account for expected sidecar errors when Elasticsearch is not ready yet.\n\n**Monitoring and Alerting:**\n\n1. Monitor the sidecar health check status to detect any failures or errors.\n2. Set up alerts for sidecar failures, allowing for prompt notification and investigation of the issue.\n3. Integrate the sidecar health check status with the cluster monitoring system to provide a comprehensive view of the cluster's health.\n\nBy implementing this decision, we can ensure that the sidecar is properly monitored and alerted on, reducing the risk of undetected failures and improving the overall reliability and availability of the Elasticsearch cluster.","GenTime":"2024-08-10 23:22:28"}
{"File Name":"cloud-on-k8s\/0005-configurable-operator.md","Context":"## Context and Problem Statement\\nMost operators out there operate in one of these two modes:\\n1. Cluster-wide operator. Can manage resources in all namespaces, with cluster-wide admin privilege. A single operator running on the cluster.\\n2. Namespaced operator. Can manage resources in the namespace it's deployed in, with admin permissions in that namespace. Several operators can be running in different namespaces.\\nThe first option (cluster-wide single operator) has some major drawbacks:\\n* it does not scale well with the number of clusters\\n* it requires elevated permissions on the cluster\\nThe second option (namespace operator) also has some major drawbacks:\\n* it does not play well with cross-namespace features (a single enterprise license pool for multiple clusters in multiple namespaces, cross-cluster search and replication on clusters across namespaces)\\n* to deploy 5 clusters in 5 different namespaces, it requires 5 operators running. A single one could have been technically enough.\\n## Decision Drivers\\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\n","Decision":"* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\nChosen option: option 2 (configurable operator), because it gives us more flexibility on the deployment strategy, and allows restricting RBAC permissions to a finer-grained level.\\n### Positive Consequences\\n* Much more flexibility to cover various deployment scenarios\\n* a single cluster-wide operator\\n* one operator per namespace\\n* one operator for all production namespaces, another one for all staging namespaces\\n* and so on\\n* We don't have to require cluster-level permissions to handle enterprise licensing\\n* A single operator concept, no namespace\/global\/ecosystem vocabulary madness\\n### Negative Consequences\\n* Too many options can lead to confusion, we need proper documentation\\n* Increased yaml complexity: need to develop a tool to generate yaml specifications\\n* The controller-runtime is not ready yet for multi-namespace watches\\n","tokens":354,"id":4707,"Prediction":"## Decision\n\nWe will implement a hybrid operator architecture that combines the benefits of both cluster-wide and namespaced operators. This will allow us to scale down to single-cluster deployments while also providing the ability to manage tens of thousands of clusters simultaneously.\n\n## Context and Problem Statement\n\nThe provided context highlights the challenges of implementing a scalable and secure operator architecture for managing Elasticsearch clusters. The two existing options, cluster-wide and namespaced operators, have significant drawbacks. The cluster-wide operator does not scale well and requires elevated permissions, while the namespaced operator does not play well with cross-namespace features and requires multiple operators to be deployed.\n\n## Decision Drivers\n\nThe decision drivers emphasize the importance of scalability, both up and down, security, and adherence to the principle of least privilege. The solution should also ensure that the responsiveness of one cluster does not negatively affect the operations of other clusters.\n\n## Decision Description\n\nWe will implement a hybrid operator architecture that consists of a central, cluster-wide operator that manages the overall configuration and licensing for the Elasticsearch clusters. This operator will be responsible for handling cross-namespace features and will have elevated permissions to manage the cluster-wide configuration.\n\nIn addition to the central operator, we will also implement a namespaced operator that will be responsible for managing the Elasticsearch clusters within a specific namespace. This operator will have limited permissions and will only be able to manage the resources within the namespace it is deployed in.\n\nThis hybrid architecture will provide the benefits of both cluster-wide and namespaced operators, allowing us to scale down to single-cluster deployments while also providing the ability to manage tens of thousands of clusters simultaneously. The central operator will handle the complex, cluster-wide configuration and licensing, while the namespaced operators will focus on managing the individual clusters within their respective namespaces.\n\n## Trade-Offs\n\nThe hybrid operator architecture will require more complex implementation and management compared to a single, cluster-wide operator. However, it will provide the benefits of scalability, security, and adherence to the principle of least privilege.\n\n## Rationale\n\nThe hybrid operator architecture is the best solution because it addresses the scalability, security, and complexity concerns while providing a scalable and manageable solution for managing Elasticsearch clusters. It also provides a clear story around credentials, RBAC permissions, and service accounts, ensuring that the solution is easy to understand and maintain.","GenTime":"2024-08-10 23:22:54"}
{"File Name":"latis3\/0005-Operations-defined-in-FDML-schema-as-elements.md","Context":"## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"\/>\\n<\/xs:complexType>\\n<\/xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n","Decision":"* consistency\\n* expressiveness\\nChosen option: Try to describe operations as elements only\\n### Positive Consequences\\n* consistency\\n### Negative Consequences\\n* future operations may not be definable\\n","tokens":115,"id":4713,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Use XML Schema's `xs:element` with `xs:complexType` and `xs:attribute` to define operations in FDML, allowing for both element-only and element-attribute-based definitions.\n\n**Context:** Operations can be defined in FDML as elements and attributes or simply as elements.\n\n**Problem Statement:** The FDML needs a consistent and expressive way to define operations, considering the examples provided.\n\n**Decision Drivers:**\n\n* Consistency: Using XML Schema's `xs:element` with `xs:complexType` and `xs:attribute` ensures consistency in defining operations across the FDML.\n* Expressiveness: This approach allows for both element-only and element-attribute-based definitions, providing a flexible and expressive way to describe operations.\n\n**Rationale:** By using XML Schema's built-in features, we can leverage its strengths in defining complex data structures and provide a consistent and expressive way to define operations in FDML. This approach also enables us to take advantage of XML Schema's validation capabilities, ensuring that the defined operations are valid and consistent.\n\n**Consequences:**\n\n* Pros:\n\t+ Consistent and expressive way to define operations\n\t+ Flexibility in defining operations as elements or elements with attributes\n\t+ Ability to leverage XML Schema's validation capabilities\n* Cons:\n\t+ May require additional complexity in the FDML schema definition\n\t+ May require additional processing to handle the complexity of XML Schema\n\n**Status:** Accepted\n\n**Implementation:** The FDML schema will be updated to use XML Schema's `xs:element` with `xs:complexType` and `xs:attribute` to define operations.","GenTime":"2024-08-10 23:24:33"}
{"File Name":"latis3\/0001-Scala-programming-language.md","Context":"## Context and Problem Statement\\nThe hybrid object-oriented\/functional language Scala is ideally suited for developing LaTiS.\\n## Decision Drivers <!-- optional -->\\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\n","Decision":"* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\nChosen option: Scala is the language-of-choice for LaTiS development.  The only decision driver above that is not in alignment with this decision is the fact that Python is the preferred language for most scientific programmers.\\n### Positive Consequences <!-- optional -->\\n* immutable data types in Scala reduce the challenges of introducing parallelism\\n* for an extensible library like LaTiS a functional programming language like Scala provides better abstractions and tools to reason about transformations of large data sets\\n* a strongly typed language like Scala helps to prevent code rot and reduces the chance that defects will slip into the code base\\n* well designed Scala applications are based on sound software engineering principles even though they may take more effort to build than similar Java or Python programs\\n* that being said, Scala programs tend to be easier to maintain and refactor which actually minimizes effort over the life of a project\\n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use cases\\n* Apache Spark, written in Scala, is the leading framework for processing large amounts of data and is becoming on of the primary framework for implementing machine learning algorithms\\n* Scala supports the creation of DSLs which will allow users of LaTiS to specify processing instructions for datasets without themselves nedding to learn Scala\\n* the full gamut of Java libraies is available to Scala developers\\n* many successful Java libraries eventually get re-written in Scala\\n* Scala encourages software craft-people to follow sound engineering principles instead of just developing something that just gets the job done, this is probably the most important factor in choosing Scala, but the hardest to document\\n### Negative Consequences\\n* Scala developers are not as common as Python developers, especially in the field of scientific computing\\n* Python is clearly the dominant language used by scientific programmers\\n","tokens":97,"id":4716,"Prediction":"## Decision\n\nAfter careful consideration of the context and problem statement, we have decided to adopt a functional programming paradigm with strong typing for the development of LaTiS using Scala. This decision is driven by the following key factors:\n\n* Merits of functional programming languages: Scala's strong support for functional programming concepts, such as immutability, recursion, and higher-order functions, will enable us to write more concise, composable, and predictable code.\n* Merits of strongly typed languages: Scala's static type system will ensure that our code is robust, maintainable, and free from runtime type errors, allowing us to catch errors early and avoid costly debugging.\n* Availability of developers: Scala is a widely used and well-established language, making it easier to find and hire developers with relevant skills.\n* Consensus of community of scientific programmers: The Scala community is well-established and has a strong following among scientific programmers, ensuring that we can leverage existing knowledge and resources.\n* Suitability for processing large data sets: Scala's ability to handle large data sets and its integration with popular data processing libraries such as Apache Spark and Apache Flink make it an ideal choice for processing large datasets.\n* Concepts involving code correctness: Scala's strong type system and functional programming paradigm will enable us to write code that is easier to reason about and more likely to be correct.\n* Transformations to data are explicit and can be reasoned about: Scala's functional programming paradigm and strong type system will ensure that data transformations are explicit and can be reasoned about, making it easier to maintain and debug our code.\n* Support for DSLs (Domain Specific Languages): Scala's ability to create DSLs will enable us to create domain-specific languages that are tailored to the specific needs of LaTiS, making our code more expressive and easier to maintain.\n\nBy adopting a functional programming paradigm with strong typing, we can create a robust, maintainable, and efficient implementation of LaTiS that is well-suited to the needs of scientific programmers and the processing of large data sets.","GenTime":"2024-08-10 23:25:35"}
{"File Name":"jskatas.org\/000-use-adrs.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4722,"Prediction":"Here is a sample Architectural Decision Record (ADR) for the provided context:\n\n## Decision\n\n**ADR-001: Choosing a Frontend Framework for the Project**\n\n## Context\n\nAs we are building a web-based application, we need to decide on a frontend framework to use for the project. The framework should be able to handle complex user interfaces, provide a good development experience, and be maintainable in the long run.\n\n## Problem\n\nChoosing a frontend framework is a crucial decision that will impact the development speed, maintainability, and scalability of the project. The team needs to consider factors such as the framework's popularity, ease of use, and the availability of resources (e.g., documentation, community support).\n\n## Decision\n\nAfter considering the options, we have decided to use React as the frontend framework for the project. React is a popular and widely-used framework that provides a robust set of features for building complex user interfaces. Its virtual DOM approach ensures efficient rendering and updates, making it suitable for large-scale applications.\n\n## Rationale\n\nWe chose React for the following reasons:\n\n1. **Popularity**: React is one of the most widely-used frontend frameworks, with a large and active community of developers.\n2. **Ease of use**: React has a relatively low learning curve, making it accessible to developers with varying levels of experience.\n3. **Scalability**: React's virtual DOM approach ensures efficient rendering and updates, making it suitable for large-scale applications.\n4. **Resources**: React has an extensive ecosystem of libraries and tools, including Redux for state management and React Router for client-side routing.\n\n## Consequences\n\nBy choosing React as the frontend framework, we will:\n\n1. **Improve development speed**: React's popularity and extensive documentation will enable the team to develop features quickly and efficiently.\n2. **Enhance maintainability**: React's modular architecture and robust set of features will make it easier to maintain and update the application over time.\n3. **Increase scalability**: React's virtual DOM approach will ensure efficient rendering and updates, allowing the application to handle a large number of users and complex user interfaces.\n\n## History\n\n* 2023-02-15: Decision made to use React as the frontend framework.\n* 2023-03-01: Initial implementation of React in the project.\n* 2023-04-15: Review and refinement of the React implementation.\n\nThis ADR provides a record of the decision, the context, the problem, the decision, the rationale, the consequences, and the history of the decision. It serves as a reference point for future development and provides transparency and accountability within the team.","GenTime":"2024-08-10 23:27:36"}
{"File Name":"biosamples-v4\/0004-inverse-relationships-sample-update.md","Context":"## Context\\nWhen a relation is created between two samples, the sample target of the relationship doesn't change the update date.\\nStated in another way, the relationship inversion process doesn't change the update date of the sample.\\nThis is associated with BSD-1088 - https:\/\/www.ebi.ac.uk\/panda\/jira\/browse\/BSD-1088\\n","Decision":"We decided that is good for the relationship inversion process to not change the update date of the sample as we don't\\nhave any actual usecase for this to happen and it would also create issues for NCBI as they don't care about relationships\\nand no real information is added to the sample\\n","tokens":74,"id":4726,"Prediction":"## Decision\n\n**Decision:** Update the sample target's update date only when the relationship inversion process changes the sample target's attributes, not when the relationship is created or updated.\n\n**Context:** When a relation is created between two samples, the sample target of the relationship doesn't change the update date. This is associated with BSD-1088.\n\n**Rationale:** The update date of the sample target should only be updated when the sample target's attributes are modified, not when a relationship is created or updated. This ensures that the update date reflects the actual changes made to the sample target, rather than being updated unnecessarily when a relationship is created or updated.\n\n**Consequences:**\n\n* The update date of the sample target will only be updated when the sample target's attributes are modified, which reduces unnecessary updates and improves data consistency.\n* The relationship inversion process will not update the update date of the sample target when the relationship is created or updated, which aligns with the expected behavior.\n\n**Alternatives Considered:**\n\n* Updating the update date of the sample target every time a relationship is created or updated, regardless of whether the sample target's attributes are modified. However, this would result in unnecessary updates and could lead to inconsistencies in the data.\n\n**Status:** Approved\n\n**Implementation:** The update date of the sample target will be updated only when the sample target's attributes are modified, and not when a relationship is created or updated. This will be implemented in the relationship inversion process to ensure that the update date is updated only when necessary.","GenTime":"2024-08-10 23:28:47"}
{"File Name":"jabref\/0015-support-an-abstract-query-syntax-for-query-conversion.md","Context":"## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n","Decision":"Chosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.\\nFurthermore parsers for lucene already exist and are tested.\\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\\n* All terms in the query are whitespace separated and will be ANDed\\n* Default and certain fielded terms are supported\\n* Fielded Terms:\\n* `author`\\n* `title`\\n* `journal`\\n* `year` (for single year)\\n* `year-range` (for range e.g. `year-range:2012-2015`)\\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\\n* The `year` and `year-range` fields are mutually exclusive\\n* Example:\\n* `author:\"Igor Steinmacher\" author:\"Christoph Treude\" year:2017` will be converted to\\n* `author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" AND year:2017`\\nThe supported syntax can be expressed in EBNF as follows:\\nQuery := {Clause} \\\\nClause:= \\[Field\\] Term \\\\nField := author: | title: | journal: | year: | year-range: | default:\\\\nTerm  := Word | Phrase \\\\nWord can be derived to any series of non-whitespace characters.\\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\\nsuch a query does not make sense semantically and therefore will not be executed.\\n### Positive Consequences\\n* Already tested\\n* Well known\\n* Easy to implement\\n* Can use an existing parser\\n","tokens":84,"id":4738,"Prediction":"## Decision\n\n**Decision:** We will use a variant of the Elasticsearch Query DSL (Domain Specific Language) as the abstract query syntax for our system. This syntax will be used to create an Abstract Syntax Tree (AST) that can be mapped to the query syntax of each library.\n\n**Rationale:**\n\n* The Elasticsearch Query DSL is a widely used and well-established query syntax that is designed to be flexible and expressive.\n* It has a large community of users and developers, which means there are many resources available for learning and troubleshooting.\n* The syntax is designed to be easy to read and write, making it a good choice for users who are not experts in query languages.\n* The syntax is also extensible, which means we can add support for new features and operators as needed.\n* By using a variant of the Elasticsearch Query DSL, we can leverage the existing knowledge and expertise of our users and developers.\n\n**Features:**\n\n* Support for boolean operators (AND, OR, NOT)\n* Support for field-based queries (e.g. `title:hello`, `author:john`)\n* Support for range queries (e.g. `publishedDate: [2020-01-01 TO 2020-12-31]`)\n* Support for fuzzy matching (e.g. `title:hello~`)\n* Support for wildcard matching (e.g. `title:*hello*`)\n* Support for grouping and nesting queries (e.g. `(title:hello AND author:john) OR (title:goodbye AND author:jane)`)\n\n**Alternatives Considered:**\n\n* We considered using a query syntax based on the Lucene query parser, but ultimately decided against it because it is less widely used and has fewer resources available.\n* We also considered using a custom query syntax, but decided against it because it would require more development and maintenance effort, and would likely be less intuitive for users.\n\n**Conclusion:**\n\nBy using a variant of the Elasticsearch Query DSL as our abstract query syntax, we can provide a flexible and expressive way for users to formulate their search queries. This will make it easier for users to find the information they need, and will reduce the complexity and overhead of integrating with multiple libraries.","GenTime":"2024-08-10 23:33:23"}
{"File Name":"jabref\/0021-keep-study-as-a-dto.md","Context":"## Context and Problem Statement\\nThe study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\\nThis poses the question: should the study remain a pure DTO object or should it contain direct object instances?\\n","Decision":"Chosen option: \"Keep study as DTO and use transformators\", because comes out best (see below).\\n","tokens":49,"id":4741,"Prediction":"## Decision\n\n**Decision:** The study will remain a pure DTO (Data Transfer Object) object, and not contain direct object instances.\n\n**Context:** The study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\n\n**Problem Statement:** The question is whether the study should remain a pure DTO object or should it contain direct object instances.\n\n**Rationale:**\n\n* Keeping the study as a pure DTO object maintains the separation of concerns between the data layer and the business logic layer.\n* This approach ensures that the study remains a simple, lightweight object that only holds data, without any complex logic or dependencies.\n* It also allows for easier testing and mocking of the study object, as it is not coupled to any specific implementation details.\n* The query and library entries can be injected as dependencies, allowing for easier testing and swapping out different implementations.\n\n**Alternatives Considered:**\n\n* Having the study contain direct object instances, which would couple the study to specific implementations and make it harder to test and maintain.\n* Creating a separate class that wraps the study and contains the query and library entries, which would add unnecessary complexity and indirection.\n\n**Consequences:**\n\n* The study will remain a simple, lightweight object that only holds data.\n* The separation of concerns between the data layer and the business logic layer will be maintained.\n* The study will be easier to test and maintain.\n\n**Status:** Accepted\n\n**Implementor:** [Name]\n\n**Date:** [Date]\n\nThis decision ensures that the study remains a pure DTO object, which maintains the separation of concerns and makes it easier to test and maintain.","GenTime":"2024-08-10 23:34:29"}
{"File Name":"jabref\/0002-use-slf4j-for-logging.md","Context":"## Context and Problem Statement\\nUp to version 4.1 JabRef uses apache-commons-logging 1.2 for logging errors and messages. However, this is not compatible with java 9 and is superseded by log4j.\\n## Decision Drivers\\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\n","Decision":"* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\nChosen option: \"SLF4J with Log4j2 binding\", because comes out best \\(see below\\).\\n","tokens":104,"id":4742,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Migrate JabRef's logging from Apache Commons Logging 1.2 to SLF4J with Log4j, to ensure compatibility with Java 9 and leverage the existing Log4j dependency.\n\n**Context:** JabRef's current logging implementation, Apache Commons Logging 1.2, is not compatible with Java 9 and has been superseded by Log4j.\n\n**Decision Drivers:**\n\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including Log4j, and supports Java 9.\n* Log4j is already defined as a dependency and SLF4J has already been required by a third-party dependency.\n\n**Consequences:**\n\n* JabRef will be able to use a logging framework compatible with Java 9.\n* The existing Log4j dependency will be leveraged, reducing the number of dependencies and potential issues.\n* The migration will require changes to the logging configuration and potentially some refactoring of the code to use SLF4J.\n\n**Rationale:** The decision to migrate to SLF4J with Log4j is driven by the need to ensure compatibility with Java 9 and the existing dependencies. SLF4J provides a fa\u00e7ade for several logging frameworks, including Log4j, making it a suitable replacement for Apache Commons Logging. By leveraging the existing Log4j dependency, we can reduce the number of dependencies and potential issues.","GenTime":"2024-08-10 23:34:45"}
{"File Name":"jabref\/0010-use-h2-as-internal-database.md","Context":"## Context and Problem Statement\\nWe need to store data internally in a structured way to gain performance.\\n## Decision Drivers\\n* Easy to integrate\\n* Easy to use\\n* Common technology\\n","Decision":"* Easy to integrate\\n* Easy to use\\n* Common technology\\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\\n","tokens":39,"id":4747,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** We will use a relational database management system (RDBMS) to store data internally in a structured way to gain performance.\n\n**Context and Problem Statement:** We need to store data internally in a structured way to gain performance.\n\n**Decision Drivers:**\n\n* Easy to integrate: RDBMS are widely supported and have numerous libraries and tools available for various programming languages, making it easy to integrate with our existing technology stack.\n* Easy to use: RDBMS have a well-established syntax and query language, making it easy for developers to write and maintain database queries.\n* Common technology: RDBMS are a common technology used in many industries and have a large community of developers and experts, making it easier to find resources and support.\n\n**Rationale:** We have chosen to use a relational database management system to store data internally in a structured way to gain performance. This decision is based on the need for easy integration, ease of use, and the widespread adoption of RDBMS technology.\n\n**Consequences:**\n\n* We will need to design and implement a database schema to store our data in a structured way.\n* We will need to write database queries to retrieve and manipulate our data.\n* We will need to ensure that our database is properly secured and backed up to prevent data loss.\n\n**Alternatives Considered:** We considered using a NoSQL database, but ultimately decided against it due to the complexity and learning curve associated with NoSQL databases. We also considered using a cloud-based database service, but decided against it due to concerns about data security and control.\n\n**Status:** This decision is approved and has been implemented in our system.","GenTime":"2024-08-10 23:36:14"}
{"File Name":"jabref\/0014-separate-URL-creation-to-enable-proper-logging.md","Context":"## Context and Problem Statement\\nFetchers are failing.\\nThe reason why they are failing needs to be investigated.\\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\\nHow to properly log the URL used for fetching?\\n## Decision Drivers\\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\n","Decision":"* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n","tokens":132,"id":4748,"Prediction":"Here is the Architectural Decision Record:\n\n## Decision\n\n**Log URL used for fetching in a readable format**\n\n## Context and Problem Statement\n\nFetchers are failing, and we need to investigate the reason. Knowing the URL used to query the fetcher eases debugging. Claim 1 suggests that logging the URL would be beneficial for debugging. Additionally, Claim 2 suggests that logging the URL instead of showing it only in the debugger would be more effective.\n\n## Decision Drivers\n\n* Code should be easy to read: We want to log the URL in a way that is easy to read and understand, without cluttering the code with unnecessary complexity.\n* Include URL in the exception instead of logging in case an exception is thrown already: We want to follow best practices for exception handling and include the URL in the exception message instead of logging it separately.\n\n## Decision\n\nWe will log the URL used for fetching in a readable format, using a logging framework that provides a way to format the log message with the URL. We will include the URL in the exception message if an exception is thrown, to provide additional context for debugging.\n\n## Consequences\n\n* Code readability: The logging code will be easy to read and understand, with a clear and concise format for logging the URL.\n* Debugging: The URL will be logged in a way that is easy to read and understand, making it easier to debug and identify the cause of the fetcher failure.\n* Exception handling: The URL will be included in the exception message, providing additional context for debugging and making it easier to identify the cause of the exception.","GenTime":"2024-08-10 23:36:32"}
{"File Name":"insight\/0002-tweepy.md","Context":"## Context\\nTo programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.\\n","Decision":"We choose [Tweepy](https:\/\/github.com\/tweepy\/tweepy) as our Twitter API Pyhon library.\\n","tokens":36,"id":4760,"Prediction":"## Decision\n\nWe will use the Tweepy library to programmatically access Twitter APIs in our Python application.\n\n## Context\n\nAs per the requirement, we need a Python library that can support the APIs we need to access. The chosen library should be well-reviewed and have a wide community support.\n\n## Problem\n\nWe need a reliable and efficient way to access Twitter APIs programmatically in our Python application. The chosen library should be able to handle the complexities of the Twitter API and provide a simple and intuitive interface for our developers to use.\n\n## Decision\n\nAfter researching and evaluating various options, we have decided to use Tweepy, a Python library for accessing the Twitter API. Tweepy is a well-reviewed and widely-used library that provides a simple and intuitive interface for accessing the Twitter API. It supports a wide range of Twitter API endpoints and has a large and active community of developers who contribute to its development and provide support.\n\n## Rationale\n\nWe chose Tweepy for several reasons:\n\n* Wide community support: Tweepy has a large and active community of developers who contribute to its development and provide support.\n* Well-reviewed: Tweepy has a high rating on PyPI and is widely recommended by other developers.\n* Easy to use: Tweepy provides a simple and intuitive interface for accessing the Twitter API, making it easy for our developers to use.\n* Supports a wide range of Twitter API endpoints: Tweepy supports a wide range of Twitter API endpoints, including the Twitter API, Twitter Streaming API, and Twitter Ads API.\n\n## Consequences\n\nBy using Tweepy, we will be able to access Twitter APIs programmatically in our Python application in a reliable and efficient way. Our developers will be able to easily access and manipulate Twitter data, and we will be able to build robust and scalable applications that integrate with the Twitter API.\n\n## Pros and Cons\n\nPros:\n\n* Easy to use\n* Wide community support\n* Well-reviewed\n* Supports a wide range of Twitter API endpoints\n\nCons:\n\n* May require additional setup and configuration to work with the Twitter API\n* May have limitations on the number of requests that can be made to the Twitter API\n\n## Alternatives\n\nWe considered the following alternatives:\n\n* Twitter API wrapper: A custom wrapper around the Twitter API that provides a simple interface for accessing the API.\n* Python-twitter: A Python library that provides a simple interface for accessing the Twitter API.\n\nHowever, we decided to use Tweepy because of its wide community support, well-reviewed, and easy to use interface.","GenTime":"2024-08-10 23:40:42"}
{"File Name":"insight\/0001-python.md","Context":"## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n","Decision":"We choose Python as our base programming language.\\n","tokens":33,"id":4761,"Prediction":"## Decision\n\n**Decision:** Python will be chosen as the primary programming language for implementing the various functionalities of the software.\n\n## Context\n\nAs stated, we need a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\n\n## Problem\n\nChoosing a programming language for our software is crucial as it will have a significant impact on the development process, maintainability, and scalability of the system. We need a language that is easy to learn, has a vast range of libraries and tools, and is widely adopted in the industry.\n\n## Reasoning\n\nAfter careful consideration, we have decided to choose Python as our primary programming language. Here are some reasons why:\n\n* Python is a high-level language that is easy to learn and has a simple syntax, making it an excellent choice for beginners and experienced developers alike.\n* Python has a vast range of libraries and tools that make it an ideal choice for various tasks, such as data analysis, machine learning, web development, and more.\n* Python is widely adopted in the industry, with many popular frameworks and libraries available, such as Django, Flask, and NumPy.\n* Python is known for its large and active community, which means there are many resources available for troubleshooting and learning.\n\n## Consequences\n\nBy choosing Python as our primary programming language, we can expect the following benefits:\n\n* Faster development time due to the ease of learning and the availability of libraries and tools.\n* Improved maintainability and scalability due to the simplicity and flexibility of the language.\n* Increased collaboration and knowledge sharing within the development team due to the widespread adoption of Python in the industry.\n\n## Rationale\n\nWhile other languages, such as Java or C++, may be suitable for specific tasks, Python's versatility, ease of use, and large community make it an excellent choice for our software. We will use Python as the primary language for implementing the various functionalities, and other languages will be used as needed with proper justification.","GenTime":"2024-08-10 23:41:03"}
{"File Name":"island.is-glosur\/0009-naming-files-and-directories.md","Context":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n","Decision":"* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","tokens":93,"id":4766,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will adopt the PascalCase naming convention for files and directories in the monorepo, following the convention of naming components with a prefix indicating their type (e.g. `MyComponent` for a React component).\n\n**Context**\n\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has defaults that differ between schematic types. In order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files and directories.\n\n**Decision Drivers**\n\n* Provide consistency when navigating the codebase: By adopting a single naming convention, we can reduce confusion and make it easier for team members to find and understand the code.\n* The earlier we decide on this, the better: Establishing a naming convention early on will help prevent inconsistent naming from becoming ingrained in the codebase, making it easier to maintain and evolve over time.\n\n**Consequences**\n\n* All new files and directories created in the monorepo will follow the PascalCase naming convention.\n* Existing files and directories will be refactored to conform to the new convention, with a gradual timeline to minimize disruption to ongoing development.\n* The naming convention will be documented in the project's style guide to ensure consistency across the team.\n\n**Rationale**\n\nAfter considering the pros and cons of different naming conventions, we have chosen PascalCase as the most suitable option for our monorepo. This convention is widely used in the JavaScript and React communities, making it easier for team members to understand and maintain the code. Additionally, PascalCase provides a clear and consistent way to distinguish between file and directory names, reducing the risk of confusion and errors.","GenTime":"2024-08-10 23:42:43"}
{"File Name":"web-mapviewer\/2020_05_28_test_framework.md","Context":"## Context\\nAn integration test framework (and possibly a combo unit\/integration test) has to be chosen for this project.\\nAfter reviewing what's the state of testing with JS in 2020, here's a list of contenders.\\n### Selenium\\nHas been the standard of the industry for many years. Start to struggle with single page application framework (SPA) like Angular, Vue.js or React.js.\\n- Desktop browsers, emulators \/ simulators and real devices (E2E testing all the way)\\n- Highly scalable\\n- Has many \"sister frameworks\" that enhance the feature set (like Appium for mobile testing, Nightwach or WebdriverIO)\\nWhile very efficient for E2E testing, this framework would be cumbersome to have in development on a local machine.\\nCould be used for E2E testing for cross browser compatibility tests.\\n### TestCafe\\nGood alternative to Selenium, is compatible with Saucelab (we already have some account with them).\\n- Fast to set up\\n- Cross Browser and Devices (with SauceLabs or BrowserStack)\\nThis could also be a good cross browser testing framework, but the cross browser feature will be hard to have deployed on every developers' machine.\\n### Cypress\\nMade by devs for devs.\\n- Very convenient running and debugging tools\\n- Native access to all your application\u2019s variables\\n- Solid and clear documentation\\n- Newest addition (2017) to the big frameworks (very active community)\\n","Decision":"After playing a bit with these frameworks, Cypress clearly had something for it.\\nThe documentation is well maintained and complete, tests are described with Mocha which is already known by many in the team (it's what we used to write our tests on the older viewer)\\nCypress will be a great help during the development phase of the new viewer.\\nBut we have to keep in mind that it doesn't really do cross browser testing.\\n","tokens":307,"id":4785,"Prediction":"## Decision\n\nAfter careful consideration of the options, we have decided to use Cypress as our integration test framework for this project.\n\n## Context\n\nAs discussed, we have three contenders for our integration test framework: Selenium, TestCafe, and Cypress. Each has its pros and cons, and we need to choose the one that best fits our needs.\n\n## Problem\n\nWe need a test framework that is efficient, scalable, and easy to use for our development team. We also need to consider the limitations of each framework, such as the difficulty of setting up Selenium on local machines and the limitations of TestCafe's cross-browser testing capabilities.\n\n## Decision\n\nWe have decided to use Cypress as our integration test framework for several reasons:\n\n1. Convenience: Cypress is designed for developers, by developers. It has a very convenient running and debugging toolset, making it easy to use and debug tests.\n2. Native access: Cypress provides native access to all application variables, making it easy to test complex application logic.\n3. Solid documentation: Cypress has solid and clear documentation, making it easy for new developers to get started.\n4. Active community: Cypress is a relatively new addition to the big frameworks, but it has a very active community, which ensures that it will continue to evolve and improve.\n5. Cross-browser testing: While Cypress is not as strong as TestCafe in terms of cross-browser testing, it is still a viable option, and we can use SauceLabs or BrowserStack to achieve cross-browser testing.\n\n## Trade-offs\n\nWe have decided to trade off the scalability of Selenium and the cross-browser testing capabilities of TestCafe for the convenience and ease of use of Cypress. We believe that the benefits of using Cypress outweigh the limitations, and it will be a better fit for our development team.\n\n## Consequences\n\nWe will implement Cypress as our integration test framework and use it for all our integration testing needs. We will also use SauceLabs or BrowserStack to achieve cross-browser testing capabilities. This decision will allow us to focus on developing high-quality software and reduce the time and effort required for testing.","GenTime":"2024-08-10 23:48:16"}
{"File Name":"nr-arch\/2020-04-24-ARCH-OpenJDK-Versions.md","Context":"## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https:\/\/apps.nrs.gov.bc.ca\/int\/jira\/browse\/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n","Decision":"* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\nJDK 8 & Later\\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\\nAs an example, the archives for JDK 13 may be found on jdk.java.net\/13 and may be extracted on the command line using\\n$ tar xvf openjdk-13*_bin.tar.gz\\nor\\n$ unzip openjdk-13*_bin.zip\\ndepending on the archive type.\\n### Positive Consequences\\n* removes the dependencies on Oracle JDK Licensing\\n* reduces security vulnerabilities of older JDK versions\\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\\nFurther vulnerabilities -\\n* <https:\/\/www.cvedetails.com\/product\/19117\/Oracle-JRE.html?vendor_id=93>\\n*\t<https:\/\/www.cvedetails.com\/product\/23642\/Oracle-Openjdk.html?vendor_id=93>\\n### Negative Consequences\\n* slow performance may occur\\n* migration issues will need to be addressed\\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\\n","tokens":403,"id":4802,"Prediction":"## Decision\n\nThe IITD Architecture team proposes to adopt OpenJDK as the default Java runtime environment for all IITD hosted Java applications, effective immediately.\n\n## Context and Problem Statement\n\nThe IITD Infrastructure Team and others have requested an official statement on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications. Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases. This change requires IITD to re-evaluate its Java strategy and make a decision on the future direction of Java\/JDKs for IITD hosted applications.\n\n## Decision Drivers\n\n* The need for an official IITD statement on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.\n* Oracle's change to a subscription model for its Oracle Java SE product releases.\n* The end of life for Java SE 8 and the recommendation to upgrade to at least JDK 11 for security reasons.\n* The availability of OpenJDK as a viable alternative to Oracle JDK.\n* The potential cost savings and flexibility offered by OpenJDK.\n\n## Decision Outcome\n\nThe IITD Architecture team recommends adopting OpenJDK as the default Java runtime environment for all IITD hosted Java applications. This decision is based on the following rationale:\n\n* OpenJDK is a free and open-source implementation of the Java SE specification, which provides a stable and secure alternative to Oracle JDK.\n* OpenJDK is widely used and supported by the open-source community, which ensures that it will continue to receive updates and security patches.\n* OpenJDK is compatible with Oracle JDK, which means that applications developed using Oracle JDK can be easily migrated to OpenJDK.\n* Adopting OpenJDK will help IITD reduce costs associated with licensing Oracle JDK.\n* OpenJDK provides greater flexibility and control over the Java runtime environment, which is essential for IITD's mission-critical applications.\n\n## Pros and Cons\n\nPros:\n\n* Cost savings\n* Increased flexibility and control\n* Greater security and stability\n* Wide community support and updates\n\nCons:\n\n* Potential disruption to existing applications and services\n* Need for migration and testing of existing applications\n* Potential for compatibility issues with third-party libraries and frameworks\n\n## Rationale\n\nThe IITD Architecture team believes that adopting OpenJDK as the default Java runtime environment is the best decision for IITD's hosted Java applications. OpenJDK provides a stable and secure alternative to Oracle JDK, and its open-source nature ensures that it will continue to receive updates and security patches. Additionally, OpenJDK is compatible with Oracle JDK, which makes it easy to migrate existing applications to OpenJDK. The potential cost savings and flexibility offered by OpenJDK are also significant advantages. While there may be some disruption to existing applications and services, the benefits of adopting OpenJDK outweigh the potential drawbacks.","GenTime":"2024-08-10 23:53:55"}
{"File Name":"nr-arch\/2020-03-12-ARCH-record-architecture-decisions-simple-template.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4804,"Prediction":"## Decision\n\n**Use a Service-Based Architecture for the New Feature**\n\n## Context\n\nThe issue motivating this decision is the need to develop a new feature for our software that requires integrating with multiple external services and systems. The feature, which we will refer to as \"Feature X\", will allow users to import data from various sources and perform complex analytics on that data. The complexity of the feature, combined with the need to integrate with multiple external services, has raised concerns about the maintainability and scalability of the architecture.\n\nThe context that influences this decision includes:\n\n* The existing monolithic architecture of the software, which makes it difficult to maintain and scale individual components.\n* The need to integrate with multiple external services, which requires a flexible and scalable architecture.\n* The requirement for high availability and reliability of the feature, which necessitates a architecture that can handle high traffic and sudden spikes in usage.\n* The need to ensure data security and compliance with regulatory requirements, which requires a architecture that can provide fine-grained access control and auditing.\n\n## Problem\n\nThe problem is that the existing monolithic architecture of the software is not well-suited to handle the complexity and integration requirements of Feature X. The architecture is rigid and inflexible, making it difficult to add new features or services without disrupting the entire system. Additionally, the architecture does not provide adequate support for scalability and high availability, which are critical requirements for the feature.\n\n## Solution\n\nThe solution is to adopt a service-based architecture for Feature X. This architecture will consist of a set of loosely coupled services that can be developed, deployed, and scaled independently. Each service will be responsible for a specific function, such as data import, data processing, and analytics. This will allow us to develop and deploy each service independently, without affecting the other services or the rest of the system.\n\nThe service-based architecture will also provide a number of benefits, including:\n\n* Improved scalability and high availability, as each service can be scaled and replicated independently.\n* Improved maintainability, as each service can be developed and deployed independently.\n* Improved flexibility, as new services can be added or removed without affecting the rest of the system.\n* Improved data security and compliance, as each service can be configured to provide fine-grained access control and auditing.\n\n## Consequences\n\nThe consequences of this decision include:\n\n* The need to develop a new set of skills and expertise in the team, as service-based architecture requires a different set of skills and tools than traditional monolithic architecture.\n* The need to invest in new infrastructure and tools, such as containerization and orchestration, to support the service-based architecture.\n* The need to re-architect and re-factored some of the existing code to fit the new architecture.\n* The need to develop and implement new testing and deployment strategies to support the service-based architecture.\n\nOverall, the decision to adopt a service-based architecture for Feature X is a critical step in ensuring the long-term maintainability, scalability, and security of the software.","GenTime":"2024-08-10 23:54:27"}
{"File Name":"figgy\/0002-queues.md","Context":"## Context\\nWe have background jobs that are processed by Sidekiq workers on several dedicated background processing\\nmachines.  The background jobs include a variety of different tasks from different sources:\\n* processing user requests (e.g., ingesting files, (re)generating derivatives)\\n* cleaning up after user actions (e.g., propagating state and visibility, removing files before deleting\\ntheir parent resource)\\n* refreshing metadata from Voyager\/PULFA\\n* bulk-ingesting content\\n* routine cleanup (cleaning out guest users)\\n* fixity checking\\n","Decision":"We will have three queues for processing background jobs:\\n1. `default` for processing user-initiated requests needed for ingesting and displaying objects\\n2. `low` for bulk processing, validation, cleanup, etc.\\n3. `super_low` for fixity checking and other long-term preservation actions\\n","tokens":116,"id":4821,"Prediction":"## Decision\n\n**Decoupling Background Jobs from Sidekiq Workers using a Message Queue**\n\n## Context\n\nWe have background jobs that are processed by Sidekiq workers on several dedicated background processing machines. The background jobs include a variety of different tasks from different sources, such as processing user requests, cleaning up after user actions, refreshing metadata, bulk-ingesting content, routine cleanup, and fixity checking.\n\n## Problem\n\nThe current implementation tightly couples the background jobs with the Sidekiq workers, making it difficult to:\n\n* Scale the number of workers independently of the number of jobs\n* Easily add or remove job types without modifying the worker code\n* Monitor and troubleshoot job processing in a centralized manner\n* Implement retries and failures handling for individual jobs\n\n## Decision\n\nTo address these issues, we will decouple the background jobs from the Sidekiq workers by introducing a message queue (e.g., RabbitMQ, ActiveMQ) between the job creators and the workers. This will allow us to:\n\n* Send jobs to the message queue for processing, decoupling the job creators from the workers\n* Scale the number of workers independently of the number of jobs\n* Easily add or remove job types by creating and configuring separate queues for each job type\n* Monitor and troubleshoot job processing in a centralized manner using the message queue's built-in monitoring tools\n* Implement retries and failures handling for individual jobs by configuring the message queue's retry and failure mechanisms\n\n## Consequences\n\n* Additional complexity in the system architecture\n* Increased overhead in terms of message queue maintenance and monitoring\n* Potential for increased latency in job processing due to the additional hop in the processing pipeline\n\n## Rationale\n\nThe benefits of decoupling the background jobs from the Sidekiq workers outweigh the additional complexity and overhead. By introducing a message queue, we can improve the scalability, flexibility, and maintainability of our background job processing system, ultimately leading to a more robust and reliable system.","GenTime":"2024-08-11 00:00:06"}
{"File Name":"ansible-docker\/choose_database.md","Context":"### Context: A method of data storage is necessary to fufill the requirements for the assignment.\\n* Having used mySQL in previous courses and personal projects, I found mySQL a suitable choice for supporting database capabilities.\\n* Many of the examples provided by my instructor are using mySQL, thus for the benefit of staying consistent with course material mySQL is a good choice.\\n### Decision: The change proposed to the current implementation is to add mySQL as a method for data storage.\\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n","Decision":"### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n### Consequences: Acid Transactions can be considered \"pessimistic\" and not as flexible as base transactions.\\n","tokens":138,"id":4832,"Prediction":"Here is an Architectural Decision Record (ADR) based on the context you provided:\n\n**Title:** [Use MySQL as a Method for Data Storage]\n\n**Context:** The need to implement a method for data storage arises from the requirements of the assignment. Given my familiarity with MySQL from previous courses and personal projects, and the consistency with course material, MySQL is a suitable choice for supporting database capabilities.\n\n**Decision:** We will add MySQL as a method for data storage in our implementation.\n\n**Context for the Decision:** The primary consideration is the need for a reliable and consistent method for data storage. MySQL is a popular and widely-used database management system that meets these requirements. Additionally, using MySQL allows us to stay consistent with the course material and examples provided by the instructor.\n\n**Consequences:**\n\n* **Pros:** MySQL is a well-established and widely-used database management system, ensuring a high level of reliability and compatibility.\n* **Cons:** There may be more modern tools with additional features that could be used instead, potentially limiting the functionality of our implementation.\n\n**Rationale:** While there may be more modern tools available, MySQL's popularity and widespread adoption make it a suitable choice for our implementation. Additionally, the consistency with course material and examples provided by the instructor makes it a good choice for staying on track with the assignment.\n\n**Status:** Approved\n\n**Implementation:** MySQL will be integrated into our implementation as a method for data storage.\n\n**Reevaluation:** This decision will be reevaluated if new requirements or constraints arise that make it necessary to reconsider the choice of data storage method.","GenTime":"2024-08-11 00:03:38"}
{"File Name":"ansible-docker\/choose_webserver.md","Context":"### Context: A webserver is a necessary requirment to complete the assignment\\n* A webserver is needed to host the web application.\\n* There is a PHP-apache bundle offered by docker (see Dockerfile in php directory), thus apache was chosen purely out of convenience.\\n* I do not have much experience with Web Servers and there is a lot of documentation on apache. Again, convenience was a major factor in making the decision to use apache.\\n### Decision: The change proposed to the current implementation is to add an apache web server to host the php web application created in the php directory\\n### Consequences: No forseen consequences or drawbacks in using apache.\\n","Decision":"### Consequences: No forseen consequences or drawbacks in using apache.\\n","tokens":139,"id":4833,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**: Use Apache as the web server for hosting the PHP web application.\n\n**Context**: The PHP-apache bundle offered by Docker was chosen due to convenience, and since there is a lot of documentation available on Apache, it was a major factor in the decision-making process.\n\n**Problem**: The need to host a PHP web application requires a web server to serve the application.\n\n**Decision**: Apache was chosen as the web server due to its convenience and the availability of documentation.\n\n**Consequences**:\n\n* No foreseeable consequences or drawbacks in using Apache.\n\n**Rationale**: The decision to use Apache was made based on the convenience of the PHP-apache bundle offered by Docker and the availability of documentation on Apache. This decision allows for a straightforward implementation of the web server without requiring extensive expertise in web servers.\n\n**Status**: Implemented\n\n**Date**: [Insert date]\n\n**Responsible**: [Insert name]\n\nThis Architectural Decision Record provides a clear and concise summary of the decision to use Apache as the web server for hosting the PHP web application. It outlines the context, problem, decision, consequences, and rationale for the decision, making it a useful reference for future development and maintenance of the application.","GenTime":"2024-08-11 00:03:52"}
{"File Name":"opg-use-an-lpa\/0002-persistent-storage-uses-dynamodb.md","Context":"## Context\\n* The application will require persistent storage for storing LPA ownership, granted access, and possibly user credentials\\n* Current MoJ strategy is to use managed services where possible\\n","Decision":"Use DynamoDB for persistent storage\\n","tokens":37,"id":4841,"Prediction":"## Decision\n\n**Decision:** Use Amazon DynamoDB as the primary storage solution for storing LPA ownership, granted access, and user credentials.\n\n**Context:** The application requires persistent storage for storing LPA ownership, granted access, and possibly user credentials. Additionally, the current MoJ strategy is to use managed services where possible.\n\n**Rationale:**\n\n* Amazon DynamoDB is a fully managed NoSQL database service that can handle large amounts of data and scale horizontally to meet the needs of the application.\n* DynamoDB provides high levels of availability and durability, with built-in support for automatic backups and data replication.\n* As a managed service, DynamoDB eliminates the need for the development team to manage and maintain the underlying infrastructure, allowing them to focus on developing the application.\n* DynamoDB's strong consistency model and support for transactions make it well-suited for storing sensitive data such as user credentials.\n* The use of DynamoDB aligns with the MoJ strategy of using managed services where possible, reducing the complexity and cost of managing and maintaining infrastructure.\n\n**Consequences:**\n\n* The application will require additional configuration and setup to integrate with DynamoDB.\n* The development team will need to ensure that data is properly formatted and stored in DynamoDB, which may require additional testing and validation.\n* The use of DynamoDB may require additional costs for data storage and retrieval.\n\n**Status:** Accepted\n\n**Implementation:** The development team will implement DynamoDB as the primary storage solution for storing LPA ownership, granted access, and user credentials. The team will work with the infrastructure team to set up and configure the DynamoDB instance, and will ensure that data is properly formatted and stored in the database.","GenTime":"2024-08-11 00:06:17"}
{"File Name":"opg-use-an-lpa\/0008-session-storage-using-an-encrypted-cookie.md","Context":"## Context\\n* We will be storing a very small amount of data in the session.\\n* Whilst the above holds true we can avoid additional infrastructure by using client side storage.\\n* The session _may_ hold somewhat sensitive details (e.g. an LPA Share code), thus its content is secret.\\n* As the cookie is client side, we also need authentication to ensure the message isn't tempered with.\\n","Decision":"To use a cookie who's payload is encrypted with AES GCM. This provides secrecy and authentication.\\nNot to use JWT, because:\\n* To ensure message secrecy, additional libraries are needed.\\n* The resulting cookie value is significantly larger.\\n* Concerns over the general suitability around using JWT for client side sessions.\\n","tokens":86,"id":4843,"Prediction":"## Decision\n\nTo store a small amount of sensitive data in the session, we will use a combination of client-side storage and authentication to ensure the data remains secure.\n\n## Context\n\nAs mentioned in the context, we will be storing a small amount of data in the session, which may hold sensitive details such as an LPA Share code. To avoid additional infrastructure, we can use client-side storage, but this also means that the data will be stored on the client's machine, making it vulnerable to tampering. To mitigate this risk, we will implement authentication to ensure that the data cannot be modified or accessed by unauthorized parties.\n\n## Decision Criteria\n\n* Security: The data stored in the session is sensitive and requires protection from unauthorized access and tampering.\n* Infrastructure: Using client-side storage can help avoid additional infrastructure costs and complexity.\n* Performance: Client-side storage can provide faster access to the data compared to server-side storage.\n\n## Decision Outcome\n\nWe will use a combination of client-side storage and authentication to store and protect the sensitive data in the session. This approach will provide a balance between security, infrastructure, and performance.\n\n## Rationale\n\nUsing client-side storage allows us to store the data locally on the client's machine, reducing the load on the server and improving performance. However, this also means that the data is vulnerable to tampering and unauthorized access. To mitigate this risk, we will implement authentication to ensure that the data can only be accessed and modified by authorized parties.\n\n## Pros and Cons\n\nPros:\n\n* Reduced infrastructure costs and complexity\n* Improved performance due to local storage\n* Enhanced security through authentication\n\nCons:\n\n* Data is vulnerable to tampering and unauthorized access\n* Requires additional implementation of authentication\n\n## Alternatives Considered\n\n* Server-side storage: This would provide better security and control over the data, but would require additional infrastructure and may impact performance.\n* Third-party storage: This would provide additional security and scalability, but would require additional implementation and may impact performance.\n\n## Implementation\n\nWe will implement client-side storage using a secure protocol such as HTTPS and implement authentication using a library such as JSON Web Tokens (JWT). We will also implement data encryption to protect the data in transit and at rest.","GenTime":"2024-08-11 00:07:01"}
{"File Name":"opg-use-an-lpa\/0007-split-terraform-configuration.md","Context":"## Context\\nEngineers in the team want to have their code changes built as real environments in the Development AWS account so that they can have their work evaluated.\\nEvaluation includes automated and user acceptance testing.\\nWhen evaluation is finished it should be possible to destroy the environment, while retaining important artifacts and data (logs for example).\\nIt will be possible to host multiple PR environments in the Development AWS account\\n","Decision":"### Terraform Configuration\\nWe will identify and manage separately, resources at an Account level and resources at an Environment level.\\nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.\\nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.\\nAccount level resources will remain in place after an Environment is destroyed.\\n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)\\n### Name Spacing\\nWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resources\\nFor example, DNS records will be created for each environment using the namespace `https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uk`\\nwhile an ECS cluster will be named `025-UML-93-use-an-lpa`\\n### Environment lifecycle\\nAn Environment will be built when a PR is opened\\nOnce evaluation is complete, the Envirnoment can be destroyed.\\nTo begin with the pipeline will print instructions for destroying an Environment.\\nLater, destroying an Environment will be automated and scheduled with a maximum life span of a day or two.\\n### CI\/CD Pipeline\\nThe Terraform steps in the pipeline will always first provision the Account level shared resources, then provision the Environment level resources.\\n![pipeline terraform steps](..\/diagrams\/Use-An-LPA_Terraform_pipeline.png)\\n","tokens":82,"id":4847,"Prediction":"## Decision\n\n**Title:** Implement Automated Environment Creation and Destruction for PRs in Development AWS Account\n\n**Context:** As requested by the engineering team, we need to create a solution that allows engineers to have their code changes built as real environments in the Development AWS account, enabling automated and user acceptance testing. The environment should be disposable, allowing us to destroy it when evaluation is complete, while retaining important artifacts and data.\n\n**Decision:** We will implement a solution using AWS CloudFormation templates and AWS CodePipeline. The solution will consist of the following components:\n\n1. AWS CodePipeline: We will create a pipeline that triggers on pull requests (PRs) and builds the environment using the PR's code.\n2. AWS CloudFormation templates: We will create CloudFormation templates that define the environment's infrastructure, including EC2 instances, RDS databases, and other necessary resources.\n3. Environment creation and destruction: We will create a script that uses the AWS SDK to create the environment using the CloudFormation template and then destroy it when the evaluation is complete.\n4. Artifact retention: We will use AWS S3 to store important artifacts, such as logs, and retain them even after the environment is destroyed.\n\n**Rationale:**\n\n* AWS CodePipeline provides a robust and scalable way to automate the build and deployment process.\n* AWS CloudFormation templates allow us to define the environment's infrastructure in a declarative way, making it easy to manage and version control.\n* Environment creation and destruction scripts provide a flexible way to manage the environment's lifecycle.\n* Artifact retention using AWS S3 ensures that important data is preserved even after the environment is destroyed.\n\n**Consequences:**\n\n* Engineers will be able to see their code changes built as real environments in the Development AWS account, enabling automated and user acceptance testing.\n* The solution will provide a scalable and repeatable way to create and destroy environments, reducing the risk of errors and increasing efficiency.\n* The solution will retain important artifacts, such as logs, for future reference and debugging.\n\n**Monitoring and Feedback:**\n\n* We will monitor the solution's performance and provide feedback to the engineering team on its effectiveness.\n* We will also monitor the environment's creation and destruction process to identify any issues and optimize the solution.\n\n**Implementation:**\n\n* The implementation will be done in two phases:\n\t1. Phase 1: Set up the AWS CodePipeline and AWS CloudFormation templates.\n\t2. Phase 2: Implement the environment creation and destruction script and artifact retention using AWS S3.\n\n**Timeline:**\n\n* Phase 1: 2 weeks\n* Phase 2: 1 week\n\n**Responsible:**\n\n* [Your Name]\n\n**Verified by:**\n\n* [Name of the person verifying the decision]","GenTime":"2024-08-11 00:08:38"}
{"File Name":"opg-use-an-lpa\/0009-use-aws-kms-to-manage-session-encryption-keys.md","Context":"## Context\\nFollowing on from [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md)\\n* The encrypted cookie will need to be encrypted using a key.\\n* Keys should be able to be rotated easily and often.\\n* Key rotations should have no effect on active users.\\n","Decision":"* We will use AWS' KMS to manage our encryption keys.\\n* Keys will be cached at the contained level in volatile memory.\\n","tokens":67,"id":4848,"Prediction":"## Decision\n\n**Decision:** Implement a Key Rotation Mechanism using a Hardware Security Module (HSM) to store and manage encryption keys for session storage.\n\n## Context\n\nAs mentioned in [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md), we need to encrypt the cookie using a key and ensure that key rotations can be performed easily and often without affecting active users.\n\n## Problem\n\nWe need a secure and reliable way to store and manage encryption keys, allowing for easy rotation and revocation of keys without disrupting active user sessions.\n\n## Solution\n\nWe will use a Hardware Security Module (HSM) to store and manage our encryption keys. An HSM is a physical device that provides secure storage and processing of sensitive data, such as encryption keys. This will ensure that our keys are protected from unauthorized access and tampering.\n\nKey Rotation Mechanism:\n\n1. The HSM will store the current encryption key used for session storage.\n2. When a key rotation is required, a new key will be generated and stored in the HSM.\n3. The new key will be used for encrypting new session cookies, while the old key will be used for decrypting existing session cookies.\n4. Once all existing session cookies have been decrypted using the old key, the old key will be revoked and the new key will be used for all new and existing session cookies.\n\n## Rationale\n\nUsing an HSM provides several benefits:\n\n* Secure storage: The HSM provides a secure environment for storing encryption keys, protecting them from unauthorized access and tampering.\n* Easy key rotation: The HSM allows for easy key rotation, which can be performed without disrupting active user sessions.\n* Compliance: The use of an HSM can help organizations meet compliance requirements, such as PCI-DSS, HIPAA, and GDPR, which require secure storage and management of sensitive data.\n\n## Consequences\n\n* Additional hardware and infrastructure costs for the HSM.\n* Increased complexity in key management and rotation.\n* Requires additional security measures to ensure the HSM is properly secured and configured.\n\n## Alternatives\n\n* Using a software-based key management system, such as a cloud-based key management service.\n* Using a secure storage solution, such as a secure database or a secure file storage system.\n* Not using a key rotation mechanism at all, which would compromise the security of the system.","GenTime":"2024-08-11 00:09:04"}
{"File Name":"opg-use-an-lpa\/0016-logging-and-tracing.md","Context":"## Context\\nWe want to have an application wide logging infrastructure in place so that we can trace user requests through the\\napplication. It should be possible to trace each request through the application layers so that we can see what outgoing\\ncalls were made for incoming requests.\\n","Decision":"The use of `monolog\/monolog` to provide configurable logging levels throughout the application. Unique tracing\\ninformation is already provided by the amazon loadbalancers so this should be made available to the logging library and\\nattached to the logged information.\\n* Logging of service code to be main source of log information. If needed handlers can also be logged.\\n* Most logging to be done at an _info_ level.\\n* User actions that need monitoring (e.g. authentication failures) at _notice_ level\\n* Personally Identifiable Information (PII) to **not** be logged anywhere but _debug_\\n","tokens":55,"id":4852,"Prediction":"## Decision\n\n**Decision:** Implement a centralized logging infrastructure using a logging framework and a logging service, with a focus on traceability and visibility across the application layers.\n\n**Context:** As described, we want to have an application-wide logging infrastructure in place to trace user requests through the application, allowing us to see what outgoing calls were made for incoming requests.\n\n**Reasoning:**\n\n* A centralized logging infrastructure will provide a single source of truth for logging data, making it easier to monitor and troubleshoot the application.\n* Using a logging framework will enable us to log messages in a standardized way, making it easier to read and analyze logs.\n* A logging service will provide a scalable and reliable way to store and process log data, allowing us to handle high volumes of log data and ensure that logs are not lost.\n* By focusing on traceability and visibility across the application layers, we will be able to see the entire request lifecycle, including outgoing calls made for incoming requests.\n\n**Consequences:**\n\n* We will need to implement logging in all layers of the application, including the presentation layer, business logic layer, and data access layer.\n* We will need to configure the logging framework and logging service to meet the specific requirements of our application.\n* We will need to monitor and analyze logs to ensure that they are being collected and processed correctly, and to identify and troubleshoot issues.\n\n**Alternatives Considered:**\n\n* Implementing a logging mechanism in each layer of the application separately, which could lead to inconsistencies and make it harder to monitor and troubleshoot the application.\n* Using a third-party logging service that does not provide the level of customization and control we need, which could limit our ability to troubleshoot and optimize the application.\n\n**Status:** Accepted\n\n**Implementation:** The implementation will involve setting up a logging framework (e.g. Log4j, Logback) and a logging service (e.g. ELK Stack, Splunk) in the application. We will also implement logging in all layers of the application, and configure the logging framework and logging service to meet the specific requirements of our application.","GenTime":"2024-08-11 00:10:25"}
{"File Name":"bfi-discovery\/0002-choice-of-auth-provider.md","Context":"## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\n","Decision":"* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting \/ infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost \/ interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n","tokens":96,"id":4857,"Prediction":"## Decision\n\nWe will implement an OpenID Connect (OIDC) based identity and access management solution using Auth0 as the Identity Provider. This decision is based on the following considerations:\n\n### Decision Drivers\n\n* Ease of initial deployment and configuration: Auth0 provides a simple and streamlined setup process, allowing us to quickly deploy the solution and focus on other aspects of the project.\n* Ongoing hosting and maintenance costs: Auth0 is a cloud-based service, which means that we don't have to worry about hosting and maintaining the infrastructure, reducing our costs and administrative burden.\n* Availability of core features: Auth0 provides a wide range of features out-of-the-box, including user creation and registration, user management, and authentication and authorization, which meet the project requirements.\n\n### Rationale\n\nWe considered other options such as building our own identity and access management solution from scratch, or using other Identity Providers like Okta or Google. However, after evaluating the pros and cons of each option, we decided that Auth0 provides the best balance of ease of deployment, cost-effectiveness, and feature availability.\n\n### Consequences\n\n* We will need to integrate our application with Auth0 using the OIDC protocol, which will require some development effort.\n* We will need to configure Auth0 to meet our specific requirements, which may involve some customization.\n* We will benefit from Auth0's scalability, reliability, and security features, which will ensure that our identity and access management solution is robust and secure.\n\n### Monitoring and Review\n\nWe will monitor the performance and security of the Auth0-based identity and access management solution and review its effectiveness in meeting the project requirements. If any issues arise, we will work with Auth0's support team to resolve them.","GenTime":"2024-08-11 00:12:11"}
{"File Name":"cape-cod-coastal-planner\/0004-use-postgresql-as-db.md","Context":"## Context\\nThe API for CHIP needs some sort of persistence for storing its data. Although the data needs for the app are pretty lightweight, it's likely that they will grow in the future as new features are desired or more data is acquired. To this end, sticking with a simple, powerful, reliable, and flexible technology that is widely understood makes sense.\\n","Decision":"A RDBMS fits this bill and PostgreSQL does so even more snugly. It's a top contender, it's free, it's currently the best supported traditional database for the Elixir\/Erlang ecosystem and it's unparalleled at its geospatial capabilities.\\n","tokens":73,"id":4859,"Prediction":"## Decision\n\n**Persistence Technology for CHIP API**\n\n### Context\n\nAs discussed, the API for CHIP requires a persistence mechanism to store its data, considering the potential growth of data needs in the future.\n\n### Decision\n\nWe will use PostgreSQL as the persistence technology for the CHIP API. PostgreSQL is a widely used, powerful, reliable, and flexible relational database management system that meets our requirements.\n\n### Rationale\n\n1. **Simple**: PostgreSQL has a simple and intuitive query language, making it easy to learn and use.\n2. **Powerful**: It supports advanced features such as transactions, stored procedures, and views, allowing for complex queries and data manipulation.\n3. **Reliable**: PostgreSQL is known for its high reliability and fault tolerance, ensuring data integrity and availability.\n4. **Flexible**: It supports a wide range of data types, allowing us to store various types of data, and its extensibility features enable us to customize it to our needs.\n5. **Widely understood**: PostgreSQL is a widely used and well-documented technology, making it easy to find resources, talent, and support.\n\n### Consequences\n\n* We will need to design our database schema to optimize performance and data retrieval.\n* We will need to implement data validation and error handling to ensure data consistency and integrity.\n* We will need to consider scalability and performance when designing our database architecture.\n\n### Alternatives Considered\n\nWe considered alternative persistence technologies such as MySQL, MongoDB, and Cassandra. While each has its strengths, PostgreSQL was chosen due to its comprehensive feature set, reliability, and flexibility.\n\n### Status\n\nThis decision is final and has been implemented in the CHIP API.","GenTime":"2024-08-11 00:12:44"}
{"File Name":"GDD-app\/0002-use-preferences-for-data-storage.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4867,"Prediction":"## Decision\n\nAs our team continues to develop and maintain the architecture of our software, we have reached a critical point where we need to decide on the best approach to handle user authentication and authorization. This decision will have a significant impact on the overall security and scalability of our system.\n\n## Context\n\nOur software is a cloud-based platform that provides a suite of tools for businesses to manage their operations. As the user base grows, we need to ensure that our system can handle a large number of concurrent users while maintaining a high level of security and performance. Currently, we are using a simple username and password combination for authentication, which is vulnerable to various types of attacks.\n\nThe context that influences this decision includes:\n\n* The need for a more secure and robust authentication mechanism to protect user data and prevent unauthorized access.\n* The requirement for scalability to handle a large and growing user base.\n* The need for ease of use and minimal disruption to existing functionality.\n* The constraints of working within a cloud-based environment with limited control over infrastructure and resources.\n\n## Decision Criteria\n\nThe decision criteria for this architectural decision include:\n\n* Security: The chosen approach should provide a high level of security to protect user data and prevent unauthorized access.\n* Scalability: The chosen approach should be able to handle a large and growing user base without compromising performance.\n* Ease of use: The chosen approach should be easy to use and integrate with existing functionality.\n* Cost: The chosen approach should be cost-effective and align with our budget constraints.\n\n## Decision\n\nAfter careful consideration of the context and decision criteria, we have decided to implement a token-based authentication system using JSON Web Tokens (JWT). This approach provides a high level of security, scalability, and ease of use, while also meeting our budget constraints.\n\nThe benefits of this decision include:\n\n* Improved security: JWT provides a secure way to authenticate users and protect their data.\n* Scalability: JWT can handle a large and growing user base without compromising performance.\n* Ease of use: JWT is easy to implement and integrate with existing functionality.\n* Cost-effective: JWT is a cost-effective solution that aligns with our budget constraints.\n\n## Consequences\n\nThe consequences of this decision include:\n\n* The need to implement a new authentication mechanism, which may require additional resources and time.\n* The need to update existing functionality to work with the new authentication mechanism.\n* The potential for minor disruptions to existing functionality during the transition period.\n\nOverall, we believe that this decision will provide a more secure and scalable authentication mechanism that meets our needs and aligns with our budget constraints.","GenTime":"2024-08-11 00:15:31"}
{"File Name":"GDD-app\/0003-use-bitrise-for-ci.md","Context":"## Context\\nWe need an easy way to integrate and test out code that is fast and reliable.\\n","Decision":"We choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\\nIt also allows us to notify users in a easy way and with different roles.\\n","tokens":21,"id":4868,"Prediction":"## Decision\n\nTo achieve an easy way to integrate and test out code that is fast and reliable, we will implement a testing framework that utilizes a mocking library to isolate dependencies and allow for fast and isolated testing.\n\n## Context\n\nWe need an easy way to integrate and test out code that is fast and reliable.\n\n## Decision\n\nThe decision to use a testing framework with a mocking library is based on the following factors:\n\n* Fast and isolated testing: By using a mocking library, we can isolate dependencies and test our code in isolation, which will allow for faster and more reliable testing.\n* Easy integration: The testing framework will provide a simple and easy-to-use API for writing tests, which will make it easy to integrate new code into our testing infrastructure.\n* Reliable testing: The mocking library will allow us to control the behavior of dependencies, which will make it easier to write reliable tests that cover a wide range of scenarios.\n\n## Status\n\nThis decision is currently in the \"proposed\" status, and we will be implementing the testing framework and mocking library in the next sprint.\n\n## Pros and Cons\n\nPros:\n\n* Fast and isolated testing\n* Easy integration\n* Reliable testing\n\nCons:\n\n* May require additional setup and configuration\n* May require additional learning and training for developers\n\n## Rationale\n\nThe decision to use a testing framework with a mocking library is based on the following rationale:\n\n* Fast and isolated testing: By using a mocking library, we can isolate dependencies and test our code in isolation, which will allow for faster and more reliable testing.\n* Easy integration: The testing framework will provide a simple and easy-to-use API for writing tests, which will make it easy to integrate new code into our testing infrastructure.\n* Reliable testing: The mocking library will allow us to control the behavior of dependencies, which will make it easier to write reliable tests that cover a wide range of scenarios.\n\n## References\n\n* [Testing Framework Documentation](https:\/\/testingframework.com\/docs)\n* [Mocking Library Documentation](https:\/\/mockinglibrary.com\/docs)\n\nNote: This is just a sample Architectural Decision Record, you should adjust it according to your needs and the specific requirements of your project.","GenTime":"2024-08-11 00:15:54"}
{"File Name":"content-publisher\/0008-restricted-deletion.md","Context":"## Context\\nWe use foreign keys to help enforce database integrity e.g.\\n```\\n# ensure creator_id on a document matches a row in the users table\\n# when we try to delete a user, raise an error if its the creator of a document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\\n```\\nThere are other `on_delete` behaviours instead of `restrict`.\\n```\\n# when we delete a user, set creator_id to NULL on all of its documents\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\\n# when we delete a user, also delete all of its document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\\n```\\nDifferent options are appropriate depending on the foreign key.\\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\\n","Decision":"Currently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\\n","tokens":341,"id":4872,"Prediction":"Here is the Architectural Decision Record (ADR) for the given context:\n\n**Decision**\n\nADR-001: Foreign Key `on_delete` Behavior Selection\n\n**Context**\n\nWe use foreign keys to enforce database integrity by defining relationships between tables. The `on_delete` behavior of a foreign key determines what happens when the referenced record is deleted. There are three options: `restrict`, `nullify`, and `cascade`.\n\n**Problem**\n\nDifferent foreign keys have different requirements for the `on_delete` behavior. Choosing the correct behavior for each foreign key is important to ensure data consistency and integrity.\n\n**Decision**\n\nWe will use the following guidelines to select the `on_delete` behavior for each foreign key:\n\n* `restrict` will be used in child\/parent scenarios where deleting the child would make the parent invalid.\n* `nullify` will be used in optional relationships where the foreign key can be set to NULL if the referenced record is deleted.\n* `cascade` will be used in parent\/child scenarios where it makes sense to delete the child when the parent is deleted.\n\n**Rationale**\n\nThis decision is based on the principles of database design and the specific requirements of our application. By using these guidelines, we can ensure that our foreign keys are properly configured to maintain data consistency and integrity.\n\n**Status**\n\nThis decision is currently in effect and will be reviewed and updated as necessary.\n\n**Changelog**\n\n* 2023-02-20: Initial decision made\n* 2023-03-15: Review and update of decision to reflect new requirements\n\n**References**\n\n* [Foreign Key Documentation](https:\/\/www.postgresql.org\/docs\/current\/sql-createtable.html#SQL-CREATETABLE-FOREIGN-OID)\n* [Database Design Patterns](https:\/\/www.tutorialspoint.com\/dbms\/database_design_patterns.htm)","GenTime":"2024-08-11 00:17:18"}
{"File Name":"content-publisher\/0009-modelling-history.md","Context":"## Context\\nThe Content Publisher domain model is focused on storing current\\ninformation. There is a documents table which stores the most recent content of\\na document and an images table which stores the most recent version of\\na document's images. History of these changes is stored using [Papertrail][],\\nwhich is not [intended as a permanent store][papertrail-pr].\\nThe presentation of a document's history is done via a TimelineEntry model\\nwhich stores that a user did something, but lacks any further detail. In places\\nwhere richer information was required there have been models such as Removal or\\nWithdrawal associated with a TimelineEntry.\\nThis has led to a number of pain points:\\n- users cannot discard a new draft of a published document, because creating a\\nnew draft overwrites the data stored for the published edition\\n- Content Publisher can't show an accurate link or status for the live edition\\nof a document when a new draft of a published document is created;\\n- users cannot edit or remove images on a document once the first\\nedition is published;\\n- the TimelineEntry model stores aspects of a document's state, resulting in it\\nneeding to be queried outside a timeline context which limits flexibility\\nfor the timeline.\\nAnd this prevents a number of intended features for Content Publisher:\\n- comparing different editions of a document;\\n- republishing live content if there are any problems (currently a common\\nsupport task for Whitehall publisher);\\n- showing users what changes a user made in a particular edit.\\n","Decision":"This ADR proposes changes to the domain model to resolve the aforementioned\\npain points and provide a means to support the future intended features. These\\nchanges provide the means to store the individual editions of a document,\\neach revision of the content of a document and each status an edition has held.\\nAs per [ADR-3](0003-initial-domain-modelling.md) it does not consider the\\noption of sharing data between translations of a document as there are not\\nthe appropriate product decisions for this.\\nA common theme in this decision is\\n[immutablity in models](#approach-to-mutabilityimmutability), which is used\\nas an implicit means of storing a history. Immutability is a key consideration\\nin modelling [revisions of a document](#breakdown-of-revision) and\\n[images](#image-modelling). This ADR then considers the impacts of\\nstoring history for [timeline](#timeline) and [topics](#topics), both areas\\nwhere the usage\/need of history is less clear. Finally, this ADR concludes with\\na [collated diagram](#collated-diagram) of the domain model concepts.\\n### Core Concepts\\n![Main concepts](0009\/main-concepts-diagram.png)\\n**Document**: A record that represents all versions of a piece of content in a\\nparticular locale. It has many editions and at any time it will have a current\\nedition - shown on Content Publisher index - and potentially a live edition\\nwhich is currently on GOV.UK. The live and current edition can be\\nthe same. Each iteration of a document's content is represented as a revision\\non the current edition, thus a document has many revisions. Document is a\\nmutable entity that is used to store data common across all editions (such as\\nfirst publishing date) and it is expected to be a joining point for\\ndocument-related data that is not associated with a particular edition.\\n**Edition**: A numbered version of a document that has been, or is\\nexpected to be, published on GOV.UK. It is associated with a revision\\nand a status. It is mutable so that it can be a consistent object that\\njoins to immutable data. It is a place where any edition-level\\ndatabase constraints can be placed, such as the constraint that only one live\\nedition can exist per document. It is supported that two editions of the same\\ndocument share the same revision. This allows them to explicitly reference the\\nsame content, which supports a future ability to revert a document to past\\ncontent.\\n**Revision**: Represents an immutable snapshot of the content of a document at a\\nparticular point in time. It has a number to indicate which revision of the\\ndocument it is and stores who created it. Any request by a user that changes\\ncontent should result in a single new revision. This is to directly map the\\nconcept of a revision to each time a user revises a document. Data outside of\\ncontent, such as state, should not be stored in a revision to ensure that\\ndifferences between revisions can be represented to a user. The\\n[anatomy of a Revision model](#breakdown-of-revision) is explored further in\\nthis document.\\n**Status**: Represents a state that an edition can hold such as: \"draft\" or\\n\"submitted for review\". This model is coupled to the concept of status that is\\nshown and changed by a user. Each time a user changes the status of an edition\\na new Status model is created and the user who created it stored. An edition\\ncan only have one status at any one time. If a status has data specific to\\nthat status, such as an explanatory note for a withdrawal, this can be stored\\nin a specific model associated by a polymorphic relation. This allows for\\nmodels, such as Removal or Withdrawal, to no longer be the responsibility of\\nTimelineEntry. Initially this object is intended to be immutable, however this\\nmay be changed if status changes become asynchronous operations. This is so\\nthat a single status change performed by a user can still be represented by\\na single record.\\n### Approach to mutability\/immutability\\nA number of the models in Content Publisher are defined as immutable, most\\nsignificantly [Revision and associated models](#breakdown-of-revision). These\\nmodels should be persisted to the database once and never be updated or deleted.\\nAny need to change them requires creating a new record. This allows us to store\\na full history by only appending to the database.\\nFor simplicity, performance and consistency with Rails idioms the accessing\\nof immutable models is intended to be done by foreign key and not by the usage\\nof `SELECT MAX` style queries. This maintains the ability to use the regular\\napproach to ActiveRecord associations and the means to require the existence of\\na association (by specifying a foreign key cannot be null). An example of this\\nmodelling is the mutable Edition model which references an immutable model,\\nRevision, that stores the content. Edition is accessed by a\\nconsistent primary key and the revision accessed by a foreign key stored on\\nthe edition.\\nSince the data on a mutable model can be lost when the model is updated these\\nshould not be used for data where there is a need for history. For example, to\\nstore the statuses an edition has held there are individual status models that\\nreference the Edition. This allows an edition to reference a single status that\\nis replaced while a history is maintained.\\nThe choice of this immutability strategy is to store both present and\\nhistorical concerns in the same way, thus ensuring history remains a\\nfirst class citizen. A nice side effect of having immutable models is\\nthis opens options for caching. Since data for that\\nmodel will never change it can effectively be cached forever.\\n### Breakdown of Revision\\nAs Revision is an immutable model, used to store each edit of a Document, there\\nis likely to be a large amount of these with often only minor differences\\nbetween them. To address this a Revision is not stored as a single model but\\ninstead as a collection of models, where the Revision model stores little data\\nand joins to other models. This can be visualised as:\\n![Revision breakdown](0009\/revision-diagram.png)\\nThe intention of breaking this up is to be conservative with the amount of data\\nduplicated between consecutive revisions. For example when a user edits\\nthe title of an edition a new ContentRevision is created and the existing\\nTagsRevision, MetadataRevision and ImageRevisions models are associated with\\nthe next revision. An ImageRevision is modelled in a similar way to a Revision\\nand this is explained further in [Image modelling](#image-modelling).\\nIt is intended that [delegation][delegate] be used when interfacing with a\\nrevision so that the caller need not be concerned with which sub-revision\\nstores particular fields. This allows a revision to have a rich interface\\ndespite storing a low amount of data directly.\\n### Image modelling\\nContent Publisher supports a user uploading image files and referencing them\\nin a revision of a document. They have metadata and editable properties that a\\nuser can change, of which a history is stored. A single image file uploaded\\nproduces multiple files that are uploaded to Asset Manager for different sizing\\nvariations. Images are modelled in a similar way to Revision with an\\nimmutable Image::Revision model, as represented below:\\n![Image Revision breakdown](0009\/image-revision-diagram.png)\\nThe Image model itself is used for continuation between image revisions. It is\\nknown that two Image::Revisions are versions of the same item if they share the\\nsame Image association. The id of the Image is used in Content Publisher URLs\\nto consistently reference the Image no matter which revision it is.\\nThe data of an Image::Revision is stored between an Image::FileRevision and an\\nImage::MetadataRevision. Both are immutable and they differ by the fact that\\nany change to Image::FileRevision requires changes to the resultant Asset\\nManager files (such as crop dimensions), whereas Image::MetadataRevision stores\\naccompanying data that doesn't affect the Asset Manager files (such as alt\\ntext).\\nEach Image::FileRevision is associated with an ActiveStorage::Blob object that\\nis responsible for managing the storage of the source file. It also has a one\\nto many association with Image::Asset. Each Image::Asset represents resultant\\nfiles that are uploaded to Asset Manager for the various image sizes. The\\nImage::Asset model stores the URL to the Asset Manager file and what state the\\nfile is on Asset Manager.\\n### Timeline\\nThe TimelineEntry model represents an event that should be shown to a user as\\npart of a visual timeline of a document's history. In order for the timeline to\\nbe a flexible feature that can be iterated, this model should not be used\\noutside of the timeline context. Previously models such as Removal and\\nWithdrawal were associated directly with a TimelineEntry which\\nmeant state was accessed through the timeline. These are now suggested to be\\nassociated with a Status model.\\nAt the time of writing it wasn't yet determined what the\\ntimeline would show, and therefore it wasn't clear exactly how\\nbest to model an entry for it. Because of this TimelineEntry is modelled in a\\nspeculative way with a number of references to relevant data, including a\\npolymorphic association for flexibility.\\nThe TimelineEntry model should not store data which could not be\\nderived from other aspects of a document. This is the allow the ability to\\nrebuild TimelineEntry models if the needs of the timeline changed and to avoid\\ntimeline being an aspect of a document's state.\\n### Topics\\nNo data related to topics (otherwise known as GOV.UK taxonomy) is intended to\\nbe stored in Content Publisher at this current point in time. This is due to\\ntopics being accessed and edited by directly interacting with the Publishing\\nAPI. Other applications, notably Content Tagger, can also edit topics which has\\nthe consequence that the Publishing API is the source of truth for this data\\nrather than Content Publisher.\\nThis inconsistency makes it difficult to store the history of topics in a\\nreliable way. Thus, until needs are determined to store past topics, Content\\nPublisher will only know current topics (by querying Publishing API) and\\nthe history of topics will not be available.\\n### Collated diagram\\nThe various aspects of this decision can be collated together to be represented\\nin the following, somewhat daunting, diagram. It has been simplified for ease\\nof reading with noticeable omissions being a User model, since most models have\\na creator, and the speculative joins of a TimelineEntry.\\n![Domain model diagram](0009\/domain-model-diagram.png)\\n","tokens":317,"id":4875,"Prediction":"## Decision\n\nTo address the pain points and limitations mentioned, we will introduce a new `DocumentVersion` model that represents a specific version of a document, including its associated images, and a `DocumentState` model that captures the current state of a document, including its published status, draft status, and any other relevant information.\n\n### Rationale\n\nThe current approach of using a single `documents` table and `images` table to store the most recent version of a document and its images, respectively, has led to several limitations and pain points. By introducing a `DocumentVersion` model, we can store and manage different versions of a document, allowing users to discard new drafts, edit or remove images, and compare different editions of a document.\n\nThe `DocumentState` model will provide a centralized location to store information about a document's state, such as its published status, draft status, and any other relevant information. This will enable us to query the state of a document in a flexible way, allowing for features such as comparing different editions of a document, republishing live content, and showing users what changes were made in a particular edit.\n\n### Consequences\n\n* Introduce a new `DocumentVersion` model to store and manage different versions of a document.\n* Introduce a new `DocumentState` model to capture the current state of a document.\n* Modify the `documents` table to store a reference to the current `DocumentVersion`.\n* Modify the `images` table to store a reference to the current `DocumentVersion`.\n* Update the `TimelineEntry` model to store a reference to the `DocumentVersion` it corresponds to.\n* Implement logic to handle the creation, editing, and removal of `DocumentVersion` instances.\n* Implement logic to handle the creation, editing, and removal of `DocumentState` instances.\n* Update queries and views to take into account the new `DocumentVersion` and `DocumentState` models.\n\n### Alternatives Considered\n\n* One alternative was to continue using the current approach and try to work around the limitations and pain points. However, this would likely lead to more complex and brittle code, making it harder to maintain and extend the system.\n* Another alternative was to introduce a new `Document` model that represents a specific version of a document, but this would not provide the same level of flexibility and scalability as the proposed `DocumentVersion` model.\n* A third alternative was to use a third-party library or service to manage document versions and states, but this would introduce additional dependencies and complexity, and may not provide the level of customization and control needed for our specific use case.","GenTime":"2024-08-11 00:18:18"}
{"File Name":"content-publisher\/0004-editing-microcopy.md","Context":"## Context\\nEvery feature we add to the app comes with its own static text, which is either embedded in the code (Ruby or JavaScript) or in the HTML. Static text can be anything from the page title, to the text of a button, to an entire page of guidance.\\nWriting text 'inline' makes it hard for us to audit all of strings in our application, some of which can only be seen under special conditions e.g. error messages. It also makes it hard to change strings consistently across the application - a task which has to be done by a developer. Finally, using inline strings in code distracts from the logical flow of that code.\\n[Rails Internationalization](https:\/\/guides.rubyonrails.org\/i18n.html) (also referred to as 'translations') are a way to extract all of the strings in the application to a central location in `config\/locales\/en`. The strings can be organized in a hierarchy over one or more files, as below, where we can refer to the reviewed title by writing `I18n.t(\"publish.published.reviewed.title\")`.\\n```\\n# publish_document\/published.yml\\nen:\\npublish_document:\\npublished:\\nreviewed:\\ntitle: Content has been published\\nbody: |\\n\u2018%{title}\u2019 has been published on GOV.UK.\\nIt may take 5 minutes to appear live.\\n```\\nRails translations have a few special behaviours, such as pluralization, raw HTML, and variables. The `%{title}` string in the above is an example of a variable, which a developer will set to the title of the document being published.\\n","Decision":"Although we could use translations to extract all of the strings in the application, in some cases we felt this wasn't necessary, or that a different method should be used. The following is a summary of the rules we currently use.\\n* **Link and button labels** are not extracted. We think link and button labels are unlikely to change, and extracting them made the application tests harder to read by obfuscating some of the crucial steps in the test with translation keys.\\n* **Publishing component strings** are not extracted. This ensures we are able to migrate these components to the [govuk_publishing_components](https:\/\/github.com\/alphagov\/govuk_publishing_components) repo, which wouldn't be able to access our local translations.\\n* **Big guidance** is extracted into it's own Markdown files and stored alongside the corresponding HTML page that shows it. For example, the guidance for creating a new document is stored in `app\/views\/new_document\/guidance.md`.\\n* **Domain data** that's static is stored in a number of custom YAML files. This application has two static models (for document types and their selection) that encapsulate domain concepts where the data is part of the application. We have split up domain data based on whether it's used in a backend setting or as a string for use in the frontend. The latter are extracted at the top-level of the translation hierarchy.\\n* **Global strings** (states and validation messages) are extracted using translations. As these strings aren't page-specific, we put them at the top-level of the translation hierarchy (in `states.yml` and `validations.yml`).\\n* **All other strings** are extracted using translations, in a hierarchy that follows the structure of the `app\/views` directory. For example, the above example relates to `app\/views\/publish_document\/published.html.erb`.\\n* **Small amounts of govspeak and HTML** are extracted using translations as for other strings, with '\\_html' or '\\_govspeak' appended to the final component of the key to indicate they support rich text.\\nEvery instance of a string in the tests has been replaced according to the above rules, such that the tests continue to pass when an extracted string is changed. **Link and button labels** are not replaced, as they are not extracted in the code.\\nWe also configured Rails to raise an exception when we hit a page where a translation is missing, as we don't test all of the translations; this is done by setting `config.action_view.raise_on_missing_translations = true` in `application.rb`.\\n","tokens":343,"id":4878,"Prediction":"## Decision\n\nTo improve the maintainability, scalability, and consistency of the application's static text, we will adopt the Rails Internationalization (I18n) approach, also known as translations, to extract and manage all strings in the application to a central location in `config\/locales\/en`.\n\n## Context\n\nAs mentioned in the context, writing static text inline in the code makes it difficult to audit, change, and maintain strings consistently across the application. This approach also distracts from the logical flow of the code. The Rails I18n approach provides a centralized location for managing strings, allowing for easier auditing, maintenance, and consistency.\n\n## Rationale\n\nThe benefits of using Rails I18n include:\n\n* Centralized management of strings, making it easier to audit and maintain\n* Improved consistency across the application\n* Reduced distractions from the logical flow of code\n* Support for pluralization, raw HTML, and variables, making it easier to handle complex text requirements\n\n## Decision Drivers\n\n* Maintainability: Extracting and managing strings in a centralized location improves maintainability and reduces the risk of errors\n* Scalability: The centralized approach makes it easier to add new features and manage existing ones\n* Consistency: The I18n approach ensures that strings are used consistently across the application\n* Code quality: By separating concerns, the I18n approach improves the logical flow of the code\n\n## Consequences\n\n* Increased upfront effort to set up the I18n system\n* Potential learning curve for developers who are new to the I18n approach\n* Additional complexity in handling complex text requirements, such as pluralization and variables\n\n## Status\n\nThis decision has been made and implemented. The I18n approach will be used for all new features and existing features will be migrated to the new approach.\n\n## Affected Components\n\n* Code: All code will be updated to use the I18n approach for static text\n* Configuration: The `config\/locales\/en` directory will be created and populated with translation files\n* Development process: The development process will be updated to include I18n best practices and guidelines","GenTime":"2024-08-11 00:19:03"}
{"File Name":"content-publisher\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4886,"Prediction":"Here is an Architectural Decision Record (ADR) for the software:\n\n**Decision**\n\nTitle: Use of Microservices Architecture for Scalability and Flexibility\n\n**Context**\n\nWe need to record the architectural decisions made on this project.\n\n**Problem**\n\nAs the project grows, we are facing scalability issues with our monolithic architecture. We need a solution that allows us to scale individual components independently, improve fault tolerance, and reduce the complexity of the system.\n\n**Decision**\n\nWe will adopt a microservices architecture for the project. This will allow us to break down the monolithic application into smaller, independent services that can be developed, tested, and deployed independently. Each service will be responsible for a specific business capability and will communicate with other services through APIs.\n\n**Rationale**\n\nThe microservices architecture will provide several benefits, including:\n\n* Scalability: We can scale individual services independently, which will improve the overall system's ability to handle increased traffic and load.\n* Flexibility: With microservices, we can develop and deploy services at different rates, which will allow us to adapt to changing business requirements more quickly.\n* Fault tolerance: If one service experiences issues, it will not bring down the entire system, as other services can continue to function independently.\n* Improved maintainability: With a smaller, more focused scope, individual services will be easier to maintain and update.\n\n**Consequences**\n\n* Increased complexity: With more services and APIs to manage, the system will become more complex, which will require additional resources and expertise to maintain.\n* Additional overhead: We will need to invest in additional infrastructure and tools to support the microservices architecture, such as service discovery and load balancing.\n* Changes to testing and deployment: We will need to adapt our testing and deployment processes to accommodate the new microservices architecture.\n\n**Status**\n\nThis decision is approved and will be implemented in the next release.\n\n**History**\n\n* 2023-02-20: Decision made\n* 2023-03-15: Implementation started\n* 2023-04-20: Implementation completed","GenTime":"2024-08-11 00:20:36"}
{"File Name":"klokwrk-project\/0002-strategic-project-structure.md","Context":"## Context\\nExcluding the simplest hello-world-like cases, any useful project typically contains several modules. The traditional way to organize project modules is just to put them under the project root.\\nWe can call that structure simply **flat structure**.\\nWhile the flat structure is appropriate and sufficient for simpler projects, when the project grows and the number of modules increases, the flat structure starts suffering from many drawbacks:\\n* Flat structure does not scale well when the number of modules grows.\\n* Flat structure is difficult and confusing to navigate with numerous modules at the same hierarchy level.\\n* Flat structure does not suggest a direction of dependencies between modules.\\n* Flat structure does not suggest abstraction levels of modules.\\n* Flat structure does not suggest where are the system's entry points.\\n* Flat structure can use only module names to provide hints about relations between modules. Unfortunately, even that possibility is rarely leveraged.\\n* Flat structure does not use any high-level constructs that may suggest how modules are organized and related.\\n* Negative usage aspects are getting worse and worse as we add additional modules.\\n* Flat structure often requires extracting modules in separate repositories just because confusion becomes unbearable with a larger number of modules.\\n* When using microservices, the flat structure practically forces us to use one project per microservice.\\n> Note: Terms **flat structure** and **strategic structure** (see below) are ad-hoc terms introduced just for this document. However, in the `klokwrk-project`, we may use them in other places for\\n> convenience.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**\\nOur project organization will follow principles and recommendations of **strategic structure** as defined below.\\n### Decision Details\\nWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategic\\nstructure focusing on the differences to the given concrete example.\\n#### Strategic structure in klokwrk\\nThe current project layout in the klokwrk looks like this:\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 asd\\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 booking\\n\u2502   \u2502       \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-commandside\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-view\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-rdbms-management\\n\u2502   \u2502       \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-boundary-web\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-out-customer\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502               cargotracking-booking-test-component\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-queryside\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-testcontainers\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502               cargotracking-lib-axon-cqrs\\n\u2502   \u2502               cargotracking-lib-axon-logging\\n\u2502   \u2502               cargotracking-lib-boundary-api\\n\u2502   \u2502               cargotracking-lib-boundary-query-api\\n\u2502   \u2502               cargotracking-lib-domain-model-command\\n\u2502   \u2502               cargotracking-lib-domain-model-event\\n\u2502   \u2502               cargotracking-lib-web\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-datasourceproxy-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-jackson-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-context\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-data-jpa\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-validation-springboot\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-archunit\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-datasourceproxy\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-hibernate\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-jackson\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-uom\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-constraint\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-validator\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-base\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-match\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-simple\\n\u2502   \u2502\\n\u2502   \u2514\u2500\u2500 other\\n\u2502       \u251c\u2500\u2500 platform\\n\u2502       \u2502       klokwrk-platform-base\\n\u2502       \u2502       klokwrk-platform-micronaut\\n\u2502       \u2502       klokwrk-platform-spring-boot\\n\u2502       \u2502\\n\u2502       \u2514\u2500\u2500 tool\\n\u2502               klokwrk-tool-gradle-source-repack\\n\u251c\u2500\u2500 support\\n\u2502   \u2514\u2500\u2500 ... (other files or directories)\\n\u2514\u2500\u2500 ... (other files or directories)\\nAt the top of the hierarchy, we have a project folder  - `klokwrk-project`. It is the equivalent of the whole system. In the strategic structure, the system name appears in the names of artifacts\\nconsidered to be conceptually at the level of a system.\\nRight below the root, we have `modules` and `support` folders. These should be the area of 99% of everyday work, with the `modules` folder taking a vast majority of that percentage.\\nThe `support` folder houses all kinds of supportive files like scripts, documentation, git hooks, etc. The `support` folder is free-form, and the strategic structure does not impose any\\nrecommendations or rules on its content. On the contrary, the strategic structure is applied to the content of the `modules` directory - the home of all source code modules in the system.\\nAt the 1st level of strategic structure - the system level, we have the content of the `modules` directory. It is divided into three subdirectories: `bc` (bounded context modules),\\n`lib` (system-level libraries), and `other` (miscellaneous helper modules).\\nAt the 2nd level - the bounded context level, we have the content of the `modules\/bc` directory that is further organized into three parts, `asd` (asd stands for **A** **S**ub**D**omain),\\n`domain-model` (bounded context domain model), and `lib` (bounded context libraries).\\nAt the 3rd level of a hierarchy, we have the content of the `modules\/bc\/[bounded-context-name]\/asd` directory that holds all bounded context's subdomains. The modules for each subdomain are further\\ndivided into `app` and `lib`. The `modules\/bc\/[bounded-context-name]\/asd\/[subdomain-name]\/app` directory contains the **subdomain applications** responsible for implementing concrete subdomain\\nscenarios. From the abstraction level and dependency perspectives, subdomain applications are at the top of the hierarchy. Subdomain applications speak the language of domain - the bounded context's\\nubiquitous language. They even contribute to it through the naming and meaning of use cases.\\nThe first thing that **subdomain libraries** (`modules\/bc\/[bounded-context-name]\/asd\/subdomain-name\/lib)` can hold is infrastructural code related to the technological choices made for that\\nparticular subdomain and are not reusable outside the subdomain. However, they can temporarily have infrastructural modules intended to be more reusable (either on the bounded context or system\\nlevels) at the end. Still, for whatever reason, it was more convenient to hold them at the subdomain level for a limited time.\\nThe second thing that can be found in subdomain libraries are business-related reusable modules that connect technological choices with the domain model. One characteristic example is the\\n`cargotracking-booking-lib-queryside-model-rdbms-jpa` module. Those kinds of modules do speak bounded context's ubiquitous language.\\nThe bounded context's **domain model** is implemented in `modules\/bc\/[bounded-context-name]\/domain-model`. Those modules contain the essence of the bounded context business logic. Implementation of\\nthe domain model should be free of technology as much as possible and practical. Adding external libraries is not strictly forbidden, but each addition should be conscious and must be carefully\\nevaluated. It is best to have tests that monitor and control the dependencies of a domain model. The domain model implements the majority of code-level representation of the bounded context's\\nubiquitous language and must be consistent across all bounded context's subdomains.\\nBy default, the directory `modules\/bc\/[bounded-context-name]\/lib` is the home of shareable **bounded context infrastructural libraries**. It contains modules with infrastructural code that is\\nreusable across the bounded context. Those modules are at a lower abstraction level than subdomain libraries. Bounded context infrastructural libraries do not speak domain language. However, they can\\nsupport the implementation of the domain model and other module groups higher in the hierarchy. Domain model should not generally depend on bounded context infrastructural libraries. Exceptions are\\nallowed but should be conscious and carefully managed.\\nDo note that another variant of bounded context libraries is also possible. It is a variant supporting the sharing of business logic at the bounded context level when necessary. In that case, instead\\nof a single `lib` directory, we would have `blib` and `ilib` directories. The `blib` directory would contain business-related modules that can depend on a domain model. On the contrary, the `ilib`\\ndirectory cannot use the domain model because it should contain infrastructural code only. The `ilib` directory role is the same as the role of `lib` directory from the default variant of bounded\\ncontext libraries.\\nLet's return to the `modules\/lib` directory containing general **system-level libraries**. It is divided into `hi`, `lo`, and `xlang` subdirectories. All system-level libraries are at lower\\ndependency and abstraction levels than any bounded context module.\\nAlthough separation on the high (`hi`) and low-level (`lo`) system libraries is somewhat arbitrary, it is helpful in practice. The `hi` directory is intended to contain\\n**high-level system libraries**, which are general infrastructural modules closer to the high-level technological frameworks (something like Spring, Spring Boot, or Axon frameworks) used in the\\nsystem. They could contain some specifics of our system, but usually, they do not. In that later case, they are general enough to be reused even outside of our system.\\nThe **low-level system libraries** from the `lo` directory deal with the customizations and extensions of widely used 3rd party libraries like Hibernate, Jackson, Java Bean validations, and similar.\\nBoth types of system-level libraries should not be, in general, dependencies of a domain model.\\nAt the lowest abstraction level, we have the **language extensions** (`modules\/lib\/xlang`). They focus on adding features to the programming language itself or its accompanying SDK (JDK in our case).\\nLanguage extensions can be used from everywhere, even from the domain model, without restrictions. Some of them are often written to ease the implementation of the domain model by making it more\\nexpressive and concise.\\n#### Characteristics of strategic structure\\nThe most important thing about strategic structure is not the structure itself but rather the distinguishing characteristics that it provides.\\nWe already mentioned abstraction levels and dependencies between groups of modules. If you look again at the example, you will notice that both of them are constantly flowing top to bottom through\\nthe strategic structure. For instance, subdomain applications depend on subdomain libraries. They both can depend on the domain model, which can depend on bounded context libraries and language\\nextensions. At the level of system libraries, high-level modules can depend on low-level modules, and they both can depend on the language extensions. However, none of the dependencies can come the\\nother way around. Dependencies are not allowed to flow from the bottom to the top.\\nWe have managed to do this because we applied strategic DDD concepts of bounded context and subdomains to the project structure. They provide sense and meaningfulness by connecting our code to the\\nbusiness. Without that business context, we will be left exclusively to the technical aspects, which are just insufficient. Technical aspects know nothing about the purpose of our system. They do not\\nknow anything about the business context.\\nDescribed characteristics bring important benefits when trying to understand or navigate through the system's code. Finding the desired functionality is much easier because we usually know, at least\\napproximately, where we should look for it. This can greatly reduce cognitive load while exploring unfamiliar (or even familiar) codebases.\\nIn addition, if you follow the proposed naming conventions for modules and their packages (see below), the same easy orientation can be applied at the package level or even if you pull out all\\nmodules into the flat structure. You will always know where to look for.\\n#### Naming conventions\\nYou have probably noticed that modules have very particular names reflecting their position in the strategic structure. The following table summarizes them as used in the example:\\n| Module group    | Naming scheme                                            | Example                                  |\\n|-----------------|----------------------------------------------------------|------------------------------------------|\\n| subdomain apps  | `[bounded-context-name]-[subdomain-name]-app-[app-name]` | `cargotracking-booking-app-commandside`  |\\n| subdomain libs  | `[bounded-context-name]-[subdomain-name]-lib-[lib-name]` | `cargotracking-booking-lib-boundary-web` |\\n| domain model    | `[bounded-context-name]-domain-model-[model-part-name]`  | `cargotracking-domain-model-aggregate`   |\\n| bc libs         | `[bounded-context-name]-lib-[lib-name]`                  | `cargotracking-lib-boundary-api`         |\\n| sys hi libs     | `[system-name]-lib-hi-[lib-name]`                        | `klokwrk-lib-hi-spring-context`          |\\n| sys lo libs     | `[system-name]-lib-lo-[lib-name]`                        | `klokwrk-lib-lo-jackson`                 |\\n| lang extensions | `[system-name]-lib-xlang-[lib-name]`                     | `klokwrk-lib-xlang-groovy-base`          |\\nModule naming conventions are essential because our modules are not always presented (i.e., try the Packages view in the IntelliJ IDEA's Project tool window) or used as a part of the hierarchy (think\\nof JAR names put in the same directory). For those reasons, our naming scheme closely follows the strategic structure hierarchy where parts of module names are directly pulled from corresponding\\nsubdirectory names. That way, we can keep the match between alphabetical order and the direction of dependencies.\\n> Note: When you have multiple bounded contexts and\/or multiple subdomains in the project, to get the exact match between alphabetical order and the direction of dependencies, you can use the `bc-`\\n> prefix in front of bounded context names and the `asd-` prefix for subdomain names.\\nThe same naming principles should also be applied to packages. Here are a few examples of package names:\\norg.klokwrk.cargotracking.booking.app.commandside.*\\norg.klokwrk.cargotracking.booking.lib.boundary.web.*\\norg.klokwrk.cargotracking.domain.model.aggregate.*\\norg.klokwrk.cargotracking.lib.boundary.api.*\\norg.klokwrk.lib.hi.spring.context.*\\norg.klokwrk.lib.lo.jackson.*\\norg.klokwrk.lib.xlang.groovy.base.*\\nWith those naming conventions, we should be able to avoid naming collisions on the module and package levels.\\n#### The general scheme of strategic structure\\nIn some circumstances, we may need additional elements in the strategic structure to deal with shared libraries at different levels. Examples of those, with sparse explanations, are given in the\\ngeneral scheme of strategic structure below:\\nmodules\\n\u251c\u2500\u2500 bc\\n\u2502   \u251c\u2500\u2500 my_food\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 restaurant\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 menu_management\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zshared         \/\/ sharing code between subdomains if necessary\\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib                 \/\/ bounded context libraries - default variant\\n\u2502   \u2502           ... *           \/\/ Can be split into \"blib\" and \"ilib\" directories when the sharing of\\n\u2502   \u2502                           \/\/ business logic is necessary at the level of a single bounded context\\n\u2502   \u251c\u2500\u2500 my_carrier\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 zshared                 \/\/ shared code between multiple bounded contexts (if necessary).\\n\u2502       \u2502                       \/\/ \"z\" prefix - funny reference to \"zee Germans\" from Snatch movie.\\n\u2502       \u2502                       \/\/ Moves \"zshared\" at the last place alphabetically, which matches\\n\u2502       \u2502                       \/\/ the proper place in terms of dependencies and abstraction levels.\\n\u2502       \u251c\u2500\u2500 domain-model\\n\u2502       \u2502       ... *\\n\u2502       \u2514\u2500\u2500 lib\\n\u2502               ... *\\n\u251c\u2500\u2500 lib\\n\u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502       ... *\\n\u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502       ... *\\n\u2502   \u2514\u2500\u2500 xlang\\n\u2502           ... *\\n\u2514\u2500\u2500 other            \/\/ supportive project's code for various \"other\" purposes\\n\u251c\u2500\u2500 build\\n\u2502       ... *\\n\u251c\u2500\u2500 tool\\n\u2502       ... *\\n\u2514\u2500\u2500 ...\\n#### Simplification - the case of bounded context boundaries matching 1:1 with subdomain\\nThe one-to-one match between bounded context boundaries and corresponding subdomain is considered to be the \"ideal\" case, and it is relatively common in practice. When we know how a fully expanded\\nstrategic structure works and looks like, it is relatively easy to come up with simplification for this particular case.\\nHere are \"refactoring\" steps and the example based on our concrete example from the beginning of this document:\\n- move subdomain applications to the bounded context level\\n- merge subdomain libraries with bounded context libraries\\n- split bounded context libraries into `blib` and `ilib` directories if necessary\\n- rename corresponding modules and packages\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       cargotracking-app-commandside\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-view\\n\u2502   \u2502       \u2502       cargotracking-app-rdbms-management\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 blib\\n\u2502   \u2502       \u2502       cargotracking-blib-out-customer\\n\u2502   \u2502       \u2502       cargotracking-blib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 ilib\\n\u2502   \u2502               cargotracking-ilib-axon-cqrs\\n\u2502   \u2502               cargotracking-ilib-axon-logging\\n\u2502   \u2502               cargotracking-ilib-boundary-api\\n\u2502   \u2502               cargotracking-ilib-boundary-query-api\\n\u2502   \u2502               cargotracking-ilib-boundary-web\\n\u2502   \u2502               cargotracking-ilib-domain-model-command\\n\u2502   \u2502               cargotracking-ilib-domain-model-event\\n\u2502   \u2502               cargotracking-ilib-web\\n\u2502   \u2502               cargotracking-test-component\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502               cargotracking-test-support-queryside\\n\u2502   \u2502               cargotracking-test-support-testcontainers\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 other\\n\u2502           ... *\\n\u251c\u2500\u2500 support\\n\u2502       ... *\\n\u2514\u2500\u2500 ... *\\n","tokens":335,"id":4889,"Prediction":"## Decision\n\nTo address the limitations of the traditional flat structure and improve the organization and navigation of the project's modules, we will adopt a strategic structure for the `klokwrk-project`. This structure will provide a clear and scalable way to organize modules, suggest dependencies, abstraction levels, and entry points, and reduce confusion and complexity.\n\n## Context\n\nThe context for this decision is the growing complexity of the `klokwrk-project`, which currently consists of multiple modules and is expected to continue to grow. The traditional flat structure is no longer sufficient to manage the complexity and provide a clear understanding of the relationships between modules.\n\n## Solution\n\nWe will adopt a strategic structure for the `klokwrk-project`, which will consist of the following components:\n\n* A clear and consistent naming convention for modules and their subdirectories\n* A hierarchical organization of modules, with higher-level modules containing lower-level modules\n* A clear distinction between modules that provide entry points and those that provide internal functionality\n* A use of high-level constructs, such as packages and subdirectories, to suggest how modules are organized and related\n* A clear documentation of the relationships between modules and their dependencies\n\n## Rationale\n\nThe strategic structure will provide a clear and scalable way to organize modules, reducing confusion and complexity and making it easier to navigate the project. It will also provide a clear understanding of the relationships between modules and their dependencies, making it easier to maintain and extend the project.\n\n## Consequences\n\nThe strategic structure will require a significant reorganization of the project's modules and will require additional documentation and communication to ensure that all team members understand the new structure. However, the benefits of a clear and scalable organization will outweigh the costs, and the project will be easier to maintain and extend in the long run.\n\n## Decision Owner\n\nThe decision owner for this architectural decision is [Your Name].\n\n## Reviewers\n\nThe reviewers for this architectural decision are [List of Reviewers].\n\n## Links\n\n* [Link to relevant documentation or design documents]\n* [Link to relevant code or architecture diagrams]","GenTime":"2024-08-11 00:21:48"}
{"File Name":"klokwrk-project\/0014-commit-message-format.md","Context":"## Context\\nUsing commit messages without any structure looks convenient for developers as they do not have to think about messages too much. Unfortunately, that freedom and lack of thinking can impose some\\nadditional burden on long-term project maintenance.\\nQuite often, we can find incomprehensible commit messages that do not communicate anything useful. Hopefully, imposing some lightweight rules and guidance will help developers create commit messages\\nthat are helpful for their colleagues.\\nIn addition, with unstructured commit messages, there is much less opportunity to introduce any tools on top of commit history. For example, we would like to employ an automated changelog generator\\nbased on extracting some semantical meaning from commits, but this will not work if commit messages lack any structure. Without the commit message structure, we can just dump the commit log in the\\nchangelog, which does not make the changelog more helpful than looking at the history of commits in the first place.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use a customized [conventional commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) format for writing commit messages.**\\nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:\\n<type>(optional <scope>): <description> {optional <metadata>}\\nOur customization:\\n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-message-guidelines)\\n- allows adding additional metadata in the message title if useful and appropriate (see details section for more info)\\n- requires format compliance only for messages of \"significant\" commits (see details section for more info)\\n### Decision details\\nDetails about the decision are given mainly as a set of strong recommendations (strongly recommended) and rules enforced by tooling (rule). In our case, the tooling is implemented as git commit hooks.\\nEvery contributor should install git hooks provided in this repository. That can be done with following command (executed from the project root):\\ngit config core.hooksPath support\/git\/hooks\\nThere might be cases when implemented rules are not appropriate and should be updated or removed or just temporarily ignored. In such scenarios, hooks can be skipped with git's `--no-verify` option.\\nWhile describing details, following terms are used as described:\\n- *commit message title*: refers to the first line of a commit message\\n- *commit message description*: refers to the part of the title describing a commit with human-readable message. In conventional commits specification that part is called `description`.\\n#### General guidance and rules for all commit messages\\n- (strongly recommended) - avoid trivial commit messages titles or descriptions\\n- (strongly recommended) - use imperative mood in title or description (add instead of adding or added, update instead of updating or updated etc.) as you are spelling out a command\\n- (rule) - message title or description must start with the uppercase letter <br\/>\\n<br\/>\\nThe main reason is a desire for better readability as we want easily spot the beginning message description or title. There are some arguments for using the lowercase like \"message titles are not\\nsentences\". While this is true, we prefer to have better readability than comply with some vague constraints.<br\/>\\n<br\/>\\n- (rule) - message title or description should not end with common punctuation characters: `.!?`\\n- (strongly recommended) - message title or description should not be comprised of multiple sentences\\n- (rule) - message title should not be longer than 120 characters. Use the message body if more space for description is needed<br\/>\\n<br\/>\\nActually, there is a common convention that we should not use more than 69 characters in the message title. It looks like the main reason for it is that GitHub truncates anything above 69 chars\\nfrom message titles. Having such a tight constraint seems unreasonable today, and the apparent shortcomings of any tool shouldn't restrict us, even if the tool is GitHub.<br\/>\\n<br\/>\\n- (strongly recommended) - commit message title or description should describe \"what\" (and sometimes \"why\"), instead of \"how\"<br\/>\\n<br\/>\\nFor describing \"why\", the message body is more appropriate as we have more space there. If needed, the message body may contain \"how\" too, but it should be clearly separated (at least with a blank\\nline) from \"what\" and \"why\".<br\/>\\n<br\/>\\n- (recommended) - commit message title should provide optional scope (from conventional commit specification) if applicable\\n- if commit refers to multiple scopes, scopes should be separated with `\/` character\\n- if commit refers to the work which influences the whole project, the scope should be `project` or it can be left out\\n- the scope should be a single word in lowercase<br\/>\\n<br\/>\\n- (strongly recommended) - message body must be separated from message title with a single blank line\\n- (option) - message body can contain additional blank lines\\n- (recommended) - message body should not use lines longer than 150 characters\\n- (strongly recommended) - include relevant references to issues or pull request to the metadata section of message title<br\/>\\n<br\/>\\nExample: `feat(some-module): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}`<br\/>\\n<br\/>\\n- (option) - include relevant feature\/bug ticket links in message footer according to conventional commits guidelines<br\/>\\n<br\/>\\nFooter is separated from body with a single blank line.\\n#### Guidance and rules for \"normal\" commits to the main development branch\\n- (rule) - all commits to the main development branch must have a message title in customized conventional commit format\\n#### Guidance and rules for merge commits to the main development branch\\nWhen used with [semi-linear commit history](.\/0007-git-workflow-with-linear-history.md), merge commits are the primary carriers of completed work units. As such, they are the most interesting for\\ncreating a changelog.\\nBefore merging, merge commits must be rebased against main development branch, and merging must be executed with no-fast-forward option (`--no-ff`).\\n- (rule) - merge commits must have 'merge' metadata (`{m}`) present at the end of the title <br\/>\\n<br\/>\\nThat way, merge commits can be easily distinguished on GitHub and in the changelog.\\n- (option) - merge commit metadata can carry additional information related to the issues and PRs references like in the following example\\nfeat(klokwrk-tool-gradle-source-repack): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}\\nHere, `i#123` is a reference to the issue, while `pr#12` is a reference to the pull request. Additional metadata are not controlled or enforced by git hooks.\\n#### Guidance and rules for normal commits to the feature branches\\n- (option) - normal commits don't have to follow custom conventional commits format for message title\\n- (strongly recommended) - normal commits should use conventional commits format when contained change is significant enough on its own to be placed in the changelog\\nWhen all useful changelog entries are contained in normal commits of a feature branch, we can do two different things depending on the situation:\\n- use merge commit with type of `notype`. Such merge commit will be ignored when creating a changelog.\\n- merge a branch with fast-forward option (no merge commit will be present)\\nPreferably, use `notype` merge commits, as they are still useful for clear separation of related work.\\n#### Types for conventional commits format\\n- common (angular)\\n- `feat` or `feature` - a new feature\\n- `fix` - a bug fix\\n- `docs` - documentation only changes\\n- `style` - changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\\n- `test` - adding missing tests or correcting existing tests\\n- `build` - changes that affect the build system or external dependencies\\n- `ci` - changes to our CI configuration files and scripts\\n- `refactor` - a code change that neither fixes a bug nor adds a feature\\n- `perf` - a code change that improves performance\\n- `chore` - routine task\\n- custom\\n- `enhance` or `enhancement` - improvements to the existing features\\n- `deps` - dependencies updates (use instead of `build` when commit only updates dependencies)<br\/>\\n<br\/>\\nThere are two main scenarios when upgrading dependencies, a simple version bump and the more involved upgrade requiring resolving various issues like compilation errors, API upgrades, etc.<br\/>\\n<br\/>\\nSimple version bumps should be contained in a single individual commit with a description message starting with the word \"Bump\". For example: `deps: Bump Micronaut to 2.5.2 version`.<br\/>\\n<br\/>\\nMore complicated upgrades should be organized as feature branches where each non-conventional commit resolves a single step in the process. When finished, the feature branch should be merged\\ninto the main development branch with a description starting with the word \"Upgrade\". For example: `deps: Upgrade Spring Boot to 2.5.0 version {m}`.<br\/>\\n<br\/>\\n- `task` - same meaning as `chore`. Prefer using `task`.\\n- `article` - use instead of `docs` when changes are related only to articles\\n- `misc` - anything that does not fit into previous categories\\n- `notype` - only for merge commits in situations where contained plain commits carries all relevant types and merge commit doesn't add anything useful. It is ignored in changelog.<br\/>\\n<br\/>\\nTypical example is when we are updating number of dependencies through commits in a feature branch. In this situation each feature branch commit should contain updates to a single dependency\\nand communicate the update through its own commit message in conventional commit format. When all updates in the branch are tested, we can merge them in the main branch. However, that merge\\nshould be ignored by changelog as it does not communicate anything new.\\n","tokens":205,"id":4890,"Prediction":"## Decision\n\nTo improve the maintainability and usability of our commit history, we will adopt a structured commit message format. This will help developers create clear and concise commit messages that provide useful information for their colleagues, and also enable us to introduce tools that can extract semantical meaning from commits.\n\n### Context\n\nUsing commit messages without any structure looks convenient for developers as they do not have to think about messages too much. Unfortunately, that freedom and lack of thinking can impose some additional burden on long-term project maintenance. Quite often, we can find incomprehensible commit messages that do not communicate anything useful. Hopefully, imposing some lightweight rules and guidance will help developers create commit messages that are helpful for their colleagues.\n\n### Decision Criteria\n\n* Improve maintainability and usability of commit history\n* Enhance clarity and concision of commit messages\n* Enable extraction of semantical meaning from commits for automated changelog generation\n\n### Decision\n\nWe will adopt the following structured commit message format:\n\n* Type: Briefly describe the type of change (e.g. fix, feature, refactor)\n* Scope: Identify the affected area or component (e.g. core, frontend, backend)\n* Description: Provide a concise description of the change\n* References: Include relevant issue or task numbers, if applicable\n\n### Consequences\n\n* Developers will need to follow a structured format for commit messages, which may require some additional effort upfront\n* The commit history will become more readable and maintainable, making it easier for colleagues to understand the changes made\n* We will be able to introduce automated changelog generation tools that can extract semantical meaning from commits, providing a more helpful changelog for users","GenTime":"2024-08-11 00:22:06"}
{"File Name":"klokwrk-project\/0003-cqrs-and-event-sourcing-for-applications.md","Context":"## Context\\nWhen building monoliths or microservices that contain non-trivial business logic, we want to have a high-level component architecture pattern that natively leverages a strategic Domain Driven\\nDesign (DDD) concepts like ubiquitous language and bounded context. It should also support or allow, at least, the usage of application design architecture patterns like hexagonal architecture.\\nAt the lower technical level, component architecture pattern should provide native support for tactical DDD concepts like aggregates, entities, value objects, and events.\\nWhen building microservices, the chosen component architecture pattern should natively support asynchronous communication through events.\\nIt would be ideal to have all of these in a single coherent open-source framework and platform.\\nFor anemic CRUD style applications, more traditional approaches, like layered architecture, would be just fine.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**\\nEspecially as one of the major `klokwrk-project` goals is to demonstrate the creation of complex CQRS\/ES microservice applications. We can use more traditional approaches like classic layered\\narchitecture and anemic model for anemic CRUD style applications.\\n","tokens":182,"id":4896,"Prediction":"## Decision\n\nAfter careful consideration of the requirements and constraints, we have decided to adopt the Clean Architecture pattern as the high-level component architecture for our system (`klokwrk-project`). This pattern provides a robust foundation for leveraging Domain-Driven Design (DDD) concepts like ubiquitous language and bounded context, while also supporting application design architecture patterns like hexagonal architecture.\n\nClean Architecture is well-suited for our needs because it:\n\n1. Natively supports strategic DDD concepts like ubiquitous language and bounded context.\n2. Allows for the usage of application design architecture patterns like hexagonal architecture.\n3. Provides native support for tactical DDD concepts like aggregates, entities, value objects, and events.\n4. Can be easily adapted to support asynchronous communication through events in microservices.\n\nAdditionally, Clean Architecture is a flexible and scalable pattern that can be applied to a wide range of applications, from monoliths to microservices. It also provides a clear separation of concerns, which makes it easier to maintain and evolve the system over time.\n\nWhile Clean Architecture may require a bit more upfront investment in terms of learning and implementation, we believe it is the best choice for our system given its benefits and the requirements outlined in the context.","GenTime":"2024-08-11 00:24:17"}
{"File Name":"klokwrk-project\/0007-git-workflow-with-linear-history.md","Context":"## Context\\nThe value of tidy and [semi-linear commit history](https:\/\/fangpenlin.com\/images\/2013-09-30-keep-a-readable-git-history\/source_tree_new_branch_rebase_merge.png) is often overlooked in many Git-based\\nprojects. This is unfortunate since non-linear git commit history might be a [horrible mess](https:\/\/tugberkugurlu.blob.core.windows.net\/bloggyimages\/d773c1fe-4db8-4d2f-a994-c60f3f8cb6f0.png) that\\ndoes not provide any useful information. We want to use as simple as possible git workflow that promotes and ensures a semi-linear history.\\n> * **Semi-linear** commit history usually refers to a history that uses merge commits (git \"no-fast-forward\" merge option) to clearly denote which commits are meant to be together and represent a\\n>   coherent whole.\\n> * **Linear** commit history usually refers to completely flat history (git default \"fast-forward\") where it is impossible to tell at first glance which commits belong together.\\nWhen working on individual features, related git commits can be organized either as \"work log\" or as a \"recipe\". When working in a team, it is crucial that team members and\/or reviewers can easily\\ncomprehend what is going on in a particular feature. For this reason, we prefer features to be organized as \"recipes\".\\n> * **Work log** style of organizing feature commits refers to the style without any organization. Commits are added solely as they are developed through time.\\n> * **Recipe** style of organizing feature commits refers to the style where commits have a sensible organization where peer developers can clearly see and learn how the feature is created. This\\n>   style requires some additional work as its primary goal is communication, instead of just implementing a feature.\\nVery often, in bigger teams, common git workflows have a problem of broken continuous integration builds. We want to embrace and use as simple as possible workflow that resolves that problem.\\nChosen git workflow should seamlessly support release versioning and, if needed, related work on release branches.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use a [stable mainline branching model for Git](https:\/\/www.bitsnbites.eu\/a-stable-mainline-branching-model-for-git\/).** It\\n[supports semi-linear Git history](https:\/\/www.bitsnbites.eu\/a-tidy-linear-git-history\/) and helps to resolve the problem of broken continuous integration builds.\\n**We will, however, introduce several tweaks to the \"stable mainline branching model\":**\\n* We will use the following naming pattern for feature branches: **`feature_<name>`** instead of `feature\/name`.\\n* We will not use new branches when remote feature branches need rebasing. Instead, we will just inform all collaborators that rebasing is pending for a feature branch. It is important\\nto communicate with collaborators **before** rebasing and force-pushing.\\n* We will not create a release branch for each release. In general, we will just tag a release. However, we will create a release branch when a particular release needs fixing.\\nWe will organize our [feature commits as recipes](https:\/\/www.bitsnbites.eu\/git-history-work-log-vs-recipe\/) because we want to promote the team's learning and communication.\\nWe will also use specific commit message format as described in [ADR-0014 - Commit Message Format](.\/0014-commit-message-format.md)\\n","tokens":461,"id":4899,"Prediction":"## Decision\n\n**Decision:** Use a Git workflow that promotes a semi-linear commit history, organized as \"recipes\", to ensure a clear and comprehensible commit history.\n\n**Context:** As described in the provided context, we want to use a simple Git workflow that promotes and ensures a semi-linear history, which is easily comprehensible by team members and reviewers. We prefer features to be organized as \"recipes\" rather than \"work logs\", and we want to resolve the problem of broken continuous integration builds.\n\n**Rationale:** A semi-linear commit history using merge commits will provide a clear and organized representation of the feature development process, making it easier for team members and reviewers to understand the changes made. This will also help to ensure that the feature development process is transparent and reproducible. The \"recipe\" style of organizing feature commits will provide a clear and structured approach to feature development, making it easier for others to learn from and build upon the work.\n\n**Consequences:**\n\n* The team will need to adopt a more structured approach to feature development, which may require additional effort upfront.\n* The commit history will be more organized and easier to understand, making it easier for team members and reviewers to comprehend the changes made.\n* The workflow will support release versioning and related work on release branches seamlessly.\n* The continuous integration builds will be less likely to break due to the structured approach to feature development.\n\n**Implementation:** We will implement this workflow by using Git merge commits to denote the start and end of feature development, and by organizing feature commits in a structured and logical manner. We will also establish guidelines and best practices for feature development and commit organization to ensure that the team follows the workflow consistently.","GenTime":"2024-08-11 00:25:23"}
{"File Name":"architecture-decision-log\/0016-analytics-foundations.md","Context":"## Context\\nOur company is starting to growth fast. With that growth, it is common to see the need of complex data analysis. We've solved that by installing Metabase in a read-replica of our OKR transactional database, but even that structure lacks more complex analytics. Concurrently with the previous statement, our company plans to create an analytics product for our customers, enabling real-time complex analysis of their users.\\nWe can't ignore the need to have a proper analytics foundations inside Bud. Also, we can't afford investing a large amount of time building that infrastructure, since everything could change fast. We need to find a way to create a flexible analytics infrastructure that could:\\n(a) Provide meaningful data regarding our customers;\\n(b) Be flexible enought to integrate with multiple sources;\\n(c) Allow the usage from external applications.\\nIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.\\n## Decision Drivers\\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\n","Decision":"1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\nAfter evaluating all options, we've decided to proceed with Airbyte. It meets almost every specification that we have. It is extremelly easy to implement and follows all the best standards. It isn't an in-house solution, but in the current scenario we're on that would not be a big deal with it. Also, we could learn from it and maybe create a new tool in the future, designed to met our needs.\\n### Positivo Consequences\\nWith this infrastructure, we're going to achieve a robust ELT infrastructure, with little effort. We can easily create an analytics application that is going to serve all our business requirements with minimal effort. Also, Airbyte uses DBT under the hood, that being said, even if we need to change our ELT structure, we would still be able to migrate our DBT project.\\n### Negative Consequences\\nThere are two main negative consequences of this decision:\\n#### 1. Not being able to query real time data\\nAs pointed by Marcelo Travi, with Airbyte we would not be able to query real-time data. In most use-cases that would not be an issue, but considering that our first requirement is serving an evolution graph of a given key-result percentual proggress, that should be an issue. To fix that, we suggest using only D-1 data from our analytics, and using the transactional data to enrich it.\\n#### 2. Large queries could affect our applications\\nAs pointed by Victor Perin, since this is not a common implementation, we should be aware that complex analytics queries could impact our applications, since we're integrating them directly. There are plenty of ways to fix that, the easiest one would be creating a read-replica of our data warehouse, and using that to query the analytics. By doing so, we would separate both analytics and BI, minizing this issue.\\n","tokens":240,"id":4909,"Prediction":"## Decision\n\nAfter careful consideration of the context and decision drivers, I propose that we implement a data warehousing solution using Apache Druid and Apache Superset.\n\n### Rationale\n\n1. **Flexibility**: Apache Druid is a distributed, column-oriented database that allows for flexible schema design and real-time data ingestion. It can handle large amounts of data and scale horizontally, making it an ideal choice for complex analytics workloads.\n2. **Ease of integration with external sources**: Apache Superset is a modern, open-source business intelligence platform that provides a simple and intuitive interface for data exploration and visualization. It supports multiple data sources, including Druid, and allows for easy integration with external applications.\n3. **Implementation difficulty**: While implementing a data warehousing solution requires significant effort, Apache Druid and Apache Superset are well-established projects with large communities and extensive documentation, making it easier to find resources and expertise.\n\n### Benefits\n\n* Provides a scalable and flexible analytics infrastructure that can handle complex data analysis and real-time data ingestion\n* Allows for easy integration with multiple sources, including our OKR transactional database and external applications\n* Enables real-time complex analysis of customer data, providing meaningful insights for our analytics product\n* Supports a wide range of data visualization and exploration capabilities through Apache Superset\n\n### Trade-offs\n\n* Requires significant upfront investment in infrastructure and development resources\n* May require additional training and expertise for team members to work with Apache Druid and Apache Superset\n\n### Conclusion\n\nIn conclusion, implementing a data warehousing solution using Apache Druid and Apache Superset provides a flexible, scalable, and easy-to-integrate analytics infrastructure that meets our requirements and enables us to provide real-time complex analysis of customer data. While it requires significant upfront investment, it will provide a solid foundation for our analytics product and enable us to adapt to changing requirements in the future.","GenTime":"2024-08-11 00:28:33"}
{"File Name":"architecture-decision-log\/0013-microservices-overview.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWithin this ADR, we're going to keep an overview of our current microservices architecture. Each application has a pre-defined boundary, as well as a well-defined technical definition.\\nYou can take a look at our [live architecture](https:\/\/app.mural.co\/t\/d4c6342\/m\/d4c6342\/1602612797113\/5836de51b220fa5ebc8cefd7ad0f161125610524) or check the diagram below:\\n![Diagram showing our microservices architecture](..\/assets\/0013-microservices-overview\/diagram.png)\\n","tokens":53,"id":4910,"Prediction":"Here is an Architectural Decision Record (ADR):\n\n**Context**\nWe are building a real-time analytics platform that collects and processes large amounts of data from various sources. The platform needs to handle high volumes of data and provide fast query responses to meet the business requirements.\n\n**Decision**\nWe will use a distributed database architecture with a combination of in-memory data grid and disk-based storage to store and process the data. This architecture will provide the necessary scalability, performance, and reliability for our platform.\n\n**Status**\nApproved\n\n**Consequences**\n* The platform will be able to handle high volumes of data and provide fast query responses.\n* The distributed architecture will provide scalability and reliability, allowing the platform to grow with the increasing data volume and user base.\n* The use of in-memory data grid will provide fast access to frequently accessed data, reducing the latency of query responses.\n* The disk-based storage will provide a durable and persistent storage for the data, ensuring that it is not lost in case of a failure.\n* The platform will require additional resources and infrastructure to set up and maintain the distributed architecture.\n\n**More reading**\n* \"Distributed Database Systems\" by Goetz Graefe\n* \"In-Memory Data Grids\" by Gigaom Research\n\n**Updates**\n* 2023-02-15: The decision has been approved and the implementation has started.\n* 2023-03-15: The implementation is complete and the platform is being tested.\n* 2023-04-15: The platform has been deployed to production and is being monitored for performance and scalability.","GenTime":"2024-08-11 00:28:50"}
{"File Name":"architecture-decision-log\/0007-behavior-driven-development.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.\\nInstead of writing complex test statements, we use a BDD tool to create a readable phrase, like the following:\\n```txt\\nGiven a new created user\\nWhen that user places a new order\\nThen a notification must be sent\\n```\\nEach line of that phrase relates to a given function that will execute steps and persist the state machine's context. We can use assertions to return errors along the way.\\nBDD is a powerful tool that enhances our TDD cycle.\\n","tokens":53,"id":4918,"Prediction":"Here is an example of an Architectural Decision Record:\n\n**Context**\nAs our application grows, we are experiencing issues with scalability and performance. Our current database schema is not optimized for large amounts of data, leading to slow query times and increased latency. We need to decide on a new database schema that can handle our growing data volume and improve our application's overall performance.\n\n**Decision**\nWe will use a sharded database schema, where we split our data into multiple smaller databases based on a specific criteria (e.g. user ID). This will allow us to distribute the load more evenly and improve query performance.\n\n**Status**\nApproved\n\n**Consequences**\n* We will need to implement a load balancer to distribute traffic across the shards.\n* We will need to update our application code to handle the new database schema.\n* We may need to re-index our data to optimize for the new schema.\n* We will need to monitor our database performance closely to ensure that the shards are not becoming bottlenecks.\n\n**More reading**\n* [Sharding](https:\/\/en.wikipedia.org\/wiki\/Sharding)\n* [Database performance optimization](https:\/\/www.dbta.com\/Featured\/Articles\/Database-Performance-Optimization-Strategies-115555.aspx)\n\n**Updates**\n* 2023-02-20: Implemented load balancer and updated application code to handle sharded database schema.\n* 2023-03-15: Re-indexed data to optimize for new schema.\n\nLet me know if you want me to add anything else!","GenTime":"2024-08-11 00:30:53"}
{"File Name":"architecture-decision-log\/0002-github-template-for-issues-and-pull-requests.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\nAn easy way to improve our communication is by creating templates for both `issues` and `pull requests`. Those templates must be simple, easy to use, and express all required sections for our developers and users.\\nYou can check the suggested templates [in the following folder](..\/assets\/0002-github-template-for-issues-and-pull-requests).\\n","tokens":34,"id":4919,"Prediction":"Here is the Architectural Decision Record:\n\n**Context**\nWe are building a new e-commerce platform that needs to handle a large volume of user requests and process transactions efficiently. The platform will be built using a microservices architecture, with multiple services responsible for different aspects of the business logic.\n\n**Decision**\nTo improve the scalability and performance of the platform, we will use a message queue (Apache Kafka) to handle communication between the services. This will allow us to decouple the services and handle failures and retries in a more robust way.\n\n**Status**\nApproved\n\n**Consequences**\n\n* We will need to implement a producer and consumer for each service that needs to send and receive messages.\n* We will need to handle message retries and failures in a way that is transparent to the end user.\n* We will need to implement a mechanism for handling message acknowledgments and ensuring that messages are not lost in transit.\n* We will need to monitor and troubleshoot the message queue to ensure that it is performing correctly and efficiently.\n* The use of a message queue will add complexity to the system, but it will also provide a number of benefits, including improved scalability and fault tolerance.","GenTime":"2024-08-11 00:31:06"}
{"File Name":"road-registry\/002-track-municipalities.md","Context":"## Context\\nWe need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street.\\nWe can't rely on the geometries used in the municipality registry because they are (1) not exposed and (2) they do not match the boundary along which road segments have been split up.\\nWhen the boundaries in the municipality registry change, one can't expect all segments to change as well.\\n","Decision":"We keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.\\nWe use `ImportedMunicipality` as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.\\n","tokens":96,"id":4930,"Prediction":"## Decision\n\n**Decision:** To store and maintain a separate dataset of municipality boundaries, which will be used to derive the municipality of road segments that do not have an associated left or right street.\n\n**Context:** As per the user's requirement, we need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street. The municipality registry geometries are not exposed and do not match the boundary along which road segments have been split up. Moreover, when the boundaries in the municipality registry change, it's not feasible to expect all segments to change as well.\n\n**Rationale:**\n\n1. **Data Independence**: By storing and maintaining a separate dataset of municipality boundaries, we ensure that our system is not dependent on the municipality registry for this information. This allows us to make changes to our system independently of the municipality registry.\n2. **Data Consistency**: By having a separate dataset, we can ensure that the boundaries used to derive the municipality of road segments are consistent and accurate, even if the boundaries in the municipality registry change.\n3. **Flexibility**: Having a separate dataset provides flexibility to use different boundary sources or to update the boundaries independently of the municipality registry.\n4. **Scalability**: Storing and maintaining a separate dataset allows us to scale our system independently of the municipality registry, which may have its own scalability limitations.\n\n**Consequences:**\n\n1. **Additional Data Storage**: We will need to store and maintain a separate dataset of municipality boundaries, which will require additional storage space and maintenance efforts.\n2. **Data Synchronization**: We will need to ensure that the separate dataset of municipality boundaries is synchronized with the municipality registry boundaries, which may require additional development efforts.\n3. **Data Validation**: We will need to validate the separate dataset of municipality boundaries to ensure that it is accurate and consistent with the municipality registry boundaries.\n\n**Implementation:**\n\n1. **Data Storage**: We will store the municipality boundaries in a separate database or data storage system, such as a spatial database or a geospatial data storage service.\n2. **Data Maintenance**: We will maintain the separate dataset of municipality boundaries by periodically updating it with the latest boundaries from the municipality registry or other sources.\n3. **Data Synchronization**: We will implement a data synchronization mechanism to ensure that the separate dataset of municipality boundaries is synchronized with the municipality registry boundaries.\n4. **Data Validation**: We will implement data validation mechanisms to ensure that the separate dataset of municipality boundaries is accurate and consistent with the municipality registry boundaries.","GenTime":"2024-08-11 00:33:54"}
{"File Name":"road-registry\/006-provisional-changes.md","Context":"## Context\\nAs a result of running the feature-compare process a bunch of files are produced. These files are essentially a _diff_ between what the operator started out with, that is the dump \/ extract he \/ she started working from, and what they ended up with, after having edited shapes and associated data in their editor. Some of those files have a `_ALL.DBF|_ALL.SHP` suffix. These files are the files we base our logic on to validate and translate to an _internal_ change request. Most notably, these files have an extra column called `RECORDTYPE` which, as the name suggests, classifies a record as either being `IDENTICAL`, `ADDED`, `MODIFIED` or `REMOVED`. Next to that most of the files contain, per record, the equivalent of a primary key that identifies a row. This primary key is used in other files as a foreign key to reference a record sitting in another file. Records with a `RECORDTYPE` of `ADDED` will use a temporary identifier (a really big number that we assume is free to use) to make this work (see [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) for how that works). One gotcha is that the primary keys are not always unique, that is, they can appear multiple times in the `*_ALL.DBF|*_ALL.SHP` files, once for each record type. A common scenario is a modification represented as a removal and an addition record.\\nFor the `WEGSEGMENT_ALL.DBF` file, things are more complicated ... next to having a `WS_OIDN` column act as primary key it has a `EVENTIDN` column acting as an alternative primary key in some cases. In case the `RECORDTYPE` is `ADDED` and the `EVENTIDN` has a value differing from `0`, the `WS_OIDN` column refers to an existing road segment and the `EVENTIDN` column refers to its new representation. In such a case, other files refer to a road segment by the value found in the `EVENTIDN`, not by the value in `WS_OIDN`. Alas, such is life ...\\n","Decision":"Modifying a road segment involves data from `WEGSEGMENT_ALL.DBF`, `WEGSEGMENT_ALL.SHP` and `ATTRIJSTROKEN_ALL.DBF`, `ATTWEGBREEDTE_ALL.DBF`, `ATTWEGVERHARDING_ALL.DBF` - that is, when it is represented as an _internal_ change request command. Each of those `.DBF` files contains a `RECORDTYPE` column. As such, a road segment could be marked as identical in `WEGSEGMENT_ALL.DB`, yet it's lanes, width and \/ or surfaces could be marked as a mixture of modified, removed, added, identical. This is the reason why a road segment that is identical is appended as a `provisional` change to the list of translated changes. Because we're not sure, just yet, that it is an actual change. The order in which these files are translated causes a `provisional` change, if warranted, to be promoted to an actual change.\\nWhy go thru all this trouble? Well, lanes, widths, and surfaces are tightly coupled to the geometry of a road segment such that it makes sense to capture them as a holistic change rather than as individual fragmented changes. There's still a bigger debate to be had about which pieces of data change together ...\\n","tokens":467,"id":4931,"Prediction":"## Decision\n\n**Decision:** We will implement a custom parser to process the `_ALL.DBF|_ALL.SHP` files produced by the feature-compare process, in order to extract the necessary information for validating and translating to an internal change request.\n\n**Context:** As described in the context, the `_ALL.DBF|_ALL.SHP` files contain a complex structure with multiple columns, including `RECORDTYPE`, `WS_OIDN`, and `EVENTIDN`. The files also contain non-unique primary keys, which can appear multiple times in the same file. Additionally, the `WEGSEGMENT_ALL.DBF` file has a more complex structure with an alternative primary key.\n\n**Problem:** The existing logic is not able to correctly process these files, leading to issues with validating and translating the change requests.\n\n**Reasoning:** Implementing a custom parser will allow us to specifically handle the complexities of these files, including the non-unique primary keys and the alternative primary key in the `WEGSEGMENT_ALL.DBF` file. This will ensure that the logic is able to correctly extract the necessary information for validating and translating the change requests.\n\n**Consequences:**\n\n* The custom parser will need to be maintained and updated as the file format changes.\n* The parser will need to be thoroughly tested to ensure it is able to correctly process the files.\n* The parser will add complexity to the system, but will provide a more robust solution for processing the `_ALL.DBF|_ALL.SHP` files.\n\n**Alternatives:**\n\n* Attempting to use a generic database library to process the files, which may not be able to handle the complexities of the files.\n* Manually parsing the files using string manipulation, which may be error-prone and difficult to maintain.\n\n**Decision Criteria:**\n\n* The ability to correctly process the complexities of the files.\n* The maintainability and scalability of the solution.\n* The ability to ensure the logic is able to correctly extract the necessary information for validating and translating the change requests.","GenTime":"2024-08-11 00:34:17"}
{"File Name":"godspeed-you-blocked-developer\/003. Serverless.md","Context":"## Context\\nServices become unique and fragile when you configure them by hand. I want to have a provisioning strategy before I actually start adding components, so I don't end up with behavior that's hard to reproduce.\\nI like the principles in the book Infrastructure as Code. It suggests avoiding making changes to services except in an automated, configured manner.\\nI've had some exposure to the infrastructure provisioning tool [Serverless](https:\/\/serverless.com), so I'd like to try using it on this project. Serverless abstracts AWS CloudFormation and similar tools from other cloud vendors.\\n","Decision":"I will use Serverless to describe and provision my AWS components.\\n","tokens":119,"id":4937,"Prediction":"## Decision\n\n**Decision:** Implement a provisioning strategy using Serverless to manage the configuration of services in the application, following the principles of Infrastructure as Code.\n\n**Context:** As services become unique and fragile when configured by hand, it is essential to have a provisioning strategy in place before adding components to ensure reproducibility and avoid unexpected behavior. The principles of Infrastructure as Code suggest avoiding manual changes to services, and instead, using automated, configured changes.\n\n**Rationale:** Serverless provides a convenient and cloud-agnostic way to manage infrastructure provisioning, abstracting away the underlying infrastructure and allowing for a more modular and scalable approach. By using Serverless, we can ensure that the configuration of services is automated, reproducible, and version-controlled, reducing the risk of errors and inconsistencies.\n\n**Consequences:**\n\n* Improved reproducibility and consistency in service configuration\n* Reduced risk of errors and inconsistencies in service configuration\n* Increased modularity and scalability of the application\n* Simplified management and maintenance of service configuration\n\n**Alternatives Considered:**\n\n* Manual configuration of services\n* Using a different infrastructure provisioning tool or framework\n* Not using a provisioning strategy at all\n\n**Status:** Implemented","GenTime":"2024-08-11 00:36:14"}
{"File Name":"godspeed-you-blocked-developer\/001. TypeScript, React, Redux, and Webpack.md","Context":"## Context\\nI was once a web developer when jQuery and CSS 2 were the new hotness. I did a lot of web 1.0, single-server work. Then I spent many years building iOS apps. These days I'm interested to be more of a generalist.\\nIn the fall of 2017, I learned some TypeScript and thought it was great. JavaScript is JavaScript, and I like strong types, and TypeScript makes that happen, so it was an instant favorite.\\nI decided to learn some more web technologies. For browser apps that means React and Redux, which is commonly used with Node, NPM, and Webpack. Learning the common stuff is a good way to get my bearings in the field.\\nWhen the idea for this project came to mind, it was a good chance to learn that web stack and catch up my knowledge.\\n","Decision":"I decided to build this site in TypeScript, with React and Redux, on Node, NPM, and Webpack.\\n","tokens":177,"id":4940,"Prediction":"## Decision\n\nAs a generalist, I have decided to use TypeScript as the primary programming language for this project, in addition to using React and Redux for building the browser application. This decision is based on my prior experience with TypeScript and my preference for strong typing.\n\n## Context\n\nAs a web developer with experience in older technologies, I have been looking to modernize my skills and learn more about the latest web technologies. I have a strong foundation in JavaScript, but I am interested in exploring more advanced tools and frameworks to improve my skills as a generalist. I have already learned TypeScript and enjoyed its strong typing features, and I believe it will be a valuable addition to my skillset.\n\n## Problem\n\nWhen deciding on a programming language for this project, I considered using JavaScript alone, but I wanted to take advantage of TypeScript's strong typing features to improve the maintainability and scalability of the codebase. I also considered using other languages, such as Java or C#, but I ultimately decided to stick with JavaScript and its ecosystem, as I am already familiar with it and want to continue building on my existing knowledge.\n\n## Decision\n\nAfter considering the pros and cons, I have decided to use TypeScript as the primary programming language for this project. This will allow me to take advantage of TypeScript's strong typing features, which will improve the maintainability and scalability of the codebase. Additionally, using TypeScript will allow me to write more robust and error-free code, which will ultimately improve the quality of the final product.\n\n## Consequences\n\nUsing TypeScript as the primary programming language for this project will have several consequences. First, it will require me to write more code in TypeScript, which may take some time to get used to. Second, it will require me to use TypeScript-specific tools and plugins, such as the TypeScript compiler and the TypeScript type checker. Finally, it will require me to adapt to TypeScript's syntax and semantics, which may take some time to get used to.\n\nOverall, I believe that using TypeScript as the primary programming language for this project will be a valuable decision, as it will allow me to take advantage of TypeScript's strong typing features and improve the maintainability and scalability of the codebase.","GenTime":"2024-08-11 00:37:26"}
{"File Name":"uniffi-rs\/0004-only-threadsafe-interfaces.md","Context":"## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\n","Decision":"* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\nChosen option:\\n* **[Option 2] Immediately deprecate, then remove entirely, support for\\nnon-`Send+Sync` interfaces.**\\nThis decision was taken because our real world experience tells us that\\nnon-`Send+Sync` interfaces are only useful in toy or example applications (eg,\\nthe nimbus and autofill projects didn't get very far before needing these\\ncapabilities), so the extra ongoing work in supporting these interfaces cannot\\nbe justified.\\n### Positive Consequences\\n* The locking in all uniffi supported components will be more easily\\ndiscoverable - it will be in hand-written rust code and not hidden inside\\ngenerated code. This is a benefit to the developers of the uniffi supported\\ncomponent rather than to the consumers of it; while we are considering other\\nfeatures to help communicate the lock semantics to such consumers, that is\\nbeyond the scope of this ADR.\\n* Opens the door to enhancements that would be impossible for non-`Send+Sync`\\ninterfaces, and simpler to implement for `Send+Sync` interfaces if support\\nfor non-`Send+Sync` interfaces did not exist.\\n* Simpler implementation and documentation.\\n### Negative Consequences\\n* All consumers (both inside Mozilla and external) will need to change their\\ninterfaces to be `Send+Sync`. As an example of what this entails,\\nsee [this commit](https:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985)\\nwhich converts the `todolist` example.\\n* Simple, toy applications may be more difficult to wrap - consumers will not\\nbe able to defer decisions about `Send+Sync` support and will instead need to\\nimplement simple locking as demonstrated in [this commit](\\nhttps:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985).\\n* Existing applications that are yet to consider how to make their\\nimplementations `Send+Sync` cannot be wrapped until they have.\\n* The examples which aren't currently marked with the `[Threadsafe]` attribute\\nwill become more complex as they will all need to implement and explain how\\nthey achieve being `Send+Sync`.\\n* The perception that its more difficult to wrap interfaces will lead to less\\nadoption of the tool.\\n","tokens":468,"id":4952,"Prediction":"## Decision\n\n**ADR-0004: Require all structs supported by uniffi to be `Send+Sync`**\n\n## Context\n\nAs discussed in the previous ADR, the concept of non-thread-safe interfaces is a misconception in Rust, and the `[Threadsafe]` marker is actually an opt-out for managing locking internally. This decision proposes that uniffi forces component authors to explicitly manage locking in all cases, making all structs supported by uniffi `Send+Sync`.\n\n## Decision Drivers\n\n* The need to avoid hidden locking and its potential for unexpectedly blocking method calls.\n* The desire to allow uniffi to grow in directions that are desired in practice, such as alternative method receivers and passing interface references over the FFI.\n\n## Decision Outcome\n\nWe will require all structs supported by uniffi to be `Send+Sync`, effectively making the `[Threadsafe]` marker unnecessary. This decision aims to improve the reliability and performance of uniffi-generated code by ensuring that component authors are aware of and manage locking internally.","GenTime":"2024-08-11 00:39:55"}
{"File Name":"adr-poc\/0003-write-help-file.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4963,"Prediction":"Here is a sample Architectural Decision Record:\n\n**Decision**\n\n* **Implement a caching layer for our API endpoints using Redis**\n\n**Context**\n\nThe issue motivating this decision is the growing traffic on our API, which has led to increased latency and response times. Our monitoring tools have shown that the majority of the delay is caused by database queries, which are taking an average of 200ms to complete. This is affecting the overall performance and user experience of our application.\n\nAdditionally, our team has been experiencing issues with database connection timeouts and slow queries, which are causing errors and requiring manual intervention to resolve. To mitigate these issues, we need to find a solution that can reduce the load on our database and improve the overall performance of our API.\n\nOur team has considered implementing a caching layer using various technologies, including Redis, Memcached, and Apache Ignite. After evaluating the options, we believe that Redis is the best fit for our use case due to its ease of use, scalability, and support for various data structures.\n\n**Drivers**\n\n* Improve API performance and reduce latency\n* Reduce the load on our database and improve database query response times\n* Increase the reliability and availability of our API\n* Simplify the implementation and maintenance of our caching layer\n\n**Alternatives Considered**\n\n* Implementing a caching layer using Memcached\n* Implementing a caching layer using Apache Ignite\n* Not implementing a caching layer at all\n\n**Rationale**\n\nWe chose to implement a caching layer using Redis because it provides a simple and scalable solution for storing and retrieving data. Redis is known for its high performance and low latency, making it an ideal choice for caching API endpoints. Additionally, Redis provides support for various data structures, including strings, hashes, lists, and sets, which allows us to store and retrieve data in a flexible and efficient manner.\n\nBy implementing a caching layer using Redis, we can reduce the load on our database, improve API performance, and increase the overall reliability and availability of our application.","GenTime":"2024-08-11 00:44:02"}
{"File Name":"ios-architecture-decision-logs\/0012-use-networkManager-for-network-requests.md","Context":"## Context\\nOur team created NetworkManager for network requests. For consistency we should replace and use NetworkManager instead of Fetchable protocol\\n","Decision":"Every new interactors must use NetworkManager. Also every new endpoints should use new networker approach\\n","tokens":27,"id":4974,"Prediction":"## Decision\n\n**Use NetworkManager for network requests instead of Fetchable protocol**\n\n## Context\n\nOur team created NetworkManager for network requests. For consistency, we should replace and use NetworkManager instead of Fetchable protocol.\n\n## Problem\n\nWe have two separate implementations for network requests, NetworkManager and Fetchable protocol. This leads to inconsistencies and makes it difficult to maintain and update our codebase.\n\n## Decision\n\nWe will use NetworkManager for all network requests instead of Fetchable protocol. This decision is made to maintain consistency and reduce complexity in our codebase.\n\n## Rationale\n\nUsing a single implementation for network requests will simplify our code and make it easier to maintain. NetworkManager is a more comprehensive and robust solution compared to Fetchable protocol, and it provides more features and flexibility. Additionally, replacing Fetchable protocol with NetworkManager will reduce the risk of bugs and errors that can occur due to inconsistencies between the two implementations.\n\n## Consequences\n\n* We will need to refactor our code to use NetworkManager instead of Fetchable protocol.\n* We will need to ensure that NetworkManager is properly tested and validated before making the switch.\n* We will need to document the changes and provide training to team members on the new implementation.\n\n## Status\n\nThis decision is currently in progress. We will continue to refactor our code and test NetworkManager before making the switch.","GenTime":"2024-08-11 00:48:02"}
{"File Name":"ios-architecture-decision-logs\/0007-presenter-casting-as-delegate.md","Context":"## Context\\nWe faced a problem that different implementations of presenter's conforming delegates\\n","Decision":"We decided to add `var xxxxDelegate { get }` to `xxPresenterInterface` and passing this `presenter`  to any delegate like:  `(delegate: presenter.xxxxDelegate)` instead of  `(delegate: presenter as? xxxDelegate)` or `(delegate: presenter as! xxxDelegate)`\\n","tokens":17,"id":4980,"Prediction":"## Decision\n\n**Title:** Standardize Presenter Conforming Delegate Implementations\n\n**Context:** We faced a problem that different implementations of presenter's conforming delegates, which are responsible for handling user input and updating the UI, have led to inconsistencies and difficulties in maintaining and extending the codebase.\n\n**Problem:** The current implementation of presenter conforming delegates has resulted in:\n\n* Inconsistent behavior across different presenter implementations\n* Difficulty in debugging and troubleshooting issues due to the complexity of the delegate implementations\n* Increased maintenance costs due to the need to update multiple implementations separately\n\n**Decision:** To standardize the presenter conforming delegate implementations and improve the maintainability and scalability of the codebase.\n\n**Solution:** We will introduce a new abstract class `PresenterDelegate` that will serve as a base class for all presenter conforming delegates. This class will provide a standardized interface for handling user input and updating the UI, and will include default implementations for common delegate methods.\n\n**Rationale:** By standardizing the presenter conforming delegate implementations, we can:\n\n* Reduce complexity and improve maintainability by having a single, consistent implementation\n* Improve scalability by allowing for easier extension and modification of the delegate implementations\n* Enhance debugging and troubleshooting capabilities by providing a clear and consistent interface for handling user input and updating the UI\n\n**Consequences:**\n\n* Increased upfront effort to implement the standardized delegate interface\n* Potential for increased complexity in the short term due to the introduction of a new abstract class\n* Improved maintainability and scalability in the long term, reducing the need for future updates and modifications\n\n**Monitoring and Feedback:** We will monitor the effectiveness of this decision by tracking the number of bugs and issues related to presenter conforming delegate implementations, as well as the time and effort required to maintain and extend the codebase. We will also gather feedback from developers and stakeholders to ensure that the standardized delegate interface meets their needs and expectations.","GenTime":"2024-08-11 00:50:00"}
{"File Name":"ios-architecture-decision-logs\/0006-inject-parameters-asArguments-toPresenter.md","Context":"## Context\\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\\n","Decision":"Each module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\\n","tokens":28,"id":4983,"Prediction":"## Decision\n\n**Inject Presenter Parameters using Optional and Default Values**\n\n## Context\n\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\n\n## Problem\n\nThe increasing number of parameters in the presenter's initializer is making it difficult to maintain and understand. This is leading to a higher cognitive load for developers who need to understand the presenter's behavior and dependencies.\n\n## Decision\n\nTo address this issue, we will use optional and default values for presenter parameters. This approach will allow us to:\n\n* Reduce the number of parameters in the presenter's initializer\n* Make the code more readable and maintainable\n* Improve the overall architecture of our application\n\n## Rationale\n\nUsing optional and default values for presenter parameters will enable us to decouple the presenter from its dependencies and make it more flexible. This will also allow us to add or remove dependencies without affecting the presenter's behavior.\n\n## Consequences\n\n* The presenter's initializer will be less cluttered and easier to understand\n* The code will be more maintainable and scalable\n* The application's architecture will be improved, making it more flexible and adaptable to changing requirements\n\n## Status\n\nThis decision is currently in the \"Proposed\" status and is subject to change based on further discussion and feedback.\n\n## References\n\n* [Optional and Default Values in Swift](https:\/\/docs.swift.org\/swift-book\/LanguageGuide\/Functions.html#ID245)\n* [Dependency Injection in Swift](https:\/\/www.swiftbysundell.com\/posts\/dependency-injection-in-swift)","GenTime":"2024-08-11 00:50:55"}
{"File Name":"webwritertechandhumanity.com\/0002-how-to-implement-special-pages.md","Context":"## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n","Decision":"I'll implement solution 1 because solution 2 is complicated and I want to prioritise\\nthe release of a first working website.\\n","tokens":52,"id":5001,"Prediction":"## Decision\n\nTo address the context, we will implement the privacy policy page and the about me page as separate routes outside of the main map navigation. This will allow users to access these pages directly without being forced to navigate through the map.\n\n## Context\n\nAs you mentioned, the privacy policy page and the about me page are special pages that don't fit naturally within the map navigation. Users should be able to access these pages directly, without being forced to navigate through the map. This is especially important for the privacy policy page, as users may need to access it quickly and easily.\n\n## Decision Drivers\n\n* The need for users to access the privacy policy and about me pages directly, without navigating through the map.\n* The desire to keep the map navigation focused on the main content of the application.\n* The importance of making the privacy policy page easily accessible to users.\n\n## Decision\n\nWe will create two separate routes for the privacy policy page and the about me page, outside of the main map navigation. This will allow users to access these pages directly, without being forced to navigate through the map. We will also ensure that these pages are easily accessible and visible in the application's navigation menu.\n\n## Status\n\nThis decision is approved and will be implemented in the next release.\n\n## Implemented\n\nThe privacy policy page and the about me page are now accessible as separate routes outside of the main map navigation.","GenTime":"2024-08-11 00:56:22"}
{"File Name":"trade-access-program\/0005-viewflow-for-grant-management-portal.md","Context":"## Context\\nWe need a system to manage the lifecycle of a grant application.\\n","Decision":"Viewflow was chosen as a framework to help us achieve this goal while making the development process efficient.\\n### Build vs Buy Paper\\nVarious alternatives to Viewflow were considered when making this decision. Those alternatives are details in the\\nalpha stage of the project here: https:\/\/uktrade.atlassian.net\/l\/c\/zEAEM37j\\n### Viewflow analysis\\nSome initial analysis on how viewflow could be used was also done in alpha: https:\/\/uktrade.atlassian.net\/l\/c\/Lc77C4mq\\n","tokens":17,"id":5009,"Prediction":"## Decision\n\n**Decision:** Implement a state machine to manage the lifecycle of a grant application.\n\n## Context\n\nWe need a system to manage the lifecycle of a grant application.\n\n## Problem\n\nThe grant application lifecycle involves multiple stages, including submission, review, approval, and funding. Each stage has specific requirements and constraints that need to be tracked and managed. The system must be able to accurately reflect the current state of the application and allow for transitions between states.\n\n## Reasoning\n\nA state machine is an effective way to manage the lifecycle of a grant application because it provides a clear and concise representation of the application's state and the allowed transitions. This approach allows us to:\n\n* Define a set of well-defined states that accurately reflect the application's lifecycle\n* Specify the allowed transitions between states, ensuring that the application can only move from one state to another in a valid and logical manner\n* Implement business logic and validation rules for each state and transition\n* Easily extend or modify the state machine as the requirements of the grant application process change\n\n## Consequences\n\nImplementing a state machine to manage the lifecycle of a grant application will:\n\n* Provide a clear and intuitive representation of the application's state and the allowed transitions\n* Ensure that the application can only move from one state to another in a valid and logical manner\n* Allow for easy extension or modification of the state machine as the requirements of the grant application process change\n* Improve the overall maintainability and scalability of the system\n\n## Status\n\nThis decision has been reviewed and approved by the development team. The state machine will be implemented using a combination of domain-driven design and business logic layers to ensure a clear separation of concerns and maintainability.","GenTime":"2024-08-11 00:57:53"}
{"File Name":"link_platform\/0016-use-devise-for-admin-authentication.md","Context":"## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n","Decision":"[Devise](https:\/\/github.com\/plataformatec\/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https:\/\/github.com\/omniauth\/omniauth\/wiki\/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\\n","tokens":37,"id":5024,"Prediction":"## Decision\n\n**Authentication and Authorization for Link Platform Administrators**\n\n### Context\n\nAs described by the user, we need to implement a system for authenticating and managing Link Platform Administrators. Administrators will need to log in to their Link Instances to manage data and configuration.\n\n### Decision\n\nWe will use a combination of authentication and authorization mechanisms to ensure secure access to Link Instances. Specifically, we will:\n\n1. Use OpenID Connect (OIDC) for authentication, allowing administrators to log in with their preferred identity providers (e.g., Google, GitHub, etc.).\n2. Implement role-based access control (RBAC) to manage administrator privileges and permissions. This will enable fine-grained control over what actions administrators can perform within their Link Instances.\n3. Utilize a secure token-based system to authenticate administrators and validate their access to Link Instances. This will ensure that only authorized administrators can access and manage data and configuration.\n\n### Rationale\n\nUsing OIDC for authentication provides a widely adopted, secure, and scalable solution for managing administrator identities. RBAC enables us to define and manage complex permission structures, ensuring that administrators only have access to the resources and actions they need to perform their jobs. The token-based system provides an additional layer of security and ensures that administrators are properly authenticated and authorized before accessing Link Instances.\n\n### Consequences\n\n* Increased security and authentication capabilities for Link Platform Administrators.\n* Simplified management of administrator identities and permissions.\n* Improved scalability and flexibility for handling large numbers of administrators and Link Instances.\n\n### Implications\n\n* We will need to integrate OIDC with our existing infrastructure and implement the necessary authentication and authorization mechanisms.\n* We will need to design and implement the RBAC system, including defining roles and permissions for administrators.\n* We will need to test and validate the token-based system to ensure it is secure and functioning correctly.","GenTime":"2024-08-11 01:01:45"}
{"File Name":"link_platform\/0002-use-the-adr-tools-kit-to-manage-adr-docs.md","Context":"## Context\\nTo help automated the process of managing architectual decision records use a tool that stanadardizes the process.\\nI found a reference to this tool on the thought works techninques review page.  The github repo is:\\n`https:\/\/github.com\/npryce\/adr-tools`\\nBut I installed it using brew:\\n`brew install adr-tools`\\n","Decision":"We will use the system adr-tools to manage the ADR documents\\n","tokens":79,"id":5033,"Prediction":"## Decision\n\nWe will use the ADR (Architectural Decision Record) tool provided by Nat Pryce to standardize the process of managing architectural decision records in our software development process.\n\n## Context\n\nAs we strive to improve the efficiency and transparency of our development process, we recognize the importance of documenting and tracking architectural decisions. To achieve this, we have identified the need for a standardized process and tool to manage these records.\n\n## Problem\n\nCurrently, our team lacks a centralized and structured approach to documenting and tracking architectural decisions, which can lead to confusion, duplicated efforts, and a lack of visibility into the reasoning behind key design choices.\n\n## Decision\n\nAfter researching and evaluating various options, we have decided to use the ADR tool provided by Nat Pryce. This tool provides a standardized template and process for documenting and tracking architectural decisions, which will help us to:\n\n* Improve transparency and visibility into the decision-making process\n* Reduce confusion and duplicated efforts\n* Enhance collaboration and communication among team members\n* Provide a historical record of key design choices and their rationale\n\n## Status\n\nThe ADR tool has been installed using Homebrew and is ready for use.\n\n## Consequences\n\nBy using the ADR tool, we expect to see improved collaboration and communication among team members, reduced confusion and duplicated efforts, and a more transparent and accountable decision-making process. We will also have a centralized and structured approach to documenting and tracking architectural decisions, which will help to ensure that key design choices are well-documented and easily accessible.\n\n## Rationale\n\nWe chose the ADR tool because it provides a standardized template and process for documenting and tracking architectural decisions, which will help us to achieve our goals of improving transparency, reducing confusion, and enhancing collaboration. The tool is also widely used and well-documented, which will make it easier for new team members to learn and adopt.","GenTime":"2024-08-11 01:04:24"}
{"File Name":"link_platform\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5035,"Prediction":"## Decision\n\n**Decision:** Implement a Microservices Architecture for the Project\n\n**Context:** We need to record the architectural decisions made on this project.\n\n**Problem:** As the project grows in complexity and size, we need to ensure that it is scalable, maintainable, and adaptable to changing requirements. The current monolithic architecture is becoming increasingly difficult to manage and is limiting our ability to make changes and deploy new features quickly.\n\n**Reasoning:** A Microservices Architecture will allow us to break down the application into smaller, independent services that can be developed, tested, and deployed independently. This will enable us to scale individual services as needed, reduce coupling between services, and improve overall system resilience.\n\n**Consequences:**\n\n* We will need to invest time and resources in designing and implementing the new architecture.\n* We will need to develop new skills and expertise in areas such as service discovery, messaging, and API design.\n* We will need to establish new processes and tools for testing, deployment, and monitoring of the microservices.\n* We will need to re-architect the existing monolithic application to conform to the new microservices architecture.\n\n**Decision Drivers:**\n\n* Scalability: The current monolithic architecture is becoming difficult to scale as the project grows.\n* Maintainability: The current architecture is becoming increasingly complex and difficult to maintain.\n* Adaptability: The current architecture is limiting our ability to make changes and deploy new features quickly.\n\n**Pros and Cons:**\n\nPros:\n\n* Improved scalability and performance\n* Improved maintainability and reduced technical debt\n* Improved adaptability and ability to respond to changing requirements\n* Ability to develop and deploy new features independently\n\nCons:\n\n* Increased complexity and difficulty in managing multiple services\n* Increased overhead in terms of infrastructure and tooling\n* Potential for increased coupling between services if not properly designed\n\n**Implementation:**\n\nWe will implement the Microservices Architecture by:\n\n* Breaking down the application into smaller, independent services\n* Designing and implementing APIs for communication between services\n* Implementing service discovery and messaging mechanisms\n* Developing new processes and tools for testing, deployment, and monitoring of the microservices\n* Re-architecting the existing monolithic application to conform to the new microservices architecture\n\n**Monitoring and Review:**\n\nWe will monitor the implementation of the Microservices Architecture and review its effectiveness on a regular basis. We will also review and update this decision record as necessary to reflect any changes or lessons learned.","GenTime":"2024-08-11 01:05:08"}
{"File Name":"FlowKit\/0006-JWTs.md","Context":"## Context\\nAuthentication and access control for FlowKit has previously been very limited, making use of only of postgres usernames and passwords. WIth the introduction of the API, we can be much more granular in allowing access through the use of [JSON Web Tokens](https:\/\/jwt.io).\\nThese are an encoded and cryptographically signed string, which permit access to some set of API functionality. The string specifies the identity of the user, exactly what they can access, and the time period for which the token is valid.\\nTampering with the token will result in the signature not matching, an event which can be logged an subsequently investigated.\\nA key advantage of JWTs is that they can be centrally managed, even where the service they will be used with is not accessible to the internet. Another significant advantage is that the tokens are inherently ephemeral - should a token be stolen, the time window for it to lead to a data breach will (given appropriate usage of the system) be small.\\nJWTs are also advantageous in that the add relatively minimal overhead to the day to day experience of analysts, and have broad support across many languages.\\nBut perhaps the most important advantage is the granularity of access they afford - for example it is possible to allow access to only one specific result with this method. This is useful where developing dashboards and the like, or where access to outputs but not data must be provided to other parties.\\nOther alternatives would be to use a more traditional login system, backed by a database, integrate with the Docker host's authentication system, or to use public key based authentication. All of these require more and local administration, or necessitate the use of either a JWT equivalent to actually provide API access, or the use of cookies to avoid a need to log in for every communication with the API.\\n","Decision":"API authentication and access control will use JWTs.\\n","tokens":369,"id":5057,"Prediction":"## Decision\n\n**Decision:** Implement JSON Web Tokens (JWTs) for authentication and access control in FlowKit.\n\n**Context:** As discussed, the current authentication and access control mechanism in FlowKit is limited and does not provide sufficient granularity. With the introduction of the API, we can leverage JWTs to provide a more secure and flexible way to manage access to API functionality.\n\n**Rationale:**\n\n1. **Security:** JWTs provide a secure way to authenticate users and authorize access to API functionality. The encoded and cryptographically signed string ensures that tampering with the token will result in the signature not matching, making it difficult for unauthorized users to access the API.\n2. **Granularity:** JWTs allow for fine-grained access control, enabling us to specify exactly what API functionality a user can access, making it suitable for developing dashboards and providing access to outputs without exposing sensitive data.\n3. **Centralized management:** JWTs can be centrally managed, even when the service they are used with is not accessible to the internet.\n4. **Ephemerality:** JWTs are inherently ephemeral, reducing the risk of data breaches in the event of a token being stolen.\n5. **Minimal overhead:** JWTs do not add significant overhead to the day-to-day experience of analysts, and they have broad support across many languages.\n6. **Alternative solutions:** Other alternatives, such as traditional login systems, integrating with the Docker host's authentication system, or public key-based authentication, require more local administration or necessitate the use of JWT equivalents or cookies, making JWTs a more suitable choice.\n\n**Consequences:**\n\n1. **Implementation:** We will need to implement JWT generation and validation in our API.\n2. **Configuration:** We will need to configure the JWT generation and validation process, including setting expiration times and specifying the claims that will be included in the token.\n3. **Monitoring:** We will need to monitor the JWTs for any signs of tampering or misuse.\n\n**Status:** Accepted\n\n**Owner:** [Insert name or team responsible for implementing JWTs in FlowKit]\n\n**Start Date:** [Insert start date for implementation]\n\n**End Date:** [Insert end date for implementation]\n\n**Review Date:** [Insert review date for the decision]","GenTime":"2024-08-11 01:11:53"}
{"File Name":"FlowKit\/0009-asciidoctor-pdf-for-notebook-conversion.md","Context":"## Context\\nWe want AutoFlow to support creation of PDF reports from Jupyter notebooks. [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/) provides a method for converting Jupyter notebooks to PDF format. However, PDF conversion with nbconvert requires a full LaTeX installation, which is ~4GB in size. To keep the size of the AutoFlow container smaller, it is desirable to find an alternative that doesn't require a LaTeX installation.\\n[Asciidoctor PDF](https:\/\/asciidoctor.org\/docs\/asciidoctor-pdf\/) is a tool for converting ASCIIDoc documents to PDF format without generating an interim format such as LaTeX. Since nbconvert can convert notebooks to ASCIIDoc format, we can use Asciidoctor PDF as the second half of a two-step process to convert Jupyter notebooks to PDF via ASCIIDoc.\\n","Decision":"We will use a two-step process to convert Jupyter notebooks to PDF reports in AutoFlow: convert the notebook to ASCIIDoc format using nbconvert, and then convert the resulting ASCIIDoc document to PDF using Asciidoc PDF.\\n","tokens":178,"id":5059,"Prediction":"## Decision\n\nWe will use Asciidoctor PDF as the PDF converter for Jupyter notebooks in AutoFlow, instead of nbconvert, to avoid the requirement of a full LaTeX installation.\n\n## Context\n\nAs mentioned in the context, we want to support creation of PDF reports from Jupyter notebooks in AutoFlow. We considered using nbconvert, but it requires a full LaTeX installation, which is large in size and not desirable for our container. We also considered using Asciidoctor PDF, which can convert ASCIIDoc documents to PDF format without generating an interim format like LaTeX.\n\n## Problem\n\nThe main problem is that nbconvert requires a full LaTeX installation, which is large in size and not desirable for our container. This would increase the size of our container and make it less efficient.\n\n## Decision Rationale\n\nWe decided to use Asciidoctor PDF as the PDF converter for Jupyter notebooks in AutoFlow because it can convert ASCIIDoc documents to PDF format without generating an interim format like LaTeX. This will allow us to avoid the requirement of a full LaTeX installation and keep the size of our container smaller.\n\n## Pros and Cons\n\nPros:\n\n* Asciidoctor PDF does not require a full LaTeX installation, which will keep the size of our container smaller.\n* It can convert ASCIIDoc documents to PDF format without generating an interim format like LaTeX.\n\nCons:\n\n* We will need to convert Jupyter notebooks to ASCIIDoc format before converting them to PDF, which may add an extra step to our process.\n* Asciidoctor PDF may not support all the features and formatting options that nbconvert supports.\n\n## Conclusion\n\nWe will use Asciidoctor PDF as the PDF converter for Jupyter notebooks in AutoFlow, instead of nbconvert, to avoid the requirement of a full LaTeX installation and keep the size of our container smaller. We will need to convert Jupyter notebooks to ASCIIDoc format before converting them to PDF, and we will need to consider the limitations of Asciidoctor PDF compared to nbconvert.","GenTime":"2024-08-11 01:12:31"}
{"File Name":"google-cloud-cpp-common\/2019-03-26-binary-distributions.md","Context":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","Decision":"will not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","tokens":273,"id":5061,"Prediction":"Here is the Architectural Decision Record (ADR) for the provided context and decision:\n\n**Decision**: No Binary Package Support\n\n**Context**: Compiling google-cloud-cpp from source is not always possible nor desired by all users. Some users may want to install a binary package that contains our public headers and pre-compiled libraries so that they can link against our library. There is no single standard binary package manager in C++, instead there are a variety of others that users might want to use (e.g., dpkg, rpm, vcpkg).\n\n**Decision**: We will not directly support nor provide any binary packages. We will not test binary distributions of our code. And we will not host configs for binary distributions, since that would involve hosting files which we do not test. However, we will do our best to make our code easily packageable by others in a wide variety of formats. For more context, see #333.\n\n**Consequences**: This decision will shield from endorsing any particular binary package management system. Other individuals who want to build and maintain a binary distribution of our code may do so without our permission or knowledge. If they need small and reasonable tweaks from us to support their binary distributions, they may file an issue and we'll do our best to accommodate them within reason.\n\n**Rationale**: By not providing binary packages, we are not responsible for testing and maintaining them. This decision also allows users to use their preferred binary package manager. However, we will still provide the necessary information and tools for others to create their own binary packages.\n\n**Status**: Accepted\n\n**Related ADRs**: None\n\n**Affected Components**: google-cloud-cpp\n\n**Implementation Notes**: We will update our documentation and issue tracker to reflect this decision. We will also make sure to provide the necessary information and tools for others to create their own binary packages.\n\n**References**: #333","GenTime":"2024-08-11 01:13:12"}
{"File Name":"phpadr\/0005-phpunit-as-testing-framework.md","Context":"## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n","Decision":"It will be used the [PHPUnit](https:\/\/phpunit.de\/) as testing framework.\\n","tokens":18,"id":5070,"Prediction":"## Decision\n\nTo ensure good code quality with ease for change, integration, and error correction, we will implement a Test-Driven Development (TDD) approach in our software development process.\n\n## Context\n\nEnsuring good code quality is crucial for the maintainability, scalability, and reliability of our software. With the increasing complexity of our system, it becomes more challenging to make changes, integrate new features, and correct errors without introducing new bugs or breaking existing functionality. To overcome these challenges, we need a development approach that prioritizes code quality and makes it easier to modify and extend our codebase.\n\n## Problem\n\nOur current development process lacks a structured approach to ensure code quality. We often find ourselves debugging and refactoring code for hours, which can lead to delays and increased costs. Moreover, our codebase is becoming increasingly complex, making it difficult to identify and fix issues.\n\n## Decision\n\nTo address these challenges, we will adopt a Test-Driven Development (TDD) approach. TDD involves writing automated tests before writing the actual code. This approach ensures that our code is testable, modular, and easy to maintain. By writing tests first, we can:\n\n* Catch bugs early in the development process\n* Ensure that our code meets the required functionality and behavior\n* Make changes and refactor code with confidence\n* Integrate new features with ease\n* Reduce debugging time and costs\n\n## Consequences\n\nImplementing TDD will require significant changes to our development process, including:\n\n* Writing tests before writing code\n* Focusing on testability and modularity in our code design\n* Refactoring code to make it more testable\n* Integrating automated testing into our CI\/CD pipeline\n\nHowever, the benefits of TDD will far outweigh the costs. We can expect to see:\n\n* Improved code quality and reliability\n* Reduced debugging time and costs\n* Increased confidence in our code changes\n* Faster integration of new features\n* Better maintainability and scalability of our software\n\n## Rationale\n\nWe chose TDD as our approach to ensure good code quality because it is a well-established and widely adopted best practice in software development. TDD has been shown to improve code quality, reduce bugs, and increase developer productivity. By adopting TDD, we can ensure that our software is built with quality and reliability in mind, making it easier to maintain and extend over time.","GenTime":"2024-08-11 01:16:00"}
{"File Name":"galaxy-zoo-touch-table\/adr-2.md","Context":"### Context\\nThe touch table app should support multi touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\\n### Decision\\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","Decision":"Although considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","tokens":409,"id":5076,"Prediction":"Here is the Architectural Decision Record (ADR) based on the provided context:\n\n**Decision:** To use WPF (Windows Presentation Foundation) as the technology for building the touch table app.\n\n**Context:** The touch table app should support multi-touch and be robust enough to handle various UI demands. The app will run on a Windows device, and the team is familiar with C#.\n\n**Decision:** Although considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of JavaScript, a language most of our team is familiar with. The transition from JavaScript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the DeepTree app built in WPF.\n\n**Status:** Accepted\n\n**Consequences:** The use of WPF provides strong documentation, flexibility, and a large community of developers who can provide support. However, WPF is losing popularity, and some of the frameworks and packages used by WPF are becoming deprecated.","GenTime":"2024-08-11 01:17:44"}
{"File Name":"galaxy-zoo-touch-table\/adr-3.md","Context":"### Context\\nWe should structure files in a way that makes the codebase easy to navigate and intuitive when searching for certain components. Organization should adhere to accepted practices in the .NET community.\\n### Decision\\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","Decision":"The MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","tokens":280,"id":5082,"Prediction":"Here is the Architectural Decision Record (ADR) in a standardized format:\n\n**Decision**\n\nTitle: Adopting the MVVM Architecture Pattern for the.NET WPF Application\n\n**Context**\n\nAs we structure our files, we want to make the codebase easy to navigate and intuitive when searching for certain components. We should adhere to accepted practices in the.NET community.\n\n**Decision**\n\nWe will adopt the Model-View-View Model (MVVM) architecture pattern, which is widely used by the WPF community. This decision will make it easier to find solutions to coding problems online and provide a standard structure for our application.\n\n**Status**\n\nAccepted\n\n**Consequences**\n\n* It will be easier to find solutions to coding problems online by accepting a widely-used design pattern.\n* However, this decision does not necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\n* In retrospect, the MVVM pattern was overall beneficial, but it was sometimes unclear how strictly we should adhere to the pattern.","GenTime":"2024-08-11 01:18:58"}
{"File Name":"my-notes\/prettier.md","Context":"### Context\\nPrettier integrates with well with vscode but conflicts with Standard\\n### Decision\\nReplace Standard with Prettier\\n","Decision":"Replace Standard with Prettier\\n","tokens":26,"id":5087,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision:** Adopt Prettier as the default code formatter and disable Standard\n\n**Context:** Prettier integrates well with VSCode, but conflicts with Standard, causing formatting issues and inconsistencies in our codebase.\n\n**Rationale:** Prettier is a popular and widely-used code formatter that is known for its simplicity and ease of use. Its integration with VSCode is seamless, making it an attractive choice for our development team. On the other hand, Standard is a more opinionated code formatter that can be difficult to work with, especially when used in conjunction with Prettier. By adopting Prettier as our default code formatter, we can simplify our code formatting process and reduce the risk of formatting conflicts.\n\n**Consequences:**\n\n* Prettier will be used as the default code formatter for our codebase.\n* Standard will no longer be used as a code formatter.\n* We will need to update our build processes and CI\/CD pipelines to use Prettier instead of Standard.\n* Our codebase will be more consistent and easier to maintain, as Prettier will ensure that our code is formatted in a consistent and readable way.\n\n**Status:** Accepted\n\n**Implementation:** We will implement Prettier as the default code formatter by updating our build processes and CI\/CD pipelines to use Prettier instead of Standard. We will also update our coding guidelines and documentation to reflect the change.\n\n**Reevaluation:** We will reevaluate this decision in 6 months to ensure that it is still the best choice for our codebase and to gather feedback from the development team.","GenTime":"2024-08-11 01:20:03"}
{"File Name":"my-notes\/styled-components.md","Context":"### Context\\n- easier to maintain than traditional CSS\\n- I was not able to get my tests working with it properly\\n### Decision\\nDon't start using Styled Components\\n","Decision":"Don't start using Styled Components\\n","tokens":35,"id":5090,"Prediction":"Here is the completed Architectural Decision Record:\n\n**Decision**\n\n* **Title:** Avoid Using Styled Components\n* **Status:** Accepted\n* **Context:** Easier to maintain than traditional CSS, but I was not able to get my tests working with it properly.\n* **Problem:** We considered using Styled Components to improve the maintainability of our CSS, but after experiencing difficulties in getting our tests to work with it, we decided to abandon this approach.\n* **Reasoning:** While Styled Components may offer some benefits in terms of maintainability, the difficulties we encountered in testing it outweigh these benefits. We prioritize testability and reliability in our software development process, and therefore, we will not use Styled Components in our project.\n* **Consequences:** We will continue to use traditional CSS or alternative styling solutions that are more testable and reliable. This decision may require additional effort to maintain our CSS, but it ensures that our software is more robust and easier to test.\n* **Rationale:** The benefits of using Styled Components are not sufficient to outweigh the difficulties we encountered in testing it. We prioritize the stability and reliability of our software over the potential benefits of using a specific styling solution.","GenTime":"2024-08-11 01:20:49"}
{"File Name":"form-builder\/0006-reduce-number-of-emails-per-submission.md","Context":"## Context\\nForm Builder sends one email per file uploaded for each application. For those forms that allow multiple files to be uploaded this generates a lot of email noise for both form owners and the applicants. This can result in additional work to sift these emails or even create unforced errors in workflows that are manual in nature.\\nThe use of AWS SES as the email service means that there is a 10MB upper limit on the size of each email that is sent.\\nOptions discussed:\\n1. Attach as many files as possible to a single email up to the 10MB limit.\\n2. Hard cap limit on 10MB in total accross _all_ the files uploaded per form.\\n3. Admin interface for form owners to log in and retrieve file uploads per submission.\\n4. Email a link to the form owner allowing the ability to download all attachments in one go.\\n","Decision":"We feel that it is reasonable to reduce the number of emails that recipients of each submission get to the absolute minimum.\\nOption 1 is the one which is the most reasonable solution at this time. It requires the least amount of code change and does not change the architecture or increase the threat surface of Form Builder.\\n","tokens":181,"id":5099,"Prediction":"## Decision\n\nBased on the context provided, I propose the following decision:\n\n**Decision:** Implement option 4, \"Email a link to the form owner allowing the ability to download all attachments in one go.\"\n\n**Rationale:**\n\n* This approach allows form owners to easily access and manage multiple files in a single email, reducing the noise and clutter caused by individual emails for each file.\n* It also ensures that the 10MB email limit is not exceeded, as the files are not attached to the email but rather linked to a downloadable package.\n* This solution is more scalable and flexible than hard-capping the total file size or attaching multiple files to a single email, which could lead to issues with file corruption or email delivery.\n* The admin interface option (option 3) may add complexity and overhead, while the link-based approach is more straightforward and user-friendly.\n* This decision aligns with the use of AWS SES as the email service, as it does not exceed the 10MB email size limit.\n\n**Consequences:**\n\n* The form builder will need to implement the logic to generate and send the link to the form owner.\n* The form owner will need to be trained on how to access and download the attachments via the provided link.\n* There may be a slight increase in the email size due to the link and any additional metadata, but this should be within the 10MB limit.\n\n**Monitoring and Feedback:**\n\n* Monitor the effectiveness of this solution by tracking the number of complaints from form owners and applicants about email noise and clutter.\n* Gather feedback from form owners and applicants to refine and improve the solution as needed.\n* Continuously review and assess the solution to ensure it meets the requirements and expectations of all stakeholders.","GenTime":"2024-08-11 01:23:19"}
{"File Name":"form-builder\/0010-testing-the-editor-pipeline.md","Context":"## Context\\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\\n","Decision":"![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\\n","tokens":87,"id":5102,"Prediction":"## Decision\n\n**Decision:** Implement a parallel testing environment for the Editor to reduce testing time and blockage in the pipeline.\n\n**Context:** Our pipeline follows a continuous delivery method with a manual gate to release to production. We have one Editor in test which causes issues when the rest of the team wants to test a feature. Also, while the Editor is tied up in testing, the pipeline is blocked for deploying to live-production.\n\n**Problem:** The current testing environment for the Editor is causing bottlenecks in the pipeline, resulting in delays and blocking the deployment of new features to production.\n\n**Reasoning:** Implementing a parallel testing environment for the Editor will allow multiple testers to use the Editor simultaneously, reducing the testing time and blocking of the pipeline. This will enable the team to test features more efficiently and deploy new changes to production in a timely manner.\n\n**Consequences:**\n\n* Increased testing efficiency and reduced testing time\n* Reduced blockage in the pipeline, allowing for faster deployment of new features to production\n* Improved collaboration among team members, as multiple testers can use the Editor simultaneously\n* Potential increase in infrastructure costs to support the parallel testing environment\n\n**Alternatives Considered:**\n\n* Continue using the single Editor in test, which would result in continued bottlenecks and delays in the pipeline\n* Implement a queuing system for the Editor, which would add complexity to the testing process and may not provide the desired level of parallelism\n\n**Decision Date:** [Insert date]\n\n**Responsible Party:** [Insert name]\n\n**Links:** [Insert links to relevant documentation or diagrams]","GenTime":"2024-08-11 01:24:16"}
{"File Name":"dp\/0009.md","Context":"## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nWhen storing both datasets and code-lists in a graph database, there is some\\noverlap between the information stored on a code and on a dimension option. A\\ndesign decision was needed as to whether the dimension option and code nodes\\nin the graph database should be collapsed into a single node, with the relevant\\nrelationships for each coming off that single node.\\n","Decision":"Dimension options and codes should not be collapsed into a single node. Dimension\\noptions have relationships to every observation within an imported dataset, and\\nadding those relationships to the code node would result in relationships to\\nevery observation across all datasets - a significant and unnecessary burden.\\n","tokens":126,"id":5118,"Prediction":"## Decision\n\n**Title:** Single Node vs. Separate Nodes for Dimension Options and Codes in the Graph Database\n\n**Context:** As described, datasets are composed of observations and dimensions, where dimension options have wider definitions and metadata that apply to those options across multiple datasets. These code definitions reside in code-lists. When storing both datasets and code-lists in a graph database, there is some overlap between the information stored on a code and on a dimension option. A design decision is needed on whether to collapse dimension option and code nodes into a single node or maintain separate nodes.\n\n**Decision:** We will maintain separate nodes for dimension options and codes in the graph database.\n\n**Rationale:**\n\n1. **Data integrity:** By keeping dimension options and codes as separate nodes, we can ensure that the relationships between them are properly defined and maintained. This will help prevent data inconsistencies and errors.\n2. **Flexibility:** Separating dimension options and codes into distinct nodes allows for more flexibility in modeling the relationships between them. This will enable us to easily adapt to changing requirements and schema evolution.\n3. **Scalability:** Maintaining separate nodes will also improve the scalability of our graph database, as it allows us to efficiently store and query large amounts of data.\n4. **Readability and maintainability:** Having separate nodes will make it easier to understand and maintain the graph database schema, as the relationships between dimension options and codes will be more explicit and easier to follow.\n\n**Consequences:**\n\n1. **Increased complexity:** By maintaining separate nodes, we will need to manage additional relationships between dimension options and codes, which may increase the complexity of our graph database schema.\n2. **Additional storage requirements:** Separate nodes will require more storage space, which may impact our overall storage capacity.\n\n**Alternatives considered:**\n\n1. **Collapsing dimension options and codes into a single node:** This approach would simplify the graph database schema, but may lead to data inconsistencies and reduced flexibility.\n2. **Using a hybrid approach:** We could use a combination of both approaches, where dimension options and codes are stored as separate nodes, but with some shared properties or attributes. However, this approach would add complexity to our schema and may not provide the desired benefits.\n\n**Status:** This decision is finalized, and we will implement separate nodes for dimension options and codes in our graph database.","GenTime":"2024-08-11 01:29:34"}
{"File Name":"ikforth\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5120,"Prediction":"Here is an Architectural Decision Record (ADR) for the given context:\n\n## Decision\n\n**ADR-001: Microservices Architecture for the Project**\n\n## Context\n\nAs the project grows in complexity, we need to adopt a scalable and maintainable architecture that allows for independent development, deployment, and scaling of individual components. The current monolithic architecture is becoming a bottleneck, and we need to consider a more distributed approach.\n\n## Problem\n\nThe current monolithic architecture is:\n\n* Difficult to scale horizontally\n* Hard to maintain and update individual components\n* Limited flexibility in terms of technology stack and deployment options\n\n## Decision\n\nWe will adopt a microservices architecture for the project, where each component or feature will be developed as a separate service that communicates with other services using APIs or message queues. This will allow for:\n\n* Independent development, deployment, and scaling of individual components\n* Improved fault tolerance and resilience\n* Increased flexibility in terms of technology stack and deployment options\n\n## Consequences\n\n* Increased complexity in terms of service discovery, communication, and monitoring\n* Need for additional infrastructure and tools for service management and deployment\n* Potential for increased latency and overhead due to distributed communication\n\n## Rationale\n\nThe benefits of a microservices architecture outweigh the additional complexity and overhead. By adopting this architecture, we can improve the scalability, maintainability, and flexibility of the project, which will ultimately lead to better performance and customer satisfaction.\n\n## Status\n\nAccepted\n\n## Links\n\n* [Link to relevant design documents or diagrams]\n* [Link to relevant technical discussions or meeting notes]\n\nNote: This is just a sample ADR, and you may need to add or remove sections depending on your specific use case.","GenTime":"2024-08-11 01:30:09"}
{"File Name":"ontrack\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5122,"Prediction":"Here is an example of an Architectural Decision Record (ADR) for a software project:\n\n**Decision**\n\nTitle: Use a Microservices Architecture for the Project\n\n**Context**\n\nWe need to record the architectural decisions made on this project.\n\n**Problem**\n\nThe project requires a scalable and maintainable architecture to handle the increasing traffic and user base. The current monolithic architecture is limiting our ability to scale and maintain the system efficiently.\n\n**Decision**\n\nAfter careful consideration, we have decided to adopt a microservices architecture for the project. This will allow us to break down the monolithic application into smaller, independent services that can be developed, tested, and deployed independently.\n\n**Rationale**\n\nThe microservices architecture will provide several benefits, including:\n\n* Improved scalability: Each service can be scaled independently, allowing us to allocate resources more efficiently.\n* Increased flexibility: With a microservices architecture, we can develop and deploy services independently, without affecting the entire system.\n* Better maintainability: With smaller, independent services, it will be easier to identify and fix issues, and to make changes to individual services without affecting the entire system.\n\n**Consequences**\n\nThe microservices architecture will require additional infrastructure and operational overhead, including:\n\n* Additional servers and networks to support the multiple services\n* Increased complexity in terms of service discovery, communication, and orchestration\n* Additional testing and validation to ensure that services interact correctly\n\n**Status**\n\nThis decision is currently in the \"proposed\" status, pending further discussion and review.\n\n**History**\n\n* 2023-02-15: Initial proposal made by [Name]\n* 2023-02-20: Discussion and review with team members\n* 2023-02-25: Decision made to adopt microservices architecture\n\n**References**\n\n* [Link to relevant documentation or resources]\n\nThis is just an example, and you can adjust it according to your specific needs and requirements.","GenTime":"2024-08-11 01:30:50"}
{"File Name":"SearchServices\/0007-message-driven-content-tracker.md","Context":"## Context\\nThe ability to search on content requires a content extraction process. This relies on repo getting the document, passing it to one or multiple transformers, and finally returning the plain text content. This process does not scale as the embedded transformation is unable to cope with large volumes or large documents. Embedded transformations in general come with multiple problems, security related and scaling, which led to the introduction of the transformation service with 6.1\\nSince transformations to text for content indexing makes up a major portion of the transformation workload, it has always been intended to move these transformations to the new transformation service as well.\\nThe following are the suggested approaches to indexing with Transform Service:\\n* Refactor the current V0 API (in use by Search Services) to make use of RenditionService2.\\n* Introduce a new microservice that sits between Solr and the transformation service. The content is off loaded to the transformation service asynchronously while providing the same synchronous API for Search Services.\\n* Search Service to use the get rendition V1 Public API.\\n* New content tracker that communicates with the repository asynchronously by messages.\\n","Decision":"Based on the group discussion and design reviews, we have agreed to go with the asynchronous content tracker.\\nIn this design the Search Services will place a request in the message queue for the Repo to consume.\\nThe message will contain the NodeId and Solr identifier (name of the instance or Solr shard).\\nOnce the message is consumed by Repo it will start the process to obtain the text for the content.\\nWhen the content is ready a response message will be placed in the queue for Search Services to consume.\\nThe new content tracker will monitor the response queue and consume incoming messages. The message we expect to see in the queue will consist of an identifier, status and a URL. The status of the event can be used for handling errors. The handling of such errors prompting an abort or retry will be finalised during user story creation.\\nOn a successful completion the new content tracker will use the URL to obtain the content and retrieve the text for indexing.\\nWe use a URL in the response message rather than an identifier so that the repository can choose where to store the intermediate content at its own discretion. This will also provide the ability to leverage direct access URLs to cloud storage in the future (e.g. S3 signed URLs).\\nThe benefits of this solution gives ability to index content asynchronously. Unlike the current way which is based on a synchronous call to Repo using HTTP. This solution allows Alfresco to scale the transformation and adds the ability to index more content.\\n![Component Diagram](\/search-services\/alfresco-search\/doc\/architecture\/decisions\/diagrams\/AsyncContentTrackerComponentDiagram.png)\\nThe other options have been considered but did not full fill the requirements.\\nRefactor the current V0 API (in use by Search Services) to make use of RenditionService2:\\nThe thread in the repository will still be blocked. Although the new transform service has a higher throughput, it can have a slightly longer delay. This blocks HTTP threads even longer, or they could even time out.  Using async HTTP introduced with servlet 3.0 has been considered, but this would need to be implemented throughout the entire webscript framework.\\nUsing V1 API requires an authentication for SearchServices, which needs to be configured. There is currently no way for a system to call the V1 API without creating a new user. Creating a new user to represent the system is not the correct way to integrate systems and services. In addition, the V1 API uses the renditions for text which covered below.\\nUsing renditions for text extraction:\\nRenditions are stored long term in the repository as nodes. Using this mechanism for ephemeral text extractions would require ta new retention mechanism for renditions. All of this would put additional workload on the node tree, which defeats the design goal of handling high workloads.\\nIntroduce a new microservice:\\nThis has only been considered as an interim solution if it is not possible to change the content tracker in Search Services. It is essentially the solution above (blocking the sync request from Search Services until transformation is ready) but moved to its own micro service. This solution is slightly better as it does not block repository threads but suffers from the same problems.\\nCreating a microservice for the content tracker adds complexity which adds little benefit as part of this delivery. There are benefits for making the trackers into a microservice, but this would require a significant amount of work. The deployment of Search Services will become complicated as we would have to deploy SearchServices and a micorservice for the new ContentTracker.\\nThe current ContentTracker will remain so that the Community version continues to work with SearchServices.\\nThe V0 content tracking webscript and the ContentTracker will be removed with the next major upgrade in favour of the message base API. At this stage both Enterprise and Community will make use of the message base API. Please note that the communication via message queues between Repo and Search Service, will be configured by default in the ACS (reference) deployment templates. As for the Alfresco 6.x and SearchServices 1.x distribution zip files, will default to the legacy configuration. This would provide the customer different options to upgrade, as we have an overlap between versions.\\nAdding a new content tracker based on a message queue is the preferred solution as this provides the benefits of using the Transform Service.\\n","tokens":227,"id":5123,"Prediction":"## Decision\n\nAfter careful consideration of the options, we have decided to implement approach **2: Introduce a new microservice that sits between Solr and the transformation service**. This approach allows us to offload the content extraction process to the transformation service asynchronously, while providing a synchronous API for Search Services. This approach also provides the most flexibility and scalability, as it allows us to handle large volumes of content and large documents without impacting the performance of the Search Services.\n\n## Rationale\n\nWe chose this approach over the others for several reasons:\n\n* Refactoring the current V0 API to use RenditionService2 would require significant changes to the existing codebase, and may introduce technical debt and bugs.\n* Using the get rendition V1 Public API would require Search Services to make changes to their implementation, which could be time-consuming and may not be feasible.\n* Introducing a new content tracker that communicates with the repository asynchronously by messages would require significant changes to the repository and Search Services, and may not provide the same level of scalability and flexibility as approach 2.\n\n## Consequences\n\nBy implementing approach 2, we will need to:\n\n* Design and implement a new microservice that sits between Solr and the transformation service\n* Implement asynchronous communication between the repository and the transformation service\n* Update the Search Services to use the new microservice API\n* Test and validate the new implementation to ensure it meets the required performance and scalability standards\n\nHowever, this approach will also provide benefits such as:\n\n* Improved scalability and performance of the content extraction process\n* Reduced technical debt and bugs\n* Increased flexibility and flexibility to handle large volumes of content and large documents","GenTime":"2024-08-11 01:31:08"}
{"File Name":"profiles-db-elastic\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5139,"Prediction":"## Decision\n\n**Implement a Microservices Architecture for the Project**\n\n## Context\n\nAs the project grows in complexity and scale, we need to consider a scalable and maintainable architecture that allows for independent development, deployment, and scaling of individual components. The current monolithic architecture is becoming a bottleneck, and we need to adopt a more flexible and modular approach to ensure the project's continued success.\n\n## Problem\n\nThe current monolithic architecture is:\n\n* Difficult to maintain and update\n* Limited scalability and flexibility\n* Difficult to integrate with external services\n* Not suitable for a large and complex project\n\n## Decision\n\nWe will adopt a Microservices Architecture (MSA) for the project, where each component or service is designed to be independent, loosely coupled, and scalable. This will allow us to:\n\n* Develop and deploy individual services independently\n* Scale individual services based on demand\n* Improve fault tolerance and reliability\n* Increase the overall system's scalability and performance\n\n## Consequences\n\n* Increased complexity in terms of communication and coordination between services\n* Higher costs due to the need for additional infrastructure and personnel\n* Potential for increased coupling between services if not properly designed\n* Need for a clear service registry and communication protocol\n\n## Rationale\n\nThe benefits of a Microservices Architecture outweigh the potential drawbacks. By adopting MSA, we can:\n\n* Improve the overall system's scalability and performance\n* Increase the flexibility and adaptability of the system\n* Reduce the risk of a single point of failure\n* Improve the maintainability and upgradability of individual components\n\n## Status\n\nThis decision has been reviewed and approved by the project stakeholders and architects. The implementation of the Microservices Architecture will begin immediately, with a phased rollout to ensure minimal disruption to the current system.\n\n## History\n\n* 2023-02-15: Decision made\n* 2023-02-20: Initial implementation plan created\n* 2023-03-01: Phase 1 implementation started\n\nNote: This is a sample Architectural Decision Record (ADR) and may need to be adapted to fit your specific project's needs.","GenTime":"2024-08-11 01:35:32"}
{"File Name":"markmap\/splitting-and-bundling.md","Context":"## Context\\ncoc-markmap \/ markmap-cli has an incredibly large size in `node_modules`. (~30MB)\\n### Redundant dependencies\\nIn markmap-lib@0.9.x transforming and rendering share the same package, but they have quite different dependencies and are used in quite different scenarios. In other words many of the dependencies are useless most of the time.\\nFor example, transforming usually occurs locally when creating a markmap, while rendering happens in the browser when the markmap is opened. So if we are trying to create a markmap, it's likely that we don't need d3 and its friends. If we are exploring a markmap it's likely that we don't need all those transforming tools like remarkable and katex.\\n### Extraneous bundles\\nSome packages build multiple bundles to support different types of import. For example, `index.js` for `cjs`, and `index.min.js` for minimized `cjs` bundle, and several more. As a result, each of the bundle contains a copy of code, leading to a huge size.\\nAfter bundling into a single package, we get a single copy of the code, and optionally minimized.\\n","Decision":"- Split rendering code from `markmap-lib` into a new package `markmap-view`.\\nBy splitting the packages we can manage dependencies more independently, and get rid of unused ones.\\nIn most cases, `markmap-view` can be used as a prebuilt bundle, even without installation by leveraging CDNs.\\n- Use rollup to bundle code in `markmap-cli`.\\nUnder the hood, `coc-markmap` depends on `markmap-cli`, and `markmap-cli` depends on `markmap-lib`.\\n`markmap-lib` is supposed to be used in applications with bundlers, so it's better to keep simple and externalize the dependencies.\\n`markmap-cli` is a standalone command-line tool. It should be installed without dependency bloat. So it is a good idea to bundle everything in it and get rid of additional dependency packages.\\n`coc-markmap` is a thin layer on top of `markmap-cli` so we can just leave it as is.\\n","tokens":245,"id":5140,"Prediction":"## Decision\n\nTo address the issue of large size in `node_modules` for coc-markmap\/markmap-cli, we will implement a solution to reduce redundant dependencies and extraneous bundles.\n\n### Solution\n\nWe will introduce a new package, `markmap-core`, which will contain the shared dependencies and utilities between transforming and rendering. This package will be used as a dependency by both `markmap-lib` and `markmap-cli`.\n\nFor transforming, we will use a separate package, `markmap-transform`, which will depend on `markmap-core` and additional dependencies required for transforming. Similarly, for rendering, we will use a separate package, `markmap-render`, which will depend on `markmap-core` and additional dependencies required for rendering.\n\nWe will also remove the extraneous bundles and instead provide a single bundle for each package. For example, `markmap-lib` will have a single `index.js` file that can be imported as a CommonJS module or as an ES module.\n\n### Rationale\n\nBy introducing a new package, `markmap-core`, we can reduce the redundancy between transforming and rendering. This will also make it easier to manage dependencies and ensure that each package only includes the dependencies it needs.\n\nBy providing separate packages for transforming and rendering, we can ensure that each package has the dependencies it needs and can be used independently.\n\nRemoving the extraneous bundles will also reduce the overall size of the `node_modules` directory.\n\n### Trade-offs\n\nThe main trade-off is that we will need to manage additional packages and dependencies, which may add complexity to our build process. However, the benefits of reducing the size of `node_modules` and improving the manageability of our dependencies outweigh the costs.\n\n### Conclusion\n\nBy implementing this solution, we can reduce the size of `node_modules` for coc-markmap\/markmap-cli, improve the manageability of our dependencies, and make it easier to maintain and update our packages.","GenTime":"2024-08-11 01:35:53"}
{"File Name":"adr-generation-tool\/rate_limit_adr_0001.md","Context":"## Context\\nContext for rate limiting pattern\\n","Decision":"Prosa about the decision in the ADD\\n","tokens":9,"id":5142,"Prediction":"## Decision\n\n**Rate Limiting Pattern**\n\n**Context**\n\nAs part of our microservice architecture, we need to implement a rate limiting pattern to prevent abuse and ensure the scalability of our system. We want to limit the number of requests that can be made to our API within a certain time window to prevent overloading and ensure fair usage.\n\n**Decision**\n\nWe will implement a rate limiting pattern using the Leaky Bucket algorithm, which is a popular and effective approach for rate limiting. This algorithm works by maintaining a bucket that fills up at a certain rate and then drains at a slower rate. When the bucket is full, new requests are dropped or delayed until the bucket has enough capacity to accept them.\n\n**Rationale**\n\nWe chose the Leaky Bucket algorithm for several reasons:\n\n* It is easy to implement and maintain\n* It is effective in preventing abuse and ensuring scalability\n* It is flexible and can be easily configured to meet the specific needs of our system\n* It is well-suited for our microservice architecture, which requires a high degree of scalability and fault tolerance\n\n**Consequences**\n\nBy implementing the Leaky Bucket algorithm, we will be able to:\n\n* Prevent abuse and ensure the scalability of our system\n* Improve the overall performance and reliability of our API\n* Reduce the risk of overloading and ensure fair usage\n* Make it easier to manage and maintain our system\n\n**Alternatives Considered**\n\nWe considered several alternative rate limiting algorithms, including:\n\n* Token Bucket algorithm: This algorithm is similar to the Leaky Bucket algorithm, but it uses a token bucket instead of a leaky bucket. We rejected this approach because it is more complex and less well-suited for our microservice architecture.\n* Fixed Window algorithm: This algorithm is simpler and easier to implement, but it is less effective in preventing abuse and ensuring scalability. We rejected this approach because it is less flexible and less well-suited for our system.\n\n**Status**\n\nThis decision is approved and will be implemented in our system.","GenTime":"2024-08-11 01:36:41"}
{"File Name":"commcare-cloud\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5146,"Prediction":"## Decision\n\n**Title:** Decoupling the Frontend and Backend for Scalability and Flexibility\n\n**Context:** As our project grows in popularity, we need to ensure that our architecture can scale to meet the increasing demand while also allowing for flexibility in terms of technology stack and development pace.\n\n**Decision:** We will decouple the frontend and backend of our application using RESTful APIs and microservices architecture. This will enable us to develop and deploy each component independently, allowing for faster development and deployment cycles.\n\n**Rationale:**\n\n* Decoupling the frontend and backend will allow us to use different programming languages and frameworks for each component, giving us more flexibility in terms of technology stack.\n* With separate components, we can scale each part of the application independently, ensuring that the system as a whole remains scalable and performant.\n* This architecture will also enable us to develop and deploy new features and services more quickly, as we can focus on one component at a time.\n\n**Consequences:**\n\n* We will need to implement API gateways and service discovery mechanisms to manage communication between the frontend and backend components.\n* We will need to ensure that our database schema is designed to accommodate the decoupling of the frontend and backend, and that data is properly serialized and deserialized between components.\n* We will need to develop and implement monitoring and logging mechanisms to ensure that each component is functioning correctly and to identify and troubleshoot issues.\n\n**Status:** Accepted\n\n**Implementors:** [List the names of the team members responsible for implementing this decision]\n\n**References:** [List any relevant references or resources that were used to inform this decision]\n\n**Revisions:** [List any previous revisions of this decision, including dates and changes made]\n\nThis is an example of an Architectural Decision Record (ADR). It provides a clear and concise description of the decision, the context in which it was made, and the rationale behind it. It also outlines the consequences of the decision and who is responsible for implementing it.","GenTime":"2024-08-11 01:38:08"}
{"File Name":"skypy\/adr-01.md","Context":"## Context\\nWithin SkyPy all functions used to create a \"simulation\" will in practice be taking in some values (either parameters or columns from a table) and creating new column(s) in an output table *or* selecting specific rows from an input table.\\nThe inputs and outputs of these functions are clearly defined so a directed acyclic graph (DAG) can be constructed to determine what order the functions should be run in.\\nTo aid in the creation of the tables and the DAG a helper class or decorator should be used so the person writing the function does not have to worry about the implementation details. This class or decorator is what we are currently referring to as the `Model`.\\nFor clarity in the options below we will assume the following example function:\\n```python\\ndef redshift_gamma(shape, scale, size):\\n\"\"\"Gamma-distributed redshifts (Smail et al. 1994).\\nSample `size` redshifts from a gamma distribution with the\\ngiven `shape` and `scale` parameters. See `numpy.random.gamma`.\\n\"\"\"\\n# redshift distribution\\nredshift = np.random.gamma(shape=shape, scale=scale, size=size)\\nreturn redshift\\n```\\n## Decision Drivers\\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\n","Decision":"- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 3 has been picked.  This will be easiest for developers to write new functions and write clean unit tests.  Within the example given above `pyungo` was just used as an example, other DAG frameworks exist and picking one should be the topic of a different ADR.\\n","tokens":301,"id":5147,"Prediction":"## Decision\n\nBased on the provided context, I recommend using a decorator-based approach to implement the `Model` concept in SkyPy. This approach will provide ease of use, clarity of implementation, and flexibility for the developers.\n\nHere's a possible implementation:\n\n```python\ndef model(func):\n    def wrapper(*args, **kwargs):\n        # Implement the logic to create the output table and DAG\n        # For example, create a new column in the output table or select specific rows from the input table\n        #...\n        return func(*args, **kwargs)\n    return wrapper\n```\n\nIn this implementation, the `model` decorator is applied to the `redshift_gamma` function, which would then be wrapped with the `wrapper` function. The `wrapper` function would contain the logic to create the output table and DAG, and then call the original `redshift_gamma` function.\n\nThe benefits of this approach include:\n\n* Ease of use: The developer only needs to write the logic for the simulation function, and the decorator takes care of the implementation details.\n* Clarity of implementation: The `wrapper` function can be easily read, understood, and debugged, and the original simulation function remains untouched.\n* Flexibility: The decorator can be easily extended or modified to support additional features or edge cases.\n\nBy using a decorator-based approach, the `Model` concept can be implemented in a way that is easy to use, easy to understand, and flexible enough to support the needs of the SkyPy developers.","GenTime":"2024-08-11 01:38:25"}
{"File Name":"skypy\/adr-02.md","Context":"## Context\\nWe need to decide on a unit convention as to whether units include the factor \/h or not (for instance Mpc or Mpc\/h as a unit of distance). For further discussion see e.g. 10.1017\/pasa.2013.31\\n## Decision Drivers\\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\n","Decision":"- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/issues\/23) and offline, Mpc has been chosen to ensure the closest integration and least surprise for astropy.\\n","tokens":112,"id":5148,"Prediction":"## Decision\n\n### Decision: Include the factor \/h in the unit convention for distance units\n\n### Context: We need to decide on a unit convention as to whether units include the factor \/h or not (for instance Mpc or Mpc\/h as a unit of distance). For further discussion see e.g. 10.1017\/pasa.2013.31\n\n### Decision Drivers:\n\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\n- Consistency \/ least surprise: the default for astropy is Mpc\n\n### Rationale:\n\nWe have weighed the pros and cons of including or excluding the factor \/h in the unit convention for distance units. While excluding the factor \/h provides a more straightforward and consistent unit, it does not account for the uncertainty in the value of h. Including the factor \/h, on the other hand, allows for more flexibility and ease of propagation when dealing with uncertain values of h. Given the importance of considering the uncertainty in h, we have decided to include the factor \/h in the unit convention for distance units.\n\n### Consequences:\n\n* All distance units will include the factor \/h (e.g. Mpc\/h, kpc\/h, etc.)\n* This decision will require adjustments to the unit handling in our software to accommodate the factor \/h\n* Users will need to be aware of the uncertainty in the value of h when using these units\n\n### Affected Components:\n\n* Unit handling in our software\n* Documentation and user-facing interfaces\n\n### Implementation:\n\nWe will implement the decision to include the factor \/h in the unit convention for distance units by modifying the unit handling in our software. We will also update the documentation and user-facing interfaces to reflect this change.","GenTime":"2024-08-11 01:38:45"}
{"File Name":"search-ui-extensions\/ShowViewedByCustomer.md","Context":"## Context and Problem Statement\\n<!-- Quick 2-3 sentence background of the User Story -->\\nThe `ViewedByCustomer` component needs to be added on each result template, and therefore could be accidentally missed on one or more, especially if a new template is added at a later time. This would create an inconsistent view of what content the customer has viewed. - From JIRA\\n---\\n## Decision Drivers <!-- optional -->\\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n","Decision":"### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n-   [Option 1] - Leverage the `newResultsDisplayed` event, and loop over every result, performing further action.\\n-   [Option 2] - Leverage the `newResultDisplayed` event, and perform further action.\\n**Decision 2** - Properly adding the ViewedByDocument Component\\n-   [Option 1] - Add the component using `<div class=\"CoveoViewedByCustomer\">`.\\n-   [Option 2] - Add the component using the `ViewedByCustomer` constructor.\\n**Decision 3** - Ensure we don't add the template a second time\\n-   [Option 1] - Query the results `HTMLElement` using the `getElementsByClassName` method.\\n-   [Option 2] - Query the results `HTMLElement` using the `querySelectorAll` method.\\n**Decision 4** - There should be an option whether or not to add the component\\n-   [Option 1] - Have the option be false by default.\\n-   [Option 2] - Have the option be true by default.\\n---\\n#### Decision 1: [Option 2]\\nThere are two reason behind this decision selection: First the `newResultsDisplayed` option wasn't passing back the `args.item`, which would have made editing the dom element harder. Second, using the event trigger instead of a for loop made the methods functionality more simple.\\n#### Decision 2: [Option 2]\\nThe `newResultDisplayed` dom element was firing after the completion of the search-ui, therefore using the `<div>` wasn't possible.\\n#### Decision 3: [Option 1]\\nChoosing to use `getElementsByClassName`, in this context I don't think there is a difference between using `querySelectorAll` and `getElementsByClassName`\\n#### Decision 4: [Option 2]\\nFrom what I understand it makes sense for this feature to be true by default, as it's important for their not to be inconsistencies\\n---\\n","tokens":235,"id":5149,"Prediction":"Here is the Architectural Decision Record:\n\n## Decision\n\n**Decision:** To use an event-driven approach to add the `ViewedByCustomer` component to each result template, ensuring that it is added only once and that there is an option to toggle this behavior.\n\n## Context and Problem Statement\n\nThe `ViewedByCustomer` component needs to be added on each result template, and therefore could be accidentally missed on one or more, especially if a new template is added at a later time. This would create an inconsistent view of what content the customer has viewed.\n\n## Decision Drivers\n\n### Context\n\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\n\n### Decisions\n\n1. Need to choose when to edit the results (i.e. need an event)\n2. Ensure the `ViewedByCustomer` component is properly added to each result template\n3. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\n4. There should be an option whether or not to perform that aforementioned actions with the component\n\n**Rationale:** By using an event-driven approach, we can decouple the addition of the `ViewedByCustomer` component from the rendering of the result templates. This allows us to ensure that the component is added only once, even if multiple templates are rendered. Additionally, the option to toggle this behavior provides flexibility and control over when and how the component is added.","GenTime":"2024-08-11 01:39:02"}
{"File Name":"api-docs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5155,"Prediction":"## Decision\n\n**Title:** Implementing a Microservices Architecture for the Project\n\n**Context:** As our project grows in complexity and size, we need to consider a scalable and maintainable architecture that allows for independent development, deployment, and scaling of individual components. This decision aims to address the concerns of scalability, maintainability, and flexibility in our project.\n\n**Decision:** We will adopt a microservices architecture for the project, where each service is responsible for a specific business capability and communicates with other services using APIs.\n\n**Rationale:**\n\n* Microservices architecture allows for independent development, deployment, and scaling of individual services, enabling us to respond quickly to changing business requirements.\n* Each service can be developed in a different programming language or technology stack, providing flexibility and reducing technical debt.\n* With a clear separation of concerns, we can improve maintainability by reducing the complexity of each individual service.\n* Microservices architecture enables us to use different databases and data storage solutions for each service, allowing for better data management and scalability.\n\n**Consequences:**\n\n* We will need to implement service discovery, circuit breakers, and load balancing to manage communication between services.\n* We will need to implement API gateways to provide a unified entry point for clients and to handle API security and rate limiting.\n* We will need to implement logging and monitoring mechanisms to track service performance and identify issues.\n* We will need to establish clear communication channels and collaboration processes between teams responsible for different services.\n\n**Status:** This decision is approved and will be implemented in the next release.\n\n**Implementers:** [List the names of the team members or stakeholders responsible for implementing this decision]\n\n**Revisions:** [List any revisions to this decision, including the date and a brief description of the changes]\n\nThis decision record provides a clear and transparent documentation of the architectural decision made, including the context, decision, rationale, consequences, and status. It serves as a reference for future development and helps to ensure that all stakeholders are aligned with the project's architecture.","GenTime":"2024-08-11 01:41:11"}
{"File Name":"sepa-customer-platform\/0004-authenticate-backstage-users-azure-ad.md","Context":"## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage\/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n","Decision":"SEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\\n### Positive Consequences\\n* Reuse of existing SEPA logins\\n* Better backstage user experience\\n### Negative Consequences\\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\\n","tokens":90,"id":5157,"Prediction":"## Decision\n\nWe will use Azure Active Directory (AAD) as the identity management system to provide authentication for the frontend application, leveraging the existing Active Directory infrastructure used by SEPA for internal account management.\n\n## Context\n\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case and contact management, and to maximize vendor reuse and interoperability. As a result, we will utilize Azure's identity management system to provide authentication for the frontend application, particularly for backstage\/SEPA users.\n\n## Decision Drivers\n\n1. **Security**: By using AAD, we can provide a unified single sign-on (SSO) login experience, which will improve security by reducing the number of login credentials users need to manage.\n2. **Usability**: AAD will allow users to access the frontend application using their existing Active Directory credentials, making it easier for them to login and reducing the likelihood of forgotten passwords.\n3. **Interoperability**: By leveraging AAD, we can ensure seamless integration with the existing Active Directory infrastructure used by SEPA, minimizing the need for additional infrastructure or custom solutions.\n\n## Consequences\n\n1. **Increased security**: AAD will provide an additional layer of security for the frontend application, protecting user credentials and access to sensitive data.\n2. **Simplified user experience**: Users will have a single login experience across the frontend application and other Azure services, reducing the complexity of their login process.\n3. **Streamlined integration**: By using AAD, we will reduce the need for custom integration with Active Directory, minimizing the risk of errors and reducing the development time required for integration.\n\n## Rationale\n\nThe decision to use AAD for authentication is driven by the need to provide a secure, user-friendly, and interoperable solution that leverages the existing infrastructure used by SEPA. By utilizing AAD, we can improve the overall security and usability of the frontend application, while also reducing the complexity of the integration process.","GenTime":"2024-08-11 01:41:50"}
{"File Name":"sepa-customer-platform\/0006-relating-documents-to-cases.md","Context":"## Context and Problem Statement\\nDocument uploads will be stored in Azure Blob storage because of the requirement to store very large files & the fact that Azure Blob storage offers high availability and low costs.\\nThe Azure Blob storage system is a separate cloud service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case\/incident entities, will be linked to the the files within Azure.\\n## Decision Drivers\\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n","Decision":"* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.\\n### Positive Consequences\\n* Minimises customisation of the CRM data, reuse of existing\\n### Negative Consequences\\n* Labelling within the CRM does not match the context of the upload.\\n","tokens":142,"id":5163,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will create a new entity in Dynamics CRM to store the relationship between permits and Azure Blob storage files. This entity, named \"FileReference\", will be used to link permits (stored as cases or incidents) to the corresponding files in Azure Blob storage.\n\n**Context**\n\nDocument uploads will be stored in Azure Blob storage due to the requirement to store very large files and the fact that Azure Blob storage offers high availability and low costs. Since the Azure Blob storage system is a separate cloud service not directly linked to Dynamics CRM, we need to consider how to link permits, stored using CRM case\/incident entities, to the files within Azure.\n\n**Problem Statement**\n\nFiles will be displayed to users via a web frontend, and the entity used to store the file reference attribute should be accessible and filterable via the WebAPI. Additionally, the existing Dynamics UI should be able to display links to the Azure files for administrators.\n\n**Decision Drivers**\n\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\n\n**Consequences**\n\n* A new entity, \"FileReference\", will be created in Dynamics CRM to store the relationship between permits and Azure Blob storage files.\n* The \"FileReference\" entity will be accessible and filterable via the WebAPI, allowing for seamless integration with the web frontend.\n* The existing Dynamics UI will be able to display links to the Azure files for administrators, providing a user-friendly experience.\n\n**Rationale**\n\nCreating a new entity in Dynamics CRM to store the relationship between permits and Azure Blob storage files allows for a clear and maintainable way to link these two systems. This approach also ensures that the file reference attribute is accessible and filterable via the WebAPI, making it easy to integrate with the web frontend. Additionally, this solution provides a user-friendly experience for administrators by allowing them to display links to the Azure files directly within the Dynamics UI.","GenTime":"2024-08-11 01:44:08"}
{"File Name":"identity-site\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5170,"Prediction":"## Decision\n\n**Title:** Implement a Microservices Architecture for the Project\n\n**Context:** As the project grows in complexity and scope, it has become clear that a monolithic architecture is no longer sufficient to meet the requirements of the system. We need to adopt a more scalable and maintainable architecture that allows for greater flexibility and autonomy in the development and deployment of individual components.\n\n**Problem:** The current monolithic architecture is becoming increasingly difficult to manage, with a single codebase and a large number of dependencies. This is leading to slow development times, high maintenance costs, and a lack of scalability.\n\n**Decision:** We will implement a microservices architecture for the project, breaking down the system into smaller, independent services that can be developed, tested, and deployed separately. Each service will be responsible for a specific business capability or functionality, and will communicate with other services through well-defined APIs.\n\n**Rationale:** A microservices architecture will allow us to:\n\n* Increase scalability and reliability by allowing each service to be scaled independently\n* Improve maintainability by allowing developers to work on individual services without affecting the entire system\n* Enhance flexibility by allowing services to be developed and deployed using different technologies and frameworks\n* Reduce coupling between services, making it easier to change or replace individual services without affecting the overall system\n\n**Consequences:**\n\n* Increased complexity in terms of managing multiple services and APIs\n* Greater emphasis on communication and coordination between services\n* Potential for increased costs due to the need for additional infrastructure and support\n\n**Status:** This decision has been approved and is currently being implemented.\n\n**Links:** [Insert links to relevant documentation, such as design documents, code repositories, and deployment scripts]\n\n**Notes:** [Insert any additional notes or comments about the decision]","GenTime":"2024-08-11 01:46:28"}
{"File Name":"pace-developers\/0012-matlab-python-wrapper.md","Context":"## Context\\nBoth Euphonic and Brille are PACE projects with primarily a Python user interface (UI).\\nPACE, however, aims to provide both a Python and a Matlab UI to users,\\nand also to foster inter-operability between projects which are written both in Matlab and Python.\\nIn particular, `pyHorace` ([prototype](https:\/\/github.com\/mducle\/hugo)) cannot use the\\n[standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for Matlab to run Python code,\\nwhere calls to Python from Matlab are prefixed with `py.` followed by the full module specification.\\nFor example, `r = py.numpy.random.rand()` uses `numpy` to generate a random number.\\nThis is because such a call causes Matlab to\\n[automatically spawn](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/pyenv.html) a dependent Python interpreter,\\nwhich can be either created within the same process as the Matlab interpreter (`InProcess`)\\nor in an external process (`OutOfProcess`).\\n`pyHorace` already runs within a Python interpreter and the compiled Matlab library *must* be loaded in-process.\\nThus, if Matlab spawns a second Python intepreter with the default `InProcess` execution mode,\\nthe two Python interpreters will conflict causing memory errors and a crash.\\nWe can force Matlab to launch the dependent Python interpreter `OutOfProcess`\\nbut this imposes a significant performance penalty\\n(extensive testing was not done but Brille+SpinW runs about 10x slower than with `InProcess`).\\n","Decision":"At a meeting on Jan 7 2021, the developers of `pyHorace`, `brillem` and `horace-euphonic-interface` agreed to accept this proposal.\\n`brillem` and `horace-euphonic-interface` will be refactored to use the `light_python_wrapper` proposed here.\\nThe meeting also agreed implementation details which will be described in [ADR #13](0013-light-python-wrapper-implementation-detail.md).\\n","tokens":333,"id":5171,"Prediction":"## Decision\n\nTo resolve the issue of conflicting Python interpreters in `pyHorace`, we will use a custom wrapper to interact with the Python code from Matlab, rather than using the standard `py.` method. This wrapper will allow us to manually manage the creation and interaction with the Python interpreter, ensuring that only one instance of the interpreter is used and avoiding conflicts.\n\n## Context\n\nAs mentioned in the context, `pyHorace` cannot use the standard method for Matlab to run Python code, where calls to Python from Matlab are prefixed with `py.` followed by the full module specification. This is because such a call causes Matlab to automatically spawn a dependent Python interpreter, which can lead to conflicts and crashes when running within a Python interpreter.\n\n## Problem\n\nThe problem is that the standard method for Matlab to run Python code is not suitable for `pyHorace` because it would create a second Python interpreter, which would conflict with the existing Python interpreter and cause memory errors and crashes.\n\n## Decision Rationale\n\nWe have decided to use a custom wrapper to interact with the Python code from Matlab because it allows us to manually manage the creation and interaction with the Python interpreter, ensuring that only one instance of the interpreter is used and avoiding conflicts. This approach also provides more control over the interaction between Matlab and Python, allowing us to optimize performance and improve reliability.\n\n## Status\n\nThis decision has been implemented and tested in the `pyHorace` prototype.\n\n## Consequences\n\nThis decision has the following consequences:\n\n* The custom wrapper provides more control over the interaction between Matlab and Python, allowing us to optimize performance and improve reliability.\n* The wrapper allows us to manually manage the creation and interaction with the Python interpreter, ensuring that only one instance of the interpreter is used and avoiding conflicts.\n* The performance penalty associated with running the dependent Python interpreter `OutOfProcess` is avoided.\n\nNote: This decision record is a summary of the decision-making process and the consequences of the decision. It is intended to provide transparency and accountability in the decision-making process.","GenTime":"2024-08-11 01:46:50"}
{"File Name":"pace-developers\/0002-use-github-pages-for-user-documentation.md","Context":"## Context\\nA consistent platform is required for user documentation from the PACE projects: Brille, Euphonic and Horace\/Herbert.\\nTwo platforms support, are well used for this service:\\n- [Read the Docs](https:\/\/readthedocs.org\/)\\n- [GitHub pages](https:\/\/pages.github.com\/)\\nBoth platforms will display documentation built by [Sphinx](https:\/\/www.sphinx-doc.org\/) from reStructuredText source files.\\nBrille includes compiled C libraries that contribute APIs to the project. The Read the Docs build\/deploy pipeline does not support inclusion of documentation generated from C source.\\n","Decision":"We will use GitHub pages for all PACE project documentation.\\n","tokens":132,"id":5172,"Prediction":"## Decision\n\n**Decision:** We will use GitHub Pages as the primary platform for user documentation from the PACE projects: Brille, Euphonic, and Horace\/Herbert.\n\n**Rationale:**\n\n* Both Read the Docs and GitHub Pages are well-established platforms for hosting user documentation.\n* GitHub Pages is a more flexible option that allows us to include documentation generated from C source files, which is crucial for Brille's compiled C libraries.\n* GitHub Pages is also a more cost-effective option, as it is free for public repositories.\n* Read the Docs has a limitation in its build\/deploy pipeline that does not support inclusion of documentation generated from C source files, which makes it less suitable for Brille's requirements.\n* By using GitHub Pages, we can maintain a consistent platform for user documentation across all PACE projects.\n\n**Consequences:**\n\n* We will need to set up a GitHub Pages site for each project and configure the build process to generate documentation from reStructuredText source files using Sphinx.\n* We will need to ensure that the documentation is properly linked and organized across the different projects.\n* We will need to maintain the GitHub Pages site and ensure that it remains up-to-date and consistent with the documentation generated from the reStructuredText source files.\n\n**Status:** Implemented","GenTime":"2024-08-11 01:47:04"}
{"File Name":"pace-developers\/0007-developer-scripts-storage-location.md","Context":"## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo\/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n","Decision":"Developer scripts will be stored in an appropriately placed `scripts`\\ndirectory in the `pace-developers` repository. Depending on whether the\\nscript is tied to a particular software, or general algorithm development\\nit could be stored in `pace-developers\/euphonic\/scripts` or\\n`pace-developers\/powder_averaging\/scripts` for example.\\n","tokens":283,"id":5176,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will store demo\/visualisation scripts in a `scripts` directory in `pace-developers`.\n\n**Context**\n\nWhen developing new algorithms and features for PACE-related software, developers often create useful demo\/visualisation scripts for their own use. These scripts could be useful or interesting for other developers, and are important for reproducibility or justifying design decisions. They should be stored somewhere in version control so that they can be easily accessed by any developers and referred to later. However, they are not intended for general use and will not be actively maintained or tested.\n\n**Problem**\n\nThere are two main options for storing these scripts: in a `dev_scripts` directory in each separate project repository, or in a `scripts` directory in `pace-developers`.\n\n**Decision Criteria**\n\n* All in one place: Storing scripts in `pace-developers` keeps them in one place, making it easier to find and access them.\n* Scripts will be close to the code they are used for: Storing scripts in `pace-developers` keeps them close to the decision-making developer documentation that they support.\n* Version can be specified for any project dependencies: Storing scripts in `pace-developers` allows us to specify a version for any project dependencies.\n* Scripts may not work with the version of the code they are distributed with: This is a drawback of storing scripts in `pace-developers`, as they may not work with the version of the code they are distributed with.\n* It's unclear where scripts that use more than one project would go: This is a drawback of storing scripts in `pace-developers`, as it's unclear where scripts that use more than one project would go.\n* Despite the folder being called `dev_scripts` people might expect the scripts to actually work as they're in the main project repository: This is a drawback of storing scripts in `dev_scripts` directory in each separate project repository, as people might expect the scripts to actually work as they're in the main project repository.\n\n**Consequences**\n\n* Scripts will be stored in a central location, making it easier to find and access them.\n* Scripts will be close to the decision-making developer documentation that they support.\n* A version can be specified for any project dependencies.\n* Scripts may not work with the version of the code they are distributed with.\n* It's unclear where scripts that use more than one project would go.\n\n**Rationale**\n\nAfter considering the pros and cons of each option, we decided to store demo\/visualisation scripts in a `scripts` directory in `pace-developers`. This decision was made because it keeps the scripts in one place, close to the decision-making developer documentation that they support, and allows us to specify a version for any project dependencies. While there are some drawbacks to this decision, such as scripts not working with the version of the code they are distributed with, we believe that the benefits outweigh the costs.","GenTime":"2024-08-11 01:48:40"}
{"File Name":"pace-developers\/0005-pace-projects-must-be-semantically-versioned.md","Context":"## Context\\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build.\\n[Semantic versioning](https:\/\/semver.org\/) defines a schema in which releases are given `major.minor.patch` version numbers where increments are made to the:\\n- `major` version with incompatible API changes,\\n- `minor` version when functionality is added in a backwards compatible manner, and\\n- `patch` version for backwards compatible bug fixes.\\nSphinx plugins are available that support multiple documentation versions ([sphinx-multiversion](https:\/\/pypi.org\/project\/sphinx-multiversion\/)) - this will not work correctly for Brille where documentation is extracted from the build artifacts.\\n","Decision":"Projects will be semantically versioned and documentation will be retained and be available for users to access for each major or minor version released.\\nThere is no requirement to have patch-release specific documentation.\\n","tokens":159,"id":5177,"Prediction":"## Decision\n\nTo ensure that users can easily find the correct documentation for their build, considering the PACE projects will evolve over time and breaking changes will be introduced, we will implement a documentation versioning system that is compatible with the Brille build artifacts. \n\n## Context\n\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build. \n\n## Problem\n\nThe PACE projects will evolve over time and breaking changes will be introduced, which will make it difficult for users to find the correct documentation for their build. \n\n## Solution\n\nWe will implement a documentation versioning system that is compatible with the Brille build artifacts. This system will allow us to maintain multiple versions of the documentation, each corresponding to a specific version of the PACE projects. This will enable users to easily find the correct documentation for their build, regardless of the version of the PACE projects they are using.\n\n## Rationale\n\nThe rationale for this decision is that the PACE projects will evolve over time and breaking changes will be introduced, which will make it difficult for users to find the correct documentation for their build. By implementing a documentation versioning system that is compatible with the Brille build artifacts, we can ensure that users can easily find the correct documentation for their build, regardless of the version of the PACE projects they are using.\n\n## Consequences\n\nThe consequences of this decision are that we will need to maintain multiple versions of the documentation, which will require additional resources and effort. However, this will enable users to easily find the correct documentation for their build, which will improve the overall user experience and reduce the risk of errors and confusion.\n\n## Decision\n\nWe will implement a documentation versioning system that is compatible with the Brille build artifacts.","GenTime":"2024-08-11 01:49:00"}
{"File Name":"pace-developers\/0008-pace-syntax-guiding-principles.md","Context":"## Context\\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\\nPACE involves many other codes, including [Euphonic](https:\/\/github.com\/pace-neutrons\/Euphonic),\\n[Brille](https:\/\/github.com\/brille\/brille) and [SpinW](https:\/\/spinw.org).\\nThe way in which these programs interact with each other is presently unclear.\\nFurthermore the programs are written in several different languages.\\nIn this ADR we propose some guiding principles in the design of the interfaces\\nbetween the programs or components which make up PACE.\\nThis was discussed at a meeting whose\\n[minutes are here.](https:\/\/stfc365.sharepoint.com\/:w:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/pace_syntax_meeting1_minutes.docx))\\n","Decision":"The guiding principle for PACE-supported programs is **commonality**.\\nThat is, interacting with the different programs of PACE should be **seamless**\\nto the user, and the user interface presented to them should be **uniform**.\\nThe uniform interface means that function names and how users specify parameters\\nor arguments to functions in Matlab and Python flavours of PACE should align\\nas closely as possible.\\n","tokens":183,"id":5178,"Prediction":"## Decision\n\n**Title:** Guiding Principles for Designing Interfaces between PACE Components\n\n**Context:** As PACE involves multiple codes written in different languages, there is a need to establish clear guidelines for designing interfaces between these components to ensure seamless interaction and maintainability.\n\n**Decision:** We adopt the following guiding principles for designing interfaces between PACE components:\n\n1. **Language-agnostic interfaces**: All interfaces should be designed to be language-agnostic, allowing components written in different languages to interact with each other seamlessly.\n2. **Standardized data formats**: Standardized data formats should be used to facilitate data exchange between components, reducing the risk of data corruption or misinterpretation.\n3. **API-based interfaces**: Interfaces should be designed as APIs (Application Programming Interfaces) to provide a clear and consistent way of interacting with components.\n4. **Documentation and testing**: Documentation and testing should be prioritized to ensure that interfaces are well-documented, easy to use, and thoroughly tested.\n5. **Modular design**: Components should be designed with modularity in mind, allowing for easy integration and replacement of individual components without affecting the overall system.\n6. **Extensibility**: Interfaces should be designed to be extensible, allowing for new components or features to be added to the system without disrupting existing functionality.\n7. **Consistency**: Consistency should be maintained across all interfaces, using standardized naming conventions, data types, and error handling mechanisms.\n\n**Rationale:** These guiding principles aim to promote interoperability, maintainability, and scalability within the PACE ecosystem. By adopting these principles, we can ensure that components interact with each other seamlessly, reducing the risk of errors and making it easier to add new features or components to the system.\n\n**Status:** This decision is approved and will be implemented in all new components and interfaces developed for PACE.\n\n**Implementation:** The implementation of these guiding principles will be monitored and reviewed regularly to ensure that they are effective in promoting interoperability and maintainability within the PACE ecosystem.\n\n**Revisions:** This ADR will be reviewed and revised as necessary to reflect changes in the PACE ecosystem or new requirements that arise.","GenTime":"2024-08-11 01:49:23"}
{"File Name":"pace-developers\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5182,"Prediction":"## Decision\n\n**Decision:** Implement a Microservices Architecture for the Project\n\n## Context\n\nAs we're building a complex software system, we need to ensure that it is scalable, maintainable, and flexible. A Microservices Architecture (MSA) has been considered as a viable option to achieve these goals.\n\n## Problem\n\nThe current monolithic architecture of the system is causing scalability issues, making it difficult to maintain and update individual components. Additionally, the system's complexity is increasing, and a more modular approach is needed to ensure that each component can be developed, tested, and deployed independently.\n\n## Solution\n\nWe will adopt a Microservices Architecture, where the system is broken down into smaller, independent services that communicate with each other using lightweight protocols and APIs. Each service will be responsible for a specific business capability, and will be developed, tested, and deployed independently.\n\n## Rationale\n\nThe Microservices Architecture provides several benefits, including:\n\n* Scalability: Each service can be scaled independently, allowing us to optimize resource allocation and improve system performance.\n* Flexibility: Services can be developed and deployed in different programming languages and frameworks, allowing us to take advantage of the best tools for each service.\n* Resilience: If one service experiences issues, it will not affect the entire system, improving overall system reliability.\n* Maintainability: With a smaller, more focused codebase for each service, maintenance and updates will be easier and less error-prone.\n\n## Consequences\n\n* Increased complexity: With more services and communication between them, the system will be more complex to understand and maintain.\n* Higher operational overhead: We will need to manage multiple services, including service discovery, load balancing, and monitoring.\n* Potential for inconsistencies: With multiple services, there is a risk of inconsistencies between them, which will require careful coordination and testing.\n\n## Alternatives\n\n* Monolithic Architecture: Continue with the current monolithic architecture, which would require significant refactoring and potentially compromise scalability and maintainability.\n* Service-Oriented Architecture (SOA): Implement a SOA, which would provide some benefits of MSA, but would not provide the same level of scalability and flexibility.\n\n## Status\n\nApproved\n\n## Implemented\n\nThe Microservices Architecture has been implemented, and the system is being developed and deployed using this architecture.","GenTime":"2024-08-11 01:50:24"}
{"File Name":"umbrella\/0003-component-configuration-via-context.md","Context":"## Context\\nAn alternative configuration procedure to ADR-0002, possibly better\\nsuited for dynamic theming, theme changes and separating the component\\nconfiguration between behavioral and stylistic aspects. This new\\napproach utilizes the hdom context object to retrieve theme attributes,\\nwhereas the previous solution ignored the context object entirely.\\nA live demo of the code discussed here is available at:\\n[demo.thi.ng\/umbrella\/hdom-theme-adr-0003](https:\/\/demo.thi.ng\/umbrella\/hdom-theme-adr-0003)\\n","Decision":"### Split component configuration\\n#### Behavioral aspects\\nComponent pre-configuration options SHOULD purely consist of behavioral\\nsettings and NOT include any aesthetic \/ theme oriented options. To\\nbetter express this intention, it's recommended to suffix these\\ninterface names with `Behavior`, e.g. `ButtonBehavior`.\\n```ts\\ninterface ButtonBehavior {\\n\/**\\n* Element name to use for enabled buttons.\\n* Default: \"a\"\\n*\/\\ntag: string;\\n\/**\\n* Element name to use for disabled buttons.\\n* Default: \"span\"\\n*\/\\ntagDisabled: string;\\n\/**\\n* Default attribs, always injected for active button states\\n* and overridable at runtime.\\n* Default: `{ href: \"#\", role: \"button\" }`\\n*\/\\nattribs: IObjectOf<any>;\\n}\\n```\\n#### Theme stored in hdom context\\nEven though there's work underway to develop a flexble theming system\\nfor hdom components, the components themselves SHOULD be agnostic to\\nthis and only expect to somehow obtain styling attributes from the hdom\\ncontext object passed to each component function. How is shown further\\nbelow.\\nIn this example we define a `theme` key in the context object, under\\nwhich theme options for all participating components are stored.\\n```ts\\nconst ctx = {\\n...\\ntheme: {\\nprimaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\nsecondaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\n...\\n}\\n};\\n```\\n### Component definition\\n```ts\\nimport { getIn, Path } from \"@thi.ng\/paths\";\\n\/**\\n* Instance specific runtime args. All optional.\\n*\/\\ninterface ButtonArgs {\\n\/**\\n* Click event handler to be wrapped with preventDefault() call\\n*\/\\nonclick: EventListener;\\n\/**\\n* Disabled flag. Used to determine themed version.\\n*\/\\ndisabled: boolean;\\n\/**\\n* Selected flag. Used to determine themed version.\\n*\/\\nselected: boolean;\\n\/**\\n* Link target.\\n*\/\\nhref: string;\\n}\\nconst button = (themeCtxPath: Path, behavior?: Partial<ButtonBehavior>) => {\\n\/\/ init with defaults\\nbehavior = {\\ntag: \"a\",\\ntagDisabled: \"span\",\\n...behavior\\n};\\nbehavior.attribs = { href: \"#\", role: \"button\", ...behavior.attribs };\\n\/\/ return component function as closure\\nreturn (ctx: any, args: Partial<ButtonArgs>, ...body: any[]) => {\\n\/\/ lookup component theme config in context\\nconst theme = getIn(ctx, themeCtxPath);\\nif (args.disabled) {\\nreturn [behavior.tagDisabled, {\\n...behavior.attribs,\\n...theme.disabled,\\n...args,\\n}, ...body];\\n} else {\\nconst attribs = {\\n...behavior.attribs,\\n...theme[args.selected ? \"selected\" : \"default\"],\\n...args\\n};\\nif (args && args.onclick && (args.href == null || args.href === \"#\")) {\\nattribs.onclick = (e) => (e.preventDefault(), args.onclick(e));\\n}\\nreturn [behavior.tag, attribs, ...body];\\n}\\n};\\n};\\n```\\n### Component usage\\n```ts\\nconst darkTheme = {\\nid: \"dark\",\\nbody: {\\nclass: \"vh-100 bg-black moon-gray pa3 sans-serif\"\\n},\\nlink: {\\nclass: \"link dim b light-silver\"\\n},\\nbutton: {\\ndefault: {\\nclass: \"dib link mr2 ph3 pv2 blue hover-lightest-blue hover-b--current br3 ba b--blue\"\\n},\\nselected: {\\nclass: \"dib link mr2 ph3 pv2 red hover-gold hover-b--current br3 ba b--red\"\\n},\\ndisabled: {\\nclass: \"dib mr2 ph3 pv2 mid-gray br3 ba b--mid-gray\"\\n}\\n}\\n};\\nconst bt = button(\"theme.button\");\\nconst btFixed = button(\"theme.button\", { attribs: { style: { width: \"8rem\" } } });\\nconst app = (ctx) =>\\n[\"div\", ctx.theme.body,\\n[bt, { onclick: () => alert(\"toggle\") }, \"Toggle\"],\\n[bt, { href: \"https:\/\/github.com\/thi-ng\/umbrella\" }, \"External\"],\\n[btFixed, { onclick: () => alert(\"hi\"), selected: true }, \"Selected\"],\\n[btFixed, { disabled: true }, \"Disabled\"] ];\\n\/\/ start app with theme in context\\nstart(\"app\", app, { theme: darkTheme })\\n```\\n","tokens":117,"id":5185,"Prediction":"## Decision\n\n### Title\n\nUse HDOM Context Object for Dynamic Theming and Component Configuration\n\n### Context\n\nAs discussed in ADR-0002, we currently use a separate configuration procedure for our application. However, this approach may not be suitable for dynamic theming, theme changes, and separating the component configuration between behavioral and stylistic aspects. We need a more efficient and scalable solution that utilizes the hdom context object to retrieve theme attributes.\n\n### Decision\n\nWe will adopt a new approach that utilizes the hdom context object to retrieve theme attributes. This will enable us to dynamically change themes and separate the component configuration between behavioral and stylistic aspects.\n\n### Description\n\nThe new approach involves using the hdom context object to retrieve theme attributes. This will allow us to dynamically change themes and separate the component configuration between behavioral and stylistic aspects. The demo available at [demo.thi.ng\/umbrella\/hdom-theme-adr-0003](https:\/\/demo.thi.ng\/umbrella\/hdom-theme-adr-0003) showcases the new approach in action.\n\n### Rationale\n\nThe new approach provides several benefits, including:\n\n* Dynamic theming: We can now dynamically change themes without requiring a full application restart.\n* Separation of concerns: We can separate the component configuration between behavioral and stylistic aspects, making it easier to maintain and update our application.\n* Improved scalability: The new approach is more efficient and scalable, allowing us to handle larger and more complex applications.\n\n### Consequences\n\nThe new approach will require changes to our existing codebase, but it will provide a more efficient and scalable solution for dynamic theming and component configuration. We will need to update our documentation and provide training to our development team on the new approach.\n\n### Status\n\nThis decision is currently in the implementation phase. We will continue to monitor the progress and make any necessary adjustments to ensure the successful adoption of the new approach.","GenTime":"2024-08-11 01:51:31"}
{"File Name":"js-sdk\/0008-add-kwargs-to-3bot-start.md","Context":"## Context\\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\\n","Decision":"Add kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\\n","tokens":28,"id":5188,"Prediction":"## Decision\n\n**Decision:** Introduce a configuration mechanism for packages to store and retrieve their initial keyword arguments (kwargs) instead of passing them every time the `start` method is called.\n\n**Context:** Adding packages with kwargs has some limitations and hence kwargs are needed every time `start` is called not only once when adding package.\n\n**Problem:** Currently, the `start` method requires kwargs every time it is called, which can lead to unnecessary repetition and make the code harder to maintain. This is because the kwargs are not stored anywhere and need to be passed again and again.\n\n**Reasoning:** By introducing a configuration mechanism, we can store the initial kwargs for each package and retrieve them when needed, eliminating the need to pass them every time the `start` method is called. This will improve code readability and maintainability.\n\n**Consequences:**\n\n* The `start` method will no longer require kwargs every time it is called.\n* The configuration mechanism will need to be implemented and maintained.\n* The code will become more flexible and easier to maintain.\n\n**Alternatives Considered:**\n\n* Continue passing kwargs every time the `start` method is called, which can lead to unnecessary repetition and make the code harder to maintain.\n* Introduce a separate method to set the kwargs, which can lead to code duplication and make it harder to maintain.\n\n**Decision Outcome:** Introducing a configuration mechanism for packages to store and retrieve their initial kwargs will improve code readability and maintainability, and eliminate the need to pass kwargs every time the `start` method is called.","GenTime":"2024-08-11 01:52:34"}
{"File Name":"js-sdk\/0003-allow-only-tft-token.md","Context":"## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n","Decision":"Completely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\\n","tokens":14,"id":5198,"Prediction":"## Decision\n\n**Drop FreeTFT and TFTA tokens from SDK**\n\n## Context\n\nAs per the user's request, we are considering dropping the FreeTFT and TFTA tokens from our SDK.\n\n## Problem\n\nThe FreeTFT and TFTA tokens were included in our SDK as a way to provide additional functionality to our users. However, since their adoption has been low, and the maintenance and support required to keep them up-to-date is significant, we are faced with the decision to either continue to support these tokens or drop them from our SDK.\n\n## Decision\n\nAfter careful consideration, we have decided to drop the FreeTFT and TFTA tokens from our SDK. This decision is based on the following reasons:\n\n* Low adoption rate: Despite being included in our SDK for some time, the usage of FreeTFT and TFTA tokens has been very low. This suggests that they are not providing significant value to our users.\n* Maintenance and support: The maintenance and support required to keep these tokens up-to-date is significant, and the resources required to do so are better spent on other areas of our SDK.\n* Simplification: Dropping these tokens will simplify our SDK and reduce the complexity of our codebase, making it easier for our users to understand and use.\n\n## Consequences\n\nBy dropping the FreeTFT and TFTA tokens, we expect the following consequences:\n\n* Users who were relying on these tokens will need to find alternative solutions.\n* Our SDK will be simplified and easier to maintain.\n* We will be able to focus our resources on other areas of our SDK that provide more value to our users.\n\n## Rationale\n\nWe believe that this decision is justified because it will allow us to focus on the core functionality of our SDK and provide a better experience for our users. While it may cause some inconvenience to users who were relying on these tokens, we believe that the benefits of simplifying our SDK and reducing its complexity outweigh the costs.\n\n## Status\n\nThis decision has been made and will be implemented in the next release of our SDK.\n\n## Links\n\n* [FreeTFT token documentation](link to documentation)\n* [TFTA token documentation](link to documentation)","GenTime":"2024-08-11 01:56:07"}
{"File Name":"js-sdk\/0016-update-threebot-deployer-flist.md","Context":"## Context\\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\\n","Decision":"Update the flist with the up to date one\\n","tokens":48,"id":5201,"Prediction":"## Decision\n\n**Decision:** Implement a CI\/CD pipeline that uses the new flist build to reduce the deployment time of hosted 3Bot.\n\n**Context:** Deploying hosted 3Bot takes a lot of time because of cloning and installation time.\n\n**Problem:** The current deployment process for hosted 3Bot is time-consuming, which can lead to delays in getting new features and updates to users.\n\n**Reasoning:** By using the new flist build produced by the CI, we can reduce the installation time and ensure that the deployed 3Bot is always up-to-date. This will improve the overall deployment process and provide a better user experience.\n\n**Decision Drivers:**\n\n* Reduce deployment time to improve user experience\n* Ensure that deployed 3Bot is always up-to-date\n* Leverage the new flist build produced by the CI to streamline the deployment process\n\n**Consequences:**\n\n* The deployment process will be faster and more efficient\n* The deployed 3Bot will always be up-to-date with the latest changes\n* The CI\/CD pipeline will be more robust and reliable\n\n**Alternatives Considered:**\n\n* Continue with the current deployment process and clone the entire repository\n* Use a different build tool or process to reduce deployment time\n* Implement a caching mechanism to reduce the installation time\n\n**Status:** Implemented\n\n**Implementation Status:** The CI\/CD pipeline has been updated to use the new flist build, and the deployment process has been tested and validated.","GenTime":"2024-08-11 01:57:07"}
{"File Name":"report-a-defect\/0011-use-auth0-for-authentication.md","Context":"## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n","Decision":"Use Auth0 as the single sign-on provider, and remove basic auth from production.\\n","tokens":234,"id":5211,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will use Auth0 as a temporary single sign-on (SSO) provider for the Report a Defect service until a permanent solution is decided upon by Hackney.\n\n**Context**\n\nAs Hackney is currently reviewing their options for a single sign-on provider, we have been using Basic Authentication. We initially considered using Microsoft Azure Active Directory with OpenID Connect (OAuth2) as per the Repairs Hub, but this was deemed experimental and not supported by Hackney. With only two weeks left to meet the user need of providing agent names and security requirements, we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary solution.\n\n**Problem**\n\nWe need to provide a secure and user-friendly authentication solution for the Report a Defect service while awaiting a permanent solution from Hackney.\n\n**Reasoning**\n\nAuth0 provides a logged authentication service with one set of credentials per person, meeting our security requirements. It also allows us to provide agent names within the service, meeting our user need. Using Auth0 as a temporary solution will enable us to meet the deadline and provide a better user experience.\n\n**Consequences**\n\n* We will use Auth0 as a temporary SSO provider until a permanent solution is decided upon by Hackney.\n* We will need to integrate Auth0 with the Report a Defect service.\n* We will need to monitor and test the Auth0 solution to ensure it meets our security and user requirements.\n\n**Status**\n\nApproved\n\n**Date**\n\n[Insert Date]\n\n**Responsible Person**\n\n[Insert Name]\n\n**Reviewers**\n\n[Insert Names]\n\n**Discussion**\n\n[Insert any relevant discussion or debate that occurred during the decision-making process]\n\n**Rationale**\n\n[Insert any additional information or justification for the decision]","GenTime":"2024-08-11 02:00:34"}
{"File Name":"report-a-defect\/0008-usegovuknotifyforemailing.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThis service has a need to send emails and SMS to users. As Hackney is approved as an organisation that can use GOV.UK Notify we are able to use this service and take advantage of the reliability and low costs.\\nHackney already use Notify for other services, although it is not yet referenced in their playbook https:\/\/github.com\/LBHackney-IT\/API-Playbook\\n","Decision":"Use Notify over a third party service such as SendGrid for the sending of emails and SMS\\n","tokens":101,"id":5213,"Prediction":"## Decision\n\n**Use GOV.UK Notify for sending emails and SMS to users**\n\n## Context\n\nThe service has a need to send emails and SMS to users, and Hackney has been approved to use GOV.UK Notify, which offers reliability and low costs. Additionally, Hackney already uses Notify for other services, although it is not yet referenced in their playbook.\n\n## Problem\n\nThe service needs to send emails and SMS to users, and currently does not have a reliable and cost-effective solution for doing so.\n\n## Decision\n\nAfter considering the context, we have decided to use GOV.UK Notify for sending emails and SMS to users. This decision is based on the following reasons:\n\n* Reliability: GOV.UK Notify is a well-established and reliable service that has been used by Hackney for other services.\n* Cost-effectiveness: GOV.UK Notify offers low costs for sending emails and SMS, which is beneficial for the service.\n* Integration: Hackney already uses Notify for other services, which makes it easier to integrate with the existing infrastructure.\n\n## Consequences\n\nBy using GOV.UK Notify, the service will be able to send emails and SMS to users reliably and cost-effectively. This decision also aligns with Hackney's existing use of Notify for other services, which will simplify the integration process.\n\n## Rationale\n\nThe decision to use GOV.UK Notify is based on the need to send emails and SMS to users, and the benefits of using a reliable and cost-effective service. The service will be able to take advantage of the existing infrastructure and expertise that Hackney has developed in using Notify for other services.\n\n## Status\n\nThis decision is approved and will be implemented as part of the service's infrastructure.","GenTime":"2024-08-11 02:01:15"}
{"File Name":"report-a-defect\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5220,"Prediction":"Here is an Architectural Decision Record (ADR) for the project:\n\n**Decision:** Use a Microservices Architecture for the New Project\n\n**Context:**\n\nAs we embark on the new project, we need to decide on the overall architecture that will guide the development of the system. The project requires integration with multiple external services, has a large and complex domain model, and needs to be highly scalable and fault-tolerant.\n\n**Problem:**\n\nThe traditional monolithic architecture we have used in the past may not be suitable for this project, as it would lead to a complex and rigid system that is difficult to maintain and evolve.\n\n**Decision:**\n\nAfter careful consideration, we have decided to use a Microservices Architecture (MSA) for the new project. This approach will allow us to break down the system into smaller, independent services that can be developed, tested, and deployed independently. This will enable us to:\n\n* Improve scalability and fault-tolerance by allowing individual services to be scaled or restarted independently\n* Enhance flexibility and agility by allowing services to be developed and deployed at different rates\n* Improve maintainability by allowing each service to be maintained and updated independently\n* Improve collaboration and communication between teams by allowing each team to focus on a specific service\n\n**Consequences:**\n\n* Increased complexity in the system architecture and development process\n* Need for additional infrastructure and tooling to support the microservices architecture\n* Potential for increased communication and coordination overhead between teams\n* Potential for increased risk of errors and inconsistencies between services\n\n**Rationale:**\n\nWe believe that the benefits of a Microservices Architecture outweigh the potential drawbacks. The increased complexity and overhead will be manageable, and the benefits of improved scalability, flexibility, and maintainability will be significant.\n\n**Status:**\n\nThis decision is approved and will be implemented in the new project.\n\n**History:**\n\n* 2023-02-15: Decision made\n* 2023-02-20: Architecture and design documents updated to reflect the decision\n* 2023-03-01: Development of the microservices architecture begins\n\n**Links:**\n\n* [Link to architecture and design documents](https:\/\/example.com\/architecture-docs)\n* [Link to project roadmap](https:\/\/example.com\/project-roadmap)","GenTime":"2024-08-11 02:03:32"}
{"File Name":"reactive-interaction-gateway\/0003-for-config-prefer-prefix-over-nesting-and-don-t-hide-defaults-in-code.md","Context":"## Context\\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\\n### Issue 1: Nested keys\\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\\n```\\nApplication.fetch_env!(:rig, :required_key)\\n```\\nwill show a nice error, while\\n```\\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\\n```\\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\\n### Issue 2: Default values\\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\\n","Decision":"* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\\n","tokens":182,"id":5227,"Prediction":"## Decision\n\n**Title:** Implement a Configuration Module with Support for Nested Keys and Default Values\n\n**Context:** As discussed, there is no agreed-upon way of handling application configuration in our software, leading to issues with nested keys and default values.\n\n**Problem:** The current approach to configuration handling lacks support for nested keys, resulting in errors when accessing nested configuration values. Additionally, default values are defined in multiple places, making it difficult to maintain and inspect them.\n\n**Decision:** We will implement a dedicated Configuration module that provides support for nested keys and default values. This module will be responsible for fetching and returning configuration values, handling nested keys, and providing default values when necessary.\n\n**Rationale:** By implementing a Configuration module, we can:\n\n1. Provide a centralized location for configuration values, making it easier to maintain and update them.\n2. Support nested keys, allowing for more flexible and hierarchical configuration.\n3. Define default values in a single location, making it easier to manage and inspect them.\n4. Reduce the risk of errors and inconsistencies when accessing configuration values.\n\n**Consequences:**\n\n* We will need to refactor existing code to use the new Configuration module.\n* We will need to define default values and nested keys in the Configuration module.\n* We will need to test the Configuration module thoroughly to ensure it works correctly.\n\n**Status:** This decision is approved and will be implemented in the next release.\n\n**Implementors:** [List the names of the team members responsible for implementing this decision]\n\n**References:** [List any relevant documentation or resources that support this decision]","GenTime":"2024-08-11 02:06:12"}
{"File Name":"php-docker-template\/0004-default-php-settings-memory-limit.md","Context":"## Context\\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\\n","Decision":"Set php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\\n","tokens":92,"id":5235,"Prediction":"## Decision\n\nTo avoid potential issues with PHP fatal errors and to take advantage of the resource management capabilities of the Docker orchestrator, we will configure the PHP settings to use the `memory_limit` setting in conjunction with the `memory_limit_in_bytes` setting, and set the `memory_limit` to `-1` to allow PHP to use the available system memory without restriction.\n\n## Rationale\n\nBy setting `memory_limit` to `-1`, we allow PHP to use the available system memory without restriction, which will prevent fatal errors due to memory limits. This is particularly important in a Docker orchestrator environment, where the orchestrator is responsible for managing resources and can detect and respond to out-of-memory situations.\n\nThe `memory_limit_in_bytes` setting will ensure that the memory limit is set in bytes, which will allow us to set a specific limit if needed. However, by setting `memory_limit` to `-1`, we are effectively disabling the memory limit, allowing PHP to use the available system memory.\n\n## Consequences\n\n* PHP will no longer have a hard memory limit, which may lead to increased memory usage if not properly monitored.\n* The Docker orchestrator will be able to detect and respond to out-of-memory situations, ensuring that the application is not terminated unexpectedly.\n* This configuration may require additional monitoring and logging to ensure that the application is not consuming excessive resources.\n\n## Alternatives Considered\n\n* Setting a specific memory limit using `memory_limit_in_bytes`, which may not be sufficient in a dynamic environment like a Docker orchestrator.\n* Not setting any memory limit, which may lead to PHP consuming excessive resources and causing issues with the Docker orchestrator.\n* Using a third-party PHP extension or library to manage memory, which may add complexity and overhead to the application.","GenTime":"2024-08-11 02:08:37"}
{"File Name":"php-docker-template\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Usabilla Architecture Chapter](https:\/\/github.com\/usabilla\/architecture-chapter\/blob\/master\/docs\/adr\/0001-record-architecture-decisions.md).\\n","tokens":16,"id":5237,"Prediction":"Here is an example of an Architectural Decision Record (ADR) for the given context:\n\n## Decision\n\n**ADR-001: Technology Stack for Frontend Development**\n\n## Context\n\nAs we embark on building a modern web application, we need to decide on the technology stack for the frontend development. The chosen stack will have a significant impact on the project's maintainability, scalability, and overall performance.\n\n## Decision\n\nAfter careful consideration, we have decided to use React as the primary frontend framework for the project. This decision was made based on the following factors:\n\n* React's popularity and widespread adoption in the industry\n* Its ability to handle complex UI components and state management\n* The availability of a large and active community, ensuring easy access to resources and support\n* The compatibility with other technologies we are using in the project, such as Node.js and GraphQL\n\nWe will also use Webpack as our bundler and Babel for transpiling our code. This will allow us to write modern JavaScript code and take advantage of the latest features and syntax.\n\n## Rationale\n\nThe decision to use React was based on its ability to provide a robust and scalable solution for building complex user interfaces. Its component-based architecture and virtual DOM make it an ideal choice for handling large and dynamic UIs. Additionally, React's ecosystem is well-established, with a wide range of tools and libraries available to support development.\n\n## Consequences\n\nThe decision to use React will have the following consequences:\n\n* We will need to invest time in learning and mastering React's syntax and best practices\n* We will need to set up and configure Webpack and Babel for our project\n* We will need to consider the trade-offs between using React and other frontend frameworks, such as Angular or Vue.js\n\n## Affected Components\n\n* Frontend codebase\n* Build and deployment process\n\n## Status\n\nThis decision is considered final and will be implemented as planned.\n\nPlease let me know if you need any modifications!","GenTime":"2024-08-11 02:09:17"}
{"File Name":"digital-paper-edit-firebase\/2020-03-12-integrating-with-new-pstt-module.md","Context":"## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n","Decision":"Ashley is building a service that will have an API Gateway and an S3 bucket. Uploading to the S3 bucket will trigger the STT event. We will be uploading to a bucket, as the previous architecture, with limitations that will define how we will communicate with Newslabs' PSTT service.\\nThere will only be one shared bucket per environment for services using this service. The name of the bucket is \"newslabs-stt-media-to-transcribe\" and \"newslabs-stt-media-to-transcribe-test\".\\n### Limitations\\n- Newslabs pstt will only handle audio files (`mp3`, `mp4`, `wav`, `flac`)\\n- the client (dpe) need to ensure they upload audio not video\\n- do not use the eTag as the reference, use object key instead: `<service_name>\/<object_key>.<ext>`\\n- assume that the client is sending a unique object key\\n### Example\\n#### Upload\\nWhen uploading a file with Object Key: `280612.mp3`, the Object Key should be prepended with the service name: `dpe\/280612.mp3`\\n#### Status\\nThe endpoint for requesting the status of a transcription uses [this lambda](https:\/\/github.com\/bbc\/newslabs-stt\/tree\/master\/newslabs-stt-check-transcription), which returns the transcription status.\\nThere is an example response in the README.\\nMake a request to an API Gateway endpoint (please ask) with something like this in the request body:\\n```json\\n{\\n\"objectKey\": \"dpe\/uuid.ext\"\\n}\\n```\\n","tokens":28,"id":5243,"Prediction":"## Decision\n\n### Decision: Use a RESTful API to connect to Newslabs PSTT\n\n### Context: We want to connect to Newslabs' shared STT service (named Newslabs PSTT)\n\n### Problem Statement:\n\nWe need to establish a connection to Newslabs PSTT, a shared speech-to-text (STT) service, to enable our application to perform speech recognition tasks.\n\n### Rationale:\n\n1. **Simple Integration**: A RESTful API provides a straightforward and well-established way to integrate with the Newslabs PSTT service, allowing us to easily send and receive data.\n2. **Platform Independence**: By using a RESTful API, we can develop our application using any programming language and framework, without being tied to a specific platform or technology.\n3. **Scalability**: The Newslabs PSTT service can handle a large volume of requests, and a RESTful API allows us to scale our application independently of the service.\n4. **Flexibility**: A RESTful API provides a flexible way to interact with the service, allowing us to customize the requests and responses to meet our specific needs.\n\n### Decision Criteria:\n\n* Ease of integration\n* Platform independence\n* Scalability\n* Flexibility\n\n### Trade-Offs:\n\n* Additional complexity in handling HTTP requests and responses\n* Potential for slower performance compared to using a native library or SDK\n\n### Conclusion:\n\nWe will use a RESTful API to connect to the Newslabs PSTT service, allowing us to integrate with the service in a simple, platform-independent, and scalable manner. This decision provides the flexibility and ease of integration needed to meet our application's requirements.","GenTime":"2024-08-11 02:11:15"}
{"File Name":"digital-paper-edit-firebase\/2019-04-29-SQL-database-schema.md","Context":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","tokens":248,"id":5244,"Prediction":"Here is the Architectural Decision Record:\n\n**Decision**\n\nWe will use a schema versioning approach to manage the evolution of the SQL database schema in PostgreSQL and SQLite, utilizing a separate table to store schema versions and a mechanism to track changes between versions.\n\n**Context and Problem Statement**\n\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version. The schema should consider the project milestones and be easy to version incrementally, with three main milestones: Feature: Transcript Editing, Feature: Transcript Annotations, and Feature: Paper Editing.\n\n**Decision Drivers**\n\n* Easy to reason around: By separating the schema into different versions, we can easily understand the changes made between each milestone and manage the evolution of the schema.\n* Reusability: The schema versioning approach allows us to reuse the schema across different environments and databases, ensuring consistency and reducing the risk of errors.\n\n**Decision Consequences**\n\n* The schema will be split into different versions, each corresponding to a specific milestone.\n* A separate table will be created to store schema versions, allowing us to track changes between versions.\n* The schema versioning approach will enable easy management of schema changes and reduce the risk of errors.\n* The approach will also facilitate reusability of the schema across different environments and databases.\n\n**Rationale**\n\nBy using a schema versioning approach, we can ensure that the database schema is easy to reason around, manage, and evolve. This approach will also enable us to reuse the schema across different environments and databases, reducing the risk of errors and improving overall maintainability.","GenTime":"2024-08-11 02:11:33"}
{"File Name":"digital-paper-edit-firebase\/2019-09-28-time-estimation-for-links.md","Context":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n","Decision":"* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","tokens":49,"id":5250,"Prediction":"## Decision\n\nTo include time taken for voice overs in the estimation of the time of the rough cut, we will use a separate entity in the database to store voice over timing information and then use a calculation to estimate the total time of the rough cut.\n\n## Decision Drivers\n\n* Ease of implementation: By using a separate entity, we can easily integrate the voice over timing information into our existing workflow and avoid any complexity in the implementation.\n* Accuracy of time estimated: By having a separate entity for voice over timing information, we can ensure that the time taken for voice overs is accurately calculated and included in the estimation of the total time of the rough cut.\n* Extensibility: This approach allows us to easily add more timing information in the future, such as time taken for music or sound effects, without affecting the existing implementation.\n\n## Decision Outcome\n\nWe will create a new entity called \"VoiceOverTiming\" that will store the start and end times of each voice over. We will then use a calculation to estimate the total time of the rough cut by summing up the duration of all the voice overs.\n\n## Rationale\n\nThis decision allows us to accurately estimate the time taken for voice overs and include it in the estimation of the total time of the rough cut. It also provides a flexible solution that can be easily extended to include more timing information in the future.\n\n## Pros and Cons\n\nPros:\n\n* Accurate estimation of time taken for voice overs\n* Easy to implement and extend\n* Provides a clear and separate entity for voice over timing information\n\nCons:\n\n* Requires additional database entity and calculation\n* May require additional data validation and error handling\n\n## Future Considerations\n\n* We will need to consider how to handle cases where voice overs are not included in the rough cut.\n* We will need to consider how to handle cases where the voice over timing information is incomplete or inaccurate.","GenTime":"2024-08-11 02:12:50"}
{"File Name":"remultiform\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":5255,"Prediction":"## Decision\n\n**Use Prettier as the standard code formatter across all projects**\n\n## Context\n\nWe want to ensure we're all using one code style, that is familiar across projects. Prettier is an opinionated code formatter with support for most, if not all, of the languages in the JavaScript ecosystem. As of writing, it is used by over 1 million repositories on GitHub, and has become a standard.\n\n## Problem\n\nWithout a standardized code formatting tool, our codebases may become inconsistent, making it harder to read and maintain. This can lead to errors, bugs, and increased development time.\n\n## Decision Criteria\n\n* Consistency: Prettier provides a consistent code formatting style across all projects, making it easier to read and maintain code.\n* Wide language support: Prettier supports most languages in the JavaScript ecosystem, ensuring it can be used across all our projects.\n* Industry standard: With over 1 million repositories on GitHub using Prettier, it has become a de facto standard, making it easier for new team members to adapt.\n* Ease of use: Prettier is easy to integrate into our development workflow, with plugins available for popular IDEs and code editors.\n\n## Consequences\n\n* All projects will use Prettier as the standard code formatter.\n* Codebases will become more consistent and easier to read and maintain.\n* New team members will be able to adapt to our code style more quickly.\n* Integration with popular IDEs and code editors will make it easy to use Prettier in our development workflow.\n\n## Rationale\n\nWe choose Prettier as the standard code formatter because it provides a consistent code formatting style, supports a wide range of languages, and has become an industry standard. This decision will ensure our codebases are consistent, easy to read, and maintain, and will make it easier for new team members to adapt to our code style.","GenTime":"2024-08-11 02:14:27"}
{"File Name":"remultiform\/0008-support-indexeddb-as-a-data-store.md","Context":"## Context\\nWe need to persist the user data to the backend. Rather than building our\\nfrontend to be tightly coupled to an API, we want a layer that can sit between\\nthe application and the API.\\nWe know that we will need to support offline use of these forms. We also want\\nusers to be able to recover unsubmitted forms if they haven't gone online\\nbetween sessions.\\nWe also know that some forms will require the upload of images.\\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\\ndatabase that persists data across sessions. It supports storing blob data (such\\nas images), and has decent storage limits.\\n","Decision":"We will support IndexedDB as a data store layer, sitting between the application\\nand the API.\\n","tokens":144,"id":5257,"Prediction":"## Decision\n\nWe will use IndexedDB as the local storage solution to persist user data and form submissions, and as a cache for API responses. This decision is based on the following considerations:\n\n* We want to decouple the frontend from the API, and IndexedDB provides a layer of abstraction between the two.\n* Offline support is a requirement, and IndexedDB allows us to store data locally and sync it with the API when the user comes back online.\n* The need to recover unsubmitted forms is also a requirement, and IndexedDB's ability to persist data across sessions makes it a good fit.\n* Some forms will require the upload of images, and IndexedDB supports storing blob data, making it a suitable solution for this use case.\n\n## Rationale\n\nWe considered other options such as LocalStorage, but it has limitations such as not supporting blob data and having lower storage limits. We also considered using a third-party library or service, but IndexedDB is a well-supported and widely-used technology that provides the features we need.\n\n## Consequences\n\n* We will need to implement logic to sync data between IndexedDB and the API when the user comes back online.\n* We will need to implement logic to recover unsubmitted forms from IndexedDB when the user restarts their browser or device.\n* We will need to ensure that our implementation of IndexedDB is robust and handles edge cases such as database corruption or overflow.\n\nOverall, using IndexedDB as our local storage solution will provide a robust and scalable way to persist user data and form submissions, and will meet our requirements for offline support and image upload.","GenTime":"2024-08-11 02:15:12"}
{"File Name":"remultiform\/0003-use-rollup-to-build-distributables.md","Context":"## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n","Decision":"We will build distributables using Rollup.js.\\n","tokens":62,"id":5259,"Prediction":"## Decision\n\nTo ensure compatibility with both TypeScript and plain JavaScript (both commonJS and module) applications, we will use Rollup as our JavaScript bundler. Rollup's simplicity and support for TypeScript make it an ideal choice for our library's distribution.\n\n## Context\n\nAs mentioned by the user, we want to be able to distribute our library to be ingested by both TypeScript and plain JavaScript (both commonJS and module) applications. Rollup is a popular JavaScript bundler that supports TypeScript and has a simple configuration, making it a suitable choice for our library's distribution.\n\n## Decision Rationale\n\nWe chose Rollup over other bundlers because of its:\n\n1. **TypeScript support**: Rollup has built-in support for TypeScript, which will allow us to easily generate compatible code for both TypeScript and plain JavaScript applications.\n2. **Simple configuration**: Rollup's configuration is straightforward and easy to understand, making it less prone to errors and easier to maintain.\n3. **Wide compatibility**: Rollup supports both commonJS and module formats, ensuring that our library can be used by a wide range of applications.\n4. **Popular and well-maintained**: Rollup is a widely used and well-maintained library, which means it will likely continue to receive updates and support for a long time.\n\n## Consequences\n\nBy using Rollup as our JavaScript bundler, we can:\n\n1. Ensure compatibility with both TypeScript and plain JavaScript applications.\n2. Simplify our build process and reduce the risk of errors.\n3. Provide a consistent and reliable distribution of our library.\n\n## Status\n\nThis decision is approved and will be implemented in our library's build process.","GenTime":"2024-08-11 02:15:52"}
