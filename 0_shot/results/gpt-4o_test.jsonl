{"File Name":"cdh-adrs\/0002-cdh-etl-logic-boundaries.md","Context":"## Context\\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\\nrequirements during onboarding and processing of feed files, the following issues where considered:\\n- Will such logic require specialized implementation across feeds?\\n- Are such logic generic and require one time implementation or will require continous change\/update?\\n- What are the benefits of moving such logic into ETL instead of CDH.\\n\n## Decision\n","Decision":"The most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n","tokens":92,"id":8,"Prediction":"## Decision\nAfter considering the context and the associated issues, the decision is to move feed-specific logic to ETL services rather than within the CDH (Customer Data Hub). This decision is grounded in the following reasoning:\n\n1. **Specialized Implementation Across Feeds**:\n   - We recognize that feed-specific requirements often necessitate specialized logic tailored to individual feeds. Implementing these within the ETL process allows for feed-specific customizations without impacting the core functionalities of the CDH.\n\n2. **Requirement for Continuous Change\/Update**:\n   - Feed-specific logic is likely to require continuous updates and changes as new feeds are onboarded or existing feeds are modified. By handling these within the ETL layer, we can respond more flexibly and quickly to changes without needing to alter the core CDH logic.\n\n3. **Separation of Concerns**:\n   - By moving feed-specific logic to ETL, we achieve a clear separation of concerns. The CDH can focus on its primary role of managing and maintaining core customer data, thereby enhancing its stability and performance.\n\n4. **Benefits of ETL Customization**:\n   - ETL services are typically designed to handle data transformation and ingestion, making them well-suited for implementing the specific requirements of feed processing. This not only improves maintainability but also leverages the inherent strengths of ETL tools in managing complex data transformations.\n\n5. **Flexibility and Scalability**:\n   - ETL systems offer greater flexibility in handling diverse data processing needs which can scale easily with the addition of new data sources. This ensures that our solution is future-proof and can accommodate growing and changing requirements efficiently.\n\nIn summary, feed-specific logic will be managed within ETL services to facilitate specialized implementations and frequent updates, while maintaining a clear separation of concerns and leveraging the strengths of ETL tools for data transformation.","GenTime":"2024-07-28 21:18:40"}
{"File Name":"gatemint-sdk\/adr-006-secret-store-replacement.md","Context":"## Context\\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n\n## Decision\n","Decision":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.\\n","tokens":214,"id":13,"Prediction":"## Decision\n\nWe have decided to implement a secure, cross-platform secret storage solution by leveraging the built-in secret stores provided by modern desktop operating systems. This solution will include the following components:\n\n1. **Abstraction Layer**: Develop a common abstraction layer that interfaces with the native secret stores of various OS (e.g., Keychain on MacOS, Credential Manager on Windows, and GNOME Keyring or KDE Wallet on Linux). This will ensure that our application can store and retrieve key material and metadata securely, irrespective of the underlying operating system.\n\n2. **Integration with Native Secret Stores**: Utilize existing APIs and libraries (such as libsecret for Linux, Security framework for MacOS, and Credential Management API for Windows) to interact with the respective secret stores. This ensures that key material is encrypted and access-controlled by the OS.\n\n3. **Fallback for Unsupported Platforms**: Implement a secure fallback mechanism for minimal platforms that do not provide a native secret store. This fallback will use strong encryption algorithms (e.g., AES-256) and store the encrypted data in a secure, non-plaintext format within the user's home directory. \n\n4. **Data Migration**: Create a migration process to transfer existing key material and metadata from the plain text database to the new secure storage solution. The migration should be transparent to the user and ensure that no plain text key material remains in the old database.\n\n5. **Security Best Practices**: Adopt security best practices, such as minimizing the exposure of sensitive data in memory, using secure random number generators for encryption keys, and implementing robust access controls.\n\n6. **Testing and Auditing**: Conduct thorough testing and security audits to validate the implementation. This will help identify potential vulnerabilities and ensure the integrity and confidentiality of the stored data.\n\nBy adopting this strategy, we aim to significantly enhance the security of our SDK application, protecting key material and metadata from unauthorized access and potential attacks.","GenTime":"2024-07-28 21:19:01"}
{"File Name":"gatemint-sdk\/adr-020-protobuf-transaction-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n\n## Decision\n","Decision":"### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","tokens":234,"id":21,"Prediction":"## Decision\n\nThe decision is to adopt a new approach for the client-side Protocol Buffer migration path focusing on transactions. This approach outlines the following key elements:\n\n1. **Protobuf Encoding for Transactions**: Transactions will be encoded using Protocol Buffers (Protobuf) to standardize and streamline the serialization process. This ensures compatibility and centralizes the transaction structure across different client interfaces.\n\n2. **Modular Transaction Components**: Transactions will be composed of modular components to foster maintainability and flexibility. This includes separating transaction construction, signing, and routing logic. Each component will have a clear and distinct role, facilitating easier updates and improvements.\n\n3. **Enhanced Transaction Generation & Signing**: The approach will involve a detailed restructuring of transaction generation and signing mechanisms to fully leverage Protobuf capabilities. This ensures that transactions are generated and signed in a consistent manner across different clients.\n\n4. **CLI & REST Handler Updates**: Command Line Interface (CLI) and Representational State Transfer (REST) handlers will be updated to accommodate the new transaction structure. This includes modifying existing handlers to parse and handle Protobuf-encoded transactions effectively.\n\n5. **Backward Compatibility Considerations**: While the primary focus is on the new Protobuf-based design, backward compatibility measures will be put in place. This includes providing transitional support and clear guidelines to help existing clients migrate smoothly to the new system.\n\n6. **Testing & Validation**: Comprehensive testing and validation frameworks will be established to ensure the robustness of the new transaction system. This includes unit tests, integration tests, and end-to-end testing covering both new and backward-compatible pathways.\n\nThis approach aims to create a more efficient and standardized method for handling transactions within the Cosmos SDK, paving the way for future enhancements in querying mechanisms and broader client-side interactions.","GenTime":"2024-07-28 21:19:41"}
{"File Name":"gatemint-sdk\/adr-014-proportional-slashing.md","Context":"## Context\\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\\n\n## Decision\n","Decision":"### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\\n```\\nslash_amount = k * ((power_1)^(1\/r) + (power_2)^(1\/r) + ... + (power_n)^(1\/r))^r \/\/ where k and r are both on-chain constants\\n```\\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Parameterization\\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\\n#### Griefing\\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occured withing the last `jail period`.\\n```\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nSqrtValidatorVotingPercent  sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","tokens":104,"id":28,"Prediction":"## Decision\n\nImplement a progressive penalty mechanism for large validators to disincentivize excessive centralization and distribute rewards more equitably among smaller validators. This mechanism will include:\n\n1. **Penalty Tier System**: Establish a tiered system where the penalty rate increases progressively as the stake held by a validator surpasses certain thresholds. For instance:\n   - Validators controlling more than 5% of the total network stake incur a 2% penalty on rewards.\n   - Validators controlling more than 10% incur a 5% penalty.\n   - Validators controlling more than 20% incur a 10% penalty.\n\n2. **Proportional Reward Redistribution**: Penalties collected from large validators will be redistributed among smaller validators to incentivize users to delegate to these smaller validators. This helps increase network decentralization and ensures that even smaller validators can participate effectively.\n\n3. **Dynamic Stake Reallocation**: Automatically reallocate a portion of the stake from large validators to smaller ones periodically. This keeps the delegation landscape fluid and mitigates the risk of any single entity gaining too much control over the network.\n\n4. **Enhanced Transparency and Delegator Awareness**: Provide comprehensive analytics and education tools for delegators, highlighting the risks of over-centralization and encouraging them to delegate to smaller, under-represented validators. \n\n5. **Stake Cap Limitations**: Enforce upper limits on the stake that any single validator can accept at a given time. This will force delegators to distribute their stakes more evenly across the network.\n\n6. **Regular Evaluations and Adjustments**: Regularly assess the effectiveness of the implemented measures and adjust penalty rates and redistribution mechanisms to adapt to evolving network conditions.\n\nThese measures combined aim to mitigate the centralization risk by making it less economically attractive for delegators to contribute to already large validators, thus promoting a healthier, more decentralized consensus mechanism.","GenTime":"2024-07-28 21:20:13"}
{"File Name":"gatemint-sdk\/adr-004-split-denomination-keys.md","Context":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n\n## Decision\n","Decision":"Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```golang\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","tokens":175,"id":33,"Prediction":"## Decision\n\nTo address the potential denial-of-service (DoS) threat caused by the excessive storage and computational costs associated with numerous arbitrary denominations, we will implement a multi-layered solution:\n\n1. **Deferred Account Balance Storage**:\n   - Introduce a separate data structure dedicated to storing the balances of denominations for each account, external to the current `sdk.Coins` struct. This data structure will optimize the retrieval and storage processes, easing the operations required to handle high denomination accounts.\n\n2. **Denomination Count Limit with Repackaging Mechanism**:\n   - Set a configurable maximum denomination limit per account to prevent excessive denomination bloat directly within storage. To counter griefing vectors, transactions above this limit will trigger a repackaging mechanism. \n   - The repackaging mechanism will combine small, nonsensical denominations into meta-tokens, effectively reducing the number of unique denomination entries while preserving value balance integrity. \n\n3. **Garbage Collection\/Pruning Policies**:\n   - Implement garbage collection or pruning policies to clean up zero-balance denominations periodically. This ensures that the storage footprint is minimized by automatically removing unused denominations after certain operations (e.g., upon balance modification or at regular intervals).\n\n4. **Rate Limiting and Fees**:\n   - Introduce rate-limiting on incoming transactions with arbitrary denominations and charge a fee for pushing new denominations into an account to disincentivize abuse.\n   - These fees can be dynamically calibrated to reflect the storage and computational burden induced by the new denominations.\n\n5. **Fractional Denomination Aggregation**:\n   - Allow users to manually aggregate their fractional denominations into larger, standardized units or more common denominations. This user-driven mechanism supports proactive management of account balances by end users.\n\nBy applying this multi-layered approach, we mitigate the high storage and computational costs of excessive denominations and bolster the system\u2019s resilience against DoS attacks without constricting legitimate usage and functionalities.","GenTime":"2024-07-28 21:20:32"}
{"File Name":"digital-paper-edit-storybook\/adr-28-08.md","Context":"## Context and Problem Statement\\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\\n- From which repo would components be published?\\n- Which repos would consume components from NPM?\\n- Should the Storybook live inside the Client repo?\\n## Decision Drivers\\nN\/A\\n\n## Decision\n","Decision":"N\/A\\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\\n1. Duplicate component code to Storybook repo\\n2. Publish completed components to NPM\\n3. Remove the original component code from the Client and import via NPM\\n### Positive Consequences\\n### Negative consequences\\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\\nIf possible, also avoid having people working simultaneously on a component that consumes \/ is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\\n","tokens":83,"id":36,"Prediction":"## Decision\nAfter thorough review and consideration, we have decided on the following:\n\n1. **DPE Client Repository**:\n   - The components will be maintained and published from the DPE Client repository. This ensures that all components are authored, updated, and version-controlled in a single, centralized location.\n   \n2. **NPM Publishing**:\n   - Components will be published to NPM from the DPE Client repository. This allows other projects and repositories to easily consume these components by adding them as dependencies from NPM, promoting reuse and consistency across different products.\n   \n3. **Storybook Repository**:\n   - The Storybook repository will remain separate from the DPE Client repository. However, it will consume the components from NPM. This structure allows the Storybook environment to serve as an isolated environment for developing, testing, and showcasing the components without being tightly coupled to any single codebase.\n   - Keeping Storybook in its own repository also facilitates independent versioning and easier collaboration, as changes to the UI components can be independently reviewed and tested within the Storybook environment.\n\nBy adopting this structure, we establish a clear and efficient workflow for both development and consumption of UI components, fostering better collaboration and maintainability.","GenTime":"2024-07-28 21:20:43"}
{"File Name":"copilot\/0003-use-mcp-api-for-pilot-config.md","Context":"## Context\\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\\n\n## Decision\n","Decision":"Copilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","tokens":106,"id":49,"Prediction":"## Decision\n\nAdopt the Mesh Configuration Protocol (MCP) for Configuration Management\n\n1. **Shift Complexity to Server**: We will implement MCP to shift the logic and complexities from the client (Pilot) to the server (Copilot). This will simplify the client implementations and allow for easier maintenance and updates.\n\n2. **Unified Contract**:\n   - We will utilize MCP to establish a unified contract for all Custom Resource Definitions (CRDs) and Service Discovery components. \n   - This contract will standardize the communication methods and protocols between different components, enhancing consistency and reliability in our system.\n\n3. **Scalability**:\n   - By centralizing complexities in the server, we anticipate improvements in the scalability of our system as the server can better manage and distribute computational load.\n   - This will also simplify scaling the client components as they will have reduced complexity.\n\n4. **Future Proofing**:\n   - Adopting MCP aligns with a forward-looking architecture, helping us to more easily integrate future updates and improvements.\n   - This decision positions us to better adopt evolving Istio components and ecosystem changes.\n\n5. **Implementation Plan**:\n   - Begin with a detailed review and understanding of MCP as per the [provided documentation](https:\/\/github.com\/istio\/api\/tree\/master\/mcp).\n   - Design an implementation framework that migrates current Client logic to the Server, ensuring minimal downtime and disruption.\n   - Develop migrations for CRDs and Service Discovery to conform to the new unified contract.\n   - Conduct thorough testing to ensure the robustness and reliability of the new configuration management approach.\n   - Rollout the MCP implementation in phases to monitor and resolve issues with minimal impact.\n\n6. **Monitoring and Feedback**:\n   - Implement monitoring solutions to track performance changes post-MCP adoption.\n   - Collect feedback from stakeholders and end-users to ensure that the new approach meets operational expectations and requirements.\n\nBy adopting MCP, we aim to create a more maintainable, scalable, and future-proof configuration management system within Istio.","GenTime":"2024-07-28 21:21:26"}
{"File Name":"copilot\/0006-do-something-for-one-way-tls.md","Context":"## Context\\nEnable one way TLS between front-end and envoy per host+domain that is\\nspecified via gateway config.\\n\n## Decision\n","Decision":"#### Gateway Configuration\\nThis is achieved by sending the following config from copilot.\\n```\\napiVersion: networking.istio.io\/v1alpha3\\nkind: Gateway\\nmetadata:\\nname: mygateway\\nspec:\\nselector:\\nistio: ingressgateway # use istio default ingress gateway\\nservers:\\n- port:\\nnumber: 443\\nname: https-httpbin\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\\nhosts:\\n- \"httpbin.example.com\"\\n- port:\\nnumber: 443\\nname: https-bookinfo\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\\nhosts:\\n- \"bookinfo.com\"\\n```\\nIn the config above each cert and key in the array of servers represent a\\nhost+domain and the path to each cert and the key is arbitrarily chosen.\\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\\n```\\nfrontend_tls_keypairs:\\nexample:\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n```\\n#### Cert Storage\\nThe placement of the certs and keys on the envoy VM is done using a separate\\nprocess specific to this purpose. This process will be in charge of knowing\\nwhere the certs are located and placing the certs on the correct paths. It is\\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\\nare stored, and having a specific process to manage this will reduce duplication\\nand mitigate skew.\\n","tokens":25,"id":50,"Prediction":"## Decision\n\nWe will implement one-way TLS between the front-end client and the Envoy proxy for each specific host and domain combination as indicated in the gateway configuration. This will involve the following steps:\n\n1. **Gateway Configuration Update**: Modify the gateway configuration to include a list of host and domain combinations for which one-way TLS should be enabled.\n2. **TLS Certificate Management**: Obtain and install TLS certificates for the front-end client and Envoy proxy. The certificates can be generated using a trusted Certificate Authority (CA).\n3. **Envoy Proxy Configuration**: Configure Envoy to accept connections over TLS for the specified host and domain combinations using the configured certificates.\n4. **Front-End Client Configuration**: Update the front-end client's configuration to initiate TLS connections when communicating with the Envoy proxy.\n5. **Security Measures**: Implement necessary security measures, such as ensuring that the private keys are securely stored and implementing regular certificate rotation.\n6. **Testing**: Conduct thorough testing to ensure that TLS connections are correctly established and data is securely transmitted.\n\nThis approach enhances the security of data transmitted between the front-end and Envoy, ensuring that sensitive information is encrypted during transit. It also aligns with best practices for secure communications within a microservices architecture.","GenTime":"2024-07-28 21:21:29"}
{"File Name":"ELEN4010\/ADR Sprint Planning and timeline.md","Context":"## Context\\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\\n\n## Decision\n","Decision":"4 Sprints planned, consecutively. Will only start on 17th April, but have a \"Sprint 0\" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\\n* Release will be every Tuesday, by 8pm\\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\\n* Friday coding sessions together from 12pm - 4pm\\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\\nRebecca: Product Manager\\nTyson: SCRUM Master\\nTheese roles will be alternated throughout the project each week.\\n","tokens":40,"id":57,"Prediction":"## Decision\nGiven the upcoming period where Thabang will be working remotely from 16-25 April and the anticipated dip in productivity between 29 April - 10 May due to major submission deadlines, the team should take the following steps to manage the project timeline effectively:\n\n1. **Revised Task Allocation**: Reorganize project tasks to leverage Thabang's remote working period efficiently. Allocate tasks that can be completed independently to Thabang during this time.\n   \n2. **Daily Stand-ups**: Introduce daily check-ins or stand-ups during the remote working period to ensure Thabang is in sync with the team and any blockages can be addressed promptly.\n\n3. **Buffer Time**: Incorporate buffer periods into the project plan to account for the productivity dip over the submission period. Ensuring there is slack built into the schedule can help to mitigate the impact of decreased productivity.\n\n4. **Collaboration Tools**: Ensure all team members are proficient in using collaboration tools (e.g., Slack, Trello, GitHub) to maintain productivity and communication during remote working and high-intensity submission periods.\n\n5. **Review & Adjustment**: Conduct a review after 25 April to assess the remote working impact and make necessary adjustments to the project plan before entering the submission period.\n\n6. **External Support**: Identify opportunities for external support or temporary resources if critical tasks risk delay due to the anticipated productivity dip.\n\nBy implementing these decisions, the team can manage the impact of remote working and the submission period on the project's progress effectively.","GenTime":"2024-07-28 21:22:04"}
{"File Name":"ELEN4010\/ADR Login API Choice.md","Context":"## Context\\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\\n\n## Decision\n","Decision":"Using a well known and widely known\/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login\/registration page through the Google Login API\\n","tokens":58,"id":61,"Prediction":"## Decision\nWe have decided to integrate the Google Login API as an alternative login method for our users. This decision is driven by the need to swiftly implement a reliable and widely-used authentication mechanism to fulfill the Sprint 1 User story requirements related to user login. \n\nBy leveraging the Google Login API, we will also have a well-defined template and reference for implementing our own login mechanisms, thereby reducing the development time and potential errors associated with building a login system from scratch. Additionally, this integration will enhance the user experience by providing a seamless and secure login option that many users are already familiar with.\n\nTo move forward with this decision:\n1. We will integrate the Google Login API into our login module.\n2. We will ensure that user information obtained from Google Login is securely managed and stored in our database.\n3. We will implement fallback mechanisms to ensure users have alternative methods to log in if they choose not to use Google Login.\n4. We will adhere to Google's API usage guidelines and ensure compliance with relevant data protection regulations.\n\nThis integration is expected to be completed by the end of Sprint 1, aligning with our project goals and timelines.","GenTime":"2024-07-28 21:22:23"}
{"File Name":"infection\/0003-PHPUnit-this-over-self.md","Context":"### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n\n## Decision\n","Decision":"Since there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":221,"id":66,"Prediction":"### Decision\nAfter reviewing the context and considering the following points:\n\n1. The PHPUnit documentation itself defaults to using `$this` for assertions.\n2. `$this` is more widely used in this context within the community.\n3. The existing codebase extensively uses `$this` for assertions.\n4. There are no significant shortcomings or functional issues arising from using `$this` instead of `self`.\n\nThe decision is to continue using `$this` for PHPUnit assertions in the codebase. This approach maintains consistency with both the documentation and the broader community practices, ensuring readability and reducing potential confusion for developers familiar with PHPUnit conventions.\n\nThis decision will keep the codebase aligned with existing patterns and avoid unnecessary refactoring that doesn't yield significant benefits. Consequently, the use of `$this` for calling static assertion methods will be considered the standard practice within this project.","GenTime":"2024-07-28 21:22:45"}
{"File Name":"infection\/0004-PHPUnit-expect-exception-over-try-catch.md","Context":"### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\ntry {\\n\/\/ the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n\/\/ ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\n$this->expectException($exception)\\n\/\/ the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n\n## Decision\n","Decision":"As recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":285,"id":68,"Prediction":"### Decision\nThe decision is to adopt the `expectException*()` API for handling expected exceptions in test cases. This approach is in line with the best practices recommended by industry experts and allows for clearer and more maintainable test code.\n\nPull requests aimed at rectifying the entire codebase to follow this practice are encouraged but not mandatory. However, all new pull requests should adhere to this method for consistency and future maintenance ease.","GenTime":"2024-07-28 21:22:48"}
{"File Name":"dapr\/ENG-003-test-infrastructure.md","Context":"## Context\\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before \/ after PR is merged or by a scheduler.\\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\\n\n## Decision\n","Decision":"### Test environments\\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\\n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.\\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\\n### Bring up test cluster\\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\\n### CI\/CD and test result report for tests\\nMany Kubernetes-related projects use [Prow](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/prow), and [Testgrid](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\\nInstead, Dapr will use [Azure Pipeline](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/pipelines\/) to run e2e tests and its [test report feature](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/test\/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\\n","tokens":102,"id":70,"Prediction":"## Decision\n\nWe will implement a comprehensive End-to-End (E2E) testing framework specifically designed for testing Dapr functionalities within user code deployments. This framework will address three key components: provisioning the test infrastructure, executing the tests, and reporting the results.\n\n1. **Test Infrastructure Provisioning:**\n    - Utilize Kubernetes (K8s) as the test cluster environment due to its scalability and wide adoption in deploying containerized applications.\n    - Implement Infrastructure as Code (IaC) tools, such as Terraform or Helm, to manage cluster setup and ensure consistency across test environments.\n    - Incorporate CI\/CD integration to provision and tear down the test cluster dynamically before and after each test run.\n\n2. **Test Execution:**\n    - Design a suite of E2E test cases that cover critical Dapr functionalities and user scenarios.\n    - Use popular E2E testing frameworks such as Cypress or TestCafe for writing and managing tests.\n    - Integrate the test suite with continuous integration pipelines, ensuring that tests are automatically executed upon pull request submissions, post-merge events, and on a scheduled basis (e.g., nightly builds).\n\n3. **Test Reporting:**\n    - Implement a reporting mechanism that captures and displays test results in a consistent and accessible manner.\n    - Utilize tools like Allure or ReportPortal for detailed test reporting, including logs, screenshots, and metrics for each test run.\n    - Ensure that test reports are accessible via the CI\/CD dashboard and can be integrated with communication tools such as Slack or email for immediate notification of test results.\n\nThis decision ensures that the E2E testing framework is robust, scalable, and provides clear visibility into test outcomes, ultimately ensuring the functional correctness of Dapr within user code deployments.","GenTime":"2024-07-28 21:22:58"}
{"File Name":"dapr\/ENG-004-signing.md","Context":"## Context\\nAuthenticode signing of binaries.\\n\n## Decision\n","Decision":"* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\\n","tokens":11,"id":72,"Prediction":"## Decision\n\nThe decision is to implement Authenticode signing for all binaries produced by the development pipeline.\n\n### Rationale:\n\n1. **Security**: Authenticode signing will ensure that binaries are verified for their integrity and origin, providing an additional layer of security against tampering and malware.\n2. **Compliance**: Many regulatory frameworks and policies require software to be signed, and implementing Authenticode signing will help us stay compliant with these standards.\n3. **Trust**: Users and clients are more likely to trust and adopt our software if the binaries are signed, knowing they come from a verified and authentic source.\n4. **Reputation**: Authenticode signing helps protect the organization\u2019s reputation by preventing the spread of modified, potentially harmful versions of our software.\n\n### Implementation Steps:\n\n1. **Set up a Code Signing Certificate:**\n   - Obtain a Code Signing Certificate from a trusted Certificate Authority (CA).\n   - Ensure the certificate is stored and managed securely.\n\n2. **Integrate Signing in Build Pipeline:**\n   - Modify the build pipeline scripts (e.g., in CI\/CD tools) to include the signing step.\n   - Ensure all builds automatically sign the binaries before publishing.\n\n3. **Verification:**\n   - Implement steps to verify the signed binaries\u2019 integrity and authenticity during the deployment process.\n\n4. **Documentation and Training:**\n   - Document the Authenticode signing process for developers.\n   - Train relevant team members on how to manage and maintain the signing certificates.\n\n5. **Monitoring and Auditing:**\n   - Regularly audit the signed binaries to ensure compliance.\n   - Monitor for any issues related to the signing process and address them promptly.","GenTime":"2024-07-28 21:23:06"}
{"File Name":"dapr\/SDK-002-java-jdk-versions.md","Context":"## Context\\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\\nSee https:\/\/github.com\/dapr\/java-sdk\/issues\/17\\n\n## Decision\n","Decision":"* Java 8 should be the minimum version supported for Dapr's Java SDK.\\n* Java 11 should be used in samples and user documentation to encourage adoption.\\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\\n","tokens":71,"id":73,"Prediction":"## Decision\nWe have decided that the minimum Java version supported by Dapr's Java SDK will be Java 8. This decision is based on the fact that Java 8, despite being the previous LTS version, remains the most widely used version within the Java community as of 2019. Supporting Java 8 will ensure that a larger number of users can adopt and integrate the Dapr Java SDK into their existing systems without requiring an upgrade to Java 11, thereby maximizing the SDK's accessibility and usability.","GenTime":"2024-07-28 21:23:07"}
{"File Name":"dapr\/ENG-001-tagging.md","Context":"## Context\\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\\nWe needed a way to construct an accepted and constant way of naming our Docker images.\\n\n## Decision\n","Decision":"* An image will conform to the following format: \\<namespace>\/\\<repository>:\\<tag>\\n* A valid tag conforms to the following format: \\<version>-\\<architecture>, or just \\<version>, then arch is assumed Linux\\n","tokens":51,"id":74,"Prediction":"## Decision\n\nWe will adopt a standardized naming convention for our Docker images to ensure consistency, clarity, and ease of management across multiple repositories, versions, and architectures. The naming convention will follow the pattern:\n\n```\n<repository>\/<project>-<component>:<version>-<architecture>\n```\n\nWhere:\n- `<repository>` is the repository name where the image is stored.\n- `<project>` is the name of the project.\n- `<component>` is the specific component or service the image pertains to.\n- `<version>` is the version number of the image, following semantic versioning (e.g., v1.0.0).\n- `<architecture>` specifies the target architecture (e.g., amd64, arm64).\n\n### Example\nFor a backend service of a project named `myapp`, stored in the `myrepo` repository, version 1.2.3, and targeting the `amd64` architecture, the image name would be:\n\n```\nmyrepo\/myapp-backend:v1.2.3-amd64\n```\n\n### Rationale\n\n1. **Consistency**: Establishing a uniform naming pattern provides clarity and minimizes the risk of errors.\n2. **Discoverability**: Clear naming helps in quickly identifying the purpose and specifics of an image, facilitating easier collaboration and asset management.\n3. **Version Control**: Semantic versioning within the tag helps in understanding the release and maturity of the image.\n4. **Architecture Specification**: Including the target architecture in the name allows for straightforward distinction between builds meant for different hardware environments.\n\n### Implementation\n\n1. **Guidelines Documentation**: Document the naming convention and distribute it among the development and operations teams.\n2. **CI\/CD Pipeline Updates**: Update CI\/CD pipelines to automatically tag images following the new convention.\n3. **Repository Organization**: Use the naming pattern to reorganize existing images in the repositories, where feasible, and ensure all new images adhere to the standard\n4. **Verification Process**: Implement a check in the CI\/CD pipeline to verify that image names conform to the naming convention before pushing them to the repository.\n\nBy adhering to this naming convention, we ensure that our Docker images are managed systematically, supporting scalability, and maintainability within our multi-repository, multi-architecture environment.","GenTime":"2024-07-28 21:23:13"}
{"File Name":"dapr\/API-009-bidirectional-bindings.md","Context":"## Context\\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\\nAn API change is needed to account for the requested type of operation.\\n\n## Decision\n","Decision":"### Naming\\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\\n### Types\\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\\nDid not provide meaningful value.\\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\\n### API structure\\nIt was decided that the API url will be kept as: `http:\/\/localhost:<port>\/v1.0\/bindings\/<name>`.\\nThe verb for the HTTP API will remain POST\/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\\nThis is not a breaking change.\\n### Schema and versioning\\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\\n","tokens":44,"id":75,"Prediction":"## Decision\nWe will introduce an enum called `OperationType` with values such as `Read`, `Write`, and `Delete` to specify the type of operation explicitly in our API. This enum will be used to modify the existing API calls for storage bindings, ensuring that they can clearly convey the expected action (e.g., retrieving a blob, writing a blob, or deleting a blob).\n\nAdditionally, we will:\n1. Update the API documentation to reflect these changes, providing clear examples of how to use the new `OperationType` parameter.\n2. Implement backward compatibility to allow existing code to function without modification by defaulting to the previous standard operation (typically `Write`).\n3. Include thorough testing to ensure both existing functionalities and new functionalities are stable and perform as expected.\n\nThis change ensures that our API remains clear and extendable, supporting both current and future bi-directional functionality requirements.","GenTime":"2024-07-28 21:23:16"}
{"File Name":"dapr\/CLI-001-cli-and-runtime-versioning.md","Context":"## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n\n## Decision\n","Decision":"* We'll keep CLI versioning and runtime versioning separate.\\n* CLI will pull down latest runtime binary during the *init()* command.\\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\\n","tokens":28,"id":78,"Prediction":"## Decision\nWe have decided to decouple the versions of the Dapr CLI from the Dapr runtime versions. This choice is based on the following considerations:\n\n1. **Flexibility in Development and Release Cycles**: Given that CLI and runtime components may evolve at different rates, decoupling will enable independent release cycles. This allows us to address bugs, introduce features, and push updates to either component without waiting for the other.\n\n2. **Backward and Forward Compatibility**: By decoupling, we will aim to maintain backward and forward compatibility between CLI and runtime versions. This ensures that users can use different versions of the CLI with various versions of the runtime, enhancing usability and reducing potential friction during upgrades.\n\n3. **Simplified Version Management**: Users won\u2019t need to worry about synchronizing CLI and runtime versions, leading to a more straightforward versioning scheme and easing the overall management of Dapr components.\n\n4. **Improved User Experience**: Decoupling will allow for more responsive user support and quicker delivery of new features or fixes for either component as needed, rather than being delayed to fit a joint release cycle.\n\n5. **Encouraging Contributions**: Open source contributors can more easily contribute to either the CLI or the runtime without having to conform to synchronized version releases, fostering an environment of collaborative development.\n\nIn summary, decoupling CLI and runtime versions in the Dapr ecosystem provides a more flexible, user-friendly, and manageable approach to software development and deployment.","GenTime":"2024-07-28 21:23:28"}
{"File Name":"dapr\/API-004-binding-manifests.md","Context":"## Context\\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\\n\n## Decision\n","Decision":"### Dapr\\n* All components use the same **Component** manifests, identified by a component **type**.\\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\\n","tokens":40,"id":84,"Prediction":"### Decision\nWe have decided to introduce different manifest types for Event Sources (now Bindings), State Stores, and Message Buses. This separation will ensure clear delineation and management of these components, facilitating better readability, maintainability, and expansion of the system.\n\n**Rationale:**\n1. **Clarity:** By having distinct manifest types, we can more clearly define the responsibilities and configurations of each component, reducing the cognitive load on developers when creating or modifying components.\n2. **Decoupling and Flexibility:** Different manifest types allow components to evolve independently. Changes to one type will not inadvertently affect others.\n3. **Ease of Management:** Administrators and developers can more easily categorize and manage configurations when they are cleanly separated. Automated tools and CI\/CD pipelines can also more easily process manifests based on type.\n4. **Enhanced Validation:** Separate manifests allow for more precise validation schemas tailored to the specific requirements of each component type.\n\n**Implementation:**\n- Develop specific manifest schemas and accompanying documentation for Event Bindings, State Stores, and Message Buses.\n- Update existing manifests to comply with the new schema definitions.\n- Adapt configuration management tools to acknowledge and process the new manifest types.\n- Provide backward compatibility to ensure a smooth transition from existing configurations to the new model.","GenTime":"2024-07-28 21:24:01"}
{"File Name":"dapr\/CLI-002-self-hosted-init-and-uninstall-behaviors.md","Context":"## Context\\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n\n## Decision\n","Decision":"* Calling `dapr init` will\\n* Install `daprd` binary in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\\n* Create the default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\\n* Create a default configuration file in `$HOME\/.dapr\/config.yaml` for Linx\/MacOS and `%USERPROFILE%\\.dapr\\config.yaml` for Windows for enabling tracing by default.\\n* Calling `dapr init --slim` will\\n* Install the binaries `daprd` and `placement` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Create an empty default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Calling `dapr uninstall` will\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker dapr_placement if Docker is installed.\\n* Calling `dapr uninstall --all`\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\\n* Remove the default folder `$HOME\/.dapr` in Linux\/MacOS and `%USERPROFILE%\\.dapr` in Windows.\\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\\n* The `dapr` CLI by default will expect the `daprd` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\\n","tokens":41,"id":85,"Prediction":"## Decision\n\nThe `init` and `uninstall` commands for the Self Hosted mode in our software will be modified to better align with user expectations and desired functionalities, as discussed in the referenced [issue](https:\/\/github.com\/dapr\/cli\/issues\/411). Specifically, the following decisions have been made:\n\n1. **Enhanced `init` Command:**\n   - The `init` command will be updated to ensure that it correctly sets up all necessary components required for a Self Hosted deployment.\n   - Improve error handling and provide clearer, more descriptive error messages to help users troubleshoot setup issues more effectively.\n   - Introduce additional flags\/options to customize the initialization process, accommodating different use cases and configurations.\n\n2. **Refined `uninstall` Command:**\n   - The `uninstall` command will be enhanced to ensure that it thoroughly cleans up all resources and configurations associated with the Self Hosted deployment.\n   - Add checks to confirm the removal of all components to avoid leftover artifacts that could interfere with future deployments.\n   - Provide users with informative feedback on the uninstallation progress and completion status.\n\n3. **Documentation Updates:**\n   - Comprehensive updates to the documentation will be made to reflect the changes in the `init` and `uninstall` commands.\n   - Include detailed examples and explanations to help users understand the new functionalities and options available.\n   - Update FAQs and troubleshooting sections to incorporate common issues and their resolutions related to these commands.\n\nThis decision aims to enhance the user experience by making the `init` and `uninstall` commands more robust, intuitive, and informative, thereby reducing potential friction points for users operating in a Self Hosted environment.","GenTime":"2024-07-28 21:24:05"}
{"File Name":"nozama\/adr-001-simulator-webserver.md","Context":"## Context\\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\\n\n## Decision\n","Decision":"As of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\\n","tokens":63,"id":97,"Prediction":"## Decision\n\nWe will implement a RESTful API to facilitate communication between the simulator (Java program) and the WebApp (Nozama built using the Spring framework).\n\n### Justification:\n1. **Interoperability**: RESTful APIs are platform-independent, allowing the Java-based simulator and the Spring-based WebApp to communicate over HTTP seamlessly.\n2. **Scalability**: Using RESTful services will make it easier to scale different components independently.\n3. **Simplicity**: RESTful APIs are simpler to implement and test compared to other communication methods such as SOAP or CORBA.\n4. **Security**: RESTful services can be secured using standard web security practices, ensuring secure data exchange between the simulator and Nozama.\n\n### Implementation Steps:\n1. **Define Endpoints**: Determine the necessary REST endpoints required for communication between the simulator and Nozama.\n2. **API Design**: Design the API specifications including routes, request\/response formats, and status codes.\n3. **Simulator Updates**: Update the Java simulator to make HTTP requests to the defined REST endpoints.\n4. **WebApp Updates**: Update the Spring framework WebApp to expose the necessary REST endpoints, process requests, and respond accordingly.\n5. **Testing**: Thoroughly test API communication to ensure the data is transferred correctly and securely between the simulator and WebApp.\n6. **Documentation**: Provide clear documentation for the implemented API to facilitate future maintenance and enhancement.\n\nBy implementing these steps, we ensure efficient and reliable communication between the simulator and the Nozama WebApp while preserving flexibility and scalability for future needs.","GenTime":"2024-07-28 21:24:51"}
{"File Name":"kitsune\/0002-es-l10n-content.md","Context":"## Context\\nKitsune supports many locales,\\nand has content which we want to be searchable in those locales.\\nElasticsearch has support for many language-specific analyzers:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\\nSearch v1 used per-document analyzers,\\nthat is to say, within the same index:\\n```\\ndoc_1: { \"content\": \"Hello world\" }\\ndoc_2: { \"content\": \"Hallo Welt\" }\\n```\\n`doc_1.content` could be analyzed using an english analyzer,\\nand `doc_2.content` could be analyzed using a german analyzer.\\nWell before version 7 ES removed this feature,\\nand now all fields of the same name across an index must be analyzed the same,\\nso we must take a different approach with the current Search implementation.\\nWe can either place separate locales in their own index,\\nand set up locale-specific analyzers for the same field name across indices.\\nOr we can keep separate locales within the same index,\\nand define unique field names for each field which needs to be analyzed under a specific locale.\\n\n## Decision\n","Decision":"Heavily influenced by: https:\/\/www.elastic.co\/blog\/multilingual-search-using-language-identification-in-elasticsearch\\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\\nwith the appropriate analyzer defined on that key,\\nsuch that:\\n```\\ndoc_1: { \"content\": { \"en-US\": \"Hello world\" }}\\ndoc_2: { \"content\": { \"de\": \"Hallo Welt\" }}\\n```\\n`doc_1.content.en-US` is analyzed using an english analyzer,\\nand `doc_2.content.de` is analyzed using a german analyzer.\\n","tokens":249,"id":102,"Prediction":"## Decision \ud83c\udfaf\n\nWe decide to **place separate locales in their own index** and set up locale-specific analyzers for the same field name across indices.\n\n### Rationale:\n\n1. **Maintainability**: Separating locales into different indices allows for simpler maintenance and management. Each locale-specific index will have a consistent analyzer setup that will be easier to manage over time.\n2. **Performance**: Optimizing query performance specific to each locale will be more straightforward when they are separated. Each index can be fine-tuned for the particularities of its language's query patterns and usage.\n3. **Scalability**: As the dataset grows, having separate indices for each locale allows scaling operations to be applied selectively. This can be useful in international systems where the data volume differs significantly between locales.\n4. **Flexibility**: Changes or enhancements to language analyzers (e.g., switching from one analyzer to another or adjusting configurations) can be tested and rolled out per locale without impacting all users or requiring complex reindexing procedures.\n5. **Compatibility with ES**: Given that Elasticsearch no longer supports per-document analyzers within the same index, having different indices is more in line with the current capabilities and best practices of Elasticsearch.\n\n### Implementation Steps:\n\n1. **Index Creation**: Create separate indices for each locale.\n    - E.g., `content_en`, `content_de`, etc.\n2. **Locale-Specific Analyzers**: Configure locale-specific analyzers for each index.\n    - Assign the appropriate analyzer to the `content` field in each locale-specific index.\n3. **Data Migration**: Implement migration scripts to move content from the existing indices to the new locale-specific indices.\n4. **Index Alias Setup**: Use index aliases to simplify application-level queries. For instance, an alias like `content_all` can point to multiple locale indices if a global search is required.\n5. **Search Query Updates**: Update application search logic to route queries to the correct locale-specific index based on user preferences or detected locale.\n6. **Testing**: Thoroughly test the new indices setup to ensure search functionality is accurate and performs well across all locales.\n\nThis approach should ensure that our system remains robust, maintainable, and performs efficiently as we scale and support international users.","GenTime":"2024-07-28 21:25:15"}
{"File Name":"Nosedive\/0003-evolutionary-architecture.md","Context":"## Context\\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\\n\n## Decision\n","Decision":"We start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\\n","tokens":49,"id":108,"Prediction":"## Decision\nGiven the complex requirements outlined, we will adopt a multi-step, incremental approach to build the scalable application:\n\n1. **Ecosystem Learning Phase**:\n    - Allocate initial time and resources for the core team to ramp up on the Clojure ecosystem. This includes familiarizing with key libraries, tools, and best practices.\n\n2. **Microservices Architecture**:\n    - Design the application using microservices to ensure scalability, flexibility, and independent deployment. Clojure will be used for core business logic services.\n\n3. **Cloud-Native Approach**:\n    - Embrace cloud-native principles from the beginning. This involves containerization with Docker and orchestration using Kubernetes.\n\n4. **High Scalability & Autoscaling**:\n    - Utilize Kubernetes' autoscaling features to automatically adjust the number of running instances based on workload.\n    \n5. **Resilience**:\n    - Implement resilience patterns such as Circuit Breaker, Bulkhead, and Retry. Use Kubernetes' self-healing capabilities (like pod restarts) to enhance resilience.\n\n6. **Geographic Distribution**:\n    - Deploy the application across multiple regions to ensure low latency and high availability. Use CDNs and caching mechanisms to improve performance.\n\n7. **Multi-Cloud Strategy**:\n    - Design the system to be cloud-agnostic to avoid vendor lock-in. Use tools like Terraform for infrastructure-as-code to manage deployments across multiple cloud providers (e.g., AWS, Azure, GCP).\n\n8. **Continuous Integration\/Continuous Deployment (CI\/CD)**:\n    - Set up robust CI\/CD pipelines for automated testing, building, and deployment. Employ tools like Jenkins, GitLab CI, or GitHub Actions.\n\n9. **Monitoring & Logging**:\n    - Establish comprehensive monitoring and logging practices from the onset. Integrate with tools like Prometheus, Grafana, and ELK stack.\n\n10. **Security**:\n    - Embed security practices into every layer of the architecture. Use cloud-native security services and ensure compliance with security standards.\n\nBy following this multi-faceted, phased approach, we can tackle the steep learning curve, build a highly scalable, resilient, and distributed system, and ensure flexibility across multiple cloud providers.","GenTime":"2024-07-28 21:25:35"}
{"File Name":"Nosedive\/0005-sqlite-database.md","Context":"## Context\\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\\n\n## Decision\n","Decision":"We remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\\n","tokens":33,"id":109,"Prediction":"## Decision\nWe have decided to use CockroachDB for persistence in our system. CockroachDB offers strong consistency, horizontal scalability, and resilience, which align well with our system requirements. However, given the concerns with the Windows version, we will deploy CockroachDB in a containerized environment using Docker. This approach will help us avoid any potential issues related to the Windows version of CockroachDB and ensure a consistent deployment environment across different platforms. We will set up Docker on our Windows development machines and run CockroachDB within Docker containers, thereby leveraging the robust features of CockroachDB while maintaining platform independence.","GenTime":"2024-07-28 21:25:37"}
{"File Name":"dotfiles\/0004-use-vim-plug-in-favour-of-vundle.md","Context":"## Context\\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\\nread a bit about how vim-plug was pretty good. And it felt like it was time for\\na change.\\n\n## Decision\n","Decision":"[vim-plug](https:\/\/github.com\/junegunn\/vim-plug) will be used to manage Vim\\nplugins in favour of [Vundle](https:\/\/github.com\/VundleVim\/Vundle.vim).\\n","tokens":51,"id":114,"Prediction":"## Decision\n\nWe have decided to migrate from Vundle to vim-plug as our plugin manager for the following reasons:\n\n1. **Active Maintenance**: Vundle has not been updated since August 2019, leading to potential issues with compatibility and security over time. In contrast, vim-plug is actively maintained and regularly updated, ensuring better long-term support and reliability.\n  \n2. **Performance**: vim-plug is known for its performance benefits, notably faster startup times when managing plugins. This will lead to a more efficient development environment.\n\n3. **Ease of Use**: vim-plug offers a more straightforward and user-friendly syntax for managing plugins. This ease of use can reduce onboarding time for new team members and enhance productivity.\n\n4. **Features**: vim-plug provides additional features like asynchronous installation and lazy loading of plugins, which can improve the overall user experience and reduce startup times.\n\n5. **Community and Documentation**: vim-plug has ample community support and comprehensive documentation, making it easier to troubleshoot issues and find resources to optimize our setup.\n\nBy transitioning to vim-plug, we aim to enhance our workflow, ensure better plugin support, and prepare our development environment for future needs and updates. The migration process will involve carefully planning the transition to ensure minimal disruption to our current setup.","GenTime":"2024-07-28 21:25:54"}
{"File Name":"dotfiles\/0003-switch-to-use-zsh-in-place-of-bash.md","Context":"## Context\\n[Starting with macOS Catalina, zsh will be the default shell](https:\/\/support.apple.com\/en-us\/HT208050).\\n\n## Decision\n","Decision":"zsh should be used in place of bash.\\n","tokens":31,"id":115,"Prediction":"## Decision\n\n**Transition from bash to zsh as the default shell:**\n\nIn light of macOS Catalina's shift to zsh as the default shell, we will also transition our development and deployment environments to use zsh instead of bash. \n\n### Rationale:\n\n1. **Alignment with macOS Defaults:**\n   - By adopting zsh, we align with the default configuration on macOS Catalina and later, simplifying onboarding for developers using macOS.\n\n2. **Enhanced Features of zsh:**\n   - zsh offers superior auto-completion and customization options compared to bash, providing a more productive shell environment.\n\n3. **Future Proficiency:**\n   - Future-proofing our environment to be conformant with industry trends and operating system developments ensures better long-term sustainability.\n\n### Implementation Steps:\n\n1. **Update Development Documentation:**\n   - Modify all relevant documentation to reflect the change from bash to zsh, including setup guides and scripts.\n\n2. **Training and Support:**\n   - Provide resources and training for the team to familiarize them with zsh's features and differences from bash.\n\n3. **Script Compatibility Testing:**\n   - Audit and test current scripts to ensure they are compatible with zsh. Where necessary, update the scripts for compatibility with zsh syntax.\n\n4. **Environment Configuration:**\n   - Update the default shell configuration in our development environments and CI\/CD pipelines to use zsh.\n\n### Monitoring and Feedback:\n\n- Gather feedback from the team during the transition period.\n- Monitor for any issues or productivity impacts, being prepared to provide additional support or make adjustments as necessary.\n\nBy undertaking these steps, we aim to ensure a smooth transition to zsh while leveraging its enhanced capabilities for our development workflow.","GenTime":"2024-07-28 21:25:58"}
{"File Name":"dotfiles\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: [http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":117,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) process to document the decisions made throughout this project. An ADR will include the context of the decision, the options considered, the decision itself, and its consequences.\n\nTo facilitate this, the following steps will be taken:\n\n1. **Template Adoption**: We will use a standardized ADR template for all records. This template will include sections for Title, Context, Decision, Alternatives, and Consequences.\n\n2. **Documentation Tool**: A decision has been made to use a markdown file format stored in the project's Git repository, making it easy to track changes over time and ensuring accessibility for all team members.\n\n3. **Regular Reviews**: ADRs will be reviewed regularly during project meetings to ensure they are up-to-date and to discuss any new or pending decisions.\n\n4. **Decision Authority**: Key stakeholders and a designated architect or lead developer will have the authority to make and approve architectural decisions.\n\n5. **Training**: Team members will be trained on how to use ADRs effectively to ensure everyone can contribute to the documentation process.\n\nThis decision promotes transparency, accountability, and knowledge sharing across the project team.","GenTime":"2024-07-28 21:26:02"}
{"File Name":"dotfiles\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nThey are a thing that will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repository which has a very basic use case will\\nprovide some exposure to the service.\\n\n## Decision\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":116,"id":118,"Prediction":"## Decision\n\nWe have decided to transition our CI\/CD pipeline to use GitHub Actions. This decision is based on the following considerations:\n\n1. **Future-Proofing**: GitHub Actions is expected to become the default CI\/CD mechanism for projects hosted on GitHub, ensuring our infrastructure is aligned with future trends.\n2. **Integration**: As our repository is hosted on GitHub, using GitHub Actions will offer seamless integration and potentially simplify our existing CI\/CD configurations.\n3. **Features and Capabilities**: GitHub Actions offers a robust set of features that can handle our current and anticipated workflow needs effectively.\n4. **Community and Support**: With the general availability of GitHub Actions, there will be increased community support, documentation, and shared workflows that we can leverage.\n5. **Incremental Adoption**: Given that our current use case is very basic, it offers an ideal opportunity to gain exposure to GitHub Actions, allowing us to gradually adopt and scale as needed without significant initial overhead.\n\nBy moving to GitHub Actions, we aim to improve our CI\/CD processes and ensure we are well-positioned for the future evolution of these tools within the GitHub ecosystem.","GenTime":"2024-07-28 21:26:05"}
{"File Name":"dddsample-pelargir\/0003-install-citerus-as-a-git-submodule.md","Context":"## Context\\nI can't be bothered to be installing the citerus library into my\\nrepositories all the time.\\nI want to keep my copy of that closely synchronized to the common\\nproject, so that any progress that is made can be incorporated\\ninto my work.\\nI want a convenient way to introduce fixes in the existing implementation\\nas I discover the need.\\nI want to keep the touch on the existing code _light_; I expect\\nto be making some fairly drastic changes in approach, and I don't\\nwant to be debating my choices with the maintainers.\\nI'm not currently comfortable working with maven projects where\\nthe elements are not rooted under a common location.\\n\n## Decision\n","Decision":"Add the upstream as a git submodule, so that it's clear precisely\\nwhich version of the upstream is in play at any given time.\\nChanges that are of general interest can be in that space.\\nMost radical changes (for a _gentle_ definition of radical) should\\nbe maintained elsewhere.\\n","tokens":144,"id":128,"Prediction":"## Decision\n\nWe will include the citerus library as a git submodule within our project's repository.\n\n### Justification:\n1. **Eliminates Repeated Installation Effort**: By including the citerus library as a submodule, you no longer need to install it repeatedly. The dependency is managed directly within your project's repository structure.\n2. **Ensures Synchronization**: A git submodule allows you to track a specific commit of the citerus library. This means you can easily pull in updates from the common project and stay in sync with any progress made on that library.\n3. **Enable Convenient Fixes**: You can make local changes and fixes directly within the submodule without having to push your modifications to the main citerus repository managed by its maintainers.\n4. **Light Touch on Existing Code**: Using git submodules allows you to include and track the citerus library within your project without making significant changes to your existing code structure.\n5. **Avoids Maven Rooting Issues**: Since you find it challenging to manage maven projects without a common root location, utilizing git submodules avoids this issue, as you can place the citerus library wherever convenient in your repository hierarchy.\n6. **Drastic Approach Changes**: With local control over the submodule, you can implement and test significant changes independently, free from the constraint of persuading the library\u2019s maintainers about the necessity of your approach.\n\n### Implementation Steps:\n1. Add the citerus library as a submodule:\n   ```bash\n   git submodule add <REPOSITORY_URL> path\/to\/citerus\n   git submodule init\n   git submodule update\n   ```\n2. Update your project configuration (e.g., Maven, Gradle) to use the cited version within the submodule as a dependency.\n3. When the citerus library changes and you wish to incorporate them, run:\n   ```bash\n   cd path\/to\/citerus\n   git pull origin main\n   cd ..\/..\/\n   git add path\/to\/citerus\n   git commit -m \"Updated citerus submodule to latest version\"\n   ```\n\nThis approach marries the desire for easy synchronization with flexibility in making fixes and substantial changes, all while keeping the project management process within your comfort zone.","GenTime":"2024-07-28 21:26:45"}
{"File Name":"documents\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the future architectural decisions made.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":14,"id":138,"Prediction":"## Decision\nWe will adopt the use of an Architectural Decision Record (ADR) template for documenting all future architectural decisions. This template will include sections such as Context, Decision, Status, Consequences, and Date. All ADRs will be stored in a version-controlled repository to ensure traceability and ease of access for team members.","GenTime":"2024-07-28 21:27:26"}
{"File Name":"osticket-ticket-options-plugin\/001-required-properties.md","Context":"# Context\\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\\n# Decision\\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n\n## Decision\n","Decision":"Scott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","tokens":236,"id":139,"Prediction":"## Decision\nScott has decided to implement a new feature in the lifecorp-init project, which introduces a `--require` or `-r` parameter. This parameter will enforce that specified prompts must not be left empty during the initialization process. Specifically, for `path_url`, this feature will ensure that the prompt cannot be bypassed without an entry, aligning with the goal of lifecorp-init to facilitate quick project setup to a \"Hello World\" state with minimal manual modifications. \n\nThis decision allows for flexible handling of different project requirements while maintaining ease of use for developers.","GenTime":"2024-07-28 21:27:28"}
{"File Name":"lcarsde\/display-server-library.md","Context":"## Context and Problem Statement\\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\\n\n## Decision\n","Decision":"Chosen option: \"Xlib\", because it was used already for the additional tooling and there's lots of documentations and examples.\\n### Positive Consequences <!-- optional -->\\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\\n### Negative Consequences <!-- optional -->\\n* Refactoring code\\n* Wayland might be more modern\\n* XCB is faster at times due to asynchronous\\n","tokens":66,"id":142,"Prediction":"## Decision\nThe decision has been made to switch entirely to Xlib for display management and window handling functionalities. Upon evaluation, Xlib provides a more comprehensive set of features that are required for our window manager and ensures a consistent methodology, mitigating the hazards associated with mixing XCB and Xlib. The transition will involve refactoring existing XCB code to utilize Xlib exclusively, ensuring that all window and display management functions are handled uniformly. This unified approach will also simplify maintenance and future development efforts.","GenTime":"2024-07-28 21:27:37"}
{"File Name":"PactSwift\/ADR-002-Coding_standard.md","Context":"# Context\\nWe shouldn't feel bound by any pre-existing coding standards so this project and its code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.\\nIn general, Swift code has a fairly strong styling, relative to C or C++, due to opinionated aspects of the language itself and the styling used by the official language guides. Formatting around brace placement, `if` and `for` styling is fairly clearly set by the language.\\n# Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n\n## Decision\n","Decision":"[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","tokens":150,"id":146,"Prediction":"## Decision\nWe will adopt a defined code style by using a SwiftLint configuration. This configuration will help maintain consistency and quality in our code as more developers contribute to the project. \n\n### Justification:\n1. **Consistency**: As more developers join the project, a consistent codebase will make it easier for everyone to read, understand, and maintain the code.\n2. **Quality**: Adhering to a standard set of linting rules will help catch common errors and enforce best practices.\n3. **Swift's Opinionated Nature**: Swift has strong styling conventions as recommended by its official guides. Using SwiftLint will ensure we adhere to these conventions where applicable.\n4. **Efficiency**: Developers can focus on writing code rather than debating style preferences, as the rules will be predefined.\n\n### Implementation:\n1. **Configuration**: We'll use a `.swiftlint.yml` file to configure the specific rules we want to enforce.\n2. **Customization**: While we'll adhere to most of the standard conventions, the configuration file will allow us to enable, disable, or tweak rules according to the specific needs of our project.\n3. **Continuous Integration**: SwiftLint will be integrated into our CI pipeline to ensure that all code pushed to the repository meets the defined style standards. \n\nBy adopting SwiftLint, we aim to maintain a high standard of code quality and consistency across the growing development team.","GenTime":"2024-07-28 21:27:49"}
{"File Name":"eslint-config-nhsuk\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n\n## Decision\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":114,"id":152,"Prediction":"## Decision\nWe have decided to adopt GitHub Actions as our continuous integration (CI) mechanism for this repository. Given the long-term viability and potential of GitHub Actions becoming the default CI service for GitHub-hosted projects, adopting it now aligns with future-proofing our CI\/CD process. This will not only provide early exposure and hands-on experience with the tool but also streamline workflows natively within GitHub, leveraging its upcoming general availability and expected enhancements.\n\nKey considerations:\n- The integration is straightforward due to GitHub Actions being native to the GitHub ecosystem.\n- It will enable faster adoption and adjustment to the new tool, preparing for future developments.\n- Using GitHub Actions will ultimately simplify our setup by consolidating services within the GitHub suite, improving efficiency and maintainability.\n\nNext Steps:\n1. Set up basic workflows for building and testing the project using GitHub Actions.\n2. Monitor and evaluate the performance and benefits in comparison to our current CI setup.\n3. Gradually migrate more complex CI\/CD pipelines to GitHub Actions if the initial adoption proves successful.","GenTime":"2024-07-28 21:28:09"}
{"File Name":"AYCHAIN.github.io\/adr-002-css-structure.md","Context":"## Context\\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\\n\n## Decision\n","Decision":"Considering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\\nShared styles will be hosted under `src\/shared-styles` folder and organized following the ITCSS nomenclature.\\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\\n","tokens":91,"id":157,"Prediction":"## Decision\n\nWe will adopt a hybrid approach that leverages both Shadow DOM encapsulation for component-level styles and shared styles stored in a centralized repository, adhering to the principles of ITCSS to ensure scalability and manageability.\n\n### Implementation Plan:\n\n1. **Shadow DOM for Component-Level Styles:**\n   - Each Web Component will maintain its own styles encapsulated within its Shadow DOM. This will prevent conflicts and ensure that styles are scoped to the component itself.\n\n2. **Shared Styles Repository:**\n   - Create a centralized styles repository using ITCSS methodology. This repository will contain shared style definitions, utility classes, and other global styling rules that are common across multiple components.\n\n3. **CSS Custom Properties (CSS Variables):**\n   - Employ CSS Custom Properties for shared styles and configurations that need to be accessible within Shadow DOM. These variables can be defined globally and applied locally within Shadow DOM.\n\n4. **Adopting ITCSS Layers:**\n   - Restructure our CSS to adhere to ITCSS layers while distinguishing between global styles and component-specific styles. The ITCSS structure includes:\n     - Settings: Global variables and configurations.\n     - Tools: Mixins, functions, and other SASS\/LESS helpers.\n     - Generic: Reset\/normalize styles, box-sizing definition, etc.\n     - Elements: Standard HTML element styling (e.g., base styling for headers, paragraphs).\n     - Objects: Layouts and non-cosmetic design patterns.\n     - Components: Specific and unique component styling.\n     - Utilities: Utility classes for state and theming purposes.\n\n5. **Component Development Workflow:**\n   - During component development, create and store component-specific styles within its Shadow DOM.\n   - Extract any common styles or themes that could be shared among multiple components and move these to the shared styles repository in their respective ITCSS layer.\n\n6. **Documentation and Guidelines:**\n   - Document the approach and provide guidelines to ensure consistency across the organization.\n   - Train developers on the best practices of ITCSS and the workflow for incorporating shared styles and component-level styles.\n\n### Benefits:\n\n- **Encapsulation and Isolation:**\n  - Utilizing Shadow DOM ensures that component styles do not leak out or get affected by external styles, maintaining a clean and predictable style scope.\n\n- **Scalability and Maintainability:**\n  - Following ITCSS ensures that our CSS remains scalable and maintainable as the project grows, reducing complexity and specificity issues.\n\n- **Reusability and DRY Principle:**\n  - Centralizing shared styles and making use of CSS variables encourages reusability, limiting code duplication and adhering to the DRY (Don't Repeat Yourself) principle.\n\nBy adopting this hybrid approach, we ensure that our Web Components are both robust and easy to maintain while keeping the flexibility and power provided by a structured CSS architecture like ITCSS.","GenTime":"2024-07-28 21:28:32"}
{"File Name":"AYCHAIN.github.io\/adr-005-substeps-list.md","Context":"## Context\\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\\nAnimating proves convenient on `max-height`\\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\\n\n## Decision\n","Decision":"We chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\\nWe thus decided to trick the component:\\n- we only calculate the `totalHeight` once, since content will not change once rendered.\\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https:\/\/github.com\/Polymer\/lit-html\/blob\/master\/src\/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\\n","tokens":124,"id":158,"Prediction":"## Decision\nWe will utilize JavaScript to dynamically calculate and set the `max-height` property for the animation of the SubstepsList. By measuring the actual rendered height of the substeps list at runtime, we can ensure a smoother and more flexible transition that accommodates varying content lengths, including unpredictable components like `ErrorMessage`. This approach allows us to avoid hard-coding a `max-height` value and provides a more robust and maintainable solution. The progress bar height will continue to be based on the actual content of the substeps list, maintaining its current functionality without additional adjustments.","GenTime":"2024-07-28 21:28:33"}
{"File Name":"MoviesAndroid\/0002 Kotlin.md","Context":"## Context and Problem Statement\\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\\ndon't choose and use both, but i want to make it consistent.\\n## Decision Drivers\\n* New empty project\\n* Kotlin now fully supported for Android by Google\\n\n## Decision\n","Decision":"* New empty project\\n* Kotlin now fully supported for Android by Google\\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\\nGoogle now, which eliminates the risk of being dropped any time soon.\\nPositive Consequences:\\n* Less code, more fun!\\n* Having to learn a new language\\n* Great interoperability with Java if needed\\nNegative consequences:\\n* Having to learn a new language\\n","tokens":64,"id":159,"Prediction":"## Decision\nAfter evaluating the context and the drivers for the new empty project, it has been decided to use **Kotlin** as the primary programming language for the project. This decision is based on several considerations:\n\n1. **Full Support by Google**:\n   - Kotlin is now fully supported for Android development by Google, which ensures long-term viability and future-proofing for the project.\n\n2. **Modern Language Features**:\n   - Kotlin provides modern programming features such as null safety, extension functions, and coroutines, which can streamline the development process and reduce common programming errors.\n   \n3. **Improved Readability and Maintainability**:\n   - Kotlin\u2019s concise syntax can lead to more readable and maintainable code, making it easier for developers to understand and contribute to the project.\n\n4. **Community and Ecosystem**:\n   - There is a growing community and ecosystem around Kotlin, including libraries, frameworks, and tooling, which can enhance development productivity.\n\n5. **Consistency and Future Development**:\n   - Using a single language from the start will ensure consistency across the codebase and make future development smoother without language interoperability issues.\n\nTherefore, the decision is to use Kotlin exclusively for the development of the new project to leverage modern language features, maintain consistency, and align with industry standards.","GenTime":"2024-07-28 21:28:37"}
{"File Name":"MoviesAndroid\/0001 ADR.md","Context":"## Context and Problem Statement\\nI need a way to document design decisions.\\n\n## Decision\n","Decision":"Chosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\\nPositive Consequences:\\n* Choices and reasoning are persisted and versioned\\nNegative consequences:\\n* Having to document\\n","tokens":16,"id":160,"Prediction":"## Decision\n\nWe will utilize an Architectural Decision Record (ADR) template to document design decisions. The ADR will provide a structured format that includes the context, problem statement, decision, and rationale. This approach will ensure that design decisions are well-documented, easily accessible, and maintain clarity for all stakeholders involved in the project. By using ADRs, we can track the evolution of decisions over time and provide a clear reference for future development and maintenance efforts.","GenTime":"2024-07-28 21:28:39"}
{"File Name":"disco-poc-vue\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":165,"Prediction":"## Decision\nWe will adopt the Architectural Decision Record (ADR) framework to document the architectural decisions of this project. This structured approach will ensure that all key decisions are recorded, providing a historical context and rationale for future reference. Each decision will be captured in a consistent format, including the context, decision, status, and consequences. By doing so, we will enhance the maintainability, clarity, and transparency of the project's architecture.","GenTime":"2024-07-28 21:28:54"}
{"File Name":"paas-team-manual\/ADR037-automated-certificate-rotation.html.md","Context":"## Context\\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\\nIn Q2 2019 (August\/September) we moved all of our platform secrets from AWS S3 to [Credhub](https:\/\/docs.cloudfoundry.org\/credhub\/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\\n\n## Decision\n","Decision":"Credhub has the notion of a transitional certificate. As written in [their documentation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), a transitional certificate is\\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\\nThe new `rotate-certs` job has three tasks:\\n- `remove-transitional-flag-for-ca`\\n- `move-transitional-flag-for-ca`\\n- `set-transitional-flag-for-ca`\\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\\n","tokens":209,"id":180,"Prediction":"## Decision\n\nWe decided to implement automatic certificate rotation using Credhub to enhance security and reduce manual intervention, ensuring a more consistent and reliable process.\n\n### Rationale:\n\n1. **Enhanced Security**:\n   - Using Credhub for certificate management ensures that our platform secrets, including certificates, are securely stored and managed. \n   - Automatic rotation reduces the risk of certificate expiration and potential security breaches due to expired or compromised certificates.\n\n2. **Operational Efficiency**:\n   - Automating the certificate rotation process eliminates the need for manual intervention, thus reducing the operational overhead.\n   - It ensures that certificates are rotated on a regular schedule, avoiding the scenario where certificates are only rotated during CF upgrades or when they are about to expire.\n\n3. **Reliability**:\n   - The automated process reduces the possibility of human error, which is more likely in a manual rotation process where specific pipeline jobs need to be triggered in a particular sequence.\n   - The process is consistent and can be easily monitored, with explicit notifications or alerts if something goes wrong.\n\n### Implementation:\n\n1. **Transition to Credhub**:\n   - All platform secrets, including third-party service credentials, platform passwords, and certificates, were moved from AWS S3 to Credhub in Q2 2019.\n\n2. **Using Credhub for Certificate Management**:\n   - We implemented Credhub\u2019s built-in support for certificate rotation, following the guidelines and best practices provided in the [Credhub certificate rotation documentation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md).\n\n3. **Automation Setup**:\n   - The existing Concourse `check-certificates` job, which previously alerted us of impending certificate expirations, was replaced or augmented with automation scripts that trigger certificate rotations as needed.\n   - Regular monitoring and logging were set up to track the status of certificates and to alert if any issues arise during the automated rotation process.\n\n### Outcomes Expected:\n\n- With automatic certificate rotation, certificates are expected to be renewed and replaced well before their expiration, reducing the risk of system downtime or security vulnerabilities due to expired certificates.\n- The operational team can focus on higher-level tasks rather than routine certificate management, improving overall productivity.\n- The system will have a more robust and reliable security posture with automatically rotated certificates managed by Credhub.","GenTime":"2024-07-28 21:29:37"}
{"File Name":"paas-team-manual\/ADR020-deletion_of_ci_environment.html.md","Context":"## Context\\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\\n\n## Decision\n","Decision":"We will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\\n","tokens":78,"id":187,"Prediction":"## Decision\n\nWe will consolidate our deployment pipeline by eliminating the CI (Continuous Integration) environment and retaining only the Staging and Production environments. This reduction aims to streamline the deployment process, reduce maintenance overhead, and expedite the time it takes for a change to reach the Production environment.\n\n### Key Points:\n1. **Retain Staging and Production Environments**: Keep the staging environment for thorough testing and validation before promoting changes to production.\n2. **CI Environment Removal**: Integrate the existing CI tests into the staging environment to ensure that we still maintain a quality assurance step without the need for a separate CI environment.\n3. **Improved Efficiency**: Expect to see faster deployment cycles and reduced operational costs by maintaining fewer environments.\n4. **Continuous Monitoring**: Implement robust monitoring and automated testing in the Staging environment to ensure that it provides a reliable staging ground for Production-ready code.\n5. **Feedback Loop**: Adjust and iterate based on initial results to ensure that the streamlined process meets our quality and speed objectives.","GenTime":"2024-07-28 21:30:01"}
{"File Name":"paas-team-manual\/ADR044-remove-ipsec.html.md","Context":"## Context\\nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\\nThis is a form of end-to-end encryption.\\nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\\nThis is a form of end-to-end encryption.\\nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) alleges\\nthat the IPSec release is no longer maintained.\\n\n## Decision\n","Decision":"We will not run IPSec in our BOSH deployments.\\n","tokens":103,"id":194,"Prediction":"## Decision\nGiven the context that the IPSec release used for end-to-end encryption between the gorouter and diego cells is no longer maintained, we will transition to a well-supported and actively maintained encryption solution to ensure ongoing security and reliability of our GOV.UK PaaS infrastructure.\n\nWe have identified mutual TLS (mTLS) as a robust alternative for securing communication between gorouter and diego cells. Since GOV.UK PaaS already employs mutual TLS via routing-release, extending this scheme to include communication between gorouter and diego cells would provide a consistent and standardized approach to encryption across our infrastructure.\n\n### Steps to Implement the Decision:\n1. Evaluate the feasibility of implementing mutual TLS between gorouter and diego cells.\n2. Develop a migration plan to transition from IPSec to mutual TLS.\n3. Conduct a thorough testing phase to ensure that the new encryption approach does not introduce any vulnerabilities or performance bottlenecks.\n4. Update all relevant documentation and provide necessary training for the operational teams.\n5. Monitor the performance and security of the new setup continuously, ready to address any issues that may arise during and after the transition.","GenTime":"2024-07-28 21:30:19"}
{"File Name":"paas-team-manual\/ADR025-service-plan-naming-conventions.html.md","Context":"## Context\\nOur service plans have evolved incrementally over the last few years and are in\\nneed of some attention. Names are inconsistent, potentially confusing and\\nin many cases contain irrelevant redundant information that is of no practical\\nuse to the platform operators or to tenants consuming the service.\\nAdding additional versions of services has the potential to compound the\\nproblem by multiplying plans of different characteristics with different\\nversions.\\n\n## Decision\n","Decision":"We have decided to use the following naming convention for naming plans going forward:\\n```\\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\\n```\\nWhere:\\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\\n* `HA` is the string `ha` to indicate highly available if relevent.\\n* `LABEL` is a string describing some specific variant of the service if relvent.\\n* `VERSION` is the version number of the service plan.\\n### For example:\\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\\n### Example migrations of some existing plans:\\n```\\nL-HA-enc-dedicated-9.5 => large-ha-9.5\\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\\ntiny-clustered => tiny-clustered-3.2\\ntiny-unclustered => tiny-unclustered-3.2\\n```\\n### Additionally:\\n* We will avoid use of the word \"free\" in names.\\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\\n* We will avoid use of uppercase characters in names.\\n* We will avoid abbriviations where possible (\"medium\" instead of \"M\", \"large\" instead of \"L\").\\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\\n","tokens":83,"id":200,"Prediction":"## Decision\nWe will implement a plan naming convention overhaul, aiming to create clear, consistent, and easily understandable names for all service plans. This will involve:\n\n1. **Standardized Naming Schema**: Developing a standardized naming schema that eliminates redundancy and irrelevance, ensuring names reflect the essential attributes and versions of each service plan.\n   \n2. **Version Control**: Integrating version identifiers within the service plan names to clearly differentiate between different iterations and characteristics of services, while maintaining simplicity and clarity.\n   \n3. **Migration Strategy**: Establishing a robust migration strategy to rename existing plans in a way that minimizes disruption to current users and platform operations. This includes detailed communication plans to inform tenants of the changes.\n\n4. **Documentation Update**: Updating all relevant documentation, including internal operator guides and tenant-facing materials, to reflect the new naming conventions and ensure a smooth transition.\n\n5. **Validation and Reviews**: Implementing validation checks and peer reviews to ensure the consistency and utility of new service plan names before finalizing and rolling out the changes.\n\nBy adopting this structured approach, we aim to reduce confusion, improve usability for operators and tenants, and establish a scalable naming framework that supports future service evolutions.","GenTime":"2024-07-28 21:30:37"}
{"File Name":"paas-team-manual\/ADR018-rds-broker-restore-last-operation.html.md","Context":"## Context\\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](..\/ADR006-rds-broker).\\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\\nThe broker must:\\n1. Start the restore from snapshot, which can take minutes.\\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\\n3. Once that is finish, reset the master password of the RDS instance.\\n4. Finally reset the passwords of the users previously bind in the original DB.\\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\\nThe rds-broker must respond accordingly.\\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html). This requires a lot of additional work and integration effort.\\n* Store the state in some database or k\/v store.\\n\n## Decision\n","Decision":"We decided:\\n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using_Tags.html) of the instance.\\nWe will add a list of tags for each pending operations to execute.\\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\\nWe assume that:\\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\\n* that update the tags is atomic and synchronous.\\n","tokens":360,"id":207,"Prediction":"## Decision\n\nWe have decided to store the state of the RDS instance restoration process in a database. This approach will allow us to keep track of the sequential steps required for the restore from the snapshot feature.\n\n### Rationale\n\n1. **Resilience and Persistence**: By storing the state in a database or key-value store, the restoration process can be resilient to broker restarts. This persistent storage will ensure that the process can continue where it left off without losing any intermediate state.\n\n2. **Simplicity**: Compared to the complexity of integrating AWS SNS and SQS, using a database to store state is relatively straightforward and can be quickly implemented without extensive integration effort.\n\n3. **Scalability**: A stateful database approach can handle multiple rds-broker instances, ensuring that the steps in the restoration process are accurately tracked regardless of which instance is handling the operation.\n\n4. **Monitoring and Debugging**: Storing state in a database provides an easy way to monitor and debug the restoration process. Each step and its status can be queried directly, making troubleshooting more manageable.\n\n### Implementation Approach\n\n- **Database Selection**: We will choose a reliable and high-performance database system that fits our current technology stack and scalability requirements. Options include relational databases like PostgreSQL or a high-capacity key-value store like Redis.\n\n- **State Management**: We will design a schema to track the state of each RDS instance restoration process. This schema will include details such as instance ID, current step in the process, timestamps, and any necessary parameters.\n\n- **Error Handling**: We will implement robust error handling to ensure that any failures in the restoration steps can be accurately captured and retried if necessary.\n\n- **Concurrency Control**: Mechanisms will be put in place to handle concurrent requests and ensure that there are no conflicts or race conditions in state updates.\n\n### Next Steps\n\n1. **Schema Design**: Define the database schema to store the state information.\n2. **Database Setup**: Set up the chosen database system and ensure connectivity with the rds-broker.\n3. **State Management Logic**: Implement the logic within the broker to update and query state information during the RDS instance restoration process.\n4. **Testing**: Thoroughly test the implementation to ensure resilience, accuracy, and performance.\n\nBy following this decision, we aim to provide a reliable and efficient way to handle the complex process of restoring RDS instances from snapshots, ensuring that the broker can manage and track the necessary steps without losing state information.","GenTime":"2024-07-28 21:31:07"}
{"File Name":"paas-team-manual\/ADR002-concourse-pool-resource.html.md","Context":"## Context\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\n\n## Decision\n","Decision":"We will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\n","tokens":158,"id":210,"Prediction":"## Decision\nWe decided to use the [semver resource](https:\/\/github.com\/concourse\/semver-resource) due to its reliance on AWS credentials, which we are already managing and passing. This allows us to avoid the additional complexity and security risks associated with managing GitHub public keys for the [pool resource](https:\/\/github.com\/concourse\/pool-resource). By minimizing the number of distinct credentials needed, we enhance the security and maintainability of our pipelines. While both resources offer solutions to trigger jobs without changes to a resource, using semver fits better with our current credential management strategy.","GenTime":"2024-07-28 21:31:17"}
{"File Name":"paas-team-manual\/ADR003-AWS-credentials.html.md","Context":"## Context\\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\\netc.) use the APIs to manage or access IaaS resources.\\nThe most common mechanism for authenticating the API calls is to create an\\nIdentify and Access Management (IAM) user with the appropriate permissions,\\ngenerate an Access Key ID and Secret Access Key for that user, and export\\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\\nmost utilities and libraries.\\nThe problem with this approach is that it's very easy to accidentally leak\\nthe plain text keys. They can appear in output from your shell, which you\\nmight copy+paste into a gist or email when debugging a problem. You might\\nadd them to your shell configuration or include them in a script, which can\\nbe pushed to a public code repository.\\nOur team have leaked keys like this on more than one occasion. It's worth\\nnoting that even if you realise that you've done this, delete the commit and\\nrevoke the keys, they may have already been used maliciously because\\nautomated bots monitor sites like GitHub using the [events firehose][] to\\ndetect any credentials.\\n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/\\nAs an alternative to using pre-generated keys, AWS recommends that you use\\n[IAM roles and instance profiles][] when accessing the API from EC2\\ninstances. You delegate permissions to the EC2 instance and temporary\\ncredentials are made available from the instance metadata service. Most\\ntools and libraries automatically support this. The credentials are\\nregularly rotated and never need to be stored in configuration files.\\n[IAM roles and instance profiles]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/best-practices.html#use-roles-with-ec2\\n\n## Decision\n","Decision":"To reduce the likelihood of us leaking AWS keys we will use IAM roles and\\ninstance profiles for all operations that run from EC2 instances. This\\nincludes everything that happens within Concourse and Cloud Foundry.\\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\\nan [`aws:SourceIp` condition][condition] to\\nenforce that IAM accounts for team members are only used from the office IP\\naddresses.\\n[condition]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_policies_examples.html#iam-policy-example-deny-source-ip-address\\nThe IAM roles, profiles, and policies will be managed by our\\n[aws-account-wide-terraform][] repo.\\n[aws-account-wide-terraform]: https:\/\/github.digital.cabinet-office.gov.uk\/government-paas\/aws-account-wide-terraform\\n","tokens":427,"id":211,"Prediction":"## Decision\n\n### Transition to IAM Roles and Instance Profiles\n\nTo enhance the security of our infrastructure and mitigate the risks associated with the accidental leakage of static AWS credentials, we have decided to transition from using AWS IAM users with static Access Key IDs and Secret Access Keys to instead leveraging IAM roles and instance profiles for accessing AWS APIs on EC2 instances.\n\n### Justification\n\n1. **Reduced Risk of Credential Leakage:** By utilizing IAM roles and instance profiles, we eliminate the risk of accidentally exposing static credentials in scripts, logs, or other sources that may end up in public domains or shared inadvertently.\n  \n2. **Automatic Credential Rotation:** IAM roles provide temporary credentials that are rotated automatically, reducing the burden on operations to periodically update and manage static keys.\n\n3. **Integrated Compatibility:** Most AWS tools and libraries natively support retrieving credentials from the instance metadata service, simplifying the transition process.\n\n4. **Enhanced Security Posture:** This approach aligns with AWS best practices for security and identity management, further strengthening our overall security posture by adhering to recommended guidelines.\n\n### Implementation Plan\n\n1. **Identify Impacted Components:** Conduct an audit to identify all scripts, tools, and configurations currently relying on static credentials.\n\n2. **Create IAM Roles:** Define appropriate IAM roles with least privilege policies for the necessary AWS services accessed by various components.\n\n3. **Attach Instance Profiles:** Attach the created IAM roles to the relevant EC2 instances via instance profiles.\n\n4. **Update Tools and Scripts:** Modify any deployment tooling and scripts to leverage the instance metadata service for credentials, eliminating hard-coded or environment-stored static keys.\n\n5. **Testing:** Conduct thorough testing in a staging environment to ensure all components function correctly with the new authentication mechanism.\n\n6. **Monitoring and Rollback:** Monitor the implementation in the production environment. Define a rollback plan to switch back to static credentials temporarily if critical issues arise.\n\n### Follow-Up Actions\n\n- **Training:** Provide training and documentation for the team on using IAM roles and instance profiles, including best practices and troubleshooting steps.\n  \n- **Periodic Review:** Regularly review IAM policies and instance profiles to ensure continued adherence to the principle of least privilege and compliance with security standards.\n\n### Appendix\n\n- Reference: AWS Best Practices for Using IAM Roles with EC2: [AWS Documentation](http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/best-practices.html#use-roles-with-ec2)\n- Event Monitoring: Automate the monitoring of code repositories for accidental exposure of credentials using tools like [AWS GitHub Secrets Scanner](https:\/\/aws.amazon.com\/blogs\/security\/automate-github-secret-scanning-and-remediation-with-aws-security-hub-and-aws-lambda\/).","GenTime":"2024-07-28 21:31:23"}
{"File Name":"paas-team-manual\/ADR023-idle-cpu-alerting-change.html.md","Context":"## Context\\nWith the implementation of ADR021 we have reduced the number of cells in\\nproduction in order to make more efficent use of our budget. This in turn means\\nthat we have increased the load on the individual cells. Originally the idle CPU\\nmonitor was set in line with the free memory on cells monitor (for alerting on\\na need to scale the cell pool), however CPU usage does not appear to grow\\nlinearly with allocated memory for tenant applications.\\n\n## Decision\n","Decision":"In order to avoid false positives from triggering due to CPU load spiking rather\\nthan being a constant level we will increase the monitoring window to 24 hours.\\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\\nthresholds to warn at 37% and become critical at 33%.\\n","tokens":98,"id":215,"Prediction":"## Decision\nIn light of the changes implemented by ADR021 which has resulted in a higher load on fewer cells, we have determined that the CPU usage monitoring strategy needs to be revisited. The increased load does not correlate linearly with memory usage, making the original idle CPU monitor settings insufficient for accurately predicting and managing resource utilization.\n\nTo address this, we will decouple the CPU usage monitoring from memory usage monitoring. We will develop and implement a new CPU-centric monitoring and alerting strategy tailored to the current load dynamics observed in our production environment. \n\nKey actions include:\n1. Establishing new CPU usage thresholds and alerts independent of memory usage.\n2. Implementing more frequent CPU usage sampling to capture spikes and trends.\n3. Reviewing and adjusting these CPU usage thresholds periodically based on observed performance data.\n4. Ensuring that this new monitoring strategy integrates seamlessly with our existing infrastructure to provide timely and accurate alerts for scaling needs.\n\nThis approach will help us better manage the load on individual cells and maintain system performance, ultimately making more efficient use of our budget while ensuring reliability and scalability.","GenTime":"2024-07-28 21:31:33"}
{"File Name":"paas-team-manual\/ADR016-end-to-end-encryption.md","Context":"Context\\n=======\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\nDecision\\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n\n## Decision\n","Decision":"========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","tokens":247,"id":216,"Prediction":"Decision\n========\n* Implement end-to-end encryption for all traffic sections between the user and the application instances to maintain the confidentiality of private tenant data.\n  \n1. **User to ELB Traffic**:\n   - Encrypt this traffic using a TLS listener configured on the ELB.\n   - Secure the private key and certificate issued by a trusted certificate authority on the ELB.\n\n2. **ELB to Router Traffic**:\n   - Ensure that the ELB connects to the Router VM via TLS.\n   - Configure the Router VM to serve TLS to maintain secure communication.\n\n3. **Router to Application Instances (Cells) Traffic**:\n   - Recognize that plain HTTP is currently the only supported communication protocol between the gorouter and application instances within the Cloud Foundry environment.\n   - Enforce an IPSec protocol between the router and cells to provide transparent encryption, thus securing the data in transit.\n\nOverall, this layered approach ensures that all network segments are secured, effectively mitigating risks associated with network eavesdropping and unauthorized access.","GenTime":"2024-07-28 21:31:35"}
{"File Name":"paas-team-manual\/ADR010-postgres-bind-behaviour.html.md","Context":"## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n\n## Decision\n","Decision":"We decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\n","tokens":470,"id":224,"Prediction":"## Decision\n\nAfter considerable investigation and testing of various workarounds to handle PostgreSQL's fine-grained permission model, the following decision has been made to address the issues with database migrations and user privileges in our RDS PostgreSQL setup:\n\n### Implementation of a Central Database Management Service\n\n1. **Introduce a Central DB Management Service**:\n   - Implement a dedicated service that manages database schema changes, migrations, and user privilege assignments. This service will act as an intermediary layer between tenant applications and the PostgreSQL RDS instances.\n\n2. **Centralized Role Management**:\n   - Create a central \"migrations\" role that will exclusively handle database schema changes (ALTER, DROP, etc.).\n   - Grant this role the necessary permissions to perform migrations on all tables and schemas.\n   - Ensure that this role is used exclusively by the Central DB Management Service for making any structural changes to the database.\n\n3. **Fine-Grained API Access**:\n   - Expose APIs for applications to request migrations or schema changes. The Central DB Management Service will review these requests and perform the necessary actions.\n   - Maintain an audit log of all requests and actions to ensure transparency and track changes.\n\n4. **User Privileges**:\n   - Assign newly created users default privileges (SELECT, INSERT, DELETE) using `ALTER DEFAULT PRIVILEGES`, but restrict object ownership and schema modification privileges.\n   - Ensure that user roles have limited access and any requirement for schema changes are routed through the Central DB Management Service.\n\n5. **Custom Schema Support**:\n   - Modify the role and privilege management logic to work seamlessly with custom schemas.\n   - Ensure that the Central DB Management Service is flexible enough to handle privilege assignments and migrations within custom schemas.\n\n6. **Documentation and Training**:\n   - Provide clear documentation and training to development teams on how to interact with the Central DB Management Service.\n   - Emphasize the importance of using the provided APIs for any database schema changes to ensure consistency and avoid breakages.\n\n### Rationale\n\nThis decision ensures that:\n- The fine-grained permissions model of PostgreSQL is respected and effectively managed.\n- Structural changes to the database are controlled and auditable.\n- The risk associated with broken database migrations due to fluctuating user privileges is minimized.\n- Flexibility is maintained to support various application requirements, including those with custom schemas.\n\n### Review and Feedback\n\nThe Central DB Management Service will be periodically reviewed to ensure it meets the operational needs of the tenant applications. Feedback from development teams will be encouraged to iterate and improve the service.","GenTime":"2024-07-28 21:31:54"}
{"File Name":"paas-team-manual\/ADR017-cell-capacity-assignment.md","Context":"Context\\n=======\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\nDecision\\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n\n## Decision\n","Decision":"========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","tokens":428,"id":231,"Prediction":"Decision\n========\nTo ensure platform availability and memory capacity resilience across three zones in case of a single AZ failure, we will implement the following strategy:\n\n1. **Over-provisioning and Capacity Management**:\n   - Each zone will be provisioned with enough spare memory capacity to handle 50% more than its typical usage. This accounts for potential failover scenarios where the capacity must be absorbed by the remaining two zones.\n   - We will calculate the total memory needed for failover by summing up the quotas of all the organizations and ensuring that each zone can support 150% of its normal load.\n\n2. **Operational Monitoring and Scaling**:\n   - We will regularly monitor memory usage, both reserved and actual consumption, using the `show-cf-memory-usage` makefile target to ensure visibility into current memory standings.\n   - Whenever a new organization is added or an existing organization's quota is increased, we will reassess available capacity to ensure it aligns with our failover requirements.\n   - Deploy additional cells as needed to maintain the required memory headroom, ensuring ongoing support for failover scenarios without degradation of service.\n\n3. **Dynamic Adjustment**:\n   - Implement a policy of periodic review and adjustment of memory usage metrics to better understand typical versus maximum memory demands.\n   - Use these insights to fine-tune memory quota allocations and over-provisioning factors as necessary, based on real usage trends and growth patterns.\n\nBy adhering to these steps, we aim to maintain system robustness, ensure high availability, and provide seamless service even in the event of significant infrastructure outages.\n\nStatus\n======\nAccepted\n\nConsequences\n============\n* **Positive Outcomes**:\n  - Increased resilience and high availability in case of a zone failure.\n  - Better capacity planning and more efficient use of resources due to regular monitoring and dynamic scaling.\n  \n* **Operational Overheads**:\n  - Increased operational efforts in monitoring, deploying additional cells, and performing periodic reviews.\n  - Potential initial costs associated with deploying additional memory capacity to meet over-provisioning targets.\n\n* **Capacity Safeguards**:\n  - The platform will better withstand unexpected memory spikes and ensure smoother app performance during failover situations.\n  - Continuous assessment will help mitigate risks of capacity shortages and maintain service quality.","GenTime":"2024-07-28 21:32:14"}
{"File Name":"Maud\/0011-drains.md","Context":"## Context\\nDrains are an important aspect of any model, they are an essential boundary\\ncondition for cases such as biomass drains. Curently rate laws are only specified using\\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\\nIntroducing drains into Maud requires implementing this into the ODE system which\\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\\nTo specify drains we create a new class independent of reactions, despite the fact that\\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\\non varying enzyme concentrations, which we've established that drains do not have.\\nDrains are considered as priors to ensure that prior predictive checks are informative\\nof the experimental condition.\\n\n## Decision\n","Decision":"Drains will not be considered as a reaction class.\\n","tokens":164,"id":233,"Prediction":"## Decision\nWe will introduce a new class specifically for handling drains within the model. This class will be independent of the existing reaction classes within Maud, even though drains will still be incorporated into the overall ODE system. This approach allows us to clearly differentiate between enzymatic reactions and non-enzymatic drains, facilitating easier post-processing and more accurate assessment of enzyme concentration variations independent of drain effects.\n\n1. **New Class Definition:** Create a class in the Maud codebase designed to represent drains. This class will manage attributes and behaviors specific to drains, separate from existing reaction classes.\n\n2. **Integration with ODE System:** Ensure that the new drain class integrates seamlessly into the ODE system, modifying the equations as needed to incorporate drain effects without disrupting existing enzymatic process modeling.\n\n3. **Post-Processing Compatibility:** Adapt post-processing techniques to account for the new drain class, ensuring that analyses and visualizations correctly reflect the distinction between drains and reactions.\n\n4. **Priors and Predictive Checks:** Implement drains as priors within the model to enhance the prior predictive checks, ensuring they remain relevant and accurate within the experimental framework.\n\nThis decision aims to improve the clarity, maintainability, and accuracy of the model while accommodating the unique characteristics and requirements of drains.","GenTime":"2024-07-28 21:32:18"}
{"File Name":"Maud\/0018-backwards-differential-solver.md","Context":"## Context\\nTo determine the steady state conditions of the model, we require an ODE solver.\\nWe switched to the adjoint ODE solver for faster gradient evaluations,\\nhowever, recent use suggests that the solver is unstable and fails silently.\\n\n## Decision\n","Decision":"Reverting to the ode_bdf_tol() solver implemented in Stan still solves\\nstiff ODE problems but slightly slower. This is overlooked as the current\\nadjoint solver is too unstable to use and fails on larger systems. The current\\nexample files in the `tests\/data\/` folder work appropriately, however, larger\\nsystems fail with the step size approaching 0.\\nAn attempt was made to make the flux calculations in the ode more stable by\\nusing built in functions and converting products to sums of logs etc.\\nThis did not help the adjoint solver and hence this revertion was made.\\nFuture versions of Stan can easily be tested by reverting to the previous\\nadjoint solver specification and the input files will still accept the tolerances\\nfor the solver.\\n","tokens":51,"id":239,"Prediction":"## Decision\n\nRevert to the previously used ODE solver, which demonstrated stability and robustness in past evaluations. While the adjoint ODE solver provided faster gradient evaluations, its instability and silent failures cannot be tolerated in our steady state analysis. Ensuring reliable and accurate solution of the ODEs is paramount.\n\nAdditionally, initiate an investigation into the underlying causes of instability in the adjoint ODE solver. If feasible, contribute to or seek enhancements in the solver to potentially revisit its use in the future, armed with a more robust understanding and assurances of its stability.","GenTime":"2024-07-28 21:32:36"}
{"File Name":"Maud\/0004-python-package-versioning.md","Context":"## Context\\nWe need a simple way to manage our package version.\\n\n## Decision\n","Decision":"We use versioneer to do this for us.\\n","tokens":15,"id":241,"Prediction":"## Decision\nWe will implement a version management strategy using Semantic Versioning (SemVer). Semantic Versioning is simple, widely adopted, and provides clear guidelines about how to version packages. It utilizes a three-part version number: MAJOR.MINOR.PATCH, where:\n\n- MAJOR version changes indicate incompatible API changes,\n- MINOR version changes add functionality in a backwards-compatible manner, and\n- PATCH version changes include backwards-compatible bug fixes.\n\nAdditionally, we will use a version management tool such as `npm version` for Node.js projects or `bumpversion` for Python projects to automate and streamline the versioning process. This decision ensures that new versions are created and managed consistently, reducing potential errors and confusion over package versions.","GenTime":"2024-07-28 21:32:42"}
{"File Name":"offender-management-architecture-decisions\/0003-use-progressive-enhancement.md","Context":"## Context\\nThe service manual clearly states that teams should use progressive enhancement\\nwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement\\nThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/service-standard\/create-a-service-thats-simple),\\nwhich is about ensuring that all users can use the service successfully the\\nfirst time they try - including users with accessibility needs.\\nThe service manual is also clear that [internal services should be held to the\\nsame standard as public-facing services](https:\/\/www.gov.uk\/service-manual\/design\/services-for-government-users).\\nSome of the services for prison and probation staff which have been built over\\nthe last couple of years are not progressively enhanced. Without JavaScript\\nthey display no content.\\nSince these services are in a similar space to our work and have overlapping\\nuser bases with ours (although they are not the only existing services in this\\nspace), we have considered whether we should take a similar approach to them.\\n\n## Decision\n","Decision":"We will use progressive enhancement for all our user-facing applications.\\n","tokens":210,"id":263,"Prediction":"## Decision\n\nWe will adopt and strictly adhere to progressive enhancement principles in the development of our services. This decision aligns with the recommendations set forth in the service manual, ensuring that all users, including those with accessibility needs or those who have JavaScript disabled, can access and use our services successfully the first time they try.\n\nWe recognize that some existing services for prison and probation staff do not currently follow progressive enhancement, which results in a poor experience for users without JavaScript. Our approach will differ by prioritizing accessibility and inclusivity from the outset, following both service manual guidance and point 12 of the service standard.\n\nTo implement this decision:\n\n1. **Architecture and Design Alignment:** All new features and services will be designed with a progressive enhancement strategy, ensuring basic functionality using standard HTML and CSS, and enhancing the experience with JavaScript where applicable.\n  \n2. **Training and Awareness:** Team members will receive training on the importance and implementation of progressive enhancement to ensure a thorough understanding across the board.\n\n3. **Quality Assurance:** Ensure that all services pass accessibility checks and can function without JavaScript before deployment.\n\n4. **Review and Refactor:** Regular audits will be conducted on existing services to identify and refactor components that do not comply with progressive enhancement principles.\n\nBy adhering to these steps, we aim to provide an equitable and high-quality user experience for all users, setting our services apart in terms of accessibility and usability.","GenTime":"2024-07-28 21:33:58"}
{"File Name":"docspell\/0010_convert_image_files.md","Context":"# Context and Problem Statement\\nHow to convert image files properly to pdf?\\nSince there are thousands of different image formats, there will never\\nbe support for all. The most common containers should be supported,\\nthough:\\n- jpeg (jfif, exif)\\n- png\\n- tiff (baseline, single page)\\nThe focus is on document images, maybe from digital cameras or\\nscanners.\\n# Considered Options\\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\\nThere are no screenshots here, because it doesn't make sense since\\nthey all look the same on the screen. Instead we look at the files\\nproperties.\\n**Input File**\\nThe input files are:\\n```\\n$ identify input\/*\\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\\ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\\n```\\nSize:\\n- jfif.jpg 240k\\n- letter-en.jpg 467k\\n- letter-en.png 191k\\n- letter-en.tiff 4.0M\\n\n## Decision\n","Decision":"Tesseract.\\nTo not use more external tools, imagemagick and img2pdf are not\\nchosen, even though img2pdf shows the best results and is fastest.\\nPdfbox library would be the favorite, because results are good and\\nwith the [twelvemonkeys](https:\/\/github.com\/haraldk\/TwelveMonkeys)\\nlibrary there is support for many images. The priority is to avoid\\nmore external commands if possible.\\nBut since there already is a dependency to tesseract and it can create\\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\\nwith images can be converted to searchable PDFs with images. And text\\nextraction is required anyways.\\n","tokens":435,"id":273,"Prediction":"## Decision\n\nWe will use the `img2pdf` external command to convert image files to PDF. The primary reasons for this choice are:\n\n1. **Format Support**: `img2pdf` natively supports the most common image formats we need: JPEG (both JFIF and EXIF), PNG, and TIFF (baseline, single page). This ensures broad compatibility with our input requirements without needing extensive customization or format-specific handling.\n\n2. **Efficiency**: `img2pdf` is designed to be efficient with the conversion process. It can handle large image files and different color spaces effectively, which reduces resource consumption and processing time when handling high-resolution digital camera or scanner images.\n\n3. **Quality Preservation**: When converting images to PDF, `img2pdf` ensures that there are no intermediate lossy format conversions that could degrade image quality. Each image is converted while maintaining its original resolution and color profile, which is crucial for document images.\n\n4. **Ease of Use**: Integrating `img2pdf` as an external command is straightforward. It allows for batch processing multiple images, making the conversion of large volumes of files efficient and manageable through simple scripting or automation tools.\n\n5. **Minimal Dependencies**: Unlike some libraries that might require complex setups or dependencies, `img2pdf` can be easily installed and used as a standalone command-line tool, simplifying the deployment process.\n\nBy choosing `img2pdf`, we strike a balance between support for common image formats, efficiency, quality preservation, ease of use, and minimal dependencies, making it the most suitable option for our requirements.","GenTime":"2024-07-28 21:34:42"}
{"File Name":"docspell\/0001_components.md","Context":"# Context and Problem Statement\\nHow should the application be structured into its main components? The\\ngoal is to be able to have multiple rest servers\/webapps and multiple\\ndocument processor components working togehter.\\n# Decision Outcome\\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n\n## Decision\n","Decision":"The following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","tokens":75,"id":276,"Prediction":"## Decision\n\nThe application will be structured into the following main components to enable clear separation of concerns and scalability. These main modules\/components include:\n\n1. **REST Servers\/Web Apps**\n   - **Frontend Module**: This module will handle the user interface, including web pages and client-side logic.\n   - **Backend API Module**: This will handle incoming RESTful API requests, business logic, and communication with other services.\n\n2. **Document Processor Components**\n   - **Document Ingestion Module**: This component will be responsible for uploading and validating documents, ensuring they meet required standards before processing.\n   - **Document Analysis Module**: This component will handle the analysis and extraction of information from documents using various processing techniques, such as OCR (Optical Character Recognition) and NLP (Natural Language Processing).\n   - **Document Storage Module**: This module will manage the storage of processed documents and their metadata, providing easy access and retrieval.\n\n3. **Shared Services**\n   - **Authentication and Authorization Module**: This service will manage user authentication and authorization across both the web apps and document processors.\n   - **Notification Service**: This will handle sending notifications (e.g., emails, SMS) to users about the status of their documents or other events.\n   - **Logging and Monitoring Service**: This service will collect logs and monitor the health of various components for diagnostic and maintenance purposes.\n\n4. **Supporting Modules**\n   - **Database Access Layer**: This module will handle interactions with the database, providing an abstraction layer to ensure modularity and flexibility in case of database changes.\n   - **Configuration Management**: This will handle application configuration and environment-specific settings.\n   - **Common Utilities**: This includes common utility classes and functions that can be reused across different modules.\n\nBy structuring the application into these components, we achieve a modular architecture where functionalities are well-separated, which enhances maintainability and scalability. Additionally, it facilitates easier testing and deployment of individual components.","GenTime":"2024-07-28 21:34:59"}
{"File Name":"docspell\/0012_periodic_tasks.md","Context":"# Context and Problem Statement\\nCurrently there is a `Scheduler` that consumes tasks off a queue in\\nthe database. This allows multiple job executors running in parallel\\nracing for the next job to execute. This is for executing tasks\\nimmediately \u2013 as long as there are enough resource.\\nWhat is missing, is a component that maintains periodic tasks. The\\nreason for this is to have house keeping tasks that run regularily and\\nclean up stale or unused data. Later, users should be able to create\\nperiodic tasks, for example to read e-mails from an inbox or to be\\nnotified of due items.\\nThe problem is again, that it must work with multiple job executor\\ninstances running at the same time. This is the same pattern as with\\nthe `Scheduler`: it must be ensured that only one task is used at a\\ntime. Multiple job exectuors must not schedule a perdiodic task more\\nthan once. If a periodic tasks takes longer than the time between\\nruns, it must wait for the next interval.\\n# Considered Options\\n1. Adding a `timer` and `nextrun` field to the current `job` table\\n2. Creating a separate table for periodic tasks\\n\n## Decision\n","Decision":"The 2. option.\\nFor internal housekeeping tasks, it may suffice to reuse the existing\\n`job` queue by adding more fields such that a job may be considered\\nperiodic. But this conflates with what the `Scheduler` is doing now\\n(executing tasks as soon as possible while being bound to some\\nresource limits) with a completely different subject.\\nThere will be a new `PeriodicScheduler` that works on a new table in\\nthe database that is representing periodic tasks. This table will\\nshare fields with the `job` table to be able to create `RJob` records.\\nThis new component is only taking care of periodically submitting jobs\\nto the job queue such that the `Scheduler` will eventually pick it up\\nand run it. If the tasks cannot run (for example due to resource\\nlimitation), the periodic scheduler can't do nothing but wait and try\\nnext time.\\n```sql\\nCREATE TABLE \"periodic_task\" (\\n\"id\" varchar(254) not null primary key,\\n\"enabled\" boolean not null,\\n\"task\" varchar(254) not null,\\n\"group_\" varchar(254) not null,\\n\"args\" text not null,\\n\"subject\" varchar(254) not null,\\n\"submitter\" varchar(254) not null,\\n\"priority\" int not null,\\n\"worker\" varchar(254),\\n\"marked\" timestamp,\\n\"timer\" varchar(254) not null,\\n\"nextrun\" timestamp not null,\\n\"created\" timestamp not null\\n);\\n```\\nPreparing for other features, at some point periodic tasks will be\\ncreated by users. It should be possible to disable\/enable them. The\\nnext 6 properties are needed to insert jobs into the `job` table. The\\n`worker` field (and `marked`) are used to mark a periodic job as\\n\"being worked on by a job executor\".\\nThe `timer` is the schedule, which is a\\n[systemd-like](https:\/\/man7.org\/linux\/man-pages\/man7\/systemd.time.7.html#CALENDAR_EVENTS)\\ncalendar event string. This is parsed by [this\\nlibrary](https:\/\/github.com\/eikek\/calev). The `nextrun` field will\\nstore the timestamp of the next time the task would need to be\\nexecuted. This is needed to query this table for the newest task.\\nThe `PeriodicScheduler` works roughly like this:\\nOn startup:\\n- Remove stale worker values. If the process has been killed, there\\nmay be marked tasks which must be cleared now.\\nMain-Loop:\\n0. Cancel current scheduled notify (see 4. below)\\n1. get next (= earliest & enabled) periodic job\\n2. if none: stop\\n3. if triggered (= `nextrun <= 'now'`):\\n- Mark periodic task. On fail: goto 1.\\n- Submit new job into the jobqueue:\\n- Update `nextrun` field\\n- Check for non-final jobs of that name. This is required to not\\nrun the same periodic task multiple times concurrently.\\n- if exist: goto 4.\\n- if not exist: submit job\\n- Unmark periodic task\\n4. if future\\n- schedule notify: notify self to run again next time the task\\nschedule triggers\\n","tokens":257,"id":279,"Prediction":"## Decision\n\nOption 2: Create a separate table for periodic tasks.\n\n### Rationale:\n\n1. **Separation of Concerns**: By creating a separate table, we clearly differentiate between immediate and periodic tasks. This promotes clarity and maintainability of the database schema.\n\n2. **Flexibility**: A dedicated periodic tasks table can be designed to accommodate future requirements specific to periodic tasks without overloading the current job table.\n\n3. **Simplicity**: Introducing additional fields like `timer` and `nextrun` to the current job table could lead to a more complex schema and querying logic. A separate table keeps things simple and focused.\n\n4. **Scalability**: As the system scales, having a dedicated table for periodic tasks allows better optimization and indexing strategies tailored for periodic task workflows.\n\n5. **Concurrency Handling**: It will be easier to manage the concurrency control for periodic tasks separately, ensuring that no two job executors schedule the same task simultaneously. This can be achieved by using database-level locks or other concurrency mechanisms specifically suited for periodic tasks.\n\n6. **Future-Proofing**: Creating a separate table now ensures that future enhancements or changes to periodic task handling can be done independently of the immediate task processing system, minimizing risk and impact on overall system stability.","GenTime":"2024-07-28 21:35:08"}
{"File Name":"docspell\/0004_iso8601vsEpoch.md","Context":"# Context and Problem Statement\\nThe question is whether the REST Api should return an ISO8601\\nformatted string in UTC timezone, or the unix time (number of\\nmilliseconds since 1970-01-01).\\nThere is quite some controversy about it.\\n- <https:\/\/stackoverflow.com\/questions\/47426786\/epoch-or-iso8601-date-format>\\n- <https:\/\/nbsoftsolutions.com\/blog\/designing-a-rest-api-unix-time-vs-iso-8601>\\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\\nis the better readability. But elm folks are on the other side:\\n- <https:\/\/package.elm-lang.org\/packages\/elm\/time\/1.0.0#iso-8601>\\n- <https:\/\/package.elm-lang.org\/packages\/rtfeldman\/elm-iso8601-date-strings\/latest\/>\\nOne can convert from an ISO8601 date-time string in UTC time into the\\nepoch millis and vice versa. So it is the same to me. There is no less\\ninformation in a ISO8601 string than in the epoch millis.\\nTo avoid confusion, all date\/time values should use the same encoding.\\n# Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n\n## Decision\n","Decision":"I go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","tokens":337,"id":280,"Prediction":"Decided to represent all date\/time values as Unix timestamps (number of milliseconds since 1970-01-01) in the REST API.\n\n### Rationale:\n- **Ease of Calculation**: The Elm application frequently performs calculations with date and time values (e.g., rendering current waiting times). These calculations are more straightforward and efficient when operating with numeric Unix timestamps, as they don't require parsing of strings.\n- **Consistency with Elm Community**: Given that the client-side application is developed using Elm, adopting the Unix timestamp format aligns with the community's preference, which can streamline development and reduce potential errors.\n- **Flexibility**: Although ISO8601 strings provide better readability, Unix timestamps carry the same information and can be easily converted to ISO8601 format if needed, ensuring flexibility for future needs or integrations.\n  \nEvery timestamp\/date-time value will be transferred as Unix timestamp, ensuring consistency throughout the API.","GenTime":"2024-07-28 21:35:11"}
{"File Name":"docspell\/0011_extract_text.md","Context":"# Context and Problem Statement\\nWith support for more file types there must be a way to extract text\\nfrom all of them. It is better to extract text from the source files,\\nin contrast to extracting the text from the converted pdf file.\\nThere are multiple options and multiple file types. Again, most\\npriority is to use a java\/scala library to reduce external\\ndependencies.\\n# Considered Options\\n\n## Decision\n","Decision":"- MS Office files: POI library\\n- Open Document files: Tika, but integrating the few source files that\\nmake up the open document parser. Due to its huge dependency tree,\\nthe library is not added.\\n- PDF: Apache PDFBox. I know this library better than itext.\\n","tokens":86,"id":282,"Prediction":"## Decision\n\nAfter evaluating various options for extracting text from a diverse range of file types, the decision is made to implement Apache Tika, a robust open-source library available in Java that supports a broad spectrum of file formats.\n\n### Rationale:\n1. **Diverse File Type Support**: Apache Tika can handle a large variety of file types, such as Word documents, PDF, HTML, various image formats, and many more, which aligns perfectly with the requirement of supporting multiple file types.\n2. **Java\/Scala Compatibility**: As Apache Tika is a library primarily written in Java, it seamlessly integrates with both Java and Scala applications, reducing the need for external dependencies and allowing for smoother development processes.\n3. **Active Community and Maintenance**: Tika has an active developer community and regular updates, ensuring that it keeps up with new file formats and security improvements.\n4. **Simplicity and Efficiency**: The library offers a simple API to extract metadata and text content, enhancing development efficiency and ease of use.\n5. **Security Considerations**: By using a mature and well-maintained library like Tika, we can mitigate potential security vulnerabilities that could arise from handling file parsing in-house.\n\n### Implementation Approach:\n- **Library Integration**: Integrate Apache Tika into the existing codebase as the primary mechanism for text extraction.\n- **Customization and Tuning**: Configure Apache Tika parsers and handlers to optimize for the most frequently encountered file types.\n- **Fallback Mechanism**: Implement fallback strategies for any edge cases or unsupported file formats, such as basic text extraction or error handling\/logging mechanisms.\n\n### Review and Future Considerations:\n- **Performance Monitoring**: Continuously monitor the performance and accuracy of text extraction, making adjustments as needed.\n- **Expansion and Update**: Keep track of any new file types or formats we start encountering and ensure that Apache Tika is updated accordingly, considering the addition of supplementary tools if required.\n\nBy choosing Apache Tika, we are ensuring a robust, scalable, and maintainable text extraction process while minimizing external dependencies and aligning with our technology stack preferences.","GenTime":"2024-07-28 21:35:23"}
{"File Name":"docspell\/0009_convert_office_docs.md","Context":"# Context and Problem Statement\\nHow can office documents, like `docx` or `odt` be converted into a PDF\\nfile that looks as much as possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [Apache POI](https:\/\/poi.apache.org) together with\\n[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)\\nlibrary\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [abiword](https:\/\/www.abisource.com\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nTo choose an option, some documents are converted to pdf and compared.\\nOnly the formats `docx` and `odt` are considered here. These are the\\nmost used formats. They have to look well, if a `xlsx` or `pptx`\\ndoesn't look so great, that is ok.\\nHere is the native view to compare with:\\nODT:\\n{{ figure(file=\"example-odt-native.jpg\") }}\\n\n## Decision\n","Decision":"Unoconv.\\nThe results from `unoconv` are really good.\\nAbiword also is not that bad, it didn't convert the chart, but all\\nfont markup is there. It would be great to not depend on something as\\nbig as libreoffice, but the results are so much better.\\nAlso pandoc deals very well with DOCX files (using the `context`\\nengine). The only thing that was not rendered was the embedded chart\\n(like abiword). But all images and font styling was present.\\nIt will be a configurable external command anyways, so users can\\nexchange it at any time with a different one.\\n","tokens":288,"id":284,"Prediction":"**Decision**\n\nAfter evaluating the options based on the quality of the PDF output and considering the requirement that the solution must be free software, we have decided to use **[Unoconv](https:\/\/github.com\/unoconv\/unoconv) as an external tool** for converting `docx` and `odt` files to PDF.\n\n### Reasons for Choosing Unoconv:\n\n1. **Quality of Output**: After testing various documents, Unoconv consistently produced PDF files that closely resembled the original `docx` and `odt` documents in terms of formatting and layout.\n\n2. **Compatibility with Formats**: Unoconv supports both `docx` and `odt` formats, ensuring that the most frequently used office document types can be converted effectively.\n\n3. **Free Software**: Unoconv is open-source software that aligns with the principles of Docspell being a free software project.\n\n4. **Integration Capability**: Although it is an external command, Unoconv can be easily integrated into a Java application through well-defined command-line interfaces and process management in Java.\n\n5. **Community and Documentation**: Unoconv has an active community and comprehensive documentation, making it easier to troubleshoot any issues and leverage community support.\n\nTherefore, for the conversion of office documents to PDF, Unoconv will be incorporated into the Docspell software workflow.","GenTime":"2024-07-28 21:35:30"}
{"File Name":"docspell\/0016_custom_fields.md","Context":"# Context and Problem Statement\\nUsers want to add custom metadata to items. For example, for invoices\\nfields like `invoice-number` or `total`\/`amount` make sense. When\\nusing a pagination stamp, every item gets a pagination number.\\nThis is currently not possible to realize in docspell. But it is an\\nessential part when organizing and archiving documents. It should be\\nsupported.\\n# Considered Options\\n\n## Decision\n","Decision":"- values are strings at the database\\n- values are strings when transported from\/to server\\n- client must provide the correct formatted strings per type\\n- numeric: some decimal number\\n- money: decimal number\\n- text: no restrictions\\n- date: a local date as iso string, e.g. `2011-10-09`\\n- bool: either `\"true\"` or `\"false\"`, case insensitive\\n","tokens":90,"id":285,"Prediction":"## Decision\n\nTo support the addition of custom metadata to items, we will introduce a flexible metadata system in docspell. This system will allow users to define custom fields for different types of items, such as invoices or paginated documents. These custom fields will be:\n\n1. **User-Defined Metadata Fields**: Allow users to create custom metadata fields that can be attached to any item type. This will support various types of data such as text, numbers, dates, and more.\n2. **Template-Based Metadata for Common Items**: Provide pre-defined templates for common item types (e.g., invoices) with commonly used fields like `invoice-number`, `total`, and `amount`. Users can choose to use these templates or modify them as needed.\n3. **Pagination Support**: Implement a system to automatically generate and attach pagination numbers to items that require them, ensuring each item can be uniquely identified within a paginated set.\n\nThe implementation will ensure that the metadata system is flexible and extensible to adapt to future requirements and different organizational needs.","GenTime":"2024-07-28 21:35:34"}
{"File Name":"docspell\/0007_convert_html_files.md","Context":"# Context and Problem Statement\\nHow can HTML documents be converted into a PDF file that looks as much\\nas possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nNative (firefox) view:\\n{{ figure(file=\"example-html-native.jpg\") }}\\nI downloaded the HTML file to disk together with its resources (using\\n*Save as...* in the browser).\\n\n## Decision\n","Decision":"wkhtmltopdf.\\nIt shows the best results.\\n","tokens":176,"id":287,"Prediction":"## Decision\n\nAfter evaluating the options, the following decision has been made:\n\n**Selected Option: [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command**\n\n### Reasons:\n\n1. **Output Quality:**\n   - **wkhtmltopdf** excels in generating PDF files that closely resemble the original HTML, including CSS styling and embedded images. It is highly regarded for preserving the look and feel of the web pages.\n\n2. **Ease of Use:**\n   - The **wkhtmltopdf** tool is straightforward to use via command line and integrates well into automated workflows. It also supports complex HTML and CSS, JS execution, and can handle large pages effectively.\n\n3. **Java Integration:**\n   - While **wkhtmltopdf** is an external command and not purely Java-based, it can still be easily invoked from Java applications using process execution features of Java (`ProcessBuilder` or similar).\n\n4. **Free and Open Source:**\n   - **wkhtmltopdf** is free and open source software, which aligns with the principles of Docspell. It can be freely distributed and used within the project.\n\n5. **Alternatives Considered:**\n   - **Pandoc**: While versatile for different document conversions, it does not excel in converting HTML to PDF with high fidelity to the original layout.\n   - **Unoconv**: Primarily focused on converting between LibreOffice-compatible formats, it may not handle complex HTML\/CSS as well as wkhtmltopdf.\n\nFor these reasons, **wkhtmltopdf** has been chosen as the tool for converting HTML documents to PDF in the Docspell project. \n\n### Implementation:\n\n- Integrate **wkhtmltopdf** using a command execution approach from the Java application.\n- Ensure the command is configurable to allow adjusting parameters like page size, margins, etc.\n- Verify and handle edge cases where conversion might not be perfect, possibly providing fallback methods if necessary.\n\nThis decision ensures high-quality PDF outputs while maintaining consistency with Docspell's open-source and free software ethos.","GenTime":"2024-07-28 21:35:45"}
{"File Name":"opg-refunds\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":294,"Prediction":"## Decision\n\nTo comprehensively document and manage the architectural decisions, we will adopt the use of an Architectural Decision Record (ADR) format. Each significant architectural decision made during the lifecycle of the project will be formally recorded in an ADR document. This format will ensure that decisions are well-documented, easily accessible, and can be referenced or revisited as needed.\n\nThe key elements of our ADR process are as follows:\n\n1. **Template Standardization:** We will use a standardized ADR template to ensure consistency across all decision records. This template will include sections such as Context, Decision, Status, Consequences, Pros and Cons, and Alternatives Considered.\n   \n2. **Version Control:** All ADR documents will be stored in a version-controlled repository (e.g., Git). This will allow us to track changes over time and ensure that the decision-making process is transparent and auditable.\n\n3. **Accessibility:** ADRs will be easily accessible to all project stakeholders, including developers, architects, project managers, and other relevant parties. This will ensure that everyone involved in the project has visibility into the architectural decisions being made.\n\n4. **Regular Review:** ADRs will be periodically reviewed to ensure that they remain relevant and reflect the current state of the project. If a decision becomes obsolete or needs to be revisited, a new ADR will be created to supersede the old one.\n\n5. **Integration with Project Workflows:** Recording and reviewing ADRs will be integrated into our existing project workflows, such as design reviews, sprint planning, and retrospectives. This will ensure that the process of documenting decisions is not an afterthought but an integral part of our project management practices.\n\nBy adopting this approach, we aim to create a clear and maintainable record of architectural decisions that aids in project alignment, knowledge sharing, and informed decision-making throughout the project's duration.","GenTime":"2024-07-28 21:36:15"}
{"File Name":"nso.aurora\/MicroserviceDesgin.md","Context":"## Context\\nTo ensure scalability, reliability and performance, microservice architecture should be used\\n\n## Decision\n","Decision":"All agreed with this decision\\n","tokens":18,"id":302,"Prediction":"## Decision\nWe have decided to adopt a microservice architecture for the application. This decision is driven by our need to ensure scalability, reliability, and performance. By decomposing the application into a set of loosely coupled, independently deployable services, we aim to achieve greater flexibility in scaling, enhanced fault isolation, and improved development velocity.\n\n### Key Points:\n1. **Scalability**: \n   - The microservice architecture allows each service to be scaled independently based on demand.\n   - This helps in efficiently utilizing resources and maintaining performance even during peak loads.\n\n2. **Reliability**:\n   - Isolation of services ensures that if one service fails, it does not affect the entire system.\n   - This improves the overall reliability and availability of the application.\n\n3. **Performance**:\n   - Services can be optimized and deployed independently, allowing for better performance tuning.\n   - Reduced contention as each service operates independently, thus minimizing bottlenecks.\n\n4. **Development and Deployment**:\n   - Smaller, focused teams can work independently on different services, speeding up development cycles.\n   - Continuous deployment becomes more manageable, as each service can be deployed without disrupting the others.\n\n5. **Technology Stack Flexibility**:\n   - Different services can use different technologies best suited for their specific needs, leading to optimal performance and productivity.\n\nBy implementing a microservice architecture, we are positioning our application to meet current and future demands effectively, ensuring it remains robust, responsive, and agile in a dynamic environment.","GenTime":"2024-07-28 21:37:00"}
{"File Name":"nso.aurora\/MultipleQueues.md","Context":"## Context\\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\\n\n## Decision\n","Decision":"The decision is to introduce another queue for Order Management\\n","tokens":25,"id":306,"Prediction":"## Decision\nPrioritize the handling and processing of messages from Order Management over messages from Rating Manager and Recommendation Manager. Implement a message prioritization mechanism in the messaging system that assigns higher priority to Order Management messages, ensuring that these messages are processed first. This can be achieved by setting up priority queues or using a similar prioritization strategy in the messaging infrastructure to ensure critical order-related communications are not delayed.","GenTime":"2024-07-28 21:37:15"}
{"File Name":"nso.aurora\/AsynchronousMessages.md","Context":"## Context\\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\\n\n## Decision\n","Decision":"Yes it can be asynchronous because we don't need to wait for an ack before sending the next one.\\n","tokens":22,"id":307,"Prediction":"## Decision\n\nYes, the messages exchanged between Order Management, Rating Manager, and Recommendation Manager will be asynchronous. This decision leverages the advantages of asynchronous communication, such as improved system scalability and decoupling of services, which enhances fault tolerance and parallel processing capabilities. This way, each service can perform its tasks independently without waiting for immediate responses, which contributes to overall system resilience and efficiency. To implement this, an appropriate message broker or queue system (e.g., RabbitMQ, Kafka, or AWS SQS) will be integrated to handle the asynchronous messaging reliably.","GenTime":"2024-07-28 21:37:16"}
{"File Name":"hodder\/0002-use-docker-compose-v-2.md","Context":"## Context\\nPeople are very used to traditional layered architectures and request\/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\\n\n## Decision\n","Decision":"We will use a docker-compose.yml (v.2) as the primary entry point of the project\\n","tokens":92,"id":311,"Prediction":"## Decision\nWe will adopt an event-driven architecture to better manage the complexity of our distributed systems and to promote greater flexibility and scalability. By utilizing asynchronous messaging and event handling, we can decouple services more effectively and avoid the constraints of tightly coupled request\/response cycles. \n\n### Key Actions:\n1. **Event Bus Implementation**: Deploy a reliable event bus (e.g., Apache Kafka, RabbitMQ) to facilitate the communication between different services. This will enable us to manage events in a more scalable and resilient manner.\n   \n2. **Microservices**: Break down the application into smaller, independent microservices that can listen to events and act upon them. Each service will focus on a specific domain or functionality, making the system easier to maintain and extend.\n\n3. **Event Sourcing**: Implement event sourcing where applicable to ensure that the state changes are stored as a sequence of events, allowing for easier auditing and testing.\n\n4. **Local Development Environment**: Provide a Docker-based local development setup that allows developers to quickly start and test the system on their local machines. This will ensure that initial attempts to work with the system are as smooth as possible.\n\n5. **Extensive Documentation and Tutorials**: Create comprehensive and accessible documentation, including tutorials and example projects, to help developers quickly understand and get started with the event-driven architecture.\n\n6. **Performance Monitoring**: Include tools for monitoring the performance and health of the system (e.g., Prometheus, Grafana) to help in quickly identifying and resolving issues in a distributed environment.\n\n7. **Education and Training**: Provide training sessions and resources to educate the team on the benefits and trade-offs of event-driven architecture compared to traditional layered architecture.\n\nBy adopting this approach, we aim to address the complexities of distributed systems, make them easier to extend, and facilitate a quick start for developers with a well-supported and documented local setup.","GenTime":"2024-07-28 21:37:34"}
{"File Name":"verify-event-store-schema\/0003-database-migrations-should-be-designed-for-zero-downtime.md","Context":"## Context\\nSee also ADR 0002 \"Database migrations are standalone releases\"\\nAs our system is designed for zero downtime, we have to be careful that\\nwe don't change the database in a way that causes production issues\\n\n## Decision\n","Decision":"Where possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https:\/\/www.postgresql.org\/docs\/current\/static\/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https:\/\/www.citusdata.com\/blog\/2018\/02\/15\/when-postgresql-blocks\/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n","tokens":48,"id":317,"Prediction":"## Decision\nTo maintain zero downtime and ensure smooth deployment, we will adopt the following strategies for database schema changes:\n\n1. **Backward-Compatible Schema Changes**: All schema changes must be backward-compatible. This means new columns should be nullable or have default values, and existing columns or tables should not be dropped or renamed in the initial deployment of the change.\n\n2. **Phased Deployment Approach**: Implement schema changes in multiple phases.\n   - **Phase 1**: Deploy additional columns or tables as required.\n   - **Phase 2**: Update the application code to use these columns\/tables. Ensure old and new schemas can be read and written by the application.\n   - **Phase 3**: Once all application instances are updated and stable, clean up unused columns or tables.\n\n3. **Feature Toggles**: Utilize feature toggles to enable new features that depend on schema changes in a controlled manner to allow for quick rollback if necessary.\n\n4. **Automated Rollbacks**: Develop automated rollback scripts for database changes to quickly revert to the previous stable state if an issue is detected.\n\n5. **Thorough Testing**: Ensure all schema changes go through rigorous automated testing and staging environments to simulate production and identify potential issues.\n\n6. **Documentation and Communication**: Clearly document the schema change process, and communicate with all stakeholders to coordinate the deployment phases.\n\n7. **Monitoring and Alerting**: Enhance monitoring and alerting mechanisms to detect any potential issues early during the schema change deployment.\n\nBy adhering to these strategies, we aim to safely evolve our database schema while maintaining zero downtime in our production environment.","GenTime":"2024-07-28 21:37:55"}
{"File Name":"re-build-systems\/0002-use-containers-and-multiple-boxes.md","Context":"## Context\\nThe aim of this project is to allow teams to quickly create their own Jenkins\\nplatform, running on AWS.\\nJenkins has a master\/agent architecture, where a single master node can trigger\\njobs running on multiple agents. This decision concerns how we deploy, configure\\nand manage both the master and the agents.\\n\n## Decision\n","Decision":"We will make both the master and the agents Docker containers, running on\\nseparate managed EC2 instances. We will provision and manage these directly for\\nnow, without using an orchestration service like ECS or Kubernetes.\\n### Use of separate instances\\nHaving separate instances for the master and the workers increases security by\\nmaking it impossible for code running in worker jobs to affect the master.\\nIn addition, this allows for teams to increase capacity by adding extra worker\\nboxes as required.\\n### Use of Docker for master\\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\\nteams' abilities to quickly respond when new Jenkins versions are released in\\nresponse to security vulnerabilities, for example.\\nIn addition, using Docker means that the configuration can be kept in the git\\nrepository along with the rest of the code, rather than managed via the Jenkins\\nUI.\\n### Use of Docker for agents\\nRunning the workers as Docker containers allows isolation of each job, ensuring\\nthat each job starts from a known state, making it possible to target a specific\\nconfiguration via the Jenkinsfile, and increasing security by making it\\nimpossible for the job to affect the underlying VM.\\n### Not using orchestration frameworks\\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\\nsimple enough that it is not worth introducing the added complexity for this\\nproject.\\n","tokens":68,"id":319,"Prediction":"## Decision\n\nThe Jenkins master and its agents will be deployed and managed using AWS services, specifically leveraging the following architecture and tools:\n\n1. **AWS Elastic Beanstalk for Jenkins Master**: \n   - The Jenkins master node will be deployed on AWS Elastic Beanstalk, utilizing its capability to handle deployment, scaling, and management. Elastic Beanstalk simplifies the process by automatically managing the infrastructure, load balancing, and scaling requirements.\n\n2. **EFS for Jenkins Master Storage**:\n   - AWS Elastic File System (EFS) will be used for persistent storage of Jenkins master data, including job definitions, configurations, and plugins. EFS provides a scalable and resilient storage solution, ensuring data persistence across restarts and deployments.\n\n3. **EC2 Instances for Jenkins Agents**:\n   - Jenkins agents will be deployed on Amazon EC2 instances. These instances will be dynamically provisioned and scaled based on workload demands using Auto Scaling groups. Each agent will be configured to connect to the Jenkins master via the appropriate Jenkins agent protocols.\n\n4. **CloudFormation Templates**:\n   - AWS CloudFormation templates will be used to define and provision all necessary AWS resources as code, ensuring a repeatable and consistent deployment process. These templates will manage the setup of Elastic Beanstalk for the Jenkins master, EFS for storage, and EC2 instances for the Jenkins agents.\n\n5. **IAM Roles and Security Groups**:\n   - Proper IAM roles and policies will be established to ensure secure access and operations. Security Groups will be configured to manage network access and isolate the Jenkins environment from other AWS resources as required.\n\n6. **Automated Configuration Management**:\n   - Configuration of the Jenkins master and agents will be managed using infrastructure-as-code principles and automated scripts to ensure environments are reproducible and consistency in setup. This will include necessary plugins, job configurations, and other custom settings required by teams.\n\n7. **Monitoring and Logging**:\n   - AWS CloudWatch will be used for monitoring system health, performance metrics, and logs. CloudWatch Alarms will be configured to notify the operations team of any significant issues or performance bottlenecks.\n\n8. **Backup and Recovery**:\n   - Regular backups of Jenkins master configuration and job data will be configured using AWS Backup or a similar solution. This ensures the ability to recover quickly in case of a disaster.\n\nBy adopting this architecture, teams will be able to quickly create and manage their own Jenkins environments on AWS, leveraging AWS's scalability, reliability, and robust set of services. This setup will ensure a flexible, consistent, and secure deployment that meets enterprise-grade standards.","GenTime":"2024-07-28 21:38:05"}
{"File Name":"docnow-infra\/0002-use-terraform-workspaces-for-environments.md","Context":"## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n\n## Decision\n","Decision":"Use workspaces to manage multiple environments.\\n","tokens":129,"id":328,"Prediction":"## Decision\n\nWe have decided to implement the environment management for our Terraform setups using Terraform Workspaces rather than multiple directories. This decision allows us to leverage a DRY (Don't Repeat Yourself) approach, which minimizes the redundancy in our Terraform configurations and maintains a cleaner and more maintainable codebase. By utilizing Terraform Workspaces, we can streamline the process of switching between different environments (staging, production, etc.) and ensure consistency across these environments.\n\nThis approach aligns with the recommended practices outlined by Terraform, providing a more effective way to manage and scale our infrastructure across multiple environments. Thus, we will create and maintain logical workspaces within a single Terraform configuration for each environment, facilitating efficient environment management and operations.","GenTime":"2024-07-28 21:38:37"}
{"File Name":"hospitalrun\/redux_to_react_query.md","Context":"# Migrate to React Query\/Context API\/Hooks from Redux\\n## Context\\nCurrently, HospitalRun Frontend uses [redux](https:\/\/react-redux.js.org\/) to manage async data\\nfetching through [redux thunk](https:\/\/github.com\/reduxjs\/redux-thunk).\\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\\nsince it always goes back to PouchDB\/CouchDB for the most recent data. Redux code is often verbose and\\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https:\/\/redux-toolkit.js.org\/),\\nthe application still has a lot of boilerplate. Due to the limited amount of global application\\nstate, the desire to reduce boilerplate redux code, and new\/other libraries available,\\nRedux is no longer the correct tool.\\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\\ndifficult to isolate testing just to the specific component. For components that save data, it\\nis difficult to mock the actions that are dispatched.\\n[react-query](https:\/\/github.com\/tannerlinsley\/react-query) is a library for \"handling fetching,\\ncaching, and updating asynchronous data in React\". This library has become a popular replacement\\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\\nstore.\\nFor the few uses cases that require global application state (i.e. session information),\\nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from the\\nReact library. The context API is designed for the use case of sharing state across\\ncomponents.\\n\n## Decision\n","Decision":"HospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\\ncontext api to manage shared application state such as user information, and hooks for sharing\\ncode and business logic.\\n","tokens":396,"id":334,"Prediction":"## Decision\n\n### Migrate from Redux to React Query and Context API\n\nBased on the context, the following steps will be taken to migrate the HospitalRun frontend from Redux to React Query and Context API:\n\n1. **Use React Query for Async Data Fetching:**\n   - Implement React Query to handle all asynchronous data fetching and data management. This will replace the current logic of managing async data using Redux and redux-thunk.\n   - Benefits:\n     - Reduced boilerplate code compared to Redux.\n     - Improved data fetching, caching, and synchronization out-of-the-box.\n     - Enhanced developer experience with built-in tools for handling server state.\n\n2. **Adopt React Context API for Global State Management:**\n   - Use React Context API to manage and share the minimal global state (e.g., session information, user authentication) across components.\n   - Benefits:\n     - Simplifies state management by avoiding the complexity and boilerplate of Redux.\n     - Provides a more straightforward and React-compatible way to handle state sharing across components.\n\n3. **Refactor Components:**\n   - Update components that currently connect to the Redux store to use React Query and the Context API.\n   - Ensure data fetching logic is moved to custom hooks or directly within components using React Query.\n   - Replace Redux selectors and dispatches with Context API consumers and context dispatch functions where needed for global state.\n\n4. **Update Testing Strategy:**\n   - Simplify testing by removing the need for a mock Redux store.\n   - Test components in isolation without dependency on Redux, using mocks and stubs as necessary for React Query and context.\n\n5. **Remove Redux and Related Dependencies:**\n   - Gradually remove Redux, redux-thunk, and Redux Toolkit from the project dependencies once all related code is migrated.\n   - Clean up any redundant Redux-related files and configurations.\n\nBy transitioning to React Query and React Context API, HospitalRun frontend will achieve a more efficient, maintainable, and testable codebase, aligned with modern React development practices.","GenTime":"2024-07-28 21:38:59"}
{"File Name":"twig-infrastructure\/0003-start-with-aws-codepipeline-and-codebuild.md","Context":"## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\\nand put newer approaches to the test.\\n\n## Decision\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":40,"id":350,"Prediction":"## Decision\nAfter careful consideration, we have decided to migrate Twig from the older riglet flavor to the newer, simpler riglet flavor. This decision is made with the intention of leveraging improved performance, enhanced maintainability, and the inclusion of modern development approaches.\n\nKey points supporting this decision include:\n\n1. **Performance Improvements:** The newer riglet flavor is optimized for better performance and resource management.\n2. **Simplicity:** It offers a more streamlined and simplified architecture, which will ease the development and deployment processes.\n3. **Modern Approaches:** The newer flavor incorporates updated methodologies and technologies that are aligned with current best practices.\n4. **Future-Proofing:** By moving to a more current platform, we position the system to better accommodate future updates and scalability needs.\n\nOverall, the migration is expected to bring substantial benefits to Twig and aligns with our strategic direction towards modernization and efficiency. The transition plan will include thorough testing and phased deployment to ensure minimal disruption to ongoing operations.","GenTime":"2024-07-28 21:39:56"}
{"File Name":"twig-infrastructure\/0005-build-and-deploy-pipelines-separate.md","Context":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n\n## Decision\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":351,"Prediction":"## Decision\n\nDecouple the build and deploy stages into separate pipelines.\n\n### Justification\n1. **Efficiency**: Separating the build and deploy stages allows for more efficient use of time and resources, especially for projects with long-running build steps.\n2. **Flexibility**: Configuration changes often do not necessitate a full rebuild. By having separate pipelines, we can make deployment-specific adjustments and redeploy without triggering a complete build process.\n3. **Reduced Downtime**: Decoupled pipelines can minimize downtime by only re-running essential stages, rather than incurring the overhead of a full rebuild.\n4. **Error Isolation**: Issues can be more easily isolated to either the build or deploy stage, making troubleshooting and resolution faster.\n5. **Incremental Improvements**: Changes to the deployment process or configuration can be tested and deployed independently of the build process, enabling faster iteration and continuous improvement.\n\n### Implementation Steps\n1. **Create Separate Pipelines**: Establish a dedicated build pipeline and a dedicated deploy pipeline.\n   - **Build Pipeline**: Executes all tasks related to compiling the code, running tests, and creating deployable artifacts.\n   - **Deploy Pipeline**: Manages tasks associated with deploying these artifacts, including applying configuration changes as needed.\n2. **Artifact Storage**: Ensure that build artifacts are stored in a central repository or artifact store that the deploy pipeline can access.\n3. **Trigger Mechanism**: Set up appropriate triggers to run the deploy pipeline upon completion of the build pipeline or in response to relevant configuration changes.\n4. **Environment Configuration**: Ensure the deploy pipeline includes steps for handling environment-specific configurations and secrets.\n5. **Documentation and Training**: Update documentation to reflect the new pipeline structure and provide training to the team to ensure a smooth transition.\n\nBy implementing these changes, the build and deploy processes can be made more efficient and manageable, especially for large projects with demanding build requirements.","GenTime":"2024-07-28 21:40:00"}
{"File Name":"Marain.Instance\/0001-marain-service-list-and-instance-manifests.md","Context":"## Context\\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\\n\n## Decision\n","Decision":"The `Marain.Instance` repo (this repo) includes a master service list, `Solutions\/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\\n","tokens":79,"id":357,"Prediction":"## Decision\nTo formalize and unambiguously define the component parts of a Marain instance, the following decisions have been made:\n\n1. **Instance Configuration Definition:**\n   - A Marain instance configuration will be specified using a standardized configuration file format (e.g., YAML or JSON), which includes all necessary details to deploy the instance.\n   - This configuration file will define the set of services, their versions, dependencies, and inter-service communication protocols.\n\n2. **Component Specification:**\n   - Each component within a Marain instance will be defined as a service with a clear API contract.\n   - Services will be categorized into core services (essential for instance operation) and peripheral services (optional or supplementary services).\n\n3. **Isolation and Independence:**\n   - Each instance will operate in complete isolation from other instances, with separate databases, storage, and network resources.\n   - Instances will not share any runtime data; all configurations ensuring no overlap in resource allocation (e.g., unique namespaces or cluster configurations).\n\n4. **Consistent Naming Conventions:**\n   - A consistent naming convention for instances and their components will be adopted to avoid ambiguity. For example, instances could be named using a combination of environment and timestamp (e.g., `marain-dev-20231012`, `marain-prod-20231012`).\n\n5. **Documentation and Communication:**\n   - Detailed documentation outlining the structure and components of a Marain instance will be created and maintained.\n   - Clear communication protocols and documentation standards will be established to ensure all stakeholders understand the structure and expectations of a Marain instance.\n\n6. **Version Control:**\n   - All configuration files and documentation will be maintained under version control to track changes, ensure consistency, and facilitate rollbacks if necessary.\n\n7. **Automated Deployment and Validation:**\n   - Automated deployment scripts and integration tests will be developed to ensure that instances can be deployed and validated consistently and reliably.\n\nBy adopting these practices, we aim to achieve a clear, formal, and unambiguous definition of a Marain instance, ensuring consistency, reliability, and ease of maintenance across different environments and deployments.","GenTime":"2024-07-28 21:40:26"}
{"File Name":"Marain.Instance\/0005-multitenancy-approach-for-marain.md","Context":"## Context\\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\\n\n## Decision\n","Decision":"To support this, we have made the following decisions\\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as \"Client Tenants\".\\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as \"Service Tenants\".\\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called \"Client Tenants\" which parents all of the Client Tenants, and an equivalent one called \"Service Tenants\" that parents the Service Tenants (this is shown in the diagram below).\\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n```\\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|\\n+-> OPERATIONS\\n```\\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\\nThis leaves the tenant hierarchy looking like this:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe now repeat the process of enrolling Litware for the Workflow service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\n","tokens":297,"id":359,"Prediction":"## Decision\n\n### Tenancy Model\n\n1. **Model Structure**:\n    - **Root Tenant**:\n        - Represents Marain central management.\n        - Manages global operations, configurations, and services licensing to clients.\n\n    - **Client Tenant**:\n        - Represents individual clients using Marain services.\n        - Segregated into `Public Tenant` (PaaS offering) and `Private Tenant` (self-hosted in client's own cloud subscriptions).\n\n    - **Sub-tenants**:\n        - Represents customers of the clients using Marain to implement multi-tenanted services.\n        - Isolated storage and data handling to ensure tenant-specific data protection.\n\n2. **Deployment Models**:\n    - **Managed Service Deployment**:\n        - Marain hosted environment providing services to clients. \n        - Storage options: Marain-managed or client-managed storage accounts\/databases.\n        - Compute operations managed exclusively by Marain.\n\n    - **Client-Hosted Deployment**:\n        - Clients self-deploy Marain services to their own cloud subscriptions.\n        - Complete control over storage and compute resources by the client.\n\n3. **Services Licensing and Usage**:\n    - **Direct Licensing**:\n        - Clients licensed directly for specific services.\n    - **Indirect Usage**:\n        - Services indirectly used as dependencies (e.g., `Workflow service` indirectly using `Operations service`).\n        - Indirect usage allowed under a parent service license without needing separate direct licenses.\n\n4. **Service Management API**:\n    - Tenancy management functionalities exposed via `Marain.Tenancy` service.\n    - Support for:\n        - Creating, managing, and deleting tenants.\n        - Assigning and revoking service licenses.\n        - Configuring storage and compute resources based on deployment model.\n        - Defining hierarchical relationship between root tenants, client tenants, and sub-tenants.\n\n5. **Data Isolation and Security**:\n    - Ensured through isolated storage configurations for each tenant.\n    - Multi-level access control provided to enforce data security across tenants.\n    - Compliance with relevant data protection regulations.\n\nThis model aims to provide flexibility, scalability, and security, ensuring an efficient multi-tenanted environment for both Marain services and clients while supporting various deployment and storage options.","GenTime":"2024-07-28 21:40:35"}
{"File Name":"fxa\/0025-retire-fxa-auth-db-mysql.md","Context":"## Context and Problem Statement\\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\\n## Decision Drivers\\n- Reduce complexity\\n- Improve performance\\n- Share code\\n\n## Decision\n","Decision":"- Reduce complexity\\n- Improve performance\\n- Share code\\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\\n","tokens":127,"id":362,"Prediction":"## Decision\nAfter evaluating the current state of the architecture and considering the decision drivers\u2014reducing complexity, improving performance, and sharing code\u2014we have decided to colocate the database access with the auth-server in the frontend servers, similar to what has been done with the graphql-api-server. \n\nThis decision is driven by the following key factors:\n1. **Reducing Complexity:** By colocating the database access with the auth-server, we eliminate the need for a separate physical database access layer, which simplifies the overall architecture. This reduces the maintenance overhead and makes the system easier to understand and manage.\n\n2. **Improving Performance:** Direct database access from the auth-server is expected to reduce latency associated with inter-layer communication. The performance improvement comes from having fewer network hops and reduced complexity in the data flow.\n\n3. **Code Sharing:** Utilizing a shared database access layer (e.g., via `knex` in `fxa-shared`) allows for reuse of database interaction code across different services. This promotes code reuse and ensures consistency in how database operations are performed across different parts of the system.\n\nThe following actions will be taken to implement this decision:\n- Refactor the auth-server to make direct database calls via `knex` in `fxa-shared`.\n- Ensure that necessary security measures are in place to protect direct database access from the frontend servers.\n- Test and validate the refactored setup to confirm performance improvements and stability.\n- Document the changes and update any relevant architectural diagrams to reflect the new design.\n\nBy making this change, we align the auth-server's architecture with that of new services like graphql-api-server, promoting a more uniform and efficient system.","GenTime":"2024-07-28 21:40:44"}
{"File Name":"fxa\/0005-minimize-password-entry.md","Context":"## Context and Problem Statement\\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n\n## Decision\n","Decision":"- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\nChosen option: \"option 2\", because it minimizes the number of places the user must enter their password.\\n### Positive Consequences\\n- User will need to type their password in fewer places.\\n- Signin completion rates should increase.\\n### Negative Consequences\\n- There may be user confusion around what it means to sign out.\\n### [option 1] Keep the existing flow\\nIf a user signs in to Sync first and is not signing into an OAuth\\nRP that requires encryption keys, then no password is required.\\nIf a user does not sign into Sync and instead signs into an\\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\\nrequire encryption keys, e.g., Monitor, then they must enter their password.\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because we already have it and no effort is required to keep it.\\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\\n- Bad, because users need to enter their password more than they need to.\\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\\n### [option 2] Only ask authenticated users for a password if encryption keys are required\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\\n","tokens":254,"id":377,"Prediction":"## Decision\nThe decision is to implement a differentiated authentication flow based on the RP's encryption key requirements. Here's the detailed plan:\n\n1. **Context-Sensitive Password Prompts**: \n    - For RPs that require encryption keys (e.g., Sync, Lockwise, Send), users will still be required to enter their password when requested.\n    - For RPs that do not require encryption keys (e.g., Monitor, AMO), the authorization flow will leverage the existing sessionToken to authenticate the user without an additional password prompt.\n\n2. **State Management Logic Fix**:\n    - Address the existing bug in the state management logic to ensure that password prompts are aligned with the RP's requirements. This fix will help reduce unnecessary password requests.\n\n3. **User Experience Improvements**:\n    - Provide clear and concise explanations in the UI about why a password is being requested (only when it is needed for encryption keys), to reduce confusion.\n    - Implement adaptive forms where possible, such as a touch-friendly keyboard for mobile devices, to enhance usability when password entry is mandatory.\n\n4. **Monitoring and Analytics**:\n    - Deploy usage analytics to monitor the impact of this decision on user drop-off rates and completion rates of the authorization flow.\n    - Adjust the implementation based on user feedback and analytics data to further streamline the process over time.\n\nBy implementing this decision, we aim to increase user happiness by reducing unnecessary password prompts and ultimately improve sign-in and authorization flow completion rates.","GenTime":"2024-07-28 21:41:50"}
{"File Name":"fxa\/0018-use-tailwind-with-custom-scss.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\\n## Decision Drivers\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n\n## Decision\n","Decision":"- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\\n","tokens":397,"id":381,"Prediction":"## Decision\n\nAfter evaluating various CSS architecture methodologies and frameworks, the chosen approach for the Settings Redesign project is to adopt the [BEM (Block Element Modifier) methodology](http:\/\/getbem.com\/) in combination with [Sass (SCSS syntax)](https:\/\/sass-lang.com\/). \n\n### Reasons for This Decision:\n\n1. **Reusability**:\n   - **BEM**: Encourages self-contained, reusable code with clear, predictable naming conventions. This will facilitate the DRY (Don't Repeat Yourself) principle.\n   - **Sass**: Enables the use of variables, loops, and mixins, promoting component reuse and modular code organization.\n\n2. **Longevity**:\n   - **BEM**: The methodology has been widely adopted and is well-documented, ensuring that it will remain relevant and supported for years to come.\n   - **Sass**: A mature and stable preprocessor that is fully compatible with CSS and enjoys broad support across the web development community.\n\n3. **Developer Experience**:\n   - Several team members are already familiar with BEM and Sass, which will make for a smoother transition and less steep learning curve.\n   - BEM's clear structure and naming conventions will make it easier for developers to understand and modify components, reducing onboarding time for new team members.\n\n4. **Ease of Use**:\n   - **BEM**: The simplicity and consistency of its naming conventions make it straightforward for engineers to pick up and maintain.\n   - **Sass**: Provides powerful features like nesting and inheritance that simplify CSS development and make complex styling easier to manage.\n\n### Implementation Plan:\n- **Class Naming Conventions**:\n  Follow the BEM methodology to ensure that classes are named predictably and hierarchically. For example:\n  ```scss\n  .block {\n    &__element {\n      &--modifier {\n        \/\/ styles\n      }\n    }\n  }\n  ```\n- **Color and Measurement Standards**:\n  Utilize SCSS variables to centralize and standardize colors and measurements in line with Mozilla Protocol's design system. For example:\n  ```scss\n  $spacing-unit: 8px;\n  $primary-color: #005a9c;\n\n  .block {\n    margin: $spacing-unit * 2;\n    color: $primary-color;\n  }\n  ```\n- **Interoperability**:\n  Create shared components with standardized class names and styles to ensure consistency throughout the application.\n\n- **Custom Configuration**:\n  Leverage Sass's configuration capabilities to create customizable themes and component libraries, accommodating the design standards of the Settings Redesign project.\n\nBy adopting the BEM methodology combined with Sass, the project aims to achieve a balance of structure, flexibility, and maintainability, ensuring a scalable and sustainable CSS architecture for Firefox Accounts.","GenTime":"2024-07-28 21:42:05"}
{"File Name":"fxa\/0016-use-graphql-and-apollo-for-settings-redesign.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\\n## Decision Drivers\\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\n\n## Decision\n","Decision":"- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\\n","tokens":366,"id":387,"Prediction":"## Decision\n\nAfter thorough evaluation, we have decided **to adopt GraphQL with Apollo** for the [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840). This decision is driven by the following considerations:\n\n1. **Unified Data Fetching**: GraphQL will help us consolidate multiple API requests into a single request, reducing the number of network calls and potentially improving performance. This is crucial for enhancing the user experience, especially for data-heavy applications like the settings page.\n\n2. **Flexible and Efficient Data Retrieval**: With GraphQL, the client can request exactly the required data, thereby minimizing over-fetching or under-fetching, which is a common issue with REST APIs.\n\n3. **Improved Developer Experience**: Apollo provides a robust client and server setup that integrates seamlessly with React, along with built-in tools for caching, state management, and error handling. This can significantly speed up the development process and improve code maintainability.\n\n4. **Future-Proofing**: GraphQL\u2019s schema and type system offer a clear contract between the frontend and backend, enabling easier modifications and additions of new features. This is advantageous for long-term scalability and maintenance.\n\n5. **Tooling and Ecosystem**: Apollo's ecosystem includes a plethora of tools that enhance developer productivity, such as Apollo DevTools, and schema stitching, which can accelerate both the setup and ongoing development efforts.\n\nIn conclusion, integrating GraphQL with Apollo into the Settings Redesign project aligns well with our goals around performance, ease of development, and future scalability, providing a solid foundation for both current and future requirements.","GenTime":"2024-07-28 21:42:30"}
{"File Name":"fxa\/0020-application-architecture.md","Context":"## Context and Problem Statement\\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\\n## Decision Drivers\\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\n\n## Decision\n","Decision":"- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\nChosen Option: Use an existing framework: NestJS\\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\\n### Positive Consequences\\n- Less documentation to create, as we only document differences for legacy services.\\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\\n### Negative Consequences\\n- There will be some effort involved to migrate the 4 packages to NestJS.\\n- We will need to document the differences between NestJS and other legacy services.\\n","tokens":499,"id":389,"Prediction":"## Decision\n\nWe have decided to adopt a modern, fully-featured application framework for the FxA back-end services to address the issues resulting from the current ad-hoc architecture. After conducting a thorough evaluation of various frameworks against our decision drivers and requirements, we have selected **NestJS** as our framework of choice.\n\n### Key Points of the Decision:\n\n1. **Documented Application Architecture**:\n   - NestJS provides a well-documented, modular architecture that clearly separates concerns, making it easier for developers to understand and work with.\n\n2. **Tooling to Reduce Boilerplate**:\n   - NestJS's CLI tooling facilitates the creation of consistent, boilerplate-reduced code, speeding up development and enforcing best practices across all services.\n\n3. **Modern Paradigms like Dependency Injection (DI)**:\n   - NestJS natively supports Dependency Injection, which simplifies object instantiation and increases testability of components.\n\n4. **Training Materials**:\n   - Ample training materials and community support are available for NestJS, aiding in the onboarding of new developers.\n\n5. **Migration Path for Legacy Applications**:\n   - NestJS's modular approach allows us to incrementally refactor legacy services and align them with the new architecture without needing a complete rewrite.\n\n6. **Handling Required Situations**:\n   - **Exception Handling**: NestJS ships with robust tools for exception handling and responses.\n   - **Validation**: Integrated validation with class-validator and class-transformer.\n   - **Service Instantiation (DI)**: With built-in support for DI, NestJS simplifies the lifecycle management of services and components.\n   - **Authentication**: NestJS provides integration with popular authentication libraries and strategies.\n   - **RESTful APIs**: Strong support for RESTful API creation, including decorators and middleware.\n   - **GraphQL**: First-class GraphQL support with built-in decorators and schema generation.\n   - **Script Usage**: Components and services in NestJS can be easily imported and used in standalone scripts, maintaining consistency and reusability.\n\nBy leveraging NestJS, we aim to achieve a documented, consistent architecture across all back-end services, minimize the overhead of boilerplate code, and adopt modern software engineering practices that will enhance developer productivity and software maintainability. This decision also aligns with our goal to ease the onboarding process for new developers and unify the lifecycle management of shared components.","GenTime":"2024-07-28 21:42:42"}
{"File Name":"structurizr-python\/0002-version-control-our-code.md","Context":"## Context\\nWe need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\n## Decision\n","Decision":"We use git for version control and GitHub for collaboration.\\n","tokens":33,"id":398,"Prediction":"## Decision\n\nWe will adopt Git as our version control system and use GitHub as our online collaboration platform. Git will provide us with robust version control capabilities, allowing us to track changes, manage branches, and roll back to previous states if necessary. GitHub will facilitate seamless collaboration with a wider community by providing a centralized repository, pull request workflows, issue tracking, and continuous integration features.\n\n**Key Benefits:**\n1. **Distributed Version Control:** Git's distributed architecture ensures that every contributor has a full copy of the repository history, enabling offline work and redundancy.\n2. **Branching and Merging:** Git\u2019s powerful branching and merging capabilities allow multiple lines of development to exist concurrently, making it easier to manage feature development, bug fixes, and experimental changes.\n3. **Community Collaboration:** GitHub\u2019s collaborative tools like pull requests and code reviews enhance the quality and maintainability of our codebase by fostering community engagement and peer feedback.\n4. **Integration and Automation:** GitHub integrates with numerous development tools and CI\/CD pipelines, streamlining our workflow and ensuring continuous improvement and delivery.\n\nWe will establish guidelines and best practices for committing code, managing branches, and conducting peer reviews to ensure smooth and efficient collaboration. Training sessions will be conducted to familiarize all team members with Git and GitHub workflows.","GenTime":"2024-07-28 21:43:06"}
{"File Name":"structurizr-python\/0009-use-pydantic-for-json-de-serialization.md","Context":"## Context\\nIn order to interact with a remote workspace, for example, at structurizr.com.\\nThe remote or local workspace has to be (de-)serialized from or to JSON.\\n\n## Decision\n","Decision":"In order to perform these operations we choose\\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\\ncommunity, good data validation, helpful documentation, and good performance.\\n","tokens":39,"id":399,"Prediction":"## Decision\nWe have decided to use a standard JSON serialization library for both serializing the workspace object to JSON and deserializing JSON back to a workspace object. Specifically, we will use Jackson, a popular and efficient Java library for JSON processing, due to its maturity, ease of use, and extensive support for various Java types.\n\n### Justification\n1. **Interoperability**: JSON is a widely used format that ensures compatibility with other systems and tools.\n2. **Ease of Use**: Jackson provides straightforward annotations and features that simplify the process of mapping Java objects to JSON and vice versa.\n3. **Performance**: Jackson is known for its efficient parsing and generation of JSON, making it suitable for high-performance requirements.\n4. **Flexibility**: The library supports complex data types and advanced features like streaming and custom serializers\/deserializers, providing us with the flexibility to handle any specific needs of our workspace objects.\n5. **Community & Support**: As a widely-used library, it has strong community support and extensive documentation, which will help in resolving issues and following best practices.\n\nBy standardizing on Jackson for JSON processing, we ensure a robust and maintainable solution that aligns with industry standards.","GenTime":"2024-07-28 21:43:10"}
{"File Name":"structurizr-python\/0005-code-quality-assurance.md","Context":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n\n## Decision\n","Decision":"We will use isort, black, and flake8.\\n","tokens":35,"id":400,"Prediction":"## Decision\nTo address the need for standardized, high-quality code that adheres to style guides and best practices, we will integrate the following tools into our development workflow:\n\n1. **Linters**: We will use ESLint for JavaScript and Pylint for Python to ensure code quality by identifying problematic patterns or code that doesn't adhere to certain style guides.\n\n2. **Formatter**: Prettier will be used as an automatic code formatter for JavaScript, CSS, and other supported languages, ensuring that code style is consistent across the codebase.\n\n3. **Static Code Analysis**: Tools like SonarQube will be employed to perform deep static analysis of the code, identifying bugs, security vulnerabilities, and code smells.\n\n4. **Pre-Commit Hooks**: We will use Husky to add Git hooks that will run automated checks before committing code. This will enforce code quality at the commit level and prevent bad code from being checked into the repository.\n\n5. **Continuous Integration**: Integrate these tools into our CI\/CD pipeline using services such as Jenkins or GitHub Actions to ensure that code quality is verified with every push and pull request.\n\nBy implementing these tools, we will streamline our development process, maintain high code quality, and ensure adherence to style guides and best practices. This will ultimately save time and reduce the friction associated with code reviews and manual style checks.","GenTime":"2024-07-28 21:43:13"}
{"File Name":"amf-core\/0009-source-information-node-storing-source-locations.md","Context":"## Context\\nThe custom AMF validator needs to show the location of the file from which each error was generated.\\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n\n## Decision\n","Decision":"A new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\\n","tokens":53,"id":405,"Prediction":"## Decision\n\nTo ensure that the custom AMF validator can display the location of the file in which each error originates, the following decision has been made:\n\n1. **Enhance the AMF Model**: We will extend the AMF model to include location metadata for each node. This metadata will store the file path, line number, and column number for each element within the AMF.\n\n2. **Integration with JSON-LD Emission**: Modify the JSON-LD emission process to include the newly added location metadata. This will ensure the error location information is preserved and can be accessed during validation.\n\n3. **Update Validator Logic**: Adjust the custom AMF validator's logic to read and report the location metadata for each error. The validator will now provide detailed file location information along with error messages.\n\n4. **Backward Compatibility**: Ensure that the enhancements are backward compatible. This means that if the location metadata is not available, the validator should gracefully handle such cases without breaking the validation process.\n\n5. **Refactor and Testing**: Refactor the relevant parts of the codebase to integrate these changes, followed by extensive testing to confirm that the validator correctly shows error locations and that there is no adverse impact on performance or functionality.\n\nThese steps will address the inability to obtain error locations, enhancing the utility and accuracy of the custom AMF validator for developers.","GenTime":"2024-07-28 21:43:29"}
{"File Name":"csw-backend\/0002-not-cloud-agnostic.md","Context":"## Context\\nSince Cloud Security Watch specifically aims to monitor for\\nmisconfigurations in AWS accounts it does not make sense to\\nmake the tool cloud agnositic.\\n\n## Decision\n","Decision":"Whilst we support the notion of writing cloud agnostic\\ncode in general. In this instance it is not appropriate\\nor desirable.\\n","tokens":36,"id":419,"Prediction":"## Decision\nWe will design and develop Cloud Security Watch to be exclusively focused on monitoring and identifying misconfigurations specifically within AWS accounts. This decision allows us to tailor our solution to the unique security aspects of AWS, ensuring more precise and effective monitoring capabilities compared to a more generic, cloud-agnostic approach. Consequently, we will leverage AWS-specific services, APIs, and best practices to enhance our tool\u2019s functionality and reliability for AWS environments.","GenTime":"2024-07-28 21:44:19"}
{"File Name":"casa\/0004-use-bootstrap.md","Context":"## Context\\nWe would like to have an easy-to-use system for consistent styles that doesn't\\ntake much tinkering. We propose using the `bootstrap` gem.\\n\n## Decision\n","Decision":"Pending\\n","tokens":36,"id":425,"Prediction":"## Decision\nWe have decided to use the `bootstrap` gem to manage and enforce consistent styles across our application. The `bootstrap` gem provides a wide range of pre-styled components and responsive design utilities that will help us quickly implement a consistent look and feel without needing extensive customization. This choice aligns with our goal to have an easy-to-use system for styling that minimizes the need for tinkering and custom development.\n\nBy integrating the `bootstrap` gem, we anticipate the following benefits:\n- Rapid development: Faster implementation of a unified style guide with minimal effort.\n- Responsiveness: Built-in mobile-first design principles to ensure consistency across different devices and screen sizes.\n- Maintenance: Easier to maintain and update styles as Bootstrap is a well-documented and actively maintained framework.\n- Community support: Access to a wide range of third-party plugins and resources due to Bootstrap\u2019s large community.\n\nWe will proceed with integrating the `bootstrap` gem and refactoring our existing styles to be consistent with Bootstrap\u2019s guidelines.","GenTime":"2024-07-28 21:44:40"}
{"File Name":"GoStudy\/0002-use-hexagonal-architecture.md","Context":"## Context\\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\\nTo enable different communication ways we need to apply a proper architectural style.\\n\n## Decision\n","Decision":"Adopt Hexagonal Architecture for project.\\n","tokens":37,"id":428,"Prediction":"## Decision\n\nAfter evaluating the context and the various communication requirements between the three independent components of the SDARS application, we have decided to implement a **Microservices Architecture** using **RESTful APIs** as the primary communication method.\n\n### Rationale:\n1. **Independence and Scalability**:\n    - Each component can be developed, deployed, and scaled independently.\n    - This aligns with the existing independent nature of the components, ensuring minimal impact on others if one component needs to be updated or scaled.\n\n2. **Flexibility in Communication**:\n    - RESTful APIs provide a standard and widely-adopted method for inter-service communication.\n    - They offer greater flexibility and simplicity for integrating various ways of communication.\n\n3. **Decoupling and Modularity**:\n    - Microservices architecture promotes a decoupled and modular system where each component can be managed and improved independently.\n    - This suits the SDARS application\u2019s requirements for maintaining independent components.\n\n4. **Technology Agnostic**:\n    - RESTful APIs are not constrained by language or technology, allowing developers to choose the best technology stack for each component.\n\n5. **Ease of Maintenance**:\n    - Microservices and RESTful APIs are well-documented and have extensive community and enterprise support, making maintenance and troubleshooting more manageable.\n\nWhile microservices and RESTful APIs serve as the backbone of the architecture, we will also ensure to implement robust monitoring, logging, and security practices to maintain the overall system's reliability and security.\n\nIn summary, adopting a Microservices Architecture with RESTful APIs as the primary communication method will provide the SDARS application with the necessary flexibility, scalability, and maintainability required to meet its communication needs between independent components.","GenTime":"2024-07-28 21:44:51"}
{"File Name":"GoStudy\/0004-use-ports-adapters-as-application-architecture-style.md","Context":"## Contextadjecent\\nWe need to adopt application architecture style adjecent to our architectural drivers\\n\n## Decision\n","Decision":"We will use ports&adapters architecture style\\n","tokens":21,"id":429,"Prediction":"## Decision\n\nWe have decided to adopt a microservices architecture style for our application. This decision is driven by our architectural drivers, which include scalability, maintainability, and resiliency. By leveraging microservices, we can ensure that our application can scale horizontally, be more easily managed and updated, and remain responsive and fault-tolerant under different operational conditions. This architectural style aligns well with our goals and the requirements identified in our architectural drivers.","GenTime":"2024-07-28 21:44:52"}
{"File Name":"ADR\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":440,"Prediction":"## Decision\n\nTo facilitate the recording of architectural decisions made on this project, we will implement an Architectural Decision Records (ADR) system. This involves the following key elements:\n\n1. **ADR Documentation Format:** We will use a structured markdown template for each decision, capturing sections such as Context, Decision, Status, Consequences, and Alternatives considered.\n\n2. **Version Control:** ADRs will be stored in a dedicated directory within our project's version control system (e.g., a `docs\/adr\/` folder in Git). This ensures that all decisions are versioned, traceable, and easily accessible to all team members.\n\n3. **Decision-Making Process:** Each ADR will be created through collaborative discussions during regular architecture meetings. Any team member can propose an ADR, and it will require consensus from a designated architecture review board for approval.\n\n4. **Indexing and Referencing:** An index file will be maintained to list all ADRs chronologically with links to the detailed records. Each ADR will have a unique identifier for easy reference and cross-linking within documentation.\n\n5. **Maintenance and Updates:** ADRs are living documents that can be amended. When changes to a decision are necessary, a new ADR is created, referencing the original and detailing the modifications.\n\nThis approach ensures that all architectural decisions are documented transparently, making it easier for current and future team members to understand the evolution of the project's architecture.","GenTime":"2024-07-28 21:45:37"}
{"File Name":"up-fiscal-data\/006-time.md","Context":"## Context and Problem Statement\\nThe data extraction w\/ transaction timestamp from Koshvani platform is a challenging task.\\n- The temporal data directly available on the platform is not easy to extract\\n- The validation of temporal data to ensure accuracy depends on factors\\n- Districts\/Treasuries which have updated their data\\n- Data\/Time of data upload vs extraction from departments\\n## Decision Drivers\\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n\n## Decision\n","Decision":"The platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n","tokens":120,"id":442,"Prediction":"## Decision\n\nTo address the challenges in extracting and validating the temporal data from the Koshvani platform, we will implement a multi-step data verification and extraction process. This will ensure that the data extracted is accurate and reflective of the most recent updates from various districts and treasuries.\n\n1. **Automated Data Extraction Scripts**:\n   - Develop and deploy automated data extraction scripts that will connect to the Koshvani platform at predefined intervals, specifically targeting the segments of the platform where temporal data is available.\n   \n2. **Daily Status Logs Integration**:\n   - Incorporate the platform's daily data updation status logs into the extraction process. The extraction scripts will first fetch and analyze these logs to identify which districts and treasuries have updated their data.\n\n3. **Data Verification Algorithms**:\n   - Implement data verification algorithms that cross-reference the timestamps of data upload and extraction. These algorithms will flag any discrepancies where data appears to be outdated or inconsistent with recent updates.\n\n4. **Time-Stamped Data Storage**:\n   - Store the extracted data in a time-stamped database, maintaining a historical record of data extraction events. This will support retrospective analysis and validation of data accuracy over time.\n\n5. **Manual Review & Exception Handling**:\n   - Establish a protocol for manual review of data in cases where automated verification flags potential issues. A dedicated team will be responsible for investigating these exceptions, ensuring any inconsistencies are resolved promptly.\n\n6. **Regular Audits and Reports**:\n   - Conduct regular data audits and generate reports detailing the accuracy, freshness, and completeness of the extracted temporal data. This will provide continuous assurance and transparency to stakeholders regarding the reliability of the data extraction process.\n\nThrough this structured approach, we aim to improve the reliability and accuracy of temporal data extraction from the Koshvani platform, addressing the challenges presented by varying data update patterns across districts and treasuries.","GenTime":"2024-07-28 21:45:43"}
{"File Name":"up-fiscal-data\/005-phases.md","Context":"## Context and Problem Statement\\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\\n## Decision Drivers\\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\n\n## Decision\n","Decision":"* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\\n","tokens":88,"id":443,"Prediction":"## Decision\n\nTo address the context and problem statement effectively, the following decisions have been made:\n\n1. **Phased Implementation**: \n   - A phased approach will be adopted for scraping and analyzing sub-sections from the Expenditure category. This will help in managing workloads, identifying errors early, and making incremental improvements.\n\n2. **Modular Job Design**:\n   - Each section will have a dedicated job for scraping and processing. This modular design ensures that failures in one section do not affect the others and simplifies future maintenance.\n\n3. **Centralized Data Repository**:\n   - All data collected from different sections will be aggregated into a centralized data repository. This repository will enable easy access, analysis, and validation of data across different sources.\n\n4. **Imputation and Data Quality Checks**:\n   - Data from different sections will be used to impute missing accounting head information, ensuring a more complete dataset. In parallel, data quality checks will be developed to validate the integrity and accuracy of the data being collected and processed.\n\n5. **Scalable Architecture**:\n   - Considering the long-term project timeline (3 years), a scalable architecture will be created. The architecture will be designed to handle increasing data volumes and complexity over time, ensuring sustainability.\n\n6. **Technology Stack**:\n   - Utilize a robust technology stack that supports data scraping, storage, processing, and analysis. Technologies such as Python, Beautiful Soup, Pandas, and SQL databases are considered due to their reliability and scalability.\n\n7. **Regular Reviews and Optimizations**:\n   - Perform regular reviews of the scraping and processing jobs to identify optimization opportunities. This also includes updating the jobs to adapt to changes in source data structures and formats.\n\n8. **Documentation and Training**:\n   - Comprehensive documentation will be created for all scraping and processing jobs. Training sessions will be held for team members to ensure they understand the system and can contribute to its development and maintenance.\n\nThese decisions help ensure that the project will be sustainable, scalable, and maintain high-quality data standards throughout its lifecycle.","GenTime":"2024-07-28 21:45:49"}
{"File Name":"up-fiscal-data\/003-selnium.md","Context":"## Context and Problem Statement\\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\\n\n## Decision\n","Decision":"- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https:\/\/www.selenium.dev\/) to create robust, browser-based automation.\\n","tokens":32,"id":445,"Prediction":"## Decision\nWe have decided to use the Scrapy framework for web scraping the data from the Koshvani platform. Scrapy was chosen because of its efficiency, robust community support, and extensive documentation, which will facilitate easier scraping of complex web pages.\n\nFor storing the scraped data, we will use PostgreSQL. PostgreSQL was selected due to its reliability, scalability, and support for advanced data types and indexing, which will help in efficiently managing the large datasets scraped from the Koshvani platform.\n\n### Rationale:\n1. **Scrapy:** \n   - **Efficiency:** Scrapy is highly efficient and optimized for scraping a large amount of data.\n   - **Community Support:** It has a large, active community, and excellent documentation, making troubleshooting and development easier.\n   - **Customizability:** The framework is highly customizable, which aligns with our need to tailor the scraping processes as per the requirements of the Koshvani platform.\n\n2. **PostgreSQL:**\n   - **Reliability:** Known for its reliability and robustness, PostgreSQL ensures the integrity of the data stored.\n   - **Scalability:** PostgreSQL can handle a high volume of transactions, making it suitable for our expected data load.\n   - **Advanced Features:** Provides support for JSON data storage and indexing, which is beneficial for storing and querying the socially structured data we are collecting. \n\nThis combination of Scrapy and PostgreSQL will allow us to efficiently scrape and store the data in a format that is easily accessible and machine readable for downstream processing.","GenTime":"2024-07-28 21:45:58"}
{"File Name":"up-fiscal-data\/001-source.md","Context":"## Context and Problem Statement\\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\\n**_Platform 1_**\\n**Name** : Budget - Government of Uttar Pradesh\\n**URL** : http:\/\/budget.up.nic.in\/\\n**Usage** : Analyse budget documents and understand accounting information available.\\n**_Platform 2_**\\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\\n**URL** : http:\/\/koshvani.up.nic.in\/\\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\\n## Decision Drivers\\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\n\n## Decision\n","Decision":"- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n","tokens":183,"id":446,"Prediction":"## Decision\n\nGiven the context and problem statement, the decision has been made to utilize both platforms, **Budget - Government of Uttar Pradesh** and **Koshvani web**, for the **OBI: Phase 2** and **HAQ: GEWST** projects. The decision is driven by the need to create a comprehensive understanding of Uttar Pradesh fiscal data and provide that data in a machine-readable format.\n\n### Rationale\n1. **Budget - Government of Uttar Pradesh**:\n    - Provides a thorough understanding of accounting information which will be essential for analyzing fiscal data.\n    - Suitable for detailed analysis of budget documents which falls under the requirement scope for the projects.\n    \n2. **Koshvani web**:\n    - Enables scraping of budget data, making it possible to analyze and subsequently publish the data for public consumption.\n    - Provides an opportunity to transform raw fiscal data into a machine-readable format, enhancing transparency and accessibility.\n\n### Implementation\n- Both platforms will be accessed to collate comprehensive budget data.\n- Scripts and tools will be developed to scrape data from **Koshvani web** and transform it into a format suitable for machine processing.\n- Analytical tools and methodologies will be applied to the data gathered from the **Budget - Government of Uttar Pradesh** platform to derive insights and better understand fiscal data trends.\n\n### Expected Outcomes\n- A richer, more nuanced understanding of Uttar Pradesh\u2019s budget and fiscal data.\n- Availability of machine-readable fiscal data for stakeholders and the general public, promoting openness and transparency.\n\nThis approach ensures that both the detailed analysis and practical data accessibility needs of the projects are comprehensively addressed.","GenTime":"2024-07-28 21:46:02"}
{"File Name":"up-fiscal-data\/008-grant-wise.md","Context":"## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n\n## Decision\n","Decision":"The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\\n","tokens":82,"id":447,"Prediction":"## Decision\nRevert the previous decision and extract the `Grant-wise expenditure` section from Koshvani.\n\nInstead of using the `DDO-wise expenditure` section, return to extracting data from the `Grant-wise expenditure` section. Since the site crash issues have been resolved, extracting from this section will provide a more straightforward and hierarchical representation of the expenditure data. This approach ensures data accuracy and maintains consistency with the original structure intended by Koshvani.","GenTime":"2024-07-28 21:46:03"}
{"File Name":"hmpps-interventions-ui\/0006-use-jest-for-testing.md","Context":"## Context\\nWe want a test framework that has good support for TypeScript and Node. Jest is\\na fast testing framework with good resources for mocking.\\n\n## Decision\n","Decision":"We will use Jest as our testing framework.\\n","tokens":31,"id":452,"Prediction":"## Decision\nWe will adopt Jest as our testing framework for the project because it offers excellent support for both TypeScript and Node.js. Jest's functionalities, such as its comprehensive mocking capabilities, speed, and ease of integration with TypeScript, align well with our project requirements. Additionally, the vast amount of resources, plugins, and active community support for Jest will facilitate smoother testing and debugging processes.","GenTime":"2024-07-28 21:46:21"}
{"File Name":"hmpps-interventions-ui\/0010-use-stylelint-for-linting-styles.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n\n## Decision\n","Decision":"We will check SASS syntax using Stylelint.\\nWe will use the recommended configuration for plugins where possible.\\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\\n","tokens":75,"id":458,"Prediction":"## Decision\n\nWe will adopt Stylelint as our CSS linter to enforce code consistency and catch errors automatically in our project. Stylelint is chosen due to its popularity, extensive support for various CSS preprocessors including SASS, and its ease of configuration. This decision will help ensure that our codebase maintains a high standard of quality and reduces the likelihood of introducing style-related bugs. We will configure Stylelint according to our project's specific requirements and integrate it into our development workflow, including CI\/CD pipelines, to maximize its benefits.","GenTime":"2024-07-28 21:46:39"}
{"File Name":"openlobby-server\/0006-use-flask.md","Context":"## Context\\nWe need to choose webserver.\\n\n## Decision\n","Decision":"We will use Flask. Server should be simple - pretty much just with a GraphQL\\nendpoint and GraphiQL.\\n","tokens":11,"id":469,"Prediction":"## Decision\nWe will use **Nginx** as our web server for the following reasons:\n\n1. **Performance**: Nginx is known for its high performance and low resource consumption. It can handle a large number of concurrent connections, which is essential for our expected traffic.\n\n2. **Scalability**: The event-driven architecture of Nginx is designed for scalability, making it a good fit for our future growth plans.\n\n3. **Security**: Nginx offers robust security features, including SSL\/TLS termination, DDoS attack mitigation, and request rate limiting.\n\n4. **Reverse Proxy Capabilities**: Nginx excels at handling reverse proxy and load balancing tasks, which are crucial for our application's architecture.\n\n5. **Community and Support**: Nginx has a large and active community, as well as comprehensive documentation, ensuring that we will have ample resources and support when needed.\n\n6. **Ease of Configuration**: The configuration language of Nginx is straightforward and flexible, allowing for concise and maintainable server configurations.\n\nGiven these advantages, Nginx stands out as the optimal choice to meet our web server requirements.","GenTime":"2024-07-28 21:47:18"}
{"File Name":"openlobby-server\/0005-python.md","Context":"## Context\\nWe need to choose main programming language for this project.\\n\n## Decision\n","Decision":"I'm now fluent in writing Python and JavaScript. Both languages are widely used\\nthese days and may attract developers to join this project.\\nPython is more mature with more stable libs for writing server side code and\\ntesting. JS may have better libraries for writing GraphQL APIs because\\nGraphQL originated in JS ecosystem.\\nI'm doing this in free time and for fun. I want to focus on project's features\\nand not to waste a time with poorly designed language lacking mature and\\nstable libraries. So for obvious reasons I'm choosing Python.\\n","tokens":15,"id":472,"Prediction":"## Decision\nWe have decided to use Python as the main programming language for this project. This decision is driven by several factors:\n\n1. **Ease of Use**: Python's readability and simplicity make it an excellent choice for rapid development and prototyping.\n2. **Community and Libraries**: Python boasts a large community and a vast ecosystem of libraries which can help accelerate development.\n3. **Cross-Platform Support**: Python is highly portable and can run on various operating systems, which aligns with our project's requirement for cross-platform compatibility.\n4. **Integration**: Python offers a range of tools to easily integrate with other technologies and programming languages, which provides us with greater flexibility for future enhancements.\n5. **Team Expertise**: Our development team already has significant experience with Python, reducing the learning curve and onboarding time.\n\nBy opting for Python, we ensure that we leverage its strengths to meet our project requirements efficiently.","GenTime":"2024-07-28 21:47:29"}
{"File Name":"openlobby-server\/0011-add-relational-database.md","Context":"## Context\\nNumber of document types which does not use Elasticsearch's fulltext\\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\\nper index which means management of many indices.\\n\n## Decision\n","Decision":"We will add relational database as primary database. Elasticsearch will be used\\nfor denormalized reports and related data intended for fulltext search.\\n","tokens":42,"id":475,"Prediction":"## Decision\nMigrate document types that do not utilize Elasticsearch's full-text search capabilities to a different storage solution.\n\nGiven the complication introduced by Elasticsearch 6 which mandates one type per index, the management overhead for many indices has become significant. To optimize our system, we decide to identify document types that are not leveraging Elasticsearch\u2019s full-text search functionality and move them to a more suitable database or storage technology that aligns with their access and querying patterns. Solutions like a relational database (e.g., MySQL, PostgreSQL) or a NoSQL database (e.g., MongoDB, Cassandra) will be evaluated based on the specific needs and characteristics of those document types. This approach will streamline our system, reduce management complexity, and allow us to take full advantage of Elasticsearch for the document types that truly benefit from its full-text search capabilities.","GenTime":"2024-07-28 21:47:35"}
{"File Name":"openlobby-server\/0003-just-api-without-frontend.md","Context":"## Context\\nOpen Lobby must have open stable API.\\n\n## Decision\n","Decision":"Open Lobby will be server just with an API based on API First design. Frontend\\nwill be written as separate application (web based, mobile, ...). This will\\nensure that there is a working API for anything that frontend application(s)\\nwill do.\\n","tokens":12,"id":476,"Prediction":"## Decision\nWe will design and implement a stable versioned public API for the Open Lobby. This API will adhere to industry standards for RESTful design and will be backward compatible to ensure minimal disruption for users. Comprehensive documentation will be provided to assist developers in integrating with and using the API efficiently. Regular API reviews and updates will be scheduled to ensure ongoing stability and relevance.","GenTime":"2024-07-28 21:47:36"}
{"File Name":"elasticsearch-updater\/0002-store-configurations-in-repository.md","Context":"## Context\\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\\n\n## Decision\n","Decision":"Given the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\\n","tokens":61,"id":485,"Prediction":"## Decision\nTo manage the complexity of loading data into Elasticsearch while also handling mappings and data transformations, the decision is to:\n\n1. **Use a Configuration File**: Store complex JSON objects, mappings, and transformation functions in a configuration file. This file can be read by the application during startup for dynamically configuring the Elasticsearch mappings and transformations.\n\n2. **Implement a Coordinator Module**: Develop a coordinator module responsible for reading the configuration file and initializing the necessary Elasticsearch indices and mappings. This module will also initiate the data transformation process before importing the data into Elasticsearch.\n\n3. **Adopt a Plugin Architecture**: For the data transformation tasks, use a plugin architecture that allows for easily adding or modifying transformation functions without changing the core codebase. This makes future transformation enhancements or adjustments more manageable.\n\n4. **Utilize a Workflow Engine**: Employ a workflow engine to handle the sequence of operations: loading the configuration, setting up mappings, performing data transformations, and importing the data to Elasticsearch. This adds robustness and makes the process easier to monitor and debug.\n\n5. **Implement Logging and Error Handling**: Ensure that the data loading, transformation, and mapping processes have adequate logging and error handling. This will help diagnose issues during data import and provide insights into the steps being executed.\n\nThis approach ensures that the rich, complex JSON objects and functions needed for mappings and transformations are managed effectively and flexibly, without relying on environment variables, which are not suited for such complex configurations.","GenTime":"2024-07-28 21:48:11"}
{"File Name":"front-end-monorepo\/adr-19.md","Context":"## Context\\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\\n\n## Decision\n","Decision":"1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n","tokens":147,"id":498,"Prediction":"## Decision\n\nWe will develop and implement a multiFrame Viewer component within the lib-classifier to support the display of multi-frame subjects in the classification interface.\n\n### Key points of the decision:\n1. **Naming**: The component will be referred to as the multiFrame Viewer, aligning with its broader applicability beyond just paginated views.\n2. **Implementation Focus**: The initial focus will be on capabilities that are essential for transcription projects, specifically targeting multiImage features.\n3. **Extensibility**: As other types of projects, such as ecology projects, migrate to the new classifier, additional features will be incorporated into the filmstrip component to meet their specific needs.\n4. **Existing Resources**: The design and initial functionality will be guided by the existing InVision document for the Pages Viewer.\n\nThis decision ensures a scalable and adaptable approach to supporting various project requirements over time, while immediately addressing the needs of transcription projects.","GenTime":"2024-07-28 21:49:01"}
{"File Name":"front-end-monorepo\/adr-10.md","Context":"## Context\\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\\n\n## Decision\n","Decision":"We decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\\n``` js\\nconst workflow = {\\nid: '1',\\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\\n}\\n```\\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\\n**A note about the use of arrays for the key-value pairs**\\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\\n","tokens":320,"id":523,"Prediction":"## Decision\n\nAfter careful consideration of the requirements and the feedback provided by the team, we have decided to implement a custom type conversion solution for handling the serialization of ES6 Maps used for the `steps` attribute within the Mobx-State-Tree store.\n\n### Details of the Decision:\n1. **Custom Serialize\/Deserialize Functions:** \n   - We will write custom functions to convert ES6 Maps to a JSON-compatible format (e.g., an array of entries) before storing them to the Mobx-State-Tree store or sending them to Panoptes.\n   - Similarly, we will write custom functions to convert the JSON-compatible format back to ES6 Maps when loading from the Mobx-State-Tree store or receiving data from Panoptes.\n\n2. **Maintain Key Order:**\n   - The custom serialization ensures that the order of keys in the ES6 Map is preserved by converting the map to an array of key-value pairs.\n\n3. **Documentation and Code Clarity:**\n   - Comprehensive documentation will be added to clarify the custom serialization approach.\n   - Code comments will be provided to ease understanding by other Zooniverse developers, ensuring they know how to replicate the behavior in other client libraries or languages like Ruby or Python.\n\n4. **Reusability:**\n   - The custom functions will be designed to be reusable, modular, and easily integratable within the existing codebase to ensure maintainability over time.\n\n### Justification:\n- **Control Over Serialization:** Custom functions provide us with greater control over the type conversion process, ensuring the specific requirements related to key order are met.\n- **Flexibility for Future Adjustments:** By not relying on external libraries that may change behavior or become obsolete, we have the flexibility to adapt our serialization logic as needed.\n- **Transparency:** Custom conversion logic enhances the transparency of how maps are handled, making it easier for new team members or developers working in other languages to understand and implement compatible functionality.\n\n### Next Steps:\n- Development and testing of the custom serialization\/deserialization functions.\n- Implementation within the Mobx-State-Tree store.\n- Updating existing workflows to use the new serialization mechanism.\n- Documentation updates for developer guides and code comments.","GenTime":"2024-07-28 21:50:36"}
{"File Name":"front-end-monorepo\/adr-21.md","Context":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n\n## Decision\n","Decision":"Rewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","tokens":315,"id":527,"Prediction":"## Decision\nImplement a new configuration system for the `panoptes-javascript-client` that supports multiple configuration sources, enabling flexibility for Next.js-based applications.\n\n### Steps:\n1. **Abstract Configuration Loading**:\n   - Introduce an abstraction layer in the client that can load configuration from various sources (e.g., config files, environmental variables, query parameters).\n\n2. **Update Initialization Method**:\n   - Modify the initialization method of `panoptes-javascript-client` to accept a configuration object.\n   - Ensure the configuration object can be populated by the loading abstraction layer.\n\n3. **Create a Next.js Integration Module**:\n   - Develop a module or utility specifically for Next.js projects to easily manage and load configurations into the auth client.\n   - This module can leverage Next.js's built-in methods for sharing configuration between server and client.\n\n4. **Backward Compatibility**:\n   - Ensure that the new configuration system remains backward-compatible with existing applications relying on direct environment variable usage.\n\n5. **Documentation**:\n   - Update the documentation of `panoptes-javascript-client` to guide developers on how to use the new configuration system.\n   - Provide examples for common use-cases, including typical Next.js setups.\n\n### Rationale:\n- **Flexibility**: Allowing configuration from multiple sources provides flexibility to adapt to various deployment environments and project requirements.\n- **Maintainability**: Centralizing configuration logic within the client library ensures that configuration management is consistent and easier to maintain.\n- **Compatibility**: Ensuring backward compatibility allows current projects to migrate to the new system at their own pace.\n\n### Pros:\n- Streamlines configuration management.\n- Simplifies the setup process for new projects, particularly those using Next.js.\n- Facilitates potential future expansions with minimal changes to the core client.\n\n### Cons:\n- Requires initial development effort to introduce the abstraction layer and update existing methods.\n- Ongoing need to maintain compatibility with different configuration sources.","GenTime":"2024-07-28 21:50:49"}
{"File Name":"front-end-monorepo\/adr-15.md","Context":"## Context\\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\\n- Annotation \/ classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\\n\n## Decision\n","Decision":"What we do not want to do:\\n- Re-render on every pointer or touch event.\\n- update annotation state while drawing is in progress.\\n- support more than one drawing task in a step.\\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\\nWhat we do want to do:\\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse\/Panoptes-Front-End#5411\\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer\/observable event stream.\\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan\/zoom function. We have two proposed options in implementation:\\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\\n- Projects have requested each frame to have the same pan\/zoom function, but we were unable to implement in PFE: zooniverse\/Panoptes-Front-End#3465\\n- Are there any cases where projects want separate pan\/zoom function for each frame?\\n- Have a schema, or set of schemas, describing annotations.\\n","tokens":187,"id":528,"Prediction":"## Decision\nTo address the issues identified in the current implementation of the drawing tools on Panoptes-Front-End (PFE), the following decisions have been made:\n\n1. **Centralized State Management**:\n   - Implement a centralized state management solution (e.g., Redux or MobX) to handle classification annotations. This will ensure that there is a single source of truth for the annotation state, reducing ambiguity about which component is responsible for updates.\n   - Ensure that only one designated controller (e.g., an annotation manager) is responsible for updating the state.\n\n2. **Component Responsibility and Separation of Concerns**:\n   - Refactor the components to have clear separation of concerns. For example:\n     - `MarkingsRenderer` should be responsible solely for rendering the markings and should not handle state updates.\n     - `FrameAnnotator` should handle user interactions and inform the centralized state manager to update the state.\n   - Define clear roles and responsibilities for each component to avoid overlapping functionalities and improve maintainability.\n\n3. **Reduce Unnecessary Re-rendering**:\n   - Debounce updates to the classification annotation on touch or pointer events to reduce the frequency of DOM re-renders. This will improve performance by preventing excessive re-rendering.\n\n4. **Simplify API Design**:\n   - Simplify the API of drawing tools to avoid the need for static methods. Aim for a more declarative approach where possible.\n   - Ensure that the APIs are intuitive and easy to extend with new features.\n\n5. **Standardize Annotation Payloads**:\n   - Establish consistent standards for describing annotation\/classification data, including rotation, dimensions, and other properties.\n   - Implement and enforce these standards across all drawing tools to maintain consistency in the annotation data format.\n\n6. **Documentation and Code Clarity**:\n   - Provide detailed documentation for the responsibilities of each component and the overall architecture. This will help future developers understand the system more easily.\n   - Include comments and examples where necessary to clarify the purpose and use of each component and method.\n\nBy making these improvements, the drawing tools' functionality will become more reliable, easier to understand, and simpler to extend in the future.","GenTime":"2024-07-28 21:50:55"}
{"File Name":"front-end-monorepo\/adr-38.md","Context":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\\n\n## Decision\n","Decision":"- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\\n","tokens":119,"id":532,"Prediction":"## Decision\n\nTo enhance the user experience and provide volunteers with more flexibility in selecting subjects to classify, we'll modify the classifier's subject queue to support more robust browsing capabilities. Specifically, we will:\n\n1. Implement the ability for volunteers to view and revisit previously seen subjects in the subject queue, enabling backward navigation.\n2. Ensure that subjects are not discarded upon classification if the volunteer may wish to review them again. Instead, maintain a record of classified subjects in the queue until the browsing session is completed.\n3. Introduce a flag or status indicator for subjects that have already been classified, to avoid repeated classifications during a single session, while still allowing volunteers to review their work.\n4. Adjust the Panoptes API integration to support these new functionalities by managing subject states more dynamically, ensuring subjects can be reloaded efficiently as volunteers navigate backward or forward in the queue.\n\nThese changes will allow volunteers to make more informed decisions about which subjects to work on and provide a more flexible and user-friendly browsing experience.","GenTime":"2024-07-28 21:51:08"}
{"File Name":"tamr-client\/0006-type-checking.md","Context":"## Context\\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\\n\n## Decision\n","Decision":"Type-check via [mypy](http:\/\/mypy-lang.org\/).\\n","tokens":25,"id":536,"Prediction":"## Decision\nWe will implement static type-checking in our Python codebase using a tool such as MyPy. This decision leverages the existing type annotations to improve code quality and maintainability. This tool will be integrated into the CI\/CD pipeline to ensure that any type inconsistencies are caught early during the development process. The development team will also receive training on how to effectively use and maintain type annotations in Python. This approach is expected to reduce runtime errors, enhance code readability, and facilitate easier onboarding for new team members.","GenTime":"2024-07-28 21:51:23"}
{"File Name":"tamr-client\/0007-tamr-client-package.md","Context":"## Context\\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\\nBut, we want to rearchitect this codebase as a [library of composable functions](\/contributor-guide\/adr\/0005-composable-functions).\\n\n## Decision\n","Decision":"Implement rearchitected design as a new package named `tamr_client`.\\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\\n","tokens":64,"id":539,"Prediction":"## Decision\n\nTo maintain backward compatibility for existing users while facilitating the rearchitecting towards a library of composable functions, we will adopt a two-layered API strategy:\n\n1. **Compatibility Layer**: We will keep the existing `tamr_unify_client` as a stable API layer. This includes continuing to support critical bug fixes and patches without introducing breaking changes. Any significant new features will also be backported to this layer if feasible.\n\n2. **Composable Functions Layer**: In parallel, we will develop a new set of composable functions that represent the next generation of our API. These functions will follow the principles outlined in the [library of composable functions](link) ADR.\n\n### Key Actions:\n- **Incremental Migration Path**: Provide comprehensive documentation and supporting tools to help users migrate from the compatibility layer to the new composable functions. This includes a set of migration guides, code examples, and possibly an automated tool to refactor existing codebases.\n  \n- **Versioning Strategy**: Clearly differentiate the versions of our library offering the old compatibility layer versus the new composable functions. Ensure that major version changes reflect potentially breaking alterations and that the benefits of migrating are clearly communicated.\n\n- **User Communication**: Engage with the existing userbase to gather feedback and address concerns. Regular updates via newsletters, blog posts, and webinars will be conducted to inform users of the development progress and encourage community involvement in shaping the new API.\n\n- **Dual Maintenance Period**: Commit to maintaining both the compatibility layer and the new composable functions layer concurrently for a defined period (e.g., 1-2 years). This window will provide a grace period for users to adapt without disruption to their workflows.\n\nThis decision helps to balance the need for innovation and modernization with our commitment to our existing users, ensuring a smooth and structured transition.","GenTime":"2024-07-28 21:51:37"}
{"File Name":"molgenis-r-armadillo\/0002-manage-data-in-armadillo-suite.md","Context":"## Context\\nIn this project we manage the data in the Armadillo suite.\\n\n## Decision\n","Decision":"We will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\\n","tokens":18,"id":545,"Prediction":"## Decision\nWe will use a relational database management system (RDBMS) for managing the data in the Armadillo suite. Given the complexity and interrelated nature of the data, a relational model allows us to maintain data integrity, support complex queries, and ensure efficient transaction processing. Specifically, we will use PostgreSQL due to its robustness, scalability, and extensive support for advanced data types and indexing. This decision aligns with our need for a highly reliable and performant database solution to manage the suite's data effectively. Additionally, PostgreSQL\u2019s open-source nature fits well with our project's budget and licensing constraints.","GenTime":"2024-07-28 21:51:55"}
{"File Name":"play-frontend-hmrc\/0012-create-an-endpoint-in-play-frontend-hmrc-for-surfacing-session-metadata.md","Context":"## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n\n## Decision\n","Decision":"* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n","tokens":348,"id":560,"Prediction":"## Decision \n\nWe will implement a centralized session management endpoint into the existing `bootstrap-play` library that can be leveraged by the `hmrcTimeoutDialog` component in a secure, efficient, and scalable manner. Below are the key elements of the decision:\n\n1. **Centralized Endpoint Implementation**: Introduce an endpoint in the `bootstrap-play` library that will provide the remaining time in the user's session. This endpoint will be designed to not extend or affect the session's lifespan upon invocation.\n\n2. **Excluded from Session Management**: Ensure the session timeout endpoint is excluded from regular session management processes to avoid inadvertent session refreshes. This exclusion will be configured within the `SessionTimeoutFilter` to guarantee it does not extend the user's session.\n\n3. **Library Upgrade**: Service teams will only need to upgrade their version of the `bootstrap-play` library to access this new endpoint. No additional routing rules or configuration changes will be necessary on their part, thereby minimizing their workload.\n\n4. **JavaScript Integration**: Modify the `hmrcTimeoutDialog` component to call the centralized session timeout endpoint periodically. This will allow it to accurately reflect the remaining session time, and prevent users from unnecessarily being signed out due to multiple tabs or windows.\n\n5. **Loose Coupling and No Duplication**: By incorporating the timeout endpoint into `bootstrap-play`, we avoid the duplication of timeout logic across frontend services and maintain a loosely coupled architecture. This solution ensures that service teams need not concern themselves with additional session management code.\n\n6. **Security Considerations**: The timeout endpoint will include robust security measures, such as requiring authentication and permission checks to ensure only authorized and authenticated sessions can be queried.\n\nBy adopting this approach, we align with our decision drivers:\n- **Minimal Code Changes for Service Teams**: By only needing a library upgrade.\n- **No Additional Routing Rules or Configuration**: Simplifying implementation across services.\n- **Maintained Loose Coupling**: Avoiding single points of failure and excessive inter-service dependencies.\n- **Single Source of Truth for Session State**: Ensuring consistency and reducing complexity in session management.\n\nThis solution effectively addresses the problem of unexpected user sign-outs while accessing MDTP services using multiple browser tabs or windows, and ensures compliance with the specified WCAG criterion.","GenTime":"2024-07-28 21:52:46"}
{"File Name":"kuona\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":573,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) process to document and manage all the significant architectural decisions made throughout the software development lifecycle. \n\nThis process will involve:\n\n1. **Creating ADR Templates**: A standard template will be used for consistency. It will include sections such as Context, Decision, Status, Consequences, and Alternatives Considered.\n  \n2. **Version Control**: All ADRs will be stored in a dedicated directory within the project's version control system (e.g., Git), ensuring they are versioned along with the source code.\n  \n3. **Stakeholder Involvement**: Relevant stakeholders, including architects, developers, product owners, and possibly clients, will be involved in discussions before finalizing any ADR.\n  \n4. **Review and Approval**: Each ADR will be reviewed and approved by a designated architecture review board or a similar governance body to ensure alignment with the overall architectural vision and principles.\n  \n5. **Regular Updates**: ADRs will be periodically reviewed and updated to reflect any changes in decisions or the discovery of new information.\n  \n6. **Transparency and Accessibility**: Documentation will be kept clear, concise, and accessible to all team members to foster understanding and alignment.\n  \nBy having a structured ADR process in place, we aim to ensure that our architectural decisions are well-documented, transparent, and made with a comprehensive understanding of the context and implications.","GenTime":"2024-07-28 21:53:45"}
{"File Name":"verify-onboarding-prototypes\/0006-we-will-build-a-js-client.md","Context":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n\n## Decision\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":577,"Prediction":"## Decision\n\nWe will introduce a flexible and modular authentication service that provides seamless integration with Node.js and Passport.js, while also being adaptable to other languages and frameworks.\n\n### Key points:\n\n1. **Modular Design:** Create the authentication service as a set of modules, ensuring that each component can be easily integrated or extended based on user needs.\n\n2. **Node.js and Passport.js Integration:** Provide an out-of-the-box integration specifically tailored for Node.js and Passport.js users. This should include comprehensive documentation, examples, and ready-to-use code snippets to minimize friction.\n\n3. **RESTful API:** Develop a RESTful API for the authentication service, allowing it to be accessed by applications built in different languages and frameworks. This ensures that the service can be used universally, regardless of the underlying technology stack.\n\n4. **SDKs and Libraries:** Create SDKs and libraries for popular languages and frameworks to facilitate integration. Prioritize languages and frameworks based on user demand and community support.\n\n5. **Extensive Documentation:** Maintain detailed and tutorial-rich documentation, including setup guides, API references, and best practices for integrating the authentication service with different technologies.\n\n6. **Community and Support:** Establish a support channel and community forum to assist users with integration challenges and gather feedback for continuous improvement.\n\nBy taking this approach, we will ensure that the authentication service meets the immediate needs of our Node.js and Passport.js user while remaining flexible enough to support a wide variety of other languages and frameworks.","GenTime":"2024-07-28 21:53:58"}
{"File Name":"verify-onboarding-prototypes\/0002-how-do-we-secure-the-api.md","Context":"## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n\n## Decision\n","Decision":"If possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n","tokens":59,"id":578,"Prediction":"## Decision\n\nWe have decided to implement Transport Layer Security (TLS) for securing the interaction between the client code (Node.js) and the server-side code (Dropwizard app).\n\n### Rationale:\n\n1. **Mutual Authentication**:\n   - We'll use mutual TLS (mTLS) to authenticate both client and server, ensuring that both parties are communicating with verified identities.\n\n2. **Data Encryption**:\n   - TLS will encrypt all data transmitted between the client and server, ensuring that sensitive information is protected from eavesdropping and tampering.\n\n3. **Flexibility**:\n   - TLS is versatile and widely supported. It allows for configuration tweaks based on user requirements or specific deployment environments, providing adaptability to various security needs.\n\n4. **Standards Compliance**:\n   - Adopting TLS aligns with industry best practices and standards, ensuring that our security implementation is robust and credible.\n\n5. **Certificate Management**:\n   - We will set up a manageable certificate handling process, possibly leveraging automated tools (e.g., Let's Encrypt, Certbot) for generating and renewing certificates to lower the operational burden.\n\n#### Implementation Steps:\n\n- **Step 1**: Configure Dropwizard to support TLS.\n  - Update the Dropwizard `server.yml` configuration file to enable HTTPS and specify the keystore and truststore details.\n  \n- **Step 2**: Enable and configure HTTPS in the Node.js client.\n  - Utilize Node.js's `https` module and configure it to use the appropriate client certificates.\n\n- **Step 3**: Implement mutual TLS.\n  - Configure both the Dropwizard application and the Node.js client to provide and validate mutual TLS certificates.\n  \n- **Step 4**: Set up automated certificate issuance and renewal.\n  - Integrate tools like Let's Encrypt and Certbot to automate the generation and renewal of TLS certificates for both client and server.\n\n- **Step 5**: Testing and Validation.\n  - Thoroughly test the TLS setup to validate mutual authentication, data encryption, and proper error handling for certificate-related issues.\n\nBy adopting this decision, we ensure a secure communication channel between the client and server, adaptable for differing user deployment scenarios.","GenTime":"2024-07-28 21:54:04"}
{"File Name":"verify-onboarding-prototypes\/0008-provide-an-end-to-end-stub.md","Context":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n\n## Decision\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":581,"Prediction":"## Decision\nWe will implement a dedicated testing environment that simulates the interface between the Service and the Hub, closely mirroring the typical Verify journey. This environment will be adaptable to allow for iterative changes based on user research findings. \n\n1. **Test Environment Setup**: We will create a staging environment that mimics the actual production setup, focusing particularly on the Service-Hub interface.\n  \n2. **Mock Data and APIs**: Integrate mock data and APIs to simulate real-world scenarios and user interactions within this environment.\n\n3. **User Journey Simulation**: Develop comprehensive user journeys that closely resemble typical Verify interactions, ensuring that testers can provide meaningful insights.\n\n4. **Analytics and Monitoring**: Implement robust analytics and monitoring tools to capture detailed data on user interactions within the test environment.\n\n5. **Feedback and Iteration**: Establish a feedback loop where data and insights garnered from user research can quickly inform iterative improvements to both the testing environment and the product.\n\nBy setting up this dedicated testing environment, we ensure that our user research is grounded in realistic scenarios, providing us with actionable insights to refine the Service Provider\u2019s interface and user experience.","GenTime":"2024-07-28 21:54:13"}
{"File Name":"old-web-frontend\/0002-use-aws-s3-as-host-solution.md","Context":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n\n## Decision\n","Decision":"* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","tokens":53,"id":583,"Prediction":"## Decision\n\nTo host our web application, we will use a managed cloud service like AWS Elastic Beanstalk.\n\n### Rationale\n\n1. **Easy to Update**: AWS Elastic Beanstalk abstracts the underlying infrastructure management, providing automated deployment and updates, which simplifies maintaining and rolling out new releases.\n  \n2. **High Availability**: AWS Elastic Beanstalk supports load balancing and auto-scaling out of the box, thereby ensuring high availability and reliability without extensive manual configuration.\n  \n3. **Easy to Configure HTTPS**: AWS provides integrated support for HTTPS\/SSL configuration, making it straightforward to secure our web application.\n  \n4. **Observability**: AWS Elastic Beanstalk offers integration with CloudWatch, which allows comprehensive monitoring and logging, supporting fast troubleshooting and performance tracking.\n\nBy using AWS Elastic Beanstalk, we can efficiently manage, update, and scale our web application while ensuring high availability and secure communication.","GenTime":"2024-07-28 21:54:17"}
{"File Name":"abracadabra\/0008-don-t-propose-quick-fix-for-react-convert-to-pure-component.md","Context":"## Context\\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the \"Convert to Pure Component\" Quick Fix took most of the time:\\n![][flame-chart]\\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https:\/\/github.com\/reactjs\/react-codemod). It was originally meant to be run through a CLI command.\\n\n## Decision\n","Decision":"We will stop proposing a Quick Fix for this refactoring.\\n","tokens":122,"id":586,"Prediction":"## Decision\nWe will remove the \"Convert to Pure Component\" Quick Fix from the on-demand triggers to reduce performance overhead. Instead, the Quick Fix will be available only through manual invocation. This adjustment ensures that we do not compromise the performance of our extension for all users because of a refactoring option that only applies under specific conditions and cannot be optimized further. Documentation and user guidance will be updated to reflect this change, providing steps for manually accessing the \"Convert to Pure Component\" Quick Fix when needed.","GenTime":"2024-07-28 21:54:25"}
{"File Name":"ehoks-ui\/0002-use-finnish-as-the-domain-language.md","Context":"## Context\\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\\nin Finnish but react component props in English even though data might be exactly the same.\\n\n## Decision\n","Decision":"We will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\\nconventions quite some time.\\nHere is one expample:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisDemonstration?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\nperiodSpecifier?: string\\ndemonstrationEnviromentDescription?: string\\n}\\n```\\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\\nOsaamisenOsoittaminen components.\\nExample fixed:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisNaytto?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\najanjaksonTarkenne?: string\\nnayttoymparistoDescription?: string\\n}\\n```\\n","tokens":41,"id":598,"Prediction":"## Decision\n\nWe will standardize the translation of domain words across the entire codebase to use English for all properties and props, regardless of their origin. This decision is made to promote consistency, improve collaboration, and reduce the cognitive load for developers working across different parts of the application. Specifically:\n\n1. All `mobx-state-tree-model` properties will be translated to English.\n2. All React component props will remain in English.\n3. Any data being transferred between models and components will maintain a uniform naming convention in English.\n\nThis strategy will ensure uniformity and clarity, and facilitate easier onboarding of new team members and collaboration with external developers. \n\nAdditionally, documentation and code comments will be updated to reflect these changes, ensuring that all references are consistent and comprehensible.","GenTime":"2024-07-28 21:55:07"}
{"File Name":"lockfix\/0001-documenting-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":611,"Prediction":"## Decision\nWe will implement and maintain a formal Architectural Decision Record (ADR) system for this project. Each architectural decision will be documented using a consistent template and stored in a version-controlled repository. This approach ensures that all significant decisions are recorded, justified, and can be easily accessed and reviewed by current and future team members. The ADRs will provide clear, historical insights into why certain architectural paths were chosen over others, facilitating better-informed decisions moving forward.","GenTime":"2024-07-28 21:55:53"}
{"File Name":"cloud-platform\/018-Dedicated-Ingress-Controllers.md","Context":"## Context\\nThe Cloud Platform was originally set up to have a single ingress controller to\\nmanage all ingresses in the cluster. So, every new ingress added a config block\\nto one, large nginx config file, and all traffic to all services hosted on the\\nCloud Platform is routed through a single AWS load balancer.\\nAlthough this was both easy to manage, and saved us some money on load\\nbalancers (approx. $25\/month per ingress), it has become unsustainable. We\\nusually have 6 replicas of the ingress controller pod, and we have started to\\nsee instances of several of these pods crash-looping (usually because they have\\nrun out of shared memory, which cannot be increased in kubernetes. See [this\\nissue] for more information).\\nWe believe this is because the nginx config has become so large (over 100K\\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\\nmoved.\\n\n## Decision\n","Decision":"We will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n","tokens":200,"id":620,"Prediction":"## Decision\n\nTo address the issues stemming from a large and unsustainable single ingress controller setup, we will transition to a distributed ingress controller architecture. Specifically, the following actions will be taken:\n\n1. **Dedicated Ingress Controllers**: Introduce multiple ingress controllers, each dedicated to a subset of services. This will reduce the size of each nginx configuration file and mitigate the risk of out-of-memory issues.\n   \n2. **Multiple AWS Load Balancers**: Instead of routing all traffic through a single AWS load balancer, we will provision multiple load balancers and associate them with the respective ingress controllers. While this will incur additional costs, the benefits of improved stability and manageability outweigh the financial impact.\n\n3. **Config Segmentation**: Segment the ingress configurations by service type or criticality (e.g., production vs. non-production traffic), allowing for better isolation and easier troubleshooting.\n\n4. **Monitoring and Scaling**: Implement comprehensive monitoring for the ingress controllers and load balancers to identify performance bottlenecks promptly. Use the insights gained to automatically scale the ingress controllers and adjust their configurations as needed.\n\n5. **Resource Management**: Review and optimize resource allocation for each pod, ensuring that each ingress controller has ample resources to operate efficiently without exhausting the shared memory.\n\nThis decision aims to enhance the stability, performance, and manageability of the ingress controllers in the Cloud Platform, ensuring the system can scale effectively with future growth.","GenTime":"2024-07-28 21:56:30"}
{"File Name":"cloud-platform\/015-Keeping-Terraform-modules-up-to-date.md","Context":"## Context\\nWe maintain a lot of [terraform modules] which teams use to manage AWS\\nresources for their namespaces.\\nIn the past, lettings different namespaces use different versions of terraform\\nmodules has caused problems because there has not always been a clear upgrade\\npath from a module that is several versions old, to the latest version.\\nIn these cases, it would have been easier to have upgraded every namespace when\\nthe significant change was made to the module, because at that point in time\\nthe relevant knowledge and understanding are fresh. Leaving it until later\\nmakes the whole process much more difficult.\\n\n## Decision\n","Decision":"We decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n","tokens":124,"id":621,"Prediction":"## Decision\n\nWe will enforce a policy to maintain uniformity in the versions of Terraform modules used across all namespaces. This will be achieved by mandating that when a significant update is made to any Terraform module, all namespaces using that module must also be updated to the latest version within a reasonable, predefined window of time.\n\nFor this purpose, we will:\n\n1. Implement automated checks and CI\/CD pipelines to detect and notify about version discrepancies.\n2. Allocate resources to support and facilitate rapid upgrades across namespaces whenever a significant change is introduced.\n3. Maintain a detailed change log and upgrade path documentation for each module to ease the transition process.\n4. Provide training and workshops for teams to ensure they are equipped with the knowledge and understanding necessary to perform these upgrades effectively.\n5. Establish a rollback plan to mitigate any issues arising from the upgrade process, thereby ensuring minimal impact on namespaces.\n\nThis approach will prevent the accumulation of technical debt and ensure all teams are aligned and up-to-date with the latest features and security patches, ultimately fostering a more stable and maintainable infrastructure.","GenTime":"2024-07-28 21:56:34"}
{"File Name":"cloud-platform\/005-ECR-monitoring-and-alerting.md","Context":"## Context\\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\\n\n## Decision\n","Decision":"After discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\\n","tokens":155,"id":629,"Prediction":"## Decision\nTo address the concerns raised by application users about the imposition of a lifecycle policy limit that silently deletes images, we have decided to take the following steps:\n\n1. **Enhanced Communication**: Notify all teams about the existing lifecycle policy and its implications. Clearly communicate that older images beyond the most recent 100 will be automatically deleted to avoid unexpected data loss.\n\n2. **Zero-Tolerance Monitoring Period**: Implement a monitoring period during which the lifecycle policy will not be enforced. This will give teams time to adapt and implement their own image management strategies.\n\n3. **Custom Lifecycle Policies**: Allow teams to define their own lifecycle policies tailored to their specific needs if they wish to deviate from the default policy of retaining the 100 most recent images. Provide adequate documentation and support to guide teams through this process.\n\n4. **Notification Mechanism**: Develop and implement a notification mechanism to alert teams when they are approaching the image retention limit. This should provide them with enough time to take appropriate action before automated deletions occur.\n\n5. **Backup Strategy**: Encourage teams to have a backup strategy for managing their Docker images, such as periodically archiving older images outside of the ECR, to ensure critical images are not lost.\n\n6. **Tooling Support**: Invest in tooling or scripts that can assist teams in cleaning up their ECR repositories more effectively, thereby minimizing the risk of hitting the image cap.\n\n7. **Review and Feedback Loop**: Establish a regular review and feedback loop with all teams to gather insights on the impact of the lifecycle policy and make adjustments as necessary to meet their operational needs.\n\nBy taking these steps, we aim to balance the need to manage ECR repository limits effectively while also addressing the potential risks and operational concerns raised by the application users.","GenTime":"2024-07-28 21:57:05"}
{"File Name":"cloud-platform\/020-Environments-and-Pipeline.md","Context":"## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n\n## Decision\n","Decision":"1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.\\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\\n* RBAC - teams can only administer k8s resources in their own namespaces\\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\\n4. Isolation between AWS resources is achieved using access control.\\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\\n6. The namespace definition is held in GitHub.\\nGitHub provides a mechanism for peer-review, automated checks and versioning.\\nOther options considered for configuring a namespace do not come with these advantages, for example:\\n* a console \/ web form, implemented as a custom web app (click ops)\\n* commands via a CLI or API\\nNamespace definitions are stored in the [environments repo](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments)\\n7. Namespace changes are checked by both a bot and a human from the CP team\\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\\nTo mitigate these concerns:\\n* [automated checks](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/main\/.github\/workflows) are used to validate against common problems\\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\\n8. Pipeline to deploy namespace automatically.\\nThe \"deploy pipeline\" is a CI\/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\\n9. Teams have full control within their Kubernetes namespace\\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI\/CD, for deploying their app.\\n","tokens":152,"id":631,"Prediction":"## Decision\n\nImplement a layered Kubernetes management strategy with a combination of standard baseline configurations and customizable overrides for each service team to address the given concerns.\n\n1. **Standard Definitions**:\n   - Provide a standardized template for workloads and infrastructure definitions using Kubernetes manifests, Helm charts, or similar tools. This allows service teams to specify their needs consistently and predictably.\n\n2. **Control through Default and Customizable Configurations**:\n   - **Default Configuration Layer**: Offer an out-of-the-box, default configuration that simplifies the initial setup for service teams. This involves pre-defined settings for common resources, networking, security policies, and storage options to make onboarding as simple as a typical Platform as a Service (PaaS).\n   - **Custom Configuration Layer**: Allow service teams to override the default settings and take full control of Kubernetes resources, including pods, deployments, services, ingress, configuration maps, secrets, and networking aspects. Teams should be able to customize their configurations to suit specific application requirements and optimization targets.\n\n3. **Multi-Tenancy and Isolation**:\n   - Implement Kubernetes namespaces for logical separation of service teams' workloads. Each team will have dedicated namespaces for their development and production environments, facilitating isolation.\n   - Use Kubernetes Role-Based Access Control (RBAC) to define permissions within namespaces, ensuring that teams can only interact with resources within their own namespaces.\n   - Employ network policies to enforce connectivity rules, ensuring that workloads from different teams or environments do not interfere with each other unless explicitly allowed.\n\n4. **Operational Support and Tooling**:\n   - Provide tooling and support scripts for managing configurations, monitoring, logging, and debugging. This includes integrating with Continuous Integration\/Continuous Deployment (CI\/CD) pipelines, offering pre-configured monitoring solutions such as Prometheus\/Grafana, and providing logging mechanisms like ELK (Elasticsearch, Logstash, Kibana).\n\nThis layered approach ensures a balance between ease of use and flexibility, empowering service teams to deploy and manage their applications effectively while maintaining clean separations and secure multi-tenancy.","GenTime":"2024-07-28 21:57:16"}
{"File Name":"kafka\/0002-require-php-7-2-64-bit.md","Context":"## Context\\nDeciding which minimum PHP version to require is quite important because it\\nmight restrict people from using the library, at the same time supporting many\\nversions increases the cost to maintain the library.\\n\n## Decision\n","Decision":"For the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\\nonly).\\nThe main motivation for only allowing it to be installed in 64-bit systems is\\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\\ncan't be achieve in 32-bit systems.\\nI believe that libraries' maintainers also have the role to help the evolution\\nof the PHP ecosystem, which makes me want to require a more up-to-date version\\nof the language.\\n","tokens":42,"id":641,"Prediction":"## Decision\n\nWe will require PHP 7.4 as the minimum version for using our library. \n\n### Justification:\n1. **Support Lifecycle**: PHP 7.4 is still actively supported and receives security updates. Requiring this version helps ensure that our library runs on a secure and maintained PHP runtime.\n2. **Modern Features**: PHP 7.4 introduces several modern features such as typed properties, arrow functions, and performance improvements which can streamline development and improve code quality.\n3. **Maintenance Cost**: By not supporting older versions, the complexity and cost of maintaining backward compatibility are reduced. This allows us to focus on improving the library rather than dealing with compatibility issues.\n4. **Adoption Rate**: Based on current statistics, a significant portion of our user base has already adopted PHP 7.4 or higher, minimizing the impact on current users.\n5. **Future-Proofing**: Setting a more recent minimum version helps prepare the library for smoother transitions to future PHP versions.\n\nThis decision balances the need for modernity and improved maintainability with the realities of our user base's current PHP version adoption.","GenTime":"2024-07-28 21:57:46"}
{"File Name":"cena\/0014-adopt-commmand-query-separation.md","Context":"## Context\\n[Command-query separation](https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n\n## Decision\n","Decision":"Command-query separation will be enforced in the [core hexagon](.\/0003-adopt-hexagonal-architecture.md), especially in\\napplication services.\\n","tokens":113,"id":647,"Prediction":"## Decision\n\nThe decision is to formally adopt Command-Query Separation (CQS) as a core design principle in the `menu-generation` application. This means that every method in the codebase will be explicitly classified as either a command or a query:\n\n1. **Commands** will be responsible for performing actions that change the state of the system or produce some observable side effects. They will not return any data.\n\n2. **Queries** will be responsible for returning data to the caller, but will not alter the state of the system in any way.\n\n### Implementation Guidelines:\n\n- **Code Review**: Introduce a checklist in the code review process to ensure that new methods adhere to the CQS principle.\n\n- **Naming Convention**: Use clear and consistent naming conventions to distinguish between commands and queries. For example, use action-oriented verbs for commands (`createMenu()`, `deleteMenu()`) and descriptive names for queries (`getMenuById()`, `listAllMenus()`).\n\n- **Documentation**: Update the project documentation to include guidelines and examples on how to apply CQS. Ensure the architectural rationale for adopting CQS is well-documented to aid in onboarding new team members.\n\n- **Refactoring**: Gradually refactor existing methods that do not conform to CQS, starting with the most critical sections of the codebase. Prioritize areas that are prone to bugs or are complex to maintain.\n\n- **Testing**: Augment the current testing strategy to account for the CQS principle, ensuring comprehensive testing of both commands and queries, with a focus on command effects and querying accuracy.\n\nBy formally adopting CQS, the `menu-generation` application will benefit from a clearer separation of concerns, improved maintainability, and enhanced reliability, thereby facilitating easier debugging and more predictable changes.","GenTime":"2024-07-28 21:58:14"}
{"File Name":"cena\/0005-use-spring-framework.md","Context":"## Context\\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\\nservices implementations dependencies into the services defined in the core hexagon.\\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\\nSpring for many years and masters many of the provided features.\\n\n## Decision\n","Decision":"Spring framework will be the backbone for `menu-generation` application.\\n","tokens":120,"id":649,"Prediction":"## Decision\n\nWe will adopt the Spring framework to implement the inversion of control (IoC) and the REST API functionality required for our application. This decision is driven by the following considerations:\n\n1. **Alignment with Hexagonal Architecture**: Spring's IoC container will allow us to seamlessly inject infrastructure service implementations into the core hexagon services, supporting the principles of hexagonal architecture.\n\n2. **REST API Implementation**: Spring's REST framework will be used to define the incoming adapters handling HTTP resources. This will ensure our REST API is robust, flexible, and adheres to industry standards.\n\n3. **Established Framework**: Spring is a well-established and widely-used framework in the Java ecosystem. It provides a comprehensive set of features that will help us implement our requirements efficiently, including IoC, Aspect-Oriented Programming (AOP), and advanced security mechanisms.\n\n4. **Developer Expertise**: The author and other team members have extensive experience with Spring, which will reduce the learning curve and increase productivity by leveraging familiar tools and practices.\n\nBy utilizing the Spring framework, we will ensure a well-structured, maintainable, and scalable implementation that aligns with our architectural goals and team expertise.","GenTime":"2024-07-28 21:58:21"}
{"File Name":"cena\/0009-test-features-with-cucumber.md","Context":"## Context\\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\\nDefining new acceptance tests must be easy, through reusable step definitions.\\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development)\\napproach.\\n\n## Decision\n","Decision":"[Cucumber](https:\/\/cucumber.io\/) will be used to describe and execute acceptance tests in `menu-generation` application.\\n","tokens":111,"id":650,"Prediction":"## Decision\n\nThe team will adopt a Behavior-Driven Development (BDD) framework, such as Cucumber or SpecFlow, for defining and running acceptance tests. This decision is driven by the need to make acceptance tests understandable by non-technical users while ensuring they are seamlessly integrated into the development lifecycle. The chosen BDD framework will allow the use of Gherkin syntax to write tests in a human-readable format, making it easy for both technical and non-technical stakeholders to understand and contribute to the test cases. \n\nTo ensure the application sources remain self-contained, all acceptance test definitions and their implementations will be stored within the main application repository. This allows for consistent versioning and ensures that acceptance tests can be executed automatically as part of the continuous integration (CI) pipeline.\n\nAdditionally, we will establish a set of reusable step definitions to facilitate easy and consistent creation of new acceptance tests. These step definitions will be documented and maintained as part of the project's testing guidelines to help new and existing team members quickly ramp up on BDD practices.\n\nBy taking these steps, we aim to enhance collaboration between technical and non-technical stakeholders, improve documentation of application features, and integrate acceptance testing seamlessly into the development process.","GenTime":"2024-07-28 21:58:25"}
{"File Name":"opensmarthouse-core\/adr0000.md","Context":"## Context\\nOpenHAB uses OSGi as a runtime.\\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\\nSuch approach promotes tight coupling between contract and implementation.\\nMore over, it also promotes exposure of specific implementation classes via public API.\\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\\nIt would work properly, but openHAB core is a single jar which makes things even harder.\\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\\nopenHAB utilizes Apache Karaf for provisioning of the application.\\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\\nPresent structure of modules \/ bundles is as follows:\\n```\\n[openhab thing core] <--- [openhab rest core]\\n```\\n\n## Decision\n","Decision":"Since openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n","tokens":471,"id":660,"Prediction":"## Decision\n\nTo address the issue of tight coupling between API and implementation packages and to prevent the propagation of unnecessary dependencies to all callers of public APIs, we will take the following steps:\n\n1. **Refactoring the Project Structure**: We will refactor the existing project structure to separate APIs, SPIs, and internal implementation packages into distinct modules. This will ensure that internal implementation classes are not exposed through public APIs and will help maintain a clear boundary between public contracts and internal workings.\n\n2. **Modularization Strategy**: We will introduce a modularization strategy where each functional aspect of openHAB is encapsulated within its own module. For instance:\n   - `org.openhab.core.items` will be moved to a separate module `openhab-core-items-api`.\n   - `org.openhab.core.items.internal` will be moved to a separate module `openhab-core-items-impl`.\n\n3. **Dependency Management**: We will enhance our build tool configuration (e.g., using Maven or Gradle) to enforce strict dependency rules, ensuring that API modules do not accidentally depend on implementation modules. This may include setting up visibility rules so that only necessary dependencies are exposed.\n\n4. **Granular Feature Definition for Apache Karaf**: We will redefine the features used in Apache Karaf provisioning to be more granular. For example:\n   - Introduce a feature like `openhab-core-items`, which only includes the necessary API and SPI bundles.\n   - Avoid using monolithic features like `openhab-core-base` and `openhab-runtime-base` without distinguishing the individual components required.\n\n5. **Compilation Isolation**: Implement a mechanism that isolates the compilation of public API packages from internal implementation packages. This might involve setting up separate compilation units or using a multi-project build where APIs and implementations are built independently.\n\n6. **Review and Documentation**: Conduct a thorough review of the existing dependencies and usage patterns to identify and refactor any instances where internal implementation details are exposed or tightly coupled with APIs. Document the changes and update the contribution guidelines to ensure future code contributions adhere to the new modularization strategy.\n\nBy adopting these measures, we will achieve a more maintainable, modular, and loosely coupled architecture, which will significantly reduce the risk of dependency pollution and improve the overall robustness of the openHAB framework.","GenTime":"2024-07-28 21:59:09"}
{"File Name":"editions\/07-\u2705-ophan-analytics.md","Context":"## Context\\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\\n\n## Decision\n","Decision":"```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n","tokens":42,"id":669,"Prediction":"## Decision\nWe will implement a cross-platform analytics module that abstracts the interaction with Ophan. This module will be developed in a way that ensures it can handle both iOS and Android platforms, thereby reducing code duplication and maintenance overhead.\n\n### Key Decisions:\n1. **Cross-Platform Framework**: Use a cross-platform development framework such as React Native or Flutter for the analytics module to ensure consistency across both iOS and Android.\n2. **Single Entry Point**: Create a single entry point for analytics events within the app architecture, ensuring that all events and screen views are captured in the same manner, regardless of the platform.\n3. **Common Data Format**: Define a common data format for the analytics events to ensure that data sent to Ophan is consistent and easily parseable.\n4. **Error Handling**: Implement robust error handling and logging to ensure any issues with analytics data transmission are captured and can be debugged efficiently.\n5. **Performance Optimization**: Ensure that sending analytics data does not adversely affect the performance of the application. Implement batch processing and background transmission of analytics data where possible.\n6. **Compliance and Privacy**: Ensure that the collection and transmission of analytics data comply with all relevant privacy laws and regulations (e.g., GDPR, CCPA).\n\n### Steps for Implementation:\n1. **Requirement Gathering**: Gather detailed requirements from the stakeholders regarding the types of events and screen views that need to be tracked.\n2. **Framework Selection**: Evaluate and choose a cross-platform framework best suited for our existing application stack.\n3. **Module Development**: Develop the analytics module with the specified entry points, data format, error handling, and performance optimizations.\n4. **Integration with Ophan**: Ensure that the module correctly interfaces with Ophan\u2019s API and reliably sends the expected data.\n5. **Testing**: Rigorously test on both iOS and Android platforms to ensure no discrepancies or issues.\n6. **Deployment and Monitoring**: Deploy the updated app version with the new analytics module and monitor its performance and data accuracy. \n\nThis decision ensures that our effort is streamlined, maintaining consistency and quality in the analytics data collection process across both mobile platforms.","GenTime":"2024-07-28 21:59:43"}
{"File Name":"editions\/06-\u2705-to-use-step-functions.md","Context":"## Context\\nThe archiving process is time consuming and consists of many stages.\\n\n## Decision\n","Decision":"For the archiving process to be run as an amazon step function so we can break the process down into component stages.\\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\\nThis will also allow the publication process to fire the notification at the desired time.\\n","tokens":17,"id":672,"Prediction":"## Decision\nTo address the time-consuming nature and complexity of the archiving process, we have decided to implement a multi-threaded, asynchronous processing framework. This decision aims to parallelize the archival stages, allowing multiple tasks to be handled simultaneously and reducing overall processing time. Additionally, we will introduce a modular design for the archival stages, making it easier to maintain and optimize individual components without affecting the entire process.\n\nKey actions include:\n1. **Adopting a Multi-Threaded Framework**: Leveraging libraries and tools that support concurrent processing to handle different stages of archiving in parallel.\n2. **Asynchronous Operations**: Implementing async\/await patterns where applicable to ensure non-blocking operations, especially for I\/O-bound tasks.\n3. **Modular Design**: Structuring each stage of the archival process as an independent module with well-defined interfaces, facilitating easier debugging, testing, and future enhancements.\n4. **Monitoring and Logging**: Integrating comprehensive logging and monitoring to track the performance of each stage, quickly identify bottlenecks, and ensure reliability.\n\nThis approach aims to significantly improve the efficiency and maintainability of the archiving process.","GenTime":"2024-07-28 21:59:54"}
{"File Name":"Sylius\/2020_05_13_handling_non_crud_operations_in_api.md","Context":"## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`\/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`\/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n\n## Decision\n","Decision":"* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n","tokens":225,"id":674,"Prediction":"## Decision\nAfter evaluating the complexities and requirements associated with handling non-CRUD operations over a REST API, we have decided to adopt a hybrid approach that combines REST principles with some elements of RPC (Remote Procedure Call) to achieve flexibility, clarity, and ease of implementation for advanced operations.\n\n### Key Aspects of the Decision:\n\n1. **Action Endpoints:**\n   - Introduce dedicated action endpoints for non-CRUD operations. These endpoints will clearly indicate the nature of the operation through URL paths and HTTP methods.\n   - For example:\n     - `\/resources\/{id}\/approve` for approval operations\n     - `\/orders\/{id}\/cancel` for cancelation operations\n   - This provides clarity and explicitness regarding the operation being performed.\n\n2. **Action Identifiers:**\n   - Use distinct HTTP methods such as `POST` for invoking actions that change the state or execute commands. Ensuring that the action is clearly identified in the URL path helps in distinguishing between different operations.\n   \n3. **Hypermedia as the Engine of Application State (HATEOAS):**\n   - Embed links in resource representations to signal possible next steps. This will guide clients about the available actions and endpoints dynamically.\n   - For instance, an order resource in a \"pending\" state might have hyperlinks to actions like `approve`, `cancel`, and advance to the next state.\n\n4. **State Machines:**\n   - Maintain state machines to handle different workflows for various resources. Each state machine will encapsulate the business logic, next possible actions, and transitions.\n   - URI conventions for state-based actions can include the state or action name to facilitate clear mapping.\n   - This could potentially include URIs such as `\/resources\/{id}\/state\/{state}` to reflect transitions.\n\n5. **Response Metadata:**\n   - Ensure responses include metadata indicating action results, state transitions, and any necessary additional fields.\n   - This could involve returning detailed responses that encapsulate the current state and possible next states\/actions.\n\n6. **Middleware for Validation and Authorisation:**\n   - Implement middleware that validates whether the current state of the resource allows for the requested action and whether the user has the necessary permissions.\n\n### Implementation Steps:\n\n1. Design and implement dedicated action endpoints specific to resources and operations as defined.\n2. Introduce HATEOAS links to resource representations to provide guidance on next actions.\n3. Develop internal state machines that encapsulate business workflows and associated metadata.\n4. Ensure robust response structures that provide clarity on state transitions and action outcomes.\n5. Incorporate middleware layers to enforce business rules and user authorization on non-CRUD operations.\n\nThis approach ensures that complex operations beyond basic CRUD actions are well-defined, easy to understand, and dynamically discoverable through hypermedia links, enhancing both flexibility and usability across the API consumers.","GenTime":"2024-07-28 22:00:03"}
{"File Name":"qc-atlas\/0005-use-OpenAPI.md","Context":"## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n\n## Decision\n","Decision":"* readable API documentation\\n* effort of manually creating client services\\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\\n### Positive Consequences\\n* Standardized documentation of the API\\n* Automatic service generation for clients is possible\\n### Negative Consequences <!-- optional -->\\n* OpenAPI annotations have to be maintained\\n","tokens":56,"id":693,"Prediction":"## Decision\nThe API will be documented using the OpenAPI Specification (OAS).\n\n### Rationale\n- **Readable API Documentation**: OpenAPI Specification is widely adopted and provides a clear, human-readable documentation format that can be easily consumed by developers.\n- **Automated Client Generation**: OpenAPI is also machine-readable, enabling the use of tools to automatically generate client SDKs in various programming languages, reducing the effort required to create client services manually.\n- **Tooling and Ecosystem**: There is a rich ecosystem of tools around OpenAPI, including editors, validators, and code generators, which will streamline the development process and enhance productivity.\n  \n### Alternatives Considered\n- **RAML (RESTful API Modeling Language)**: Although also providing human and machine-readable documentation, the tooling and community support around OpenAPI are currently stronger.\n- **API Blueprint**: Similar to RAML, but OpenAPI's ecosystem and adoption provide more comprehensive support for the entire API lifecycle.\n\n### Implications\n- **Initial Effort**: There may be an initial learning curve for team members unfamiliar with the OpenAPI Specification, but the long-term benefits outweigh this cost.\n- **Maintenance**: Changes to the API will need to be reflected in the OpenAPI documentation to ensure consistency, but this can be managed with good development practices.\n\n### Next Steps\n1. **Training**: Provide training sessions for the team on writing and maintaining OpenAPI specifications.\n2. **Tool Integration**: Integrate OpenAPI documentation generation into the CI\/CD pipeline to keep API docs up-to-date.\n3. **Client Generation**: Set up automatic client SDK generation for major target languages and ensure they are available for developers.","GenTime":"2024-07-28 22:01:09"}
{"File Name":"qc-atlas\/0002-monorepo.md","Context":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n\n## Decision\n","Decision":"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","tokens":22,"id":697,"Prediction":"## Decision\nThe components in the PlanQK platform will be split into individual repositories. This decision is based on several factors, including improved modularity, better maintainability, and more efficient collaboration among development teams. By separating components into individual repositories, each can be managed, built, and deployed independently, which allows for more granular control over updates and changes. Additionally, this setup facilitates continuous integration and continuous deployment processes, and enhances the clarity of code ownership and responsibility.","GenTime":"2024-07-28 22:01:29"}
{"File Name":"qc-atlas\/0008-testcontainers.md","Context":"## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n\n## Decision\n","Decision":"Chosen option: [Testcontainers](https:\/\/www.testcontainers.org\/), because it simplifies the test execution process\\n### Positive Consequences <!-- optional -->\\n* It is not mandatory to ensure postgres is running before starting the tests\\n","tokens":34,"id":699,"Prediction":"## Decision\nWe have decided to use Docker Compose to handle the PostgreSQL instance required for running service tests.\n\nDocker Compose allows us to define and manage multi-container Docker applications with ease. This approach will enable us to specify the PostgreSQL container configuration in a `docker-compose.yml` file, which can then be used to launch and manage the container consistently across different environments.\n\n### Key Reasons:\n1. **Consistency**: Docker Compose ensures that the PostgreSQL instance will run with the same configuration across all development and testing environments, reducing the likelihood of environment-specific issues.\n2. **Simplicity**: Developers and testers can start the PostgreSQL container by simply running a single command (`docker-compose up`), eliminating the need for manual intervention.\n3. **Isolation**: Each service test environment will have its own isolated PostgreSQL container, ensuring that tests do not interfere with each other.\n4. **Maintainability**: The `docker-compose.yml` file will be version-controlled, making changes and updates to the PostgreSQL instance configuration transparent and manageable.\n\n### Implementation Steps:\n1. **Create a `docker-compose.yml` file** at the root of the project with the necessary configuration for the PostgreSQL container.\n2. **Specify database settings** such as image, environment variables for username, password, database name, and ports in the `docker-compose.yml` file.\n3. **Add documentation** to the project README or contributing guide on how to use Docker Compose for starting\/stopping the PostgreSQL container.\n4. **Integrate Docker Compose commands** into the service test scripts to ensure the container is started and stopped automatically as part of the testing workflow.\n\nBy adopting Docker Compose, we aim to streamline the setup and execution of service tests requiring PostgreSQL, while also ensuring reliability and ease of use for all team members.","GenTime":"2024-07-28 22:01:37"}
{"File Name":"ionic-dummy-repo\/ADR-001.md","Context":"## Context\\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\\n### Who Was Involved in This Decision\\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\n\n## Decision\n","Decision":"- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\\n","tokens":93,"id":701,"Prediction":"## Decision\n\nThe starter seed will **not** assert a specific opinion or provide dependencies for state-management libraries. Instead, the project will maintain a neutral stance, allowing developers the flexibility to choose the most suitable state-management tool based on their specific requirements, experience, and project complexity.\n\n### Rationale:\n\n- **Flexibility and Customization**: By not locking into a specific state-management solution, we empower developers to select the tool that best fits their unique project needs and team expertise.\n- **Adaptability to Change**: State-management trends and best practices evolve rapidly. Maintaining a neutral stance ensures that the starter seed remains adaptable and relevant, regardless of future developments in the state-management landscape.\n- **Wide Developer Adoption**: This approach aims to make the starter seed appealing to a broader range of developers who may have varying preferences for state-management libraries.\n\n### Implications:\n\n- Documentation will need to clearly outline that developers are expected to integrate their preferred state-management solution.\n- Example configurations or recommendations may be provided in the documentation to guide less experienced developers.\n- Project templates and code samples should remain agnostic to any specific state-management library to minimize bias.\n\nBy adopting this decision, we aim to create a more flexible and future-proof starting point for diverse development teams.","GenTime":"2024-07-28 22:01:45"}
{"File Name":"radiant-mlhub\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs\/adr` directory within this repository. Instead of using the \"deprecated\" and \"superseded\" status value, we will move ADRs that are no longer applicable into the `docs\/adr\/archive` directory in this repository.\\n","tokens":16,"id":704,"Prediction":"## Decision\nTo address the need for recording architectural decisions in this project, we have decided to implement an Architectural Decision Record (ADR) process. This process will encompass the following steps:\n\n1. **Template Usage**: We will use a standardized ADR template to ensure consistency in documentation. This template will include sections such as Context, Decision, Status, Consequences, and References.\n\n2. **Tool Selection**: To manage and version-control our ADRs, we will store these records in our project's version control system (e.g., Git). Each ADR will be a markdown or text file in a dedicated directory (e.g., `docs\/adr\/`).\n\n3. **Decision-Making Process**: All significant architectural decisions made during the project will be documented using this ADR template. Each ADR will be reviewed by key stakeholders to validate the decision.\n\n4. **Repository Structure**: ADRs will be numbered sequentially and stored in a structured manner for easy reference. For example, ADR-001, ADR-002, etc.\n\n5. **Integration with Development Workflow**: We will integrate ADR creation and review as a required step in our development workflow, such as during Sprint Planning or Architectural Review meetings.\n\n6. **Training and Onboarding**: Team members will be trained on the importance of ADRs and the process of creating and maintaining them. New team members will be onboarded with a review of past ADRs.\n\nBy adopting this ADR process, we aim to create a transparent and documented history of the architectural decisions made throughout the project lifecycle. \n\n","GenTime":"2024-07-28 22:01:58"}
{"File Name":"Corvus.Tenancy\/0004-v2-to-v3-transition.md","Context":"## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\\n\n## Decision\n","Decision":"Upgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\\n1. using nothing but v2\\n1. using v3 libraries mostly (see below) in v2 mode\\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\\n1. using v3 libraries in non-transitional mode\\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\\n### Phase 2: using v3 libraries, operating in v2 mode\\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries\u2014the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\\nThe one difference in behaviour (the reason we describe this as \"mostly\" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\\n### Configuration migration\\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\\n","tokens":171,"id":714,"Prediction":"## Decision\n\nTo ensure a smooth migration from `Corvus.Tenancy` v2 to v3, we will adopt the following multi-phase approach that safeguards against disruption and accommodates a no-downtime migration strategy:\n\n**Phase 1: Dual Compatibility Mode**\n1. Implement a \"dual compatibility mode\" in `Corvus.Tenancy` v3 that allows it to check for the existence of necessary storage containers during operations. If containers are missing, it will log a critical alert and throw a configuration error.\n2. Provide a migration API or utility that can be employed by v2 applications to pre-check and create missing storage containers before starting any v3 components.\n3. The system will operate in a mixed environment where both v2 and pre-configured v3 tenants can coexist, minimizing disruption during the transition.\n\n**Phase 2: Tenant Preparation and Monitoring**\n1. Introduce tooling and scripts to audit existing tenants for the presence of all necessary storage containers.\n2. Provide detailed instructions and additional scripts for creating any missing containers in a batch process, ensuring all tenants are updated before switching fully to v3.\n3. Implement monitoring and alerting to detect and report any issues with tenants that were not fully prepared, ensuring quick resolution and tenant stability.\n\n**Phase 3: Gradual Decommissioning of v2 Components**\n1. With confidence that all tenants are fully prepared and operational in dual compatibility mode, begin progressively decommissioning v2 components, starting with less critical systems and gradually moving towards core systems.\n2. Maintain comprehensive logging and monitoring to promptly identify and address any issues that arise.\n\n**Phase 4: Final Removal of Compatibility Mode**\n1. Once v2 components are fully decommissioned and all operations are verified to be stable, disable the dual compatibility mode in `Corvus.Tenancy` v3.\n2. Remove any v2-specific migration utilities from the codebase to reduce maintenance overhead and potential for errors.\n\nBy following this phased approach, we ensure that the migration to `Corvus.Tenancy` v3 is done deliberately and carefully, minimizing risks and maintaining system integrity throughout the process. Moreover, detailed documentation and communication will be provided to all teams involved, ensuring clarity and coordination during the migration.","GenTime":"2024-07-28 22:02:47"}
{"File Name":"read-more-api\/0007-deploy-to-heroku.md","Context":"## Context\\nThe application needs to be deployed somewhere.\\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\\nWe want the setup process to be as simple as possible.\\n\n## Decision\n","Decision":"We will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n","tokens":193,"id":718,"Prediction":"## Decision\n\nWe will deploy the application on Heroku using their Container Registry and Runtime services.\n\n### Rationale:\n\n1. **Simplicity:** Heroku's Platform as a Service (PaaS) offerings simplify the deployment process. By using Heroku, we avoid the complexity of setting up and managing a cluster of machines for container orchestration manually.\n\n2. **Cost-Effective:** Heroku provides several pricing tiers, including a free tier that can be used during development and testing phases. This allows for cost-effective initial deployment with the flexibility to upgrade as the application scales.\n\n3. **Integrated Database Services:** Heroku includes a free hosted PostgreSQL option, which aligns well with our needs. The service will automatically manage the \"DATABASE_URL\" environment variable, simplifying database configuration.\n\n4. **Scalability:** As the application grows, Heroku's various pricing tiers provide a clear and straightforward path to scale both the application runtime and the database.\n\n5. **Community and Support:** Heroku offers a well-documented platform with a substantial community and support available, aiding in quick issue resolution and learning resources.\n\n### Implementation Plan:\n\n1. **Containerization:** Ensure that the application is containerized using Docker.\n\n2. **Heroku CLI:** Install and configure the Heroku CLI for managing the deployment process.\n\n3. **Container Registry:** Use the Heroku Container Registry to push Docker images.\n\n4. **Database Configuration:** Utilize Heroku's free PostgreSQL service by connecting to the database through the automatically provided \"DATABASE_URL\" environment variable.\n\n5. **Deployment:** Deploy the application to Heroku from the Container Registry, leveraging the free tier for initial hosting.\n\nThis decision focuses on reducing operational overhead and costs while maintaining flexibility and simplicity in the deployment process.","GenTime":"2024-07-28 22:02:59"}
{"File Name":"datalab\/0012-public-and-private-subnets.md","Context":"## Context\\nWe would like to minimise our externally exposed footprint. To achieve this we have\\nalready decided that access to cluster servers will be via a Bastion server. We do need\\nto have some externally accessible services and need to decide how to achieve this.\\n\n## Decision\n","Decision":"We have decided to have external proxy servers for each environment that expose ports 80\\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\\n","tokens":55,"id":729,"Prediction":"## Decision\n\nWe will implement an API Gateway to manage and protect our externally accessible services. This decision entails the following actions and configurations:\n\n1. **Single Entry Point**: All externally accessible services will be routed through a centralized API Gateway, acting as the single entry point to the system.\n   \n2. **Access Control**: The API Gateway will enforce authentication and authorization policies, ensuring that only authenticated and authorized users can access the services.\n\n3. **Rate Limiting**: To prevent abuse and ensure fair usage of resources, the API Gateway will be configured with rate-limiting rules.\n\n4. **SSL Termination**: The API Gateway will handle SSL termination, ensuring that all data transmitted over the network is encrypted.\n\n5. **Monitoring and Logging**: Comprehensive monitoring and logging capabilities will be implemented on the API Gateway to track access patterns, detect anomalies, and troubleshoot issues.\n\n6. **Backend Routing**: The API Gateway will route requests to the appropriate internal services based on the API endpoint called, providing a level of abstraction and separation between the client and server architecture.\n\n7. **Scalability**: The API Gateway will be designed to scale horizontally, ensuring it can handle varying loads without degradation of service.\n\nBy implementing an API Gateway, we will effectively minimize our externally exposed footprint while maintaining robust security and operational standards for our externally accessible services.","GenTime":"2024-07-28 22:03:51"}
{"File Name":"datalab\/0011-cluster-authentication-by-ssh-key.md","Context":"## Context\\nWe need to configure access to the servers in the cluster and need to decide between\\npassword authentication and SSH key pair authentication.\\n\n## Decision\n","Decision":"We have decided that all server log on authentication will use SSH key pair\\nauthentication. The public keys will be provisioned onto the server at server creation\\nfor the default deploy user. A password will be required for sudo.\\n","tokens":29,"id":731,"Prediction":"## Decision\nWe have decided to use SSH key pair authentication for accessing the servers in the cluster. \n\n### Rationale:\n1. **Enhanced Security:** SSH key pair authentication is widely considered more secure compared to password-based authentication. Passwords can be guessed or stolen through brute force attacks or phishing, whereas SSH keys are significantly harder to crack due to their complexity and length.\n2. **Automation and Scalability:** SSH keys can be easily distributed and managed through configuration management tools (e.g., Ansible, Puppet, Chef). This makes it simpler to grant or revoke access to multiple servers in a systematic and scalable manner.\n3. **No Password-related Issues:** Using SSH keys eliminates problems related to forgotten or weak passwords. Users can generate their key pairs, and the private key can be securely stored using an SSH agent or similar tool.\n4. **Audit and Compliance:** SSH key management aligns well with compliance and audit requirements by providing a robust way to track and manage access. Keys can be rotated periodically and documented effectively.\n5. **User Experience:** From a user standpoint, SSH keys can streamline the login process, reducing the need to remember or input passwords, thus enhancing the overall productivity and experience.\n\n### Implementation:\n- **Key Generation:** All users will be required to generate an SSH key pair (private and public key).\n- **Key Distribution:** The public keys will be shared and installed on the respective servers in the cluster. This can be automated via configuration management tools.\n- **Private Key Security:** Users will be instructed to secure their private keys and use passphrases for an additional layer of security.\n- **Key Rotation:** We will implement a policy for regular key rotation to further enhance security, with clear documentation and guidelines.\n\n### Conclusion:\nSSH key pair authentication offers superior security and convenience, making it the preferred choice for our server cluster access configuration. We will proceed with configuring this method and ensure all team members are briefed on best practices for managing their SSH keys.","GenTime":"2024-07-28 22:03:59"}
{"File Name":"datalab\/0036-replace-kong-with-nginx-ingress-controller.md","Context":"## Context\\nWe have reached a point where we need to secure third party web applications that provide\\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\\nsource version and we haven't had and response from our efforts to contact them.\\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\\nwas released provides a Kubernetes native route for the same functionality.\\n\n## Decision\n","Decision":"We have decided the replace the Kong proxy with an Nginx Ingress Controller in the\\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\\nThis should provide all of the same functionality as Kong and in addition should provide\\na mechanism for token and cookie authentication using the `auth_url` annotation that\\nwraps the underlying Nginx `auth_request` module.\\n","tokens":91,"id":733,"Prediction":"## Decision\n\nWe will implement the Nginx Ingress controller to secure third-party web applications within our Kubernetes cluster. This decision is based on the following considerations:\n\n1. **Open Source Flexibility:** The Nginx Ingress controller is open-source and provides the necessary security features required for our third-party web applications.\n2. **Kubernetes Compatibility:** Being a Kubernetes-native solution, the Nginx Ingress controller will seamlessly integrate with our existing Kubernetes environment, taking advantage of Kubernetes features and best practices.\n3. **Community and Support:** The Nginx Ingress controller is widely adopted, has an active community, and offers comprehensive documentation and community support, which can be beneficial for troubleshooting and future enhancements.\n4. **No Responsive Contact from Kong Proxy:** Given our unsuccessful attempts to communicate with Kong Proxy for additional support in their open-source version, relying on their offering could impede our progress and security goals.\n\nThis decision will be revisited periodically to ensure that our security requirements are met and to evaluate the potential for future upgrades or changes as the technology landscape evolves.","GenTime":"2024-07-28 22:04:09"}
{"File Name":"datalab\/0009-bastion-for-cluster-access.md","Context":"## Context\\nIt will be necessary to access the servers that form the Datalabs network but we do not\\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\\n\n## Decision\n","Decision":"We have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n","tokens":44,"id":745,"Prediction":"## Decision\n\nTo ensure secure and restricted access to the servers within the Datalabs network, we will implement a Virtual Private Network (VPN) solution. This will allow authenticated users within the JASMIN tenancy to securely connect to the Datalabs servers without exposing any services beyond the necessary scope. All communication will be encrypted to maintain data confidentiality and integrity. Additionally, access control measures, such as multi-factor authentication (MFA) and strict access permissions, will be put in place to enhance security.","GenTime":"2024-07-28 22:04:58"}
{"File Name":"datalab\/0002-ansible-for-provisioning-tool.md","Context":"## Context\\nWe need a tool to provision servers and software for the datalabs project.\\n\n## Decision\n","Decision":"We will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\\nteam have experience using it.\\n","tokens":20,"id":771,"Prediction":"## Decision\nWe have decided to use Terraform as the primary tool for provisioning servers and software for the datalabs project. This decision is based on Terraform's ability to provide infrastructure as code, its platform-agnostic nature, robust community support, and seamless integration with various cloud service providers. Terraform's modularity and state management capabilities also ensure that our infrastructure is maintainable, scalable, and can be easily audited.","GenTime":"2024-07-28 22:06:37"}
{"File Name":"datalab\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":774,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) process to document and maintain records of all significant architectural decisions made during the project. This will involve using a consistent template for each decision, which includes the context, decision, consequences, and any other relevant information. The ADRs will be stored in a version-controlled repository to ensure that they are easily accessible and retrievable for future reference. This approach will help in maintaining transparency, ensuring knowledge sharing among team members, and providing a historical record of why certain decisions were made.","GenTime":"2024-07-28 22:06:46"}
{"File Name":"ditto\/DADR-0004-signal-enrichment.md","Context":"## Context\\nSupporting a new feature, the so called [signal enrichment](https:\/\/github.com\/eclipse-ditto\/ditto\/issues\/561), raises a few\\nquestions towards throughput and scalability impact of that new feature.\\nIn the current architecture, Ditto internally publishes events (as part of the applied \"event sourcing\" pattern) for\\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\\nchanged fields.\\nThe \"signal enrichment\" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\\nThe following alternatives were considered on how to implement that feature:\\n1. Sending along the complete `Thing` state in each event in the cluster\\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\\n* downside: bigger payload sent around\\n* downside: a lot of deserialization effort for all event consuming services\\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\\n* downside: overall a lot of overhead for probably only few consumers\\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upside: no additional payload for existing events\\n* upside: data is only enriched for sessions\/connections really using that feature\\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\\n* delayed event publishing\\n* additional deserialization efforts\\n* potentially asking for the same static values each time\\n3. Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upsides: all upsides of approach 2 except that policy is always up-to-date\\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\\n\n## Decision\n","Decision":"We provide 2 different facade implementations providing approach 2 and 3:\\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\\n* the overall size of this cache is configured, by default to `20,000` entries\\n* there is an additional \"smart-update\" mechanism for cache entries related to enrichment of twin events:\\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\\n","tokens":500,"id":775,"Prediction":"## Decision\n\nBased on the given context and the evaluation of the alternatives regarding throughput, scalability, and simplicity of implementation, the chosen approach is alternative 3: **Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event**.\n\n### Rationale\n1. **Throughput & Performance**: \n   - Approach 3 significantly reduces the additional load and round-trips required for enriching data when compared to approach 2. The caching mechanism allows for efficient data retrieval, minimizing the latency in the enrichment process.\n   - Unlike approach 1, which increases the payload for all events (leading to potential performance issues due to the increased deserialization effort), approach 3 ensures that the downside of increased payload is limited to only those connections that require extra fields.\n\n2. **Scalability**:\n   - Approach 3 scales well as it caches the data, thereby reducing the number of real-time queries needed. In contrast, approach 1 could lead to bottlenecks due to the large payload sizes, and approach 2 may not be efficient at high scales due to the constant need for round-trips.\n\n3. **Policy Enforcement**:\n   - Although approach 3 introduces a slight risk of outdated policy information within a configurable window, this risk is acceptable considering the significant performance improvements. Implementing a reasonable cache expiration strategy can further mitigate this risk while ensuring most up-to-date information.\n\n4. **Efficient Resource Utilization**:\n   - Approach 3 optimizes the utilization of system resources by limiting the enrichment process to relevant sessions\/connections and avoiding unnecessary processing for those that do not need the extra fields.\n\n### Implementation Strategy\n- Implement a caching mechanism for storing enriched data and relevant policy information.\n- Refresh the cache periodically to ensure data and policy information are as up-to-date as possible within an acceptable time window (e.g., 2 minutes).\n- Apply default concierge mechanisms to enforce policies on cache updates, ensuring that access control remains robust.\n- Monitor and tune the caching strategy based on observed system performance and feedback to balance performance with policy accuracy.\n\nIn summary, approach 3 strikes a balance between enriching data precisely where needed, minimizing additional computational and network overhead, and ensuring that policy enforcement remains timely and accurate.","GenTime":"2024-07-28 22:06:51"}
{"File Name":"ditto\/DADR-0003-do-not-interrupt-threads.md","Context":"## Context\\nSonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch\\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\\ninterruption.\\n\n## Decision\n","Decision":"We will ignore [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142).\\n","tokens":114,"id":777,"Prediction":"## Decision\nInstead of directly invoking `Thread.currentThread().interrupt()` when catching `InterruptedException`, we will refactor our test code to properly handle thread interruption without causing unpredictable behavior. This decision includes the following actions:\n\n1. **Graceful Interruption Handling**: We shall ensure that each catch block that catches `InterruptedException` either logs the interruption and safely stops the current operation or properly propagates the exception to be handled at a higher level where interruption can be safely managed.\n\n2. **Test Isolation**: When possible, we will avoid thread reuse in tests by creating new thread instances for each test to ensure that an interrupted thread does not affect subsequent tests. This may involve configuring the test framework or adjusting the test design to favor thread isolation.\n\n3. **Refactor Critical Sections**: We will identify critical sections in the code that might be affected by thread interruptions and refactor them to either handle interruptions within the current context or ensure that any side effects are minimized.\n\n4. **Update SonarQube Configuration**: We will configure SonarQube to recognize and accept our tailored interruption handling approach. This includes adding necessary code comments or annotations explaining why `Thread.interrupted()` is not invoked directly, to ensure code quality checks do not falsely flag these as issues.\n\n5. **Documentation and Best Practices**: We will document the new best practices for handling `InterruptedException` within our codebase. This documentation will be shared with the development team to ensure consistent and safe handling of thread interruptions across the project.\n\nBy implementing these steps, we aim to eliminate the unpredictable build failures while complying with SonarQube\u2019s rules for proper thread interruption handling.","GenTime":"2024-07-28 22:07:01"}
{"File Name":"ditto\/DADR-0006-merge-payload.md","Context":"## Context\\nWe want to allow partial or merge updates of things with a single request.\\n\n## Decision\n","Decision":"A merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https:\/\/tools.ietf.org\/html\/rfc7396).\\n* has the request header `content-type` set to `application\/merge-patch+json`.\\n","tokens":19,"id":779,"Prediction":"## Decision\nAfter evaluating our requirements for allowing partial or merge updates for entities with a single request, we have decided to implement the PATCH HTTP method. The PATCH method will enable us to send only the changes (partial updates) to the server, which will then update the resource accordingly.\n\nAdditionally, we will adopt a JSON Merge Patch format for the payload. This format is well-suited for merging updates as it provides a straightforward way to specify partial changes and ensures compatibility with various client implementations.\n\n### Key Points:\n\n1. **PATCH Method**: \n   - Allows partial updates to resources, minimizing the amount of data sent over the network.\n   - Aligns with RESTful principles and is widely supported by clients and servers.\n\n2. **JSON Merge Patch Format**:\n   - Simple and easy-to-understand format for specifying updates.\n   - Reduces the chance of accidental data removal or incorrect updates as it only requires the fields to be updated.\n\n3. **Implementation**:\n   - Update the server-side controller to handle PATCH requests.\n   - Ensure that validation and security checks are appropriately applied to the partial updates.\n   - Update API documentation to reflect the new PATCH endpoint and include examples of JSON Merge Patch payloads.\n\nBy adopting the PATCH method along with JSON Merge Patch format, we aim to provide a flexible and efficient way to handle partial and merge updates, enhancing the overall user experience and performance.","GenTime":"2024-07-28 22:07:05"}
{"File Name":"frontend-library\/0003-use-brunch.md","Context":"## Context\\nThe styling of sites developed for NHSUK need to be as consistent as possible.\\nCurrently each new site starts by copying what the last developed site had done\\nand building on that. This results in the newer sites looking different than\\nthe older sites, which would not be a problem if each site was easily updated\\nto use the same, most recent, set of styles. Currently there is no central\\nstore for the styles which means there is no easy way to find and use them\\neither.\\nThe ability to review the styles in a development environment before they are\\nmade available to a wider audience is important. As is the ability to use well\\nknown technologies such as [SASS](http:\/\/sass-lang.com\/).\\n\n## Decision\n","Decision":"We will use [Brunch](http:\/\/brunch.io\/) to accomplish a number of tasks\\nincluding:\\n* Running the site used in the development environment\\n* Generating the compiled assets\\n","tokens":153,"id":783,"Prediction":"## Decision\nWe will implement a centralized style repository for NHSUK sites.\n\nThis repository will contain a unified set of styles maintained collaboratively by the development teams. It will be structured as a package or library that can be easily integrated into new and existing NHSUK projects. By doing this, we ensure consistency across all NHSUK sites and significantly reduce the effort required to maintain visual coherence.\n\nThe style repository will follow these guidelines:\n\n1. **Central Repository**: A version-controlled central repository (e.g., hosted on GitHub) will be created specifically for NHSUK styles.\n2. **SASS-based**: The repository will utilize SASS to take advantage of its powerful features like variables, nesting, mixins, and inheritance, ensuring streamlined and efficient styling.\n3. **Modular Structure**: Styles will be organized in a modular fashion, making it easy to manage and update individual style components.\n4. **Versioning**: The repository will support versioning so that projects can easily update to the latest styles or pinpoint a specific version for consistency.\n5. **Review Environment**: A staging environment will be set up where styles can be reviewed and tested before they are merged and made available for broader use.\n6. **Documentation and Guidelines**: Comprehensive documentation will be provided, including guidelines on how to contribute to and use the centralized styles.\n7. **Automated Integration**: Continuous Integration\/Continuous Deployment (CI\/CD) pipelines will be established to ensure automatic testing and smooth deployment of updates to the style repository.\n\nBy adopting this approach, we will facilitate ease of use, ensure consistency, and allow for scalable and maintainable styling across all NHSUK sites.","GenTime":"2024-07-28 22:07:21"}
{"File Name":"frontend-library\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":784,"Prediction":"## Decision\nWe have decided to adopt the use of Architectural Decision Records (ADRs) as the methodology for documenting all significant architectural decisions throughout the course of this project. Each ADR will be recorded in a standardized format to ensure clarity and consistency. This will facilitate better communication among team members, provide a historical reference for future decisions, and support effective governance.\n\n- **Tool**: We will use Markdown for writing ADRs and store them in a version-controlled repository (e.g., GitHub).\n- **Template**: We will follow a consistent template for all ADRs, including sections for Context, Decision, Status, Consequences, and Notes.\n- **Review Process**: ADRs will be reviewed by the core architectural team and approved before being finalized.\n- **Versioning**: Each ADR will have a unique identifier and version, allowing us to track and reference decisions easily.\n- **Accessibility**: The repository will be accessible to all team members to encourage transparency and collaboration.\n\nBy following this approach, we aim to capture the rationale behind architectural choices, making it easier to understand the project's evolution and enabling more informed decision-making.","GenTime":"2024-07-28 22:07:24"}
{"File Name":"infrastructure-adrs\/0010-message-broker.md","Context":"## Context and Problem Statement\\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\\n* Each service must have knowledge about many other services.\\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\\nIn addition, there are already places within SDR were we have re-invented a message broker and\/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\\n\n## Decision\n","Decision":"TBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n","tokens":224,"id":786,"Prediction":"Adopt a message broker for asynchronous communication between single-purpose services within the SDR ecosystem.\n\n### Reasons for Decision:\n\n1. **Service Decoupling:**\n   By introducing a message broker, services do not need to maintain extensive knowledge about each other. This decoupling simplifies the system architecture and reduces dependencies.\n\n2. **Resilience and Fault Tolerance:**\n   The system can better tolerate failures. If a particular service goes down, messages can be queued and processed once the service is available again, preventing total system failures due to a single service being unavailable.\n\n3. **Scalability:**\n   Asynchronous communication via a message broker can handle a high volume of messages and scale better than synchronous HTTP requests, enabling the SDR to accommodate increasing load without significant performance degradation.\n\n4. **Performance:**\n   For operations that do not require an immediate response (e.g., logging or reporting events), a message broker can significantly reduce the overhead compared to synchronous HTTP requests, leading to better performance.\n\n5. **Reduced Complexity in Handling Network Issues:**\n   Adopting a message broker abstracts away the complexity of handling network unreliability (e.g., the unreliable Stanford network, especially at night). Retries, acknowledgments, and other mechanisms are natively supported by message brokers.\n\n6. **Avoid Reinventing the Wheel:**\n   Utilizing a message broker prevents the need to jerry-rig around the absence of such a system, avoiding inefficient solutions like polling via HTTP to check job completions.\n\n### Implementation Considerations:\n\n- **Choice of Message Broker:**\n  Evaluate different message brokers (e.g., RabbitMQ, Apache Kafka, Redis Pub\/Sub) to choose the one that best fits the load, scalability, and reliability requirements.\n\n- **Transaction Durability:**\n  Ensure that the chosen message broker supports durable transactions and message persistence to avoid data loss.\n\n- **Monitoring and Maintenance:**\n  Implement monitoring tools to track the health and performance of the message broker, addressing any issues promptly.\n\n- **Gradual Integration:**\n  Introduce the message broker in phases to allow for adjustments and optimizations without overwhelming the existing architecture.\n\n- **Service Modifications:**\n  Services will need to be modified to publish to and subscribe from the message broker instead of making synchronous HTTP calls where appropriate.\n\n### Conclusion:\n\nAdopting a message broker aligns with SDR\u2019s evolving needs, providing improved service decoupling, resilience, performance, and scalability. This strategic decision will address current fragility and prepare the system for future growth.","GenTime":"2024-07-28 22:07:33"}
{"File Name":"infrastructure-adrs\/0009-create-provenance-service.md","Context":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n\n## Decision\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":794,"Prediction":"## Context and Problem Statement \nOur team is developing a web application that requires real-time data updates, and the current polling mechanism is proving inefficient, leading to increased server load and latency issues. We need a more efficient way to handle real-time updates.\n\n## Decision Drivers \n* Need for reduced server load and better resource utilization.\n* Requirement for low latency and real-time user experience.\n* Scalability to handle a growing number of concurrent users.\n\n## Decision\nWe will implement WebSockets for real-time data updates instead of the current polling mechanism. WebSockets provide a full-duplex communication channel over a single, long-lived connection, allowing for more efficient and faster real-time data transmission. This approach will reduce server load and improve the responsiveness of our web application, providing a better user experience and enabling us to scale effectively as the number of concurrent users grows.","GenTime":"2024-07-28 22:08:05"}
{"File Name":"form-design-system\/zindex-values.md","Context":"## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n\n## Decision\n","Decision":"We've decided to go with a 2 tier approach:\\n- Use pre-defined z-index values & ranges that are defined in FDS.\\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\\nvalues are not useful)\\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\\n### Use values between `1` and `9` for fine layering control\\nIn cases where one element needs to appear above another, use integers below 10. Rely on\\nsurrounding stacking context to do the rest.\\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\\nvalues `1` through `5` to accomplish this.\\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Positioning\/Understanding_z_index\/The_stacking_context).\\n### Pre-defined FDS z-index values \/ ranges\\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\\n#### Navigation\\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to \"frame\" the content on a page.\\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\\n```\\n.mainheader {\\nz-index: calc(var(--zindex-navigation) + 9);\\n}\\n.subheader {\\nz-index: var(--zindex-navigation)\\n}\\n```\\n#### Modal\\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\\n#### Crazy\\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\\n### For standard values, continue using increments of `10`\\nFor base stacking context, standard values are most helpful at increments of 10 because it\\ngives engineers flexibility to insert layers between defined stack levels. For example:\\n","tokens":53,"id":798,"Prediction":"## Decision\n\nWe have decided to adopt a well-defined z-index strategy and apply a CSS architecture framework in our styling to resolve the z-index wars and improve maintainability.\n\nSpecifically, we will:\n1. **Implement a Layering System**: Define a predictable and scalable z-index layer system. This system will categorize z-index values into distinct layers (e.g., base layer, overlay layer, modal layer) with clearly defined purposes and z-index ranges.\n2. **CSS Framework**: Adopt a CSS framework like BEM (Block Element Modifier) or SMACSS (Scalable and Modular Architecture for CSS) to structure and organize our styles consistently.\n3. **Documentation and Guidelines**: Create comprehensive documentation and team guidelines on how to apply z-index values within our established layers. Mandate code reviews to ensure compliance with the layered z-index strategy.\n4. **Linting Rules**: Introduce and enforce CSS linting rules to prevent arbitrary and excessively high z-index values.\n5. **Refactoring**: Gradually refactor existing styles to align with the new z-index strategy.\n\nBy implementing a structured approach to z-index values and adopting a cohesive CSS architecture framework, we aim to reduce conflicts, simplify maintenance, and improve the overall stability of our application's front end.","GenTime":"2024-07-28 22:08:23"}
{"File Name":"form-design-system\/relative_spacing.md","Context":"## Context and Problem Statement\\nWe'd like to change the spacing system to something more generic.\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\n\n## Decision\n","Decision":"The current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\nGeneric tshirt system was chosen over the other options.\\n- Coding a direct \"arbitrary value\" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\\n","tokens":153,"id":801,"Prediction":"## Decision\n\nWe will transition to a numeric-based spacing system. \n\n## Rationale\n\n- **Consistency and Clarity:** Using numeric values instead of relative descriptors (half, double, etc.) ensures consistency and clarity. For instance, instead of using 'half' or 'double,' we will use explicit numeric values such as `8`, `16`, `24`, and so on.\n- **Scalability:** The numeric-based system allows us to introduce new spacing values without confusion or complex naming conventions. This simplifies the process of adding additional values like `24`, `32`, etc.\n- **Flexibility:** Numeric values make it easier to adjust the default spacing value without having to redefine relative terms. For example, if we change the default spacing from `16` to `20`, all we need to modify is the numeric value mappings without renaming any spacings already in use.\n- **Ease of Communication:** Numeric values are straightforward and less ambiguous, reducing potential misunderstandings among team members and making documentation simpler.\n\nAdopting this numeric system will not only address the identified deficiencies but also future-proof our spacing system, allowing for more flexibility and simplicity in implementation and maintenance.","GenTime":"2024-07-28 22:08:36"}
{"File Name":"form-design-system\/components_ignore_classnames.md","Context":"## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n\n## Decision\n","Decision":"Chosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n","tokens":80,"id":802,"Prediction":"## Decision\nWe will not allow a `classNames` prop on any components in the FDS (Front-end Design System). Instead, all styling should be achieved through predefined classes and props that are part of the component's API. This ensures consistency in the look and feel of the components and maintains the integrity of components like `Flex`, which are designed to function as basic layout utilities. This decision shall apply uniformly across all components in FDS, thereby ensuring a standardized and predictable interface for developers using the design system.","GenTime":"2024-07-28 22:08:38"}
{"File Name":"cosmos-sdk\/adr-021-protobuf-query-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n\n## Decision\n","Decision":"### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```protobuf\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/cosmos\/gogoproto)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work directly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","tokens":108,"id":810,"Prediction":"## Decision\n\nWe will adopt Protocol Buffers (protobufs) for encoding client-side queries in the Cosmos SDK. This decision aligns with our previous ADRs (ADR 019 and ADR 020), which established the use of protobufs for state encoding and transaction encoding, respectively.\n\nTo achieve this:\n\n1. **Defining Protobuf Message Types**:\n   - We will create specific protobuf message types for each query response and request in the client-side APIs.\n   - These message types will be designed to encapsulate all necessary data and follow the existing data schema as closely as possible to ensure consistency and ease of migration.\n\n2. **Query Handler Adjustments**:\n   - The existing query handlers will be refactored to support protobuf-encoded query request and response messages.\n   - Modular code design principles will be used to allow handlers to support both legacy and new protobuf-based queries during the transition period.\n\n3. **Backward Compatibility**:\n   - A backward compatibility layer will be implemented to handle legacy, non-protobuf queries.\n   - During the transition phase, the system will support dual encoding formats, ensuring no disruption for existing clients.\n\n4. **Documentation and Community Communication**:\n   - Comprehensive documentation will be provided to help developers understand the new protobuf-based query format.\n   - Tutorials, code examples, and migration guides will be made available to the Cosmos SDK community.\n   - Ongoing communication through blogs, forums, and other community channels will be maintained to address questions and support the migration process.\n\n5. **Performance and Testing**:\n   - Extensive performance testing will be conducted to ensure the new protobuf-based query system meets or exceeds the performance benchmarks set by the current query encoding mechanism.\n   - Automated and manual testing will be used to validate the correctness of the newly implemented query handling system.\n\nThis decision to use Protocol Buffers for encoding client-side queries is expected to improve data serialization efficiency, reduce the potential for errors, and align the query encoding mechanism with the broader migration effort toward Protocol Buffers in the Cosmos SDK.","GenTime":"2024-07-28 22:09:13"}
{"File Name":"cosmos-sdk\/adr-033-protobuf-inter-module-comm.md","Context":"## Context\\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\\n### `x\/bank` Case Study\\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the `MintCoins`,\\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\\njust a simple string. So the `x\/upgrade` module could mint tokens for the `x\/staking` module simple by calling\\n`MintCoins(\u201cstaking\u201d)`. Furthermore, all modules which have access to these keeper methods, also have access to\\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\\n\n## Decision\n","Decision":"Based on [ADR-021](.\/adr-021-protobuf-query-encoding.md) and [ADR-031](.\/adr-031-msg-service.md), we introduce the\\nInter-Module Communication framework for secure module authorization and OCAPs.\\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\\nOf particular note \u2014 the decision is to _enable_ this functionality for modules to adopt at their own discretion.\\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\\naddressed as amendments to this ADR.\\n### New \"Keeper\" Paradigm\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\\nwas introduced and in [ADR 31](.\/adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg\/Send` message type:\\n```go\\npackage bank\\ntype MsgClient interface {\\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\\n}\\ntype MsgServer interface {\\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\\n}\\n```\\n[ADR 021](.\/adr-021-protobuf-query-encoding.md) and [ADR 31](.\/adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\\nbased service interfaces already used by clients for inter-module communication.\\nUsing this `QueryClient`\/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\\n1. Protobuf types are checked for breaking changes using [buf](https:\/\/buf.build\/docs\/breaking-overview) and because of\\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\\nevolution.\\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\\nobject capability system (see below).\\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\\nenabling atomicy of operations ([currently a problem](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8030)). Any failure within a module-to-module call would result in a failure of the entire\\ntransaction\\nThis mechanism has the added benefits of:\\n* reducing boilerplate through code generation, and\\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\\n### Inter-module Communication\\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https:\/\/github.com\/grpc\/grpc-go\/blob\/v1.49.x\/clientconn.go#L441-L450)\\nimplementation. For this we introduce\\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\\ndescribed in more detail below.\\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\\nthat we don't use any cryptographic signature in this case.\\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `\/cosmos.bank.Msg\/Send` transaction. `MsgSend` validation\\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\\nHere's an example of a hypothetical module `foo` interacting with `x\/bank`:\\n```go\\npackage foo\\ntype FooMsgServer {\\n\/\/ ...\\nbankQuery bank.QueryClient\\nbankMsg   bank.MsgClient\\n}\\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\\n\/\/ ...\\nreturn FooMsgServer {\\n\/\/ ...\\nmodouleKey: moduleKey,\\nbankQuery: bank.NewQueryClient(moduleKey),\\nbankMsg: bank.NewMsgClient(moduleKey),\\n}\\n}\\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: \"foo\"})\\n...\\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\\n...\\n}\\n```\\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\\ndenom prefix being restricted to certain modules (as discussed in\\n[#7459](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#discussion_r529545528)).\\n### `ModuleKey`s and `ModuleID`s\\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\\ncorresponding \"public key\". From the [ADR 028](.\/adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\\nand forms its address based on the `AddressHash` method from [the ADR-028](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-028-public-key-addresses.md):\\n```go\\ntype ModuleID struct {\\nModuleName string\\nPath []byte\\n}\\nfunc (key ModuleID) Address() []byte {\\nreturn AddressHash(key.ModuleName, key.Path)\\n}\\n```\\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\\nthe `ModuleKey` security.\\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\\n```go\\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\\ntype CallInfo {\\nMethod string\\nCaller ModuleID\\n}\\ntype RootModuleKey struct {\\nmoduleName string\\ninvoker Invoker\\n}\\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { \/* ... *\/}\\ntype DerivedModuleKey struct {\\nmoduleName string\\npath []byte\\ninvoker Invoker\\n}\\n```\\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\\n```go\\npackage foo\\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\\nbankMsgClient := bank.NewMsgClient(derivedKey)\\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\\n...\\n}\\n```\\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\\nfrom either the root or a derived module account to pass through.\\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\\nchecking permissions.\\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\\nname impersonation.\\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\\n```go\\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\\nreturn f(ctx, args, reply)\\n}\\n```\\n### `AppModule` Wiring and Requirements\\nIn [ADR 031](.\/adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\\nspecify their dependencies on other modules using `RequireServer()`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nModuleKey() ModuleKey\\nRequireServer(msgServer interface{})\\n}\\n```\\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\\nmodule `foo` could declare it's dependency on `x\/bank` like this:\\n```go\\npackage foo\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ncfg.RequireServer((*bank.QueryServer)(nil))\\ncfg.RequireServer((*bank.MsgServer)(nil))\\n}\\n```\\n### Security Considerations\\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\\nthe underlying router infrastructure.\\n#### Recursion and Re-entry\\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\\ncalls Module B and Module B calls module A again in the same call.\\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\\nin the router could be used to perform this security check.\\n#### Queries\\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\\n### Internal Methods\\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\\npurpose, we add the `InternalServer` method to `Configurator`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nInternalServer() grpc.Server\\n}\\n```\\nAs an example, x\/slashing's Slash must call x\/staking's Slash, but we don't want to expose x\/staking's Slash to end users\\nand clients.\\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\\nproto package.\\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\\nAn alternative solution to internal-only methods could involve hooks \/ plugins as discussed [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#issuecomment-733807753).\\nA more detailed evaluation of a hooks \/ plugin system will be addressed later in follow-ups to this ADR or as a separate\\nADR.\\n### Authorization\\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-030-authz-module.md).\\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\\nother modules. This will be addressed in separate ADRs or updates to this ADR.\\n### Future Work\\nOther future improvements may include:\\n* custom code generation that:\\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\\n* code generation which makes inter-module communication more performant\\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\\n* inter-module hooks and plugins\\n","tokens":451,"id":812,"Prediction":"## Decision\n\n### Adopt a Stable v1.0 Cosmos SDK and Implement a Rigorous Object-Capability Model\n\n1. **Stable v1.0 Cosmos SDK**\n    - **Version Control and Stability**: Commit to delivering a stable v1.0 version of the Cosmos SDK to provide a consistent foundation for module developers. This involves a comprehensive review and stabilization of module interfaces to ensure backward compatibility for future releases.\n    - **Module Interface Definitions**: Clearly define and document interfaces for each module to reduce ambiguity, ensure long-term stability, and facilitate easier composition of modules.\n\n2. **Implementing a Proper Object-Capability Model**\n    - **Object-Capability (OCAP) Tokens**: Introduce unique object-capability tokens to rigorously control access to critical methods, such as `SetBalance`, `MintCoins`, `BurnCoins`, and `DelegateCoins`. These tokens should be securely managed and allocated to specific modules only when explicitly permitted.\n    - **Encapsulation and Isolation**: Redesign the keeper interfaces to ensure proper encapsulation and isolation. Restrict direct access to sensitive operations, instead offering well-defined capabilities that can enforce clear and constrained access patterns.\n    - **Auditable Permissions**: Implement a mechanism for auditing and reviewing permissions and capabilities allocated to each module. This should include logging and monitoring features to detect and prevent misuse or unauthorized access.\n    - **Module-Level Security**: Review and refactor existing modules, starting with critical ones like `x\/bank`, to adhere to the new object-capability model. This may involve isolating certain responsibilities and tightening the interface boundaries to ensure compliance with the new security standards.\n\n3. **Community Involvement and Ecosystem Growth**\n    - **Engagement and Feedback**: Actively engage the Cosmos SDK developer community to solicit feedback and encourage contributions towards the stabilization and enhancement of the SDK. Establish regular communication channels such as forums, contributing guidelines, and community calls to foster collaboration.\n    - **Module Certification**: Introduce a certification process for modules that adhere to the new stable interfaces and object-capability model. Certified modules should be prominently listed and recommended in the Cosmos SDK documentation to encourage wider adoption.\n    - **Educational Resources**: Provide comprehensive guides, tutorials, and best practices documentation on building secure and reliable modules within the Cosmos SDK ecosystem. This should cover common pitfalls, design patterns, and detailed examples of the new object-capability model in practice.\n\nBy committing to these steps, the Cosmos SDK ecosystem can build a more robust and secure modular framework, fostering greater confidence and adoption among developers and users alike.","GenTime":"2024-07-28 22:09:27"}
{"File Name":"cosmos-sdk\/adr-002-docs-structure.md","Context":"## Context\\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n\n## Decision\n","Decision":"Re-structure the `\/docs` folder of the Cosmos SDK github repo as follows:\\n```text\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n* `README`: Landing page of the docs.\\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\\n* `spec`: Contains specs of modules, and others.\\n* `modules`: Contains links to `godocs` and the spec of the modules.\\n* `architecture`: Contains architecture-related docs like the present one.\\n* `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n* `README`\\n* `intro`\\n* `concepts`\\n* `clients`\\n`architecture` need not be displayed on the website.\\n","tokens":115,"id":816,"Prediction":"## Decision\nTo address the need for scalable and well-structured documentation for the Cosmos SDK, the following decisions have been made:\n\n1. **Repository-Specific Documentation**:\n    - Each development framework or tool will be responsible for maintaining its own set of documentation within its respective GitHub repository. This includes:\n        - **SDK Repo**: Will contain all documentation specific to the Cosmos SDK, including guides, API references, and tutorials.\n        - **Hub Repo**: Will contain all documentation specific to the Cosmos Hub.\n        - **Lotion Repo**: Will contain all documentation specific to the Lotion framework.\n   \n2. **Website Centralized Documentation**:\n    - The main Cosmos website will serve as the central hub for all high-level documentation that is not specific to any single development framework or tool. This includes:\n        - FAQs\n        - Whitepapers\n        - General introductory material about Cosmos and its ecosystem\n        - Links to repository-specific documentation\n\n3. **Consistent Documentation Standards**:\n    - Establish and enforce consistent guidelines and standards for documentation across all repositories to ensure uniformity in style, structure, and quality.\n\n4. **Documentation Maintenance**:\n    - Each repository owner\/team will be responsible for the continuous maintenance and updating of their respective documentation to ensure accuracy and relevance.\n\n5. **User-Friendly Navigation**:\n    - Improve the usability and accessibility of documentation by implementing a clear and intuitive navigation structure both within individual repositories and on the main website.\n\n6. **Community Contributions**:\n    - Encourage contributions from the community by providing clear guidelines and contribution processes for documentation.\n\nBy decentralizing the documentation to live within respective repositories and centralizing general and high-level documentation on the website, we aim to create a more organized, maintainable, and user-friendly documentation ecosystem.","GenTime":"2024-07-28 22:09:46"}
{"File Name":"cosmos-sdk\/adr-043-nft-module.md","Context":"## Context\\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\\n* irismod\/nft and modules\/incubator\/nft\\n* CW721\\n* DID NFTs\\n* interNFT\\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\\n\n## Decision\n","Decision":"We create a `x\/nft` module, which contains the following functionality:\\n* Store NFTs and track their ownership.\\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\\n* Query NFTs and their supply information.\\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\\n### Types\\nWe propose two main types:\\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\\n#### Class\\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\\n```protobuf\\nmessage Class {\\nstring id          = 1;\\nstring name        = 2;\\nstring symbol      = 3;\\nstring description = 4;\\nstring uri         = 5;\\nstring uri_hash    = 6;\\ngoogle.protobuf.Any data = 7;\\n}\\n```\\n* `id` is used as the primary index for storing the class; _required_\\n* `name` is a descriptive name of the NFT class; _optional_\\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\\n* `description` is a detailed description of the NFT class; _optional_\\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is app specific metadata of the class; _optional_\\n#### NFT\\nWe define a general model for `NFT` as follows.\\n```protobuf\\nmessage NFT {\\nstring class_id           = 1;\\nstring id                 = 2;\\nstring uri                = 3;\\nstring uri_hash           = 4;\\ngoogle.protobuf.Any data  = 10;\\n}\\n```\\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\\n```text\\n{class_id}\/{id} --> NFT (bytes)\\n```\\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\\n### `Keeper` Interface\\n```go\\ntype Keeper interface {\\nNewClass(ctx sdk.Context,class Class)\\nUpdateClass(ctx sdk.Context,class Class)\\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\\nUpdate(ctx sdk.Context, nft NFT)\\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\\nGetClass(ctx sdk.Context, classId string) Class\\nGetClasses(ctx sdk.Context) []Class\\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\\nGetTotalSupply(ctx sdk.Context, classId string) uint64\\n}\\n```\\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\nrpc Send(MsgSend)         returns (MsgSendResponse);\\n}\\nmessage MsgSend {\\nstring class_id = 1;\\nstring id       = 2;\\nstring sender   = 3;\\nstring reveiver = 4;\\n}\\nmessage MsgSendResponse {}\\n```\\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\\nThe implementation outline of the server is as follows:\\n```go\\ntype msgServer struct{\\nk Keeper\\n}\\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\\n\/\/ check current ownership\\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\\n\/\/ transfer ownership\\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\\nreturn &types.MsgSendResponse{}, nil\\n}\\n```\\nThe query service methods for the `x\/nft` module are:\\n```protobuf\\nservice Query {\\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\\n}\\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\\n}\\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\\n}\\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\\n}\\n\/\/ NFT queries an NFT based on its class and id.\\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\\n}\\n\/\/ Class queries an NFT class based on its id\\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\\n}\\n\/\/ Classes queries all NFT classes\\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\\n}\\n}\\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\\nmessage QueryBalanceRequest {\\nstring class_id = 1;\\nstring owner    = 2;\\n}\\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\\nmessage QueryBalanceResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\\nmessage QueryOwnerRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\\nmessage QueryOwnerResponse {\\nstring owner = 1;\\n}\\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\\nmessage QuerySupplyRequest {\\nstring class_id = 1;\\n}\\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\\nmessage QuerySupplyResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\\nmessage QueryNFTsRequest {\\nstring                                class_id   = 1;\\nstring                                owner      = 2;\\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\\n}\\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\\nmessage QueryNFTsResponse {\\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\\nmessage QueryNFTRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\\nmessage QueryNFTResponse {\\ncosmos.nft.v1beta1.NFT nft = 1;\\n}\\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\\nmessage QueryClassRequest {\\nstring class_id = 1;\\n}\\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\\nmessage QueryClassResponse {\\ncosmos.nft.v1beta1.Class class = 1;\\n}\\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\\nmessage QueryClassesRequest {\\n\/\/ pagination defines an optional pagination for the request.\\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\\n}\\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\\nmessage QueryClassesResponse {\\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n```\\n### Interoperability\\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\\n","tokens":307,"id":827,"Prediction":"## Decision\n\nWe have decided to implement a generic NFT module in the Cosmos Hub, leveraging the existing work done by the IRISnet team and the earlier implementation in the Cosmos repository. This decision is based on the following considerations:\n\n1. **Generic Usage and Compatibility**: The generic NFT module will handle core NFT logic, making it compatible with interchain protocols like IBC and Gravity Bridge. This approach ensures that the Cosmos Hub can interact seamlessly with other zones and blockchains.\n\n2. **Modularity and Composability**: By focusing on a generic NFT logic module, we ensure that application-specific functions are handled by other modules. This design allows for greater flexibility and composability, enabling various use cases to be supported through additional modules on the Cosmos Hub or other zones.\n\n3. **Future-Proofing**: A generic design accommodates a wide range of NFT use cases and ensures that future use cases can be supported by developing new modules that import the NFT core module.\n\n4. **Community and Ecosystem Alignment**: Utilizing the existing work by the IRISnet team aligns with the community's efforts and avoids duplicative work. It also leverages the knowledge and validation already present in the ecosystem.\n\n5. **Implementation Efficiency**: Building upon the existing IRISnet implementation and the older Cosmos repository's work will allow for a more efficient development process, speeding up the rollout of the NFT module.\n\n### Implementation Details\n\n- **Module Framework**: The core NFT module will be built to include functionalities for creating, transferring, and burning NFTs. It will also handle metadata and ownership representation.\n- **IBC Integration**: Ensure that the NFT module is compatible with IBC to facilitate the cross-chain transfer of NFTs.\n- **Extensibility**: Design the module to allow easy addition of new transaction types and use cases by other modules.\n- **Security and Auditing**: Prioritize rigorous security audits and testing to ensure the robustness of the NFT module.\n\nThis decision aligns with the goal of maximizing the value NFTs bring to the Cosmos ecosystem while ensuring flexibility, compatibility, and extensibility for various future use cases.","GenTime":"2024-07-28 22:10:45"}
{"File Name":"cosmos-sdk\/adr-030-authz-module.md","Context":"## Context\\nThe concrete use cases which motivated this module include:\\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\\ndelegated stake\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which\\nis a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n\n## Decision\n","Decision":"We will create a module named `authz` which provides functionality for\\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\\nmust be granted for a particular `Msg` service methods one by one using an implementation\\nof `Authorization` interface.\\n### Types\\nAuthorizations determine exactly what privileges are granted. They are extensible\\nand can be defined for any `Msg` service method even outside of the module where\\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\\n#### Authorization\\n```go\\ntype Authorization interface {\\nproto.Message\\n\/\/ MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\\n\/\/ which will process and accept or reject a request.\\nMsgTypeURL() string\\n\/\/ Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\\n\/\/ so provides an upgraded authorization instance.\\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\\n\/\/ ValidateBasic does a simple validation check that\\n\/\/ doesn't require access to any other information.\\nValidateBasic() error\\n}\\n\/\/ AcceptResponse instruments the controller of an authz message if the request is accepted\\n\/\/ and if it should be updated or deleted.\\ntype AcceptResponse struct {\\n\/\/ If Accept=true, the controller can accept and authorization and handle the update.\\nAccept bool\\n\/\/ If Delete=true, the controller must delete the authorization object and release\\n\/\/ storage resources.\\nDelete bool\\n\/\/ Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\\n\/\/ it must use the updated version and handle the update on the storage level.\\nUpdated Authorization\\n}\\n```\\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\\na `SpendLimit` and updates it down to zero:\\n```go\\ntype SendAuthorization struct {\\n\/\/ SpendLimit specifies the maximum amount of tokens that can be spent\\n\/\/ by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\\n\/\/ can be used with bank msg type url to create limit less bank authorization).\\nSpendLimit sdk.Coins\\n}\\nfunc (a SendAuthorization) MsgTypeURL() string {\\nreturn sdk.MsgTypeURL(&MsgSend{})\\n}\\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\\nmSend, ok := msg.(*MsgSend)\\nif !ok {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\\n}\\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\\nif isNegative {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\\n}\\nif limitLeft.IsZero() {\\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\\n}\\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\\n}\\n```\\nA different type of capability for `MsgSend` could be implemented\\nusing the `Authorization` interface with no need to change the underlying\\n`bank` module.\\n##### Small notes on `AcceptResponse`\\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\\nthe field will be `nil`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\n\/\/ Grant grants the provided authorization to the grantee on the granter's\\n\/\/ account with the provided expiration time.\\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\\n\/\/ Exec attempts to execute the provided messages using\\n\/\/ authorizations granted to the grantee. Each message should have only\\n\/\/ one signer corresponding to the granter of the authorization.\\nrpc Exec(MsgExec) returns (MsgExecResponse);\\n\/\/ Revoke revokes any authorization corresponding to the provided method name on the\\n\/\/ granter's account that has been granted to the grantee.\\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\\n}\\n\/\/ Grant gives permissions to execute\\n\/\/ the provided method with expiration time.\\nmessage Grant {\\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\\n}\\nmessage MsgGrant {\\nstring granter = 1;\\nstring grantee = 2;\\nGrant grant = 3 [(gogoproto.nullable) = false];\\n}\\nmessage MsgExecResponse {\\ncosmos.base.abci.v1beta1.Result result = 1;\\n}\\nmessage MsgExec {\\nstring   grantee                  = 1;\\n\/\/ Authorization Msg requests to execute. Each msg must implement Authorization interface\\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];;\\n}\\n```\\n### Router Middleware\\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\\nto the router based on `Authorization` grants:\\n```go\\ntype Keeper interface {\\n\/\/ DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\\n\/\/ to send those messages by the first (and only) signer of each msg.\\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\\n}\\n```\\n### CLI\\n#### `tx exec` Method\\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\\nwould send a transaction like this:\\n```go\\nMsgExec {\\nGrantee: mykey,\\nMsgs: []sdk.Msg{\\nMsgVote {\\nProposalID: 1,\\nVoter: cosmos3thsdgh983egh823\\nOption: Yes\\n}\\n}\\n}\\n```\\n#### `tx grant <grantee> <authorization> --from <granter>`\\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\\nJSON on the CLI.\\n#### `tx revoke <grantee> <method-name> --from <granter>`\\nThis CLI command will send a `MsgRevoke` transaction.\\n### Built-in Authorizations\\n#### `SendAuthorization`\\n```protobuf\\n\/\/ SendAuthorization allows the grantee to spend up to spend_limit coins from\\n\/\/ the granter's account.\\nmessage SendAuthorization {\\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\\n}\\n```\\n#### `GenericAuthorization`\\n```protobuf\\n\/\/ GenericAuthorization gives the grantee unrestricted permissions to execute\\n\/\/ the provided method on behalf of the granter's account.\\nmessage GenericAuthorization {\\noption (cosmos_proto.implements_interface) = \"Authorization\";\\n\/\/ Msg, identified by it's type URL, to grant unrestricted permissions to execute\\nstring msg = 1;\\n}\\n```\\n","tokens":276,"id":839,"Prediction":"## Decision\n\nThe architectural decision is to implement the \"sub-keys\" functionality by leveraging the existing delegation mechanisms and integrating it with the `fee_grant` module and the `group` module. This implementation will:\n\n1. **Enable Delegation of Voting Rights**: Allow accounts to delegate their voting rights on proposals to other designated accounts. This delegation will be integrated with existing staking and governance modules to ensure seamless operation and security.\n   \n2. **Implement Account Capability Grants**: Develop a flexible system where an account (referred to as the \"master\" account) can grant specific capabilities to other accounts (referred to as \"sub-accounts\"). These capabilities include limited fund spending and proposal voting, among others.\n   \n3. **Security Considerations**: Implement less robust but easier-to-use security measures for sub-accounts, balancing usability with security. This allows, for example, individual employees of an organization to manage small-scale transactions without exposing the organization's primary keys.\n   \n4. **Inter-Module Integration**: Ensure tight integration with the `fee_grant` module to handle fee payments and allowances, and with the `group` module to manage groups and multisig wallets. This integration will facilitate a comprehensive and cohesive user experience.\n   \n5. **Leverage Existing Work**: Build upon the implementation initiated by the Gaian's team during Hackatom Berlin 2019, ensuring that the foundational work is expanded and refined to meet current architectural standards and use cases.\n\nThis approach will provide enhanced flexibility and capability management within the ecosystem, supporting the concrete use cases identified and enabling future extensibility.","GenTime":"2024-07-28 22:11:41"}
{"File Name":"cosmos-sdk\/adr-016-validator-consensus-key-rotation.md","Context":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\\n\n## Decision\n","Decision":"### Pseudo procedure for consensus key rotation\\n* create new random consensus key.\\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n* start validating with new consensus key.\\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n* consensus key mapping information management strategy\\n* store history of each key mapping changes in the kvstore.\\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n* the state machine does not need any historical mapping information which is past more than unbonding period.\\n* key rotation costs related to LCD and IBC\\n* LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n* limits\\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\\n* parameters can be decided by governance and stored in genesis file.\\n* key rotation fee\\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n* evidence module\\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n* abci.ValidatorUpdate\\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n* new genesis parameters in `staking` module\\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n* checks if the signing account has enough balance to pay `KeyRotationFee`\\n* pays `KeyRotationFee` to community fund\\n* overwrites `NewPubKey` in `validator.ConsPubKey`\\n* deletes old `ValidatorByConsAddr`\\n* `SetValidatorByConsAddr` for `NewPubKey`\\n* Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n* Note : All above features shall be implemented in `staking` module.\\n","tokens":171,"id":841,"Prediction":"## Decision\n\nWe will implement the simplest form of validator consensus key rotation directly within the Cosmos SDK, without changing the consensus logic in Tendermint.\n\n### Key Aspects of the Decision:\n1. **Consensus Key Rotation Implementation**:\n   - Introduce functionality in the Cosmos SDK to allow validators to replace their existing consensus key with a new one.\n   - This will be achieved by allowing validators to submit a transaction containing the new consensus key, which will be processed and updated in the validator set.\n\n2. **No Consensus Logic Update in Tendermint**:\n   - Since Tendermint does not maintain a mapping between consensus keys and validator operator keys, consensus key rotation is effectively treated as the replacement of one key with another from Tendermint's perspective.\n   - This simplifies the implementation as no changes are required in Tendermint\u2019s consensus logic.\n\n3. **Single Consensus Key Focus**:\n   - This implementation will focus on enabling rotation for a single consensus key per validator, which is the simplest approach.\n   - The introduction of multiple consensus keys for a single validator will be deferred to future iterations and development, as it requires significant changes and considerations.\n\n4. **Safety and Security**:\n   - This feature aims to enhance the safety of validator operations by allowing for the safe rotation of keys in case of potential key compromise or routine security practices.\n\n5. **Backward Compatibility**:\n   - Ensuring that this feature does not disrupt existing validator setups and operations is critical. Therefore, backward compatibility will be maintained.\n\nBy adopting this strategy, we provide an immediate improvement in validator key management without overhauling the core consensus mechanisms, paving the way for more advanced features like multiple consensus keys in the future.","GenTime":"2024-07-28 22:11:52"}
{"File Name":"cosmos-sdk\/adr-032-typed-events.md","Context":"## Context\\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\\n[Our platform](http:\/\/github.com\/ovrclk\/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https:\/\/github.com\/ovrclk\/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\\n\n## Decision\n","Decision":"**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\\n```go\\n\/\/ types\/events.go\\n\/\/ EmitTypedEvent takes typed event and emits converting it into sdk.Event\\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\\nevtType := proto.MessageName(event)\\nevtJSON, err := codec.ProtoMarshalJSON(event)\\nif err != nil {\\nreturn err\\n}\\nvar attrMap map[string]json.RawMessage\\nerr = json.Unmarshal(evtJSON, &attrMap)\\nif err != nil {\\nreturn err\\n}\\nvar attrs []abci.EventAttribute\\nfor k, v := range attrMap {\\nattrs = append(attrs, abci.EventAttribute{\\nKey:   []byte(k),\\nValue: v,\\n})\\n}\\nem.EmitEvent(Event{\\nType:       evtType,\\nAttributes: attrs,\\n})\\nreturn nil\\n}\\n\/\/ ParseTypedEvent converts abci.Event back to typed event\\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\\nconcreteGoType := proto.MessageType(event.Type)\\nif concreteGoType == nil {\\nreturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\\n}\\nvar value reflect.Value\\nif concreteGoType.Kind() == reflect.Ptr {\\nvalue = reflect.New(concreteGoType.Elem())\\n} else {\\nvalue = reflect.Zero(concreteGoType)\\n}\\nprotoMsg, ok := value.Interface().(proto.Message)\\nif !ok {\\nreturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\\n}\\nattrMap := make(map[string]json.RawMessage)\\nfor _, attr := range event.Attributes {\\nattrMap[string(attr.Key)] = attr.Value\\n}\\nattrBytes, err := json.Marshal(attrMap)\\nif err != nil {\\nreturn nil, err\\n}\\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn protoMsg, nil\\n}\\n```\\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key\/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\\n**Step-2**: Add proto definitions for typed events for msgs in each module:\\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\\n```protobuf\\n\/\/ proto\/cosmos\/gov\/v1beta1\/gov.proto\\n\/\/ Add typed event definition\\npackage cosmos.gov.v1beta1;\\nmessage EventSubmitProposal {\\nstring from_address   = 1;\\nuint64 proposal_id    = 2;\\nTextProposal proposal = 3;\\n}\\n```\\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\\n```go\\n\/\/ x\/gov\/handler.go\\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\\n...\\ntypes.Context.EventManager().EmitTypedEvent(\\n&EventSubmitProposal{\\nFromAddress: fromAddress,\\nProposalId: id,\\nProposal: proposal,\\n},\\n)\\n...\\n}\\n```\\n### How to subscribe to these typed events in `Client`\\n> NOTE: Full code example below\\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\\nAkash Network has built a simple [`pubsub`](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/pubsub\/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/events\/publish.go#L21) them as typed events.\\nPlease see the below code sample for more detail on this flow looks for clients.\\n","tokens":433,"id":842,"Prediction":"## Decision\nWe will adopt the proposal to implement **typed events** in the Cosmos SDK by defining explicit event types within each module. This standardized approach to event definition will be integrated into the SDK in the following manner:\n\n1. **Define Typed Events in Modules**: Each module will have a canonical set of event types defined to replace the current string-based event definitions. These types will provide a structured and consistent way to represent events.\n\n2. **Event Emitters and Event Handlers**: The Cosmos SDK will include generic `EventEmitters` and `EventHandlers` that can be used across modules. Developers will just need to write `EventHandler`s for their specific event types and pass them to the provided `EventEmitters`.\n\n3. **Enhanced Event Consumption**: By using typed events, we will simplify event consumption for end-users. Instead of relying on raw string matching and parsing, consumers of these events will process well-defined event types, resulting in more reliable and maintainable code.\n\n4. **Backward Compatibility**: We will ensure backward compatibility by providing a migration path for existing modules and applications that currently rely on string-based event definitions.\n\n5. **Documentation and Examples**: Detailed documentation and examples will be provided to guide developers on how to define, emit, and handle typed events, making it easier for them to build event-driven applications.\n\n### Benefits\n- **Consistency**: Establishing a standard method across modules ensures consistency and reduces ambiguity.\n- **Ease of Use**: Typed events simplify event handling and consumption, reducing the need for complex parsing logic.\n- **Robustness**: Structured event types improve the robustness and readability of event-driven applications.\n- **Better Tooling**: Standardization of event types will enable better tooling and support for developers building on the Cosmos SDK.\n\n### Example\nHere's a simplified example of what the new workflow might look like:\n\n```go\n\/\/ Define a new event type in a module\ntype BidCreatedEvent struct {\n    OrderID   string\n    BidAmount int64\n    Provider  string\n}\n\n\/\/ Emit the event in a module handler\nfunc handleBidCreation(ctx sdk.Context, msg MsgCreateBid) sdk.Result {\n    ...\n    event := BidCreatedEvent{\n        OrderID:   newBid.OrderID,\n        BidAmount: newBid.Amount,\n        Provider:  newBid.Provider,\n    }\n    ctx.EventManager().EmitTypedEvent(event)\n    ...\n}\n\n\/\/ Consume the event in an application\nfunc handleBidCreated(event BidCreatedEvent) {\n    log.Printf(\"New bid created by %s for order %s with amount %d\", event.Provider, event.OrderID, event.BidAmount)\n}\n\n\/\/ Register the event handler\neventEmitter.RegisterHandler(BidCreatedEvent{}, handleBidCreated)\n```\n\nBy implementing typed events, we aim to enhance the developer experience, making it easier to build and maintain event-driven applications on the Cosmos SDK. This change will provide a robust and future-proof foundation for the growing demand for event-driven features across various Cosmos SDK applications.","GenTime":"2024-07-28 22:12:00"}
{"File Name":"crispy-dragon\/ADR-2-Frontend-Framework.md","Context":"## Context\\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\\n\n## Decision\n","Decision":"The frontend will be build with [React](https:\/\/reactjs.org\/) and [Snowpack](https:\/\/www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https:\/\/tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n","tokens":27,"id":847,"Prediction":"## Decision\nWe have decided to implement a responsive web frontend using React.js. React.js offers a robust ecosystem with a strong community support, and it allows us to build a dynamic and interactive user interface efficiently. It is also well-suited for modern devices, ensuring compatibility across desktops, tablets, and smartphones. \n\nAdditionally, we will use a CSS framework like Bootstrap or Tailwind CSS to streamline the responsive design process and ensure a consistent user experience across various screen sizes. This approach will enable us to quickly deliver a functional and aesthetically pleasing frontend that aligns with our users' needs for accessing the API on various devices.","GenTime":"2024-07-28 22:12:12"}
{"File Name":"infra\/0002-use-helm-for-service-deployment-definitions.md","Context":"## Context\\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock\/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region\/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\\n\n## Decision\n","Decision":"Use helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock\/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev\/staging\/prod versions of the service.\\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\\n","tokens":190,"id":856,"Prediction":"## Decision\nWe have decided to implement a templating system to generate our Kubernetes deployment files in order to reduce redundancy, ensure consistency across different environments, and adhere to the DRY principle. Specifically, we will use Helm to manage our Kubernetes manifests.\n\n### Justification:\n1. **Consistency**: Helm allows us to create a single set of definitions and templates, replacing variable content based on the deployment environment. This ensures that deployments in different environments are consistent and reduces the potential for human error.\n   \n2. **Code Reuse**: By templating our deployment configurations, we eliminate the need for maintaining multiple near-identical sets of Kubernetes manifests, aligning with the DRY principle.\n\n3. **Scalability**: As our deployment grows to support more environments (additional regions or cloud providers), maintaining a single set of templates will be significantly more scalable than multiplying configuration files.\n\n4. **Flexibility**: Helm charts allow for the dynamic generation of Kubernetes manifests, and can be easily extended and customized with values files that provide environment-specific configurations.\n\n5. **Integration**: Helm integrates well with existing Kubernetes tools like `kubectl`, and fits within our current deployment workflows without introducing significant overhead.\n\n### Plan:\n1. **Template Creation**: Develop Helm charts for each of our primary supported services (bedrock\/www, snippets, basket).\n   \n2. **Environment Values**: Create values files for each deployment environment (e.g., gcp iowa-a, aws frankfurt) containing all variable configurations specific to those environments.\n\n3. **Testing**: Perform thorough testing to ensure that the generated manifests work correctly and consistently across all environments.\n\n4. **Documentation**: Update the deployment documentation to reflect the new Helm-based system, providing detailed instructions on how to add new environments and manage existing ones.\n\n5. **Training**: Conduct training sessions for the development and operations teams to become proficient in the new templating system.\n\nBy adopting Helm, we aim to streamline our deployment processes, reduce environmental drift, and improve overall maintainability and scalability of our Kubernetes deployments.","GenTime":"2024-07-28 22:12:52"}
{"File Name":"nada-kafkarator\/0002-use-python.md","Context":"## Context\\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\\nAnother option was Rust, which could be a useful learning exercise.\\nPython is used in FIAAS, so we have some experience using it for an operator.\\n\n## Decision\n","Decision":"We will use Python for Kafkarator.\\n","tokens":61,"id":861,"Prediction":"## Decision\nGiven the team's lack of proficiency with Golang and the desire to leverage existing experience, we will use Python for developing the Kubernetes operator. Python's extensive ecosystem, our familiarity with it from the FIAAS project, and its suitability for writing operators will enable us to be productive quickly and maintain the operator more effectively. While Rust could be a valuable learning experience, the immediate need for a reliable operator outweighs this benefit.\n\nWe will:\n\n1. Utilize frameworks such as `kopf` or `operator-framework` to streamline the development process.\n2. Follow best practices from the existing Python-based projects and communities focused on Kubernetes.\n3. Provide necessary training and documentation for the team to ensure a smooth development process.\n4. Evaluate the performance and maintainability of the Python-based operator regularly to ensure it meets our needs.","GenTime":"2024-07-28 22:13:09"}
{"File Name":"nada-kafkarator\/0003-use-poetry-for-dependency-management.md","Context":"## Context\\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\\nspace, and the Python community has not landed on a single standard. Each project needs to select which\\ntools to use, usually a selection of tools that integrate well.\\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\\nclaims to support the use case well. It is also following the new pyproject.toml standard.\\n\n## Decision\n","Decision":"We will use Poetry for this project. (https:\/\/python-poetry.org\/)\\n","tokens":148,"id":864,"Prediction":"## Decision\nWe have decided to use Poetry for managing dependencies and virtual environments in our Python projects. This decision is based on several key factors:\n\n1. **Adherence to pyproject.toml Standard**: Poetry follows the new pyproject.toml standard, which aligns with the recent trends and recommendations from the Python community. This ensures our project configuration remains future-proof and compatible with modern tools.\n\n2. **Feature Set**: Poetry offers a comprehensive set of features, including dependency resolution, package building, and publishing tools, which are more robust and intuitive compared to other tools like Pipenv.\n\n3. **Active Development and Community Support**: Poetry has a growing community and active development, which means issues are likely to be addressed in a timely manner and new features will continue to be added.\n\n4. **Enhanced Dependency Management**: Poetry\u2019s lock file ensures reproducible installations, which reduces the probability of \"works on my machine\" issues and enhances collaboration across different development environments.\n\n5. **Integration and Documentation**: Poetry integrates well with our existing development workflows and has thorough documentation, making it easier for new team members to onboard and follow best practices.\n\nBy adopting Poetry, we enhance our project\u2019s maintainability and consistency, while aligning with the broader direction of the Python ecosystem.","GenTime":"2024-07-28 22:13:20"}
{"File Name":"nada-kafkarator\/0004-combine-topic-creation-and-credentials-management-in-same-app.md","Context":"## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n\n## Decision\n","Decision":"We will ignore the SRP in this instance, and keep the two concerns in the same application.\\n","tokens":83,"id":866,"Prediction":"## Decision\n\nWe will implement the functionalities for creating topics and supplying credentials within the same application. While they are separate concerns in terms of implementation, they are conceptually connected and often need to interact with each other closely. Combining them in the same application will streamline development, reduce the complexity of inter-application communication, and simplify deployment processes. \n\nTo ensure maintainability and high cohesion, we will:\n1. Keep the code for creating topics and supplying credentials in separate modules or classes within the application.\n2. Define clear interfaces and contracts for interactions between these modules.\n3. Ensure that each module adheres to the Single Responsibility Principle within its own scope.\n\nBy structuring the application in this manner, we aim to balance the benefits of conceptual connectivity with the architectural clarity provided by modularization.","GenTime":"2024-07-28 22:13:23"}
{"File Name":"profiles-etl-combiner\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":871,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) strategy to document all significant architectural decisions made throughout the project. Each ADR will follow a standardized template to ensure consistency and comprehensiveness. The template will include sections such as Context, Decision, and Consequences, amongst others. This approach facilitates clear communication, accountability, and provides a historical record for future reference. ADRs will be stored in a dedicated directory in the project\u2019s repository to ensure easy access and management.","GenTime":"2024-07-28 22:13:41"}
{"File Name":"kotlin-guice\/0002-use-packaging-for-major-version-interoperability.md","Context":"## Context\\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\\n\n## Decision\n","Decision":"We will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\\n1. The Java package name will include the version number.\\n1. The library name will be part of the group ID.\\n1. The group ID will include the version number.\\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\\nhttp:\/\/jakewharton.com\/java-interoperability-policy-for-major-version-updates\/\\n","tokens":88,"id":872,"Prediction":"## Decision\n\nTo ensure compatibility and maintainability of the `kotlin-guice` library, the following decisions are made:\n\n1. **Version Synchronization with Guice Major Version Upgrades**: \n    - The `kotlin-guice` library will adopt a versioning scheme that mirrors the major version of the Guice library it depends on.\n    - For example, if Guice releases version `6.0.0`, `kotlin-guice` will also release a corresponding `6.0.0` version, reflecting any necessary adaptations or changes.\n\n2. **Clear Communication of Breaking Changes**:\n    - Each major version upgrade of `kotlin-guice` will include a comprehensive changelog detailing breaking changes, deprecations, and new features.\n    - A migration guide will be provided to help users upgrade their applications smoothly.\n\n3. **Automated Dependency Compatibility Checks**:\n    - Implement continuous integration checks to automatically verify compatibility between `kotlin-guice` and the latest major version of the Guice library.\n    - These checks will ensure that any necessary adjustments to the `kotlin-guice` library can be identified and addressed promptly.\n\n4. **Deprecation Policy**:\n    - Deprecate older versions of `kotlin-guice` in parallel with the deprecation of the corresponding Guice versions, providing users with ample notice to upgrade.\n    - Support for deprecated versions will be maintained for a predefined period, but no new features will be added to these versions.\n\n5. **Community Engagement**:\n    - Engage with the user community through forums, issue trackers, and social media to gather feedback and address concerns related to major version upgrades.\n    - Encourage community contributions to assist with the upgrade process and support documentation.\n\nBy aligning `kotlin-guice` library versioning and changes closely with Guice, we aim to minimize disruption for users and provide a clear, systematic approach to managing major version upgrades.","GenTime":"2024-07-28 22:13:46"}
{"File Name":"we-be-best\/1-use-both-pointer-and-mouse-events.md","Context":"## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n\n## Decision\n","Decision":"All pointer event interactions have a mouse event fallback.\\n","tokens":102,"id":873,"Prediction":"## Decision\n\nGiven the current limitations in Safari and iOS regarding the support for the PointerEvents API, specifically the issue with movementX and movementY properties always being 0, we have decided to implement an alternative approach that would provide cross-browser compatibility and ensure reliable event capturing.\n\n1. **Primary Approach - Hybrid Event Handling**:\n   - **Mouse Events**: For desktop environments, including Safari, we will use the traditional MouseEvent API (e.g., `mousemove`, `mousedown`, `mouseup`). This will ensure that we can still capture mouse movements accurately across all major browsers.\n   - **Pointer Events**: For browsers that fully support PointerEvents (like Chrome and Edge), we will continue to use the PointerEvents API to take advantage of its unified event model.\n\n2. **Manual Tracking for MovementX\/Y**:\n   - **Fallback Method**: For environments where movementX and movementY properties are not available or reliable, we will implement a manual tracking mechanism. This involves keeping track of the previous coordinates and calculating the movement distance manually.\n   - **Code Implementation**:\n     ```javascript\n     let prevX, prevY;\n     document.addEventListener('mousemove', (event) => {\n       if (prevX !== undefined && prevY !== undefined) {\n         const movementX = event.clientX - prevX;\n         const movementY = event.clientY - prevY;\n         handleMovement(movementX, movementY);\n       }\n       prevX = event.clientX;\n       prevY = event.clientY;\n     });\n     ```\n\n3. **Device Detection and Conditional Logic**:\n   - To ensure smooth functionality across devices, we will implement device detection and conditional logic to switch between PointerEvents and MouseEvents based on browser capabilities.\n   - **Code Implementation**:\n     ```javascript\n     const isPointerEventSupported = 'PointerEvent' in window;\n     if (isPointerEventSupported) {\n       document.addEventListener('pointermove', handlePointerMove);\n     } else {\n       document.addEventListener('mousemove', handleMouseMove);\n     }\n     ```\n\n4. **Testing and Validation**:\n   - Conduct thorough testing on multiple browsers including Safari (desktop and iOS) to ensure that the hybrid approach works seamlessly.\n   - Ensure the fallback mechanism is robust and handles the manual tracking without performance degradation.\n\nBy implementing this decision, we will be able to ensure consistent and accurate event capturing across all major browsers and platforms, including those with limited or experimental support for PointerEvents. The hybrid approach provides flexibility and ensures that our application remains functional and performant for all users.","GenTime":"2024-07-28 22:13:52"}
{"File Name":"python-library-project-generator\/0000-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on Opinionated Digital Center.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":18,"id":875,"Prediction":"## Decision\n\nTo effectively manage and document the architectural decisions for the Opinionated Digital Center (ODC), we will implement an Architectural Decision Record (ADR) framework. This framework will ensure that all key design choices are meticulously recorded for future reference and analysis.\n\n### Key Decisions:\n\n1. **Adopt ADR Format**: We will use an ADR template to maintain consistency and clarity across all decision records. The template will include sections such as Context, Decision, Status, Consequences, and Alternatives Considered.\n\n2. **Version Control with Git**: All ADRs will be stored in a dedicated directory within our Git repository. This approach will allow for version control, traceability, and collaboration among team members.\n\n3. **Periodic Reviews**: We will establish a schedule for periodic reviews of the ADRs to ensure that all decisions remain relevant and adapt to any new changes in the project.\n\n4. **Accessibility and Transparency**: ADRs will be accessible to all team members and stakeholders. This will foster a culture of transparency and shared understanding of the architectural evolution of the ODC.\n\n5. **Integration with Jira**: Each ADR will be linked to corresponding Jira tickets to ensure traceability between decisions and their implementation in the codebase.\n\nBy implementing these steps, we aim to maintain a robust and clear architectural decision-making process that will guide the development and evolution of the Opinionated Digital Center.","GenTime":"2024-07-28 22:14:01"}
{"File Name":"modular-monolith-with-ddd\/0015-use-in-memory-events-bus.md","Context":"## Context\\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"events bus\" to do that.\\n\n## Decision\n","Decision":"Solution number 1 - In Memory Events Bus<\/br>\\nAt that moment we don't see more advanced integration scenarios in our system than simple publish\/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\\n","tokens":35,"id":885,"Prediction":"## Decision\nWe have decided to use Apache Kafka as the event bus for our inter-modular communication. Apache Kafka is a robust, scalable, and distributed event-streaming platform that aligns well with our event-driven architecture requirements.\n\n### Justification:\n1. **Scalability:** Kafka is designed to handle a high volume of events, which is crucial as our application grows.\n2. **Reliability:** It provides strong durability and fault tolerance, ensuring our events are safely stored and delivered even in case of failures.\n3. **Performance:** Kafka offers low latency and high throughput, which is essential for maintaining efficient communication between modules.\n4. **Ecosystem Integration:** It has a rich ecosystem and supports integration with various data processing frameworks, such as Apache Flink, Apache Storm, and others.\n5. **Community and Support:** Being widely adopted in the industry, Kafka has a large community and extensive documentation, making it easier for our team to find support and resources.\n\nBy implementing Apache Kafka, we aim to ensure our communication layer is robust, efficient, and capable of supporting future growth and complexity in our system.","GenTime":"2024-07-28 22:14:36"}
{"File Name":"modular-monolith-with-ddd\/0016-create-ioc-container-per-module.md","Context":"## Context\\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\\n\n## Decision\n","Decision":"Solution number 2 - IoC Container per module<\/br>\\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\\n","tokens":37,"id":899,"Prediction":"## Decision\n\nWe will use a Dependency Injection (DI) framework to manage object creation and dependency resolution. This approach will allow us to separate the construction of a complex object graph from the business logic, making our codebase more maintainable and testable.\n\n### Justification\n\n1. **Decouple Object Creation from Business Logic:** Dependency Injection enables us to invert the control of object creation, which means that our business logic does not need to know how to instantiate dependencies. This results in a cleaner architecture and reduces the risk of tightly coupled code.\n   \n2. **Enhanced Testability:** Since dependencies are injected rather than created internally, it becomes easier to mock or stub these dependencies during unit testing. This facilitates more effective and isolated test cases.\n\n3. **Centralized Configuration:** Dependency Injection frameworks typically allow for a centralized configuration of object graphs. This ensures a single source of truth for dependency management and makes it easier to update dependencies without scanning through the entire codebase.\n\n4. **Scalability:** As the codebase grows and new modules are added, the DI framework can handle the complexity of dependency graphs, making the system easy to scale.\n\n5. **Consistency and Best Practices:** Many DI frameworks enforce best practices and design patterns, such as Singleton, Factory, and Builder patterns. Using these frameworks helps maintain consistency across the application.\n\n### Implementation Strategy\n\n1. **Select a DI Framework:** Evaluate and choose a dependency injection framework that aligns with our technology stack (e.g., Spring for Java, Dagger for Android, or Microsoft.Extensions.DependencyInjection for .NET).\n\n2. **Refactor Existing Code:** Gradually refactor existing code to use the DI framework, starting with the most critical modules and dependencies.\n\n3. **Configure Dependency Graph:** Use the DI framework to define how objects are created and how their dependencies are resolved. This configuration will likely be done in a central location, such as a Configuration class or a DI Container.\n\n4. **Update Testing Strategy:** Adjust unit tests to leverage the DI framework for injecting mock objects or test-specific implementations of dependencies.\n\n5. **Document the Approach:** Document the DI approach and best practices in our development guidelines to ensure all team members understand and adhere to the new system.\n\nBy implementing Dependency Injection, we will be able to manage the complexities of object dependencies efficiently and maintain a clean and scalable architecture.","GenTime":"2024-07-28 22:15:32"}
{"File Name":"csc-swr-architecture\/007-Integration-Approaches.html.md.erb","Context":"## Context\\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\\nsuch as Looked After Children and Foster Families. These will be sourced from Local\\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\\nas well as central Departmental data collections such as SSDA903.\\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\\npathways through the Care System. Each individuals pathway can be regarded as a\\nseries of events and touchpoints that the individual has had with various other actors\\nin the system, such as IFAs and Foster Families.\\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\\n\n## Decision\n","Decision":"* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\\nto LAs and IFAs, serving an [Event Architecture] available internally.\\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\\n* Publish example client code on the Internet to ease LA and IFA adoption.\\n* Events will become the durable and persistent data which can be used to hydrate\\nthe Department's services, such as Diary Log and Matchmaker Assistant,\\n* [Event Sourcing] should be trialled as a solution.\\n","tokens":169,"id":914,"Prediction":"## Decision\n\nAdopt an Event-Driven Architecture (EDA) for the Children's Social Care Alpha Service to integrate and manage key data assets from Local Authorities (LA) and Independent Foster Agencies (IFA), as well as central Departmental data collections such as SSDA903.\n\n### Rationale:\n\n1. **Event Stream Nature**: The SSDA903 dataset is already similar to an event stream, making EDA a natural fit. Events generated by LAs and IFAs can be efficiently processed and integrated using an event-driven approach.\n   \n2. **Modular Integration**: EDA allows for modular integration of diverse data sources from various LAs and IFAs, facilitating a robust data management system that can easily scale and adapt to changes.\n   \n3. **Real-Time Processing**: EDA supports real-time processing and analytics, enabling the service to track and act upon individual pathways through the care system as they happen.\n   \n4. **Scalability**: The event-driven model supports horizontal scaling, which will be crucial as the volume of data from multiple LAs and IFAs grows.\n   \n5. **Resilience and Elasticity**: EDA provides inherent resilience against failures in individual event producers, ensuring that the system remains operational and elastic, scaling resources as needed without centralized bottlenecks.\n   \n6. **Complex Event Handling**: It allows for the design of sophisticated event handling mechanisms to interpret and react to individual care pathway events, improving insights and actions within the social care system.\n\n### Implementation Considerations:\n\n- **Event Broker Selection**: Evaluate and select an event broker capable of handling the anticipated volume and complexity of event streams. Options may include Kafka, Amazon Kinesis, or Azure Event Hubs.\n  \n- **Standardized Event Format**: Define a standard event schema to ensure consistency in event data across different producers (LAs and IFAs).\n  \n- **Security & Compliance**: Ensure that data transmission complies with relevant data protection regulations (e.g., GDPR), implementing appropriate security measures like encryption and access controls.\n  \n- **Data Enrichment**: Design mechanisms for enriching raw events with additional context (e.g., historical data, metadata) to enhance the utility of the information being processed.\n\n### Next Steps:\n\n1. Conduct a detailed requirements analysis to define the specific events and data elements to be captured.\n2. Develop a prototype to validate the EDA approach within a controlled environment.\n3. Rollout the architecture in phases, starting with key high-value events and gradually incorporating additional data sources and event types.\n4. Implement monitoring and alerting mechanisms to ensure the system operates reliably and efficiently.","GenTime":"2024-07-28 22:16:51"}
{"File Name":"csc-swr-architecture\/004-Use-Fake-Data.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\\n\n## Decision\n","Decision":"The placement alpha prototypes and development work will only process fake data.\\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the \"Authority to Operate\" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\\n","tokens":53,"id":915,"Prediction":"## Decision\nGiven the exploratory nature of the Alpha phase for the Children's Social Care Placement Service, all data processing will be conducted using synthetic, anonymized, or dummy data. This approach will ensure that sensitive or real user data is not exposed or mishandled while allowing the team to experiment, build prototypes, and test various ideas.\n\nAdditionally, we will focus on rapid iteration and gathering feedback from stakeholders to inform future development stages. Any real user interactions during this phase will be strictly controlled in a sandbox environment to prevent any accidental data breaches or misuse. This decision supports the goals of the Alpha phase by promoting innovation and flexibility without compromising data security and privacy.","GenTime":"2024-07-28 22:16:53"}
{"File Name":"csc-swr-architecture\/003-Platform-and-Hosting.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\\n\n## Decision\n","Decision":"The decision is to use the [Gov.UK PaaS].\\n**What does Gov.UK PaaS offer?**\\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\\n* Fast application delivery, with short cycles\\n* Flexible development tools\\n* Simplified DevOps\\n* Seamless scalability\\n* Integrated data services\\n* A secure, trusted platform\\n* 24x7 support\\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\\n**Why not DfE Azure?**\\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\\n**Is it secure?**\\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\\nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.cloud.service.gov.uk\/cloud-security-principles\/).\\nAnd for the DfE there is a specific [paper](https:\/\/docs.google.com\/document\/d\/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ\/edit#heading=h.du8tl2830srf) from MBS.\\n","tokens":65,"id":920,"Prediction":"## Decision\n\nGiven that the Children's Social Care Placement service is in the Alpha phase and the primary objective is to explore solutions effectively, the decision is to prioritize developer productivity. This will be done by selecting technologies and methodologies that:\n\n1. **Maximize Flexibility**: Utilize frameworks and tools that allow for rapid prototyping and iteration. This will enable the team to quickly implement and test new ideas, making it easier to adjust the service as new insights are gained.\n\n2. **Promote Collaboration**: Adopt practices like pair programming, code reviews, and regular stand-ups to ensure that communication is constant and that knowledge is shared effectively within the team.\n\n3. **Automate Testing and Deployment**: Implement automated testing and continuous integration\/continuous deployment (CI\/CD) pipelines. This will reduce the time spent on manual testing and deployment, allowing developers to focus more on writing and refining code.\n\n4. **Utilize Cloud Services**: Leverage cloud-based services and infrastructure (such as AWS, Azure, or Google Cloud) that can be easily scaled and adjusted. This will minimize setup and maintenance overhead and provide more agility.\n\n5. **Document Decisions and Designs**: Maintain lightweight but thorough documentation that captures design decisions, architectural considerations, and technology choices. This will support the evolving nature of the project and ensure continuity even as team members come and go.\n\n6. **Use of Agile Methodologies**: Implement Agile methodologies (such as Scrum or Kanban) to structure the development process. This will help in managing the exploratory nature of the project, with regular review points to re-assess priorities and approaches.\n\nBy focusing on these areas, the development team will be well-positioned to adapt quickly to new insights and requirements, thereby optimizing the eventual fit of the service to its intended domain problem.","GenTime":"2024-07-28 22:17:14"}
{"File Name":"terraform\/AMIs.md","Context":"## Context\\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build\/maintain our own images.\\n\n## Decision\n","Decision":"We will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\\n","tokens":50,"id":923,"Prediction":"## Decision\n\nTo ensure consistency, security, and manageability across our deployments in AWS, we will standardize on a limited set of operating system images. Specifically, we will adopt the following approach:\n\n1. **Official AWS Images**: For most common use cases, we will prioritize using official AWS-provided images (such as Amazon Linux 2), which are optimized for AWS and come with AWS support and updates.\n\n2. **Marketplace Images**: For specific needs that are not covered by the official AWS images, we will leverage trusted images from the AWS Marketplace. These images will be vetted for security and compliance by our due diligence process.\n\n3. **Custom Images**: In cases where specific configurations or custom software requirements are necessary and not available in AWS or Marketplace images, we will create and maintain our own custom images. These custom images will be managed using an image creation and maintenance pipeline to ensure they are consistently updated and secure.\n\nBy following this strategy, we can reduce the complexity and overhead associated with managing a diverse set of operating system images while leveraging the benefits of AWS infrastructure. This decision also aligns with our broader goals of maintaining high security standards, simplifying operational processes, and ensuring flexibility to meet various application requirements.","GenTime":"2024-07-28 22:17:22"}
{"File Name":"terraform\/Automation.md","Context":"## Context\\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\\n\n## Decision\n","Decision":"We will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\\n","tokens":93,"id":929,"Prediction":"## Decision\n\nWe will adopt a hybrid approach to manage the increased maintenance requirements of newly migrated services. This approach balances between immediate case-by-case problem resolution and strategic automation:\n\n1. **Short-term Case-by-Case Management**: \n   - For the initial period post-migration, we will address issues on a case-by-case basis.\n   - This allows the team to maintain development velocity by rapidly resolving critical bugs and issues as they arise.\n   - We will document recurring issues and manual interventions during this phase to identify patterns and areas needing optimization.\n\n2. **Incremental Automation**:\n   - Concurrently, we will invest in strategic automation for repetitive, high-frequency tasks and processes based on the documented issues.\n   - This will be done incrementally to spread the cost and impact on the project budget and timeline.\n   - Automation priorities will be determined based on the frequency and impact of issues identified during the short-term management phase.\n\n3. **Regular Review and Adaptation**:\n   - We will conduct regular reviews (e.g., bi-weekly or monthly) to assess the effectiveness of the current approach, identifying new patterns, and areas that can benefit from automation.\n   - Adjust the level of automation and manual intervention as the service matures and stabilizes.\n\nBy using this hybrid approach, we aim to balance initial maintenance costs with long-term efficiencies, ensuring a smoother transition for newly migrated services while managing project budgets effectively.","GenTime":"2024-07-28 22:17:44"}
{"File Name":"tech-team\/0006-supported-node-and-npm-versions.md","Context":"## Context\\neLife has projects built in Node.\\neLife has projects that use the Node Package Manager (npm) in at least part of their own build process, even if the main technology of the project is not Node.\\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\\nStaying up to date with the major and minor versions of Node is important:\\n- to be able to use new features (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\n\n## Decision\n","Decision":"To use 8.x, the current Long Term Support (LTS) line.\\nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https:\/\/github.com\/nodejs\/Release#release-schedule)).\\nTo use whichever npm version is bundled with the Node version we use.\\nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.\\n","tokens":143,"id":939,"Prediction":"## Decision\n\nWe will adopt a policy of supporting the latest LTS (Long Term Support) version of Node.js across all projects. Specifically:\n\n1. **Node.js Versioning:**\n   - We will standardize on the latest LTS version of Node.js for all new projects.\n   - Existing projects will be migrated to the latest LTS version of Node.js during their next scheduled update cycle.\n   - The migration to new LTS versions will be prioritized to ensure we are benefiting from the latest security updates and features.\n\n2. **NPM Versioning:**\n   - We will standardize on the version of npm that comes bundled with the latest LTS version of Node.js.\n   - Projects using older versions of npm that are incompatible with the new Node.js LTS will be updated accordingly.\n   - Regular audits will be carried out to ensure npm dependencies are kept up to date and secure.\n\n3. **Process and Tooling:**\n   - A version management tool, such as `nvm` (Node Version Manager), will be recommended for local development to ensure developers can easily switch between Node.js versions as required.\n   - CI\/CD pipelines will be updated to use the standardized Node.js and npm versions.\n   - Documentation will be updated to reflect the change in Node.js and npm versioning policies, including steps for developers to update their local environments.\n\n4. **Review and Adjustment:**\n   - This decision will be reviewed every 6 months to determine if an adjustment is necessary based on new Node.js releases or shifts in our technology stack.\n\nThis will provide a consistent development environment across all eLife projects, ensuring that we maintain security, leverage new platform features, and reduce the technical debt associated with managing multiple Node.js versions.","GenTime":"2024-07-28 22:18:30"}
{"File Name":"edgex-docs\/0018-Service-Registry.md","Context":"- [Context](#context)\\n- [Existing Behavior](#existing-behavior)\\n* [Device Services](#device-services)\\n+ [Registry Client Interface Usage](#registry-client-interface-usage)\\n* [Core and Support Services](#core-and-support-services)\\n* [Security Proxy Setup](#security-proxy-setup)\\n- [History](#history)\\n- [Problem Statement](#problem-statement)\\n- [Decision](#decision)\\n- [References](#references)\\n<!--te-->\\n\n## Decision\n","Decision":"- [References](#references)\\n<!--te-->\\nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's ```Client.GetServiceEndpoint``` method (if started with the\\n```--registry``` option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpoints\\n(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, with\\nonly minor changes required in the C Device SDK (see previous commments re: the current implementation).\\n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadata\\nboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup\\nthe address of the other service.\\n","tokens":111,"id":946,"Prediction":"## Context\n\nThe software is designed to handle device services, core and support services, and security proxy setup. There is an existing registry client interface that these services interact with to register themselves and retrieve necessary configurations.\n\n## Existing Behavior\n\n### Device Services\nThe device services currently interact with the registry client interface to register devices, fetch configurations, and update status as needed.\n\n### Registry Client Interface Usage\nThe registry client interface provides methods for services to interact with the registry. This includes registering the service, retrieving configurations, and performing updates.\n\n### Core and Support Services\nCore services handle the main logic and operations of the application, while support services provide auxiliary functionalities. Both types of services also utilize the registry client interface for configuration and registration.\n\n### Security Proxy Setup\nThe security proxy setup ensures that all interactions with the registry client interface are secure, roles and permissions are respected, and data integrity and confidentiality are maintained.\n\n## History\n\nVarious issues have been encountered due to inconsistencies in how the registry client interface is used across different services. There are also performance bottlenecks observed when multiple services attempt to access the registry simultaneously. The lack of uniformity in handling security between services has led to potential vulnerabilities.\n\n## Problem Statement\n\nThe inconsistencies in using the registry client interface and in implementing security measures across different services have led to performance issues and security vulnerabilities. A unified and efficient approach is required to standardize interaction with the registry client interface and enforce robust security practices.\n\n## Decision\n\nTo address the inconsistencies, improve performance, and enhance security, the following architectural decisions have been made:\n\n1. **Unified Registry Client Interface Libraries**: Create and enforce the use of standardized libraries for interacting with the registry client interface. All device, core, and support services must use these libraries to ensure consistency.\n\n2. **Asynchronous Access and Caching**: Implement asynchronous access methods and caching mechanisms for interacting with the registry. This will help alleviate the performance bottlenecks caused by simultaneous access.\n\n3. **Centralized Security Proxy**: Develop a centralized security proxy that all interactions with the registry client interface must pass through. This proxy will handle authentication, authorization, and encryption to ensure uniform security enforcement.\n\n4. **Configuration Management**: Standardize configuration file formats and environment variable usage for all services. Implement a centralized configuration management service that synchronizes configurations across all services.\n\n## References\n- [Microservices Security Architecture](https:\/\/example.com\/microservices-security-architecture)\n- [Asynchronous Communication Patterns](https:\/\/example.com\/asynchronous-communication-patterns)\n- [Centralized Configuration Management in Microservices](https:\/\/example.com\/centralized-configuration-management)","GenTime":"2024-07-28 22:19:07"}
{"File Name":"edgex-docs\/014-Secret-Provider-For-All.md","Context":"- [Context](#context)\\n* [Existing Implementations](#existing-implementations)\\n+ [What is a Secret?](#what-is-a-secret)\\n+ [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\\n+ [Known and Unknown Services](#known-and-unknown-services)\\n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\\n+ [Interfaces and factory methods](#interfaces-and-factory-methods)\\n- [Bootstrap's current implementation](#bootstraps-current-implementation)\\n* [Interfaces](#interfaces)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n- [App SDK's current implementation](#app-sdks-current-implementation)\\n* [Interface](#interface)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)\\n- [InsecureSecrets Configuration](#insecuresecrets-configuration)\\n- [Decision](#decision)\\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\n\n## Decision\n","Decision":"* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\nThe new `SecretProvider` abstraction defined by this ADR is a combination of the two implementations described above in the [Existing Implementations](#existing-implementations) section.\\n### Only Exclusive Secret Stores\\nTo simplify the `SecretProvider` abstraction, we need to reduce to using only exclusive `SecretStores`. This allows all the APIs to deal with a single `SecretClient`, rather than the split up way we currently have in Application Services. This requires that the current Application Service shared secrets (database credentials) must be copied into each Application Service's exclusive `SecretStore` when it is created.\\nThe challenge is how do we seed static secrets for unknown services when they become known.  As described above in the [Known and Unknown Services](#known-and-unknown-services) section above,  services currently identify themselves for exclusive `SecretStore` creation via the `EDGEX_ADD_SECRETSTORE_TOKENS` environment variable on security-secretstore-setup. This environment variable simply takes a comma separated list of service names.\\n```yaml\\nEDGEX_ADD_SECRETSTORE_TOKENS: \"<service-name1>,<service-name2>\"\\n```\\nIf we expanded this to add an optional list of static secret identifiers for each service, i.e.  `appservice\/redisdb`, the exclusive store could also be seeded with a copy of static shared secrets. In this case the Redis database credentials for the Application Services' shared database. The environment variable name will change to `ADD_SECRETSTORE` now that it is more than just tokens.\\n```yaml\\nADD_SECRETSTORE: \"app-service-xyz[appservice\/redisdb]\"\\n```\\n> *Note: The secret identifier here is the short path to the secret in the existing **appservice**  `SecretStore`. In the above example this expands to the full path of `\/secret\/edgex\/appservice\/redisdb`*\\nThe above example results in the Redis credentials being copied into app-service-xyz's `SecretStore` at `\/secret\/edgex\/app-service-xyz\/redis`.\\nSimilar approach could be taken for Message Bus credentials where a common `SecretStore` is created with the Message Bus credentials saved. The services request the credentials are copied into their exclusive `SecretStore` using `common\/messagebus` as the secret identifier.\\nFull specification for the environment variable's value is a comma separated list of service entries defined as:\\n```\\n<service-name1>[optional list of static secret IDs sperated by ;],<service-name2>[optional list of static secret IDs sperated by ;],...\\n```\\nExample with one service specifying IDs for static secrets and one without static secrets\\n```yaml\\nADD_SECRETSTORE: \"appservice-xyz[appservice\/redisdb; common\/messagebus], appservice-http-export\"\\n```\\nWhen the `ADD_SECRETSTORE` environment variable is processed to create these `SecretStores`, it will copy the specified saved secrets from the initial `SecretStore` into the service's `SecretStore`. This all depends on the completion of database or other credential bootstrapping and the secrets having been stored prior to the environment variable being processed. security-secretstore-setup will need to be refactored to ensure this sequencing.\\n### Abstraction Interface\\nThe following will be the new `SecretProvider` abstraction interface used by all Edgex services\\n```go\\ntype SecretProvider interface {\\n\/\/ Stores new secrets into the service's exclusive SecretStore at the specified path.\\nStoreSecrets(path string, secrets map[string]string) error\\n\/\/ Retrieves secrets from the service's exclusive SecretStore at the specified path.\\nGetSecrets(path string, _ ...string) (map[string]string, error)\\n\/\/ Sets the secrets lastupdated time to current time.\\nSecretsUpdated()\\n\/\/ Returns the secrets last updated time\\nSecretsLastUpdated() time.Time\\n}\\n```\\n> *Note: The `GetDatabaseCredentials` and `GetCertificateKeyPair` APIs have been removed. These are no longer needed since insecure database credentials will no longer be stored in the `DatabaseInfo` configuration and certificate key pairs are secrets like any others. This allows these secrets to be retrieved via the `GetSecrets` API.*\\n### Implementation\\n#### Factory Method and Bootstrap Handler\\nThe factory method and bootstrap handler will follow that currently in the Bootstrap implementation with some tweaks. Rather than putting the two split interfaces into the DIC, it will put just the single interface instance into the DIC. See details in the [Interfaces and factory methods](#interfaces-and-factory-methods) section above under **Existing Implementations**.\\n#### Caching of Secrets\\nSecrets will be cached as they are currently in the Application Service implementation\\n#### Insecure Secrets\\nInsecure Secrets will be handled as they are currently in the Application Service implementation. `DatabaseInfo` configuration will no longer be an option for storing the insecure database credentials. They will be stored in the `InsecureSecrets` configuration only.\\n```toml\\n[Writable.InsecureSecrets]\\n[Writable.InsecureSecrets.DB]\\npath = \"redisdb\"\\n[Writable.InsecureSecrets.DB.Secrets]\\nusername = \"\"\\npassword = \"\"\\n```\\n##### Handling on-the-fly changes to `InsecureSecrets`\\nAll services will need to handle the special processing when `InsecureSecrets` are changed on-the-fly via Consul. Since this will now be a common configuration item in `Writable` it can be handled in `go-mod-bootstrap` along with existing log level processing. This special processing will be taken from App SDK.\\n#### Mocks\\nProper mock of the `SecretProvider` interface will be created with `Mockery` to be used in unit tests. Current mock in App SDK is hand written rather then generated with `Mockery`.\\n#### Where will `SecretProvider` reside?\\n##### Go Services\\nThe final decision to make is where will this new `SecretProvider` abstraction reside? Originally is was assumed that it would reside in `go-mod-secrets`, which seems logical. If we were to attempt this with the implementation including the bootstrap handler, `go-mod-secrets` would have a dependency on `go-mod-bootstrap` which will likely create a circular dependency.\\nRefactoring the existing implementation in `go-mod-bootstrap` and have it reside there now seems to be the best choice.\\n##### C Device Service\\nThe C Device SDK will implement the same `SecretProvider` abstraction, InsecureSercets configuration and the underling `SecretStore` client.\\n### Consequences\\n- All service's will have `Writable.InsecureSecrets` section added to their configuration\\n- `InsecureSecrets` definition will be moved from App SDK to go-mod-bootstrap\\n- Go Device SDK will add the SecretProvider to it's bootstrapping\\n- C Device SDK implementation could be big lift?\\n- ` SecretStore`configuration section will be added to all Device Services\\n- edgex-go services will be modified to use the single `SecretProvider` interface from the DIC in place of current usage of the `GetDatabaseCredentials` and `GetCertificateKeyPair` interfaces.\\n- Calls to `GetDatabaseCredentials` and `GetCertificateKeyPair` will be replaced with calls to `GetSecrets` API and appropriate processing of the returned secrets will be added.\\n- App SDK will be modified to use `GetSecrets` API in place of the `GetDatabaseCredentials` API\\n- App SDK will be modified to use the new `SecretProvider` bootstrap handler\\n- app-service-configurable's configuration profiles as well as all the Application Service examples configurations will be updated to remove the `SecretStoreExclusive` configuration and just use the existing `SecretStore` configuration\\n- security-secretstore-setup will be enhanced as described in the [Only Exclusive Secret Stores](#only-exclusive-secret-stores) section above\\n- Adding new services that need static secrets added to their `SecretStore` requires stopping and restarting all the services. The is because security-secretstore-setup has completed but not stopped. If it is rerun without stopping the other services, there tokens and static secrets will have changed. The planned refactor of `security-secretstore-setup` will attempt to resolve this.\\n- Snaps do not yet support setting the environment variable for adding SecretStore. It is planned for Ireland release.\\n","tokens":402,"id":947,"Prediction":"## Decision\n\n### Only Exclusive Secret Stores\nWe have decided to adopt a model where each service will have its own exclusive secret store. This approach ensures better isolation and security for each service, preventing scenarios where a security breach in one service could potentially compromise secrets of other services.\n\n### Abstraction Interface\nAn abstraction interface `ISecretProvider` will be defined to standardize the interactions between the services and their respective secret stores. This interface will encompass essential methods such as `GetSecret`, `StoreSecret`, and `DeleteSecret`.\n\n### Implementation\nThe implementation of the `ISecretProvider` will be done in such a way that it supports various secret store backends (e.g., HashiCorp Vault, AWS Secrets Manager). This provides flexibility and allows for backend-specific optimizations while maintaining a consistent interface.\n\n### Factory Method and Bootstrap Handler\nA factory method `CreateSecretProvider` will be implemented to facilitate the creation of `ISecretProvider` instances based on configuration settings. Additionally, a `BootstrapHandler` will be utilized during the service startup to initialize and inject the `ISecretProvider` instance.\n\n### Caching of Secrets\nTo enhance performance, a caching layer will be introduced within the `ISecretProvider` implementation. This cache will hold read secrets for a configurable duration, reducing the need for frequent calls to the backend secret store.\n\n### Insecure Secrets\nFor environments where a secure secret store is not available or required, a configuration parameter `InsecureSecrets` will be utilized to handle secrets. This configuration allows for secrets to be provided directly within the service configuration file.\n\n### Handling on-the-fly changes to `InsecureSecrets`\nThe system will be designed to accommodate runtime updates to `InsecureSecrets` without necessitating a service restart. This will involve monitoring the configuration for changes and appropriately updating the in-memory secrets.\n\n### Mocks\nMock implementations of the `ISecretProvider` interface will be provided to facilitate unit testing of services. These mocks will simulate the behavior of the secret store to ensure comprehensive test coverage without reliance on external systems.\n\n### Where will `SecretProvider` reside?\nThe `SecretProvider` will be a shared utility residing in a common library used by both Go and C-based services. This common implementation ensures consistency and reusability across different service implementations.\n\n#### Go Services\nFor Go-based services, the common library will include the necessary Go-specific bindings to seamlessly integrate with the `SecretProvider`.\n\n#### C Device Service\nFor C-based device services, the common library will expose a C-compatible API allowing them to leverage the same `SecretProvider` implementations without requiring significant modifications.\n\n### Consequences\n- Enhanced security through isolated secret stores for each service.\n- Consistent interface and interactions with secret stores across different service implementations.\n- Flexibility to support multiple backend secret stores.\n- Performance improvements via caching of secrets.\n- Ability to handle insecure secrets for non-secure environments.\n- Easier unit testing through the provision of mock secret providers.\n- Cross-language compatibility for shared secret management logic.","GenTime":"2024-07-28 22:19:14"}
{"File Name":"pharmacy-data-etl\/0004-make-etl-re-entrant.md","Context":"## Context\\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\\n\n## Decision\n","Decision":"The ETL will be re-entrant, storing state on a regular basis.\\nRestarting the ETL will pick up from the last point.\\n","tokens":47,"id":965,"Prediction":"## Decision\n\nImplement a checkpointing mechanism for the ETL process to enable resumption from the last successful checkpoint rather than restarting the entire process from the beginning. This will involve saving the state and progress of the ETL pipeline at regular intervals to ensure that in the event of an interruption, the process can restart from the last saved checkpoint, significantly reducing the total time required to complete the ETL. This approach will not only save computational resources but also improve overall system efficiency and data freshness.","GenTime":"2024-07-28 22:20:28"}
{"File Name":"operational-visualizations\/0001-about-cursors.md","Context":"## Context\\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\\nCurrent implementation of **cursor** looks like this\\n```tsx\\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\\n(row: RowCursor): ValueInRawRow;\\ncolumn: Name;\\nindex: number;\\n}\\n```\\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\\nCursor at the moment can be recieved from \"root\" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\\n**Question** raised in [one of PR](https:\/\/github.com\/contiamo\/operational-visualizations\/pull\/70\/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to \"root\" `DataFrame` along all derivative structures and \"proxy\" `getCursor` method call to it.\\n\n## Decision\n","Decision":"At the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\\n","tokens":439,"id":977,"Prediction":"## Decision\nWe will provide `getCursor` functionality in `FragmentFrame` by forwarding the call to the root `DataFrame`.\n\n## Rationale\n\n1. **Simplicity**: Adding `getCursor` directly to `FragmentFrame` will make the code simpler and more intuitive for developers. Instead of tracking multiple levels of frames, they can directly get the cursor from the frame they're working with.\n   \n2. **Maintenance**: Forwarding the call to the root `DataFrame` ensures that the core logic and caching remain centralized, avoiding redundancy and potential inconsistencies. Any changes in the cursor's retrieval mechanism would thus only need to be made in the root frame.\n\n3. **Referential Transparency**: By forwarding the requests, we retain the referential transparency of `FragmentFrame` since no additional cursor copies will be created. All derived frames will inherently use the central cache of the root `DataFrame`.\n\n4. **Flexibility**: This approach ensures that if, in the future, we decide to change the implementation details of `row` (from row-oriented storage to column-oriented or vice versa), the system's refactoring will be confined to a smaller scope. Only the core data structures will need alterations, while the derived frames will remain unaffected.\n\n## Implementation\n\n```tsx\n\/\/ FragmentFrame.ts\nimport { DataFrame } from \".\/DataFrame\";\nimport { ColumnCursor } from \".\/types\";\n\nexport class FragmentFrame {\n  private rootDataFrame: DataFrame;\n\n  constructor(rootDataFrame: DataFrame) {\n    this.rootDataFrame = rootDataFrame;\n  }\n\n  getCursor<Name extends string, ValueInRawRow = any>(column: Name): ColumnCursor<Name, ValueInRawRow> {\n    return this.rootDataFrame.getCursor(column);\n  }\n}\n\n\/\/ DataFrame.ts\nimport { ColumnCursor } from \".\/types\";\n\nexport class DataFrame {\n  private cursorCache: Map<string, ColumnCursor<any>> = new Map();\n\n  getCursor<Name extends string, ValueInRawRow = any>(column: Name): ColumnCursor<Name, ValueInRawRow> {\n    if (!this.cursorCache.has(column)) {\n      const cursor: ColumnCursor<Name, ValueInRawRow> = { \n        (row): ValueInRawRow => row, \n        column: column, \n        index: this.calculateColumnIndex(column) \n      };\n      this.cursorCache.set(column, cursor);\n    }\n    return this.cursorCache.get(column);\n  }\n\n  private calculateColumnIndex(column: string): number {\n    \/\/ Implementation details for calculating index based on column name\n  }\n}\n```\n\nBy centralizing this functionality in the `DataFrame`, we ensure a consistent, manageable, and scalable approach to accessing cursor data across all derivative frames.","GenTime":"2024-07-28 22:20:53"}
{"File Name":"xebikart-infra\/002-use-rabbitmq-with-mqtt-plugin-to-make-devices-communicate-with-each-other.md","Context":"## Context and Problem Statement\\nWe want a message broker so the devices and other applications can communicate with the backend.\\nWhat broker and protocol should we use?\\n## Decision Drivers <!-- optional -->\\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\n\n## Decision\n","Decision":"- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:\\n- It is already well-known among the team\\n- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)\\n- It seems like a good fit to iterate fast\\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\\n","tokens":84,"id":979,"Prediction":"## Decision\n\nAfter evaluating the possible options based on the provided context and decision drivers, we have selected **MQTT** as the protocol and **Eclipse Mosquitto** as the message broker for the following reasons:\n\n1. **Applicability Regarding IoT Projects**:\n   - **MQTT** is designed specifically for low-bandwidth, high-latency, or unreliable networks, making it well-suited for IoT applications. Devices with low resources will benefit from its lightweight protocol.\n   - **Eclipse Mosquitto** is a lightweight open-source message broker that implements MQTT and is well-suited for IoT devices.\n\n2. **Streaming Frames\/Images**:\n   - While MQTT is generally used for small-sized messages, it can be configured to support binary data messages suitable for transmitting frames or images from car cameras, although it is not specifically optimized for video streaming. For heavy video streaming requirements, additional consideration or a supplementary protocol may be required (e.g., HTTP\/2 or WebRTC).\n   \n3. **Ease of Deployment on Kubernetes**:\n   - **Eclipse Mosquitto** has strong community support and readily available Docker images that can be easily deployed and managed within Kubernetes clusters. This will streamline the deployment process significantly.\n   \n4. **Existing Knowledge of the Team**:\n   - Our team is already familiar with **MQTT** and **Eclipse Mosquitto**, thereby reducing the learning curve and enabling quicker implementation, maintenance, and troubleshooting.\n\nBased on these considerations, **MQTT with Eclipse Mosquitto** aligns well with our project requirements and team expertise.","GenTime":"2024-07-28 22:21:09"}
{"File Name":"dl4j-dev-tools\/0006-op_specific_enums.md","Context":"## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n\n## Decision\n","Decision":"We allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\\n`0`.\\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\\nvalues match one of the possible values (if applicable).\\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\\nthe generated enum will be derived from the name of the arg.\\n### Example\\n```kotlin\\nArg(ENUM, \"padMode\"){\\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\\ndescription = \"padding mode\"\\n}\\n```\\n","tokens":31,"id":994,"Prediction":"## Decision\nWe will introduce named enumerations (enums) for the ordinal parameters in these operations. These enums will represent the various modes the parameters can switch between, providing meaningful names for each possible value. This approach will enhance code readability, maintainability, and ease of documentation.\n\n### Details:\n1. **Define Enums**: Create enums for all ops that currently use ordinal parameters to switch modes.\n2. **Replace Ordinal Parameters**: Update the method signatures to replace ordinal parameters with the newly defined enums.\n3. **Documentation**: Update all related documentation to reflect the new usage of enums, ensuring the modes are described with their proper names.\n4. **Codebase Refactor**: Refactor existing code to utilize enums instead of raw integers, ensuring all references are updated.\n5. **Backward Compatibility**: If necessary, provide deprecation warnings for any public APIs that cannot immediately drop support for ordinal parameters, guiding users to transition to the new enums.","GenTime":"2024-07-28 22:22:04"}
{"File Name":"CCS-Architecture-Decision-Records\/0012-use-govuk-notify-for-bulk-communication-with-users.md","Context":"## Context\\nMost public-facing digital services have a need to notify its users about something at a critical business process, or just to manage their expectations to prevent incoming contact.\\nThe notifications can, for example, be:\\n* Telling a supplier they have a task to complete,\\n* Telling a supplier that a deadline is approaching,\\n* Telling a supplier that a buyer has been shortlisted for a procurement,\\n* Providing a receipt,\\n* Providing status updates,\\n* Etc...\\nGenerally, the notifications will be email, but may also need to be SMS in some cases.\\nTo reduce the need for each organisation or service team to build its own infrastructure for sending email and SMS based messages, the Government Digital Service (GDS) has built GOV.UK Notify.\\nGOV.UK Notify can send notifications either via an API call, or by uploading a CSV to the website. Sending email is free, and the service provides feedback on what messages have failed to be delivered.\\nThe service is already used in various parts of CCS, including MISO, Report MI, DigiTS etc.\\nUsing GOV.UK Notify instead of other solutions (eg using the BCC field in an email) also reduces the potential for an accidental data breach.\\n\n## Decision\n","Decision":"We will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.\\nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.\\nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal\/no training.\\nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.\\n","tokens":261,"id":996,"Prediction":"## Decision\n\nWe will integrate GOV.UK Notify for all email and SMS-based notifications within our public-facing digital services.\n\n### Rationale:\n\n1. **Standardization**: GOV.UK Notify is a proven and standardized service already in use within various parts of CCS. Integrating this service ensures consistency across different teams and services.\n  \n2. **Reliability**: The service provides assurances around message delivery, including feedback on failed messages, which helps in maintaining reliability and accountability for our notifications.\n\n3. **Security**: By leveraging GOV.UK Notify, we reduce the risk of data breaches associated with manual emailing options such as using the BCC field. The platform is designed with security in mind, which aligns with our commitment to protecting user data.\n\n4. **Cost-Effectiveness**: Sending emails via GOV.UK Notify is free, which provides a cost-effective solution for our notification needs. Even for SMS, which may incur costs, the overall value of standardization and reliability justifies its use.\n\n5. **Scalability**: The ability to send notifications via API calls or by uploading a CSV provides the flexibility needed to handle both high-volume automated messaging and manual bulk notifications as required.\n\n6. **Integration**: Since the service is already integrated with other CCS applications like MISO, Report MI, and DigiTS, the technical groundwork is familiar, reducing the learning curve and time required for implementation in new projects.\n\n### Implementation Plan:\n\n1. **API Integration**: Development teams will handle the integration of GOV.UK Notify's API within their respective services to automate email and SMS notifications.\n\n2. **Notification Templates**: Define and prepare notification templates to be used for different scenarios (e.g., task completion, approaching deadlines, status updates).\n\n3. **Testing**: Thoroughly test the integration to ensure notifications are being sent successfully and that failed message feedback is properly handled.\n\n4. **Training**: Provide necessary training for relevant team members on how to use GOV.UK Notify, particularly for scenarios involving CSV uploads.\n\n5. **Monitoring and Evaluation**: Set up monitoring to track the status and performance of notifications sent. Regularly review the feedback to identify and address any issues promptly.\n\nBy standardizing on GOV.UK Notify, we leverage an established, reliable, and secure notification infrastructure, improving our service delivery while minimizing risks and costs.","GenTime":"2024-07-28 22:22:14"}
{"File Name":"CCS-Architecture-Decision-Records\/0004-language-choices-for-new-development.md","Context":"## Context\\nWhen developing a new system, there are many possible language choices available.\\n-CCS uses a mixture of different languages for existing systems including:\\n- PHP for the Supplier Registration Service and Contracts Finder\\n- C# for MISO\\n- Java for the CCS ID and DigiTS services\\n- Python for the Digital Marketplace\\nOther services in government use a mixture of PHP, Java, Python, Ruby, Scala and GO.\\nPicking one standard language for all CCS services could restrict the pool of potential suppliers we could interact with when building new services. Conversely, allowing an unlimited selection of languages can make future management and support of services more complex and expensive.\\nAgreeing a selection of acceptable languages will allow the greatest flexibility in supplier choice, but also helps minimise future ongoing support costs.\\nAny decision we make about languages and frameworks should consider:\\n- Skills available in the current team(s) - what are the current team(s) comfortable using? What do they have experience in?\\n- Skills of a future team - is there a large enough pool of suppliers and contractors who could support and maintain the services in the future?\\n- External toolkits and modules - are there useful toolkits and modules which could reduce development effort (eg the GOV.UK Frontend Toolkit)\\n- Cost - what is the cost of developing in this way? Are there licence costs?\\n- Hosting - would picking a language restrict hosting options?\\nWe should also consider the Service Manual guide on [choosing technology](https:\/\/www.gov.uk\/service-manual\/technology\/choosing-technology-an-introduction).\\n\n## Decision\n","Decision":"New services developed by CCS should use the following languages:\\n- Java\\n- Python\\n- Ruby\\n- Javascript\\nDigital services should also use HTML5, CSS3 and JSON as required.\\nSalesforce work may continue to use the Salesforce Apex language, but where possible, we should use external web service components to reduce the volume of Apex in use.\\nNew services which do not use these languages will require an Architectural Decision to explain why.\\nAs the software development industry changes quickly, we will re-evaluate this decision in 6 months, taking account of current market trends and industry moves.\\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist. It also does not cover the development of native mobile applications.\\n","tokens":329,"id":1000,"Prediction":"## Decision\n\nTo balance flexibility in supplier choice with future management and support costs, CCS will adopt a set of approved languages for new system development. The selection will consider the criteria mentioned above, ensuring both current and future teams can effectively support and maintain the services. The approved languages are:\n\n1. **Java**: \n   - Widely used within CCS and other government services.\n   - Strong community support and a large pool of developers.\n   - Robust toolkits and frameworks such as Spring and Jakarta EE.\n   - No licensing costs beyond hosting requirements.\n   \n2. **Python**:\n   - Already in use within CCS and familiar within government tech circles.\n   - Highly versatile with a plethora of libraries and frameworks such as Django and Flask.\n   - Cost-effective and well-suited for quick development cycles.\n\n3. **C#**:\n   - Used in CCS and has good support within the Microsoft ecosystem.\n   - Strong integration with Azure services.\n   - Requires consideration of licensing costs for development tools and hosting.\n\n4. **JavaScript\/Node.js**:\n   - Essential for front-end development (both server-side and client-side).\n   - Widely used and supported with modern frameworks like React and Angular.\n   - Strong community and availability of modules like the GOV.UK Frontend Toolkit.\n\n5. **Go**:\n   - Increasing adoption in government projects and among modern system architectures.\n   - Highly performant and efficient for microservices and cloud-native applications.\n   - Strong, growing community with excellent concurrency support.\n\nThese approved languages will ensure a balance of modern capability, broad developer availability, and cost-effective hosting. They offer a range of options for different types of applications, enabling CCS to leverage the right tool for the job while maintaining a sustainable and manageable tech stack. \n\nAdditionally, it will be permissible, under special circumstances and with prior approval, to use other languages if they offer unique advantages for specific projects that align with strategic goals. Such decisions will require a thorough assessment of long-term support and cost implications.\n\nRegular reviews of the approved languages list will be conducted to ensure they remain aligned with industry trends, technological advancements, and CCS\u2019s evolving needs.","GenTime":"2024-07-28 22:22:36"}
{"File Name":"CCS-Architecture-Decision-Records\/0008-cmp-use-shared-store-for-capturing-buyer-needs.md","Context":"## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n\n## Decision\n","Decision":"Use a shared component to store user tasks in a question and answer format during buying journeys\\nThe Q&A definitions should be defined from a common repository defining the agreements\\nAgreed outcomes must be stored in the agreement records\\n","tokens":123,"id":1001,"Prediction":"## Decision\nWe will implement a centralized questioning and data storage framework that reuses common questions and patterns across different commercial agreements.\n\n1. **Question Bank**: Develop a repository for storing commonly used questions and their corresponding patterns. This repository will be accessible across all commercial agreements to ensure consistent querying and avoid redundancy.\n2. **Dynamic Questionnaires**: Implement dynamic questionnaire generation, which pulls relevant questions from the question bank based on the buyer\u2019s needs and the specific agreement\u2019s requirements.\n3. **Standardized Data Models**: Design standardized data models for buyer responses that allow for easy storage, retrieval, and comparison. These models will include metadata to track which agreement and which specific instance (e.g., call-off) the data pertains to.\n4. **Modular Storage Approach**:\n    - **Journeys**: Use temporary storage mechanisms (e.g., session databases, temporary tables) to store data collected during the buying journey. This data will be periodically synced to a more permanent storage solution where necessary.\n    - **Final Agreement Details**: Upon finalization of an agreement (e.g., call-off), data will be moved to a dedicated, immutable records storage system. This system will ensure that the finalized details are secure and auditable.\n5. **Future-Proofing for Automation**: Structure the data storage and retrieval to facilitate future enhancements, such as machine learning algorithms, to automatically match buyer needs to the supplier offerings efficiently.\n6. **APIs for Integration**: Develop APIs that allow different systems (e.g., UI front-ends, partner systems) to interact with the questioning and storage framework seamlessly.\n\nBy making these architectural decisions, the system will be more efficient in reusing information, consistent in the way it handles data, and prepared for future scalability and automation enhancements.","GenTime":"2024-07-28 22:22:41"}
{"File Name":"Data-Platform-Playbook\/010-using-pytest-for-verifying-pyspark-transformations.md","Context":"## Context\\nThe Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform.\\nExamples include:\\n- Address matching\\n- Address cleaning\\n- Repairs sheets data cleaning\\nThese jobs lack automated tests, which has meant that debugging these scripts has involved slow feedback loops, running against actual data within the platform.\\nBy introducing testing practices, frameworks and tools we hope to:\\n- Improve the speed at which PySpark scripts can be developed\\n- Provide documentation for each script with example data they expect, and what results they output\\n- Increase the proportion of defects found before they reach staging environment\\n\n## Decision\n","Decision":"We will:\\n- Use a Python testing framework, [pytest][pytest]\\n- Use the same [Docker container][awsglue-dockerhub] we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally.\\n- Integrate that framework into Apache Spark, and provide example test code\\n- Create documentation and guidance around how to productively test PySpark scripts\\n- Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging\\n[pytest]: https:\/\/docs.pytest.org\\n[awsglue-dockerhub]: https:\/\/hub.docker.com\/r\/amazon\/aws-glue-libs\\n","tokens":128,"id":1012,"Prediction":"## Decision\n\nWe will introduce automated testing practices, frameworks, and tools for the development of PySpark scripts within the Data Platform. This will include the following actions:\n\n1. **Adopt Unit Testing Frameworks:**\n   - Implement unit testing using `pytest` along with `pyspark.testing.utils` to create meaningful and comprehensive unit tests for individual transformations within the PySpark jobs.\n   - Utilize `mock` data to simulate Spark DataFrames and minimize dependencies on the actual data within the platform.\n\n2. **Integration Testing:**\n   - Set up integration tests that operate on predefined datasets to verify the end-to-end functionality of PySpark jobs.\n   - Use tools like `s3mock` or `localstack` to emulate cloud services like AWS S3, mimicking the production environment as closely as possible.\n\n3. **Data Validation and Schema Enforcement:**\n   - Add schema enforcement using Apache Spark's `StructType` and `StructField` to ensure input data quality and consistency.\n   - Implement data validation checks using tools such as `Great Expectations` to validate outputs against expected results.\n\n4. **Continuous Integration (CI):**\n   - Integrate automated testing into the CI\/CD pipeline using a CI tool like Jenkins, CircleCI, or GitHub Actions.\n   - Ensure tests are run automatically on each commit, pull request, and before deploying to staging or production environments.\n\n5. **Documentation and Examples:**\n   - Document each PySpark script with clear explanations of the input schema, expected transformations, and output schema.\n   - Provide example datasets and expected outputs within the test cases to serve as both functional documentation and as a reference for new developers.\n\n6. **Training and Best Practices:**\n   - Conduct training sessions for the Data Platform team to familiarize them with the new testing frameworks, tools, and best practices.\n   - Create internal guidelines and best practice documents for writing and maintaining reliable PySpark tests.\n\nBy implementing these testing practices and tools, we aim to streamline the development process, provide better documentation, and increase the early detection of defects, leading to more robust and reliable data transformation jobs.","GenTime":"2024-07-28 22:23:28"}
{"File Name":"Data-Platform-Playbook\/002-ingest-google-sheets-data.md","Context":"## Context\\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\\nefforts. We need to get this information pulled into the data platform for processing.\\n\n## Decision\n","Decision":"We will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\\n","tokens":41,"id":1016,"Prediction":"## Decision\n\nWe will develop an automated ETL (Extract, Transform, Load) process to consolidate the distributed datasets from the various Google Sheets Documents into our data platform. Specifically, we will use the following approach:\n\n1. **Extraction**:\n   - Use the Google Sheets API to programmatically access and read data from the different Google Sheets Documents.\n   - Set up a schedule to run this extraction process at regular intervals to ensure that any new or updated information in Google Sheets is captured in a timely manner.\n\n2. **Transformation**:\n   - Clean and normalize the data to ensure consistency across different datasets.\n   - Handle any missing or inconsistent data appropriately, applying data validation rules as needed.\n   - Transform the data into a common structure or schema that aligns with our data platform requirements.\n\n3. **Loading**:\n   - Push the cleaned and transformed data into our central data platform.\n   - Ensure the data is stored in a way that supports efficient querying and analysis.\n   - Monitor the loading process for errors and handle any issues that arise to ensure data integrity.\n\n4. **Tool Selection**:\n   - Utilize existing ETL tools or libraries that support integration with Google Sheets and our data platform.\n   - Evaluate options such as Apache Airflow, Google Cloud Dataflow, or custom Python scripts depending on the complexity and specific requirements of the data processing workflows.\n\n5. **Data Governance and Security**:\n   - Implement appropriate access controls to ensure that sensitive data is protected throughout the ETL process.\n   - Log all ETL activities for auditing and troubleshooting purposes.\n\nBy implementing this automated ETL process, we will be able to efficiently and reliably integrate data from the multiple Google Sheets Documents into our data platform, enabling better data analysis and decision-making.","GenTime":"2024-07-28 22:23:48"}
{"File Name":"lobiani\/0005-use-cqrs-architectural-style.md","Context":"## Context\\nIn non-trivial projects with complex business rules, it is often a challenge to come up with the model that scales well\\nfor reads and writes at the same time in regard to performance and maintainability.\\n\n## Decision\n","Decision":"We will adopt Command Query Responsibility Segregation architectural style where there may exist 2 models for same\\ndomain entities each respectively on the Command (write) and Query (read) sides\\n","tokens":45,"id":1031,"Prediction":"## Decision\nAfter considering the need for scalability in both read and write operations and maintaining a balance between performance and maintainability, we have decided to implement the following architecture decision:\n\n**Adopt a CQRS (Command Query Responsibility Segregation) pattern combined with event sourcing.**\n\n### Justification:\n1. **Scalability**: \n   - **CQRS Pattern**: By separating the read and write models, we can optimize each part for its specific workload. This separation allows for independent scaling of read and write operations, thus enhancing performance under high load conditions.\n   - **Event Sourcing**: Storing all changes as a series of events allows for more flexible query capabilities and easier reconstruction of various states and projections, thereby accommodating complex business rules.\n\n2. **Performance**:\n   - **Write Model**: The write model (command side) will focus solely on handling transactional changes, ensuring that complex business logic for modifying data is handled efficiently.\n   - **Read Model**: The read model can be denormalized to serve queries efficiently, optimized for read performance.\n\n3. **Maintainability**:\n   - **Separation of Concerns**: By segregating commands (writes) and queries (reads), we can ensure that changes in one area (e.g., logic for writing data) have minimal impact on the other (e.g., logic for reading data), making the codebase easier to maintain.\n   - **Event Sourcing**: A log of all events allows new projections and views to be created without changing the original data write model. This separation aids in maintaining and evolving the system without major overhauls.\n\n4. **Flexibility**:\n   - Ability to adjust read and write models separately as business rules evolve.\n   - Facilitates implementing additional business processes that depend on the history of changes.\n\n### Implementation Approach:\n1. **Write Model**: Use an aggregate-based approach to enforce business rules centrally and ensure transactional integrity.\n2. **Event Store**: Implement an event store to record all state changes (events).\n3. **Read Model**: Implement read-side projections that are optimized for specific query requirements, possibly using separate databases to cater to read-heavy traffic.\n4. **Integration**: Utilize message brokers or event streaming platforms to handle communication between the event store and readers, ensuring eventual consistency.\n\nThis decision will allow us to handle complex business logic, scale our system effectively, and maintain a clean separation of concerns to streamline future development and maintenance efforts.","GenTime":"2024-07-28 22:25:01"}
{"File Name":"lobiani\/0004-accept-coupling-with-spring-boot-in-backend.md","Context":"## Context\\nAlmost every software project needs to interact with the 3rd party libraries and frameworks to some extent.\\nGenerally, the less coupled the code is with the 3rd party libraries and frameworks (usually thanks to additional abstraction\\nlayers on top of them), more flexible the software becomes in regard to technology replacements in the future.\\nHowever, sometimes it's quite viable to compromise this flexibility in favor of practical benefits that particular\\ntechnology brings, in this case Spring Boot\/Framework. Especially that it is designed with the abstraction and\\nextensibility in mind, so introducing another layer of abstraction is senseless.\\n\n## Decision\n","Decision":"We accept direct coupling with Spring Boot\/Framework code without any abstraction layers on top of it.\\n","tokens":127,"id":1035,"Prediction":"## Decision\n\nWe will integrate our software project directly with Spring Boot\/Framework without adding additional abstraction layers on top of it.\n\n### Rationale:\n\n1. **Design Philosophy**: Spring Boot\/Framework is inherently designed with abstraction and extensibility in mind, reducing the need for additional layers of abstraction.\n\n2. **Practical Benefits**: Direct integration with Spring Boot\/Framework allows us to leverage its full set of features, streamlined configuration, and comprehensive ecosystem, facilitating faster development and easier maintenance.\n\n3. **Resource Efficiency**: Avoiding unnecessary abstractions simplifies the codebase, leading to improved performance and reduced complexity, which can save development time and resources.\n\n4. **Maintainability**: Spring Boot\/Framework's widespread adoption and robust documentation ensure that our team can efficiently maintain and extend the software, and easily onboard new developers.\n\n5. **Community and Support**: The strong community support for Spring Boot\/Framework means that any issues we encounter are likely to have existing solutions or workarounds, further accelerating our development process.\n\n### Trade-offs:\n\n1. **Technology Lock-in**: Direct integration may lead to tighter coupling with Spring Boot\/Framework, making future technology replacements more challenging.\n\n2. **Flexibility**: While we gain practical benefits, we might sacrifice some flexibility in responding to future changes in technology or project requirements.\n\n### Mitigation:\n\n- We will carefully document our integration points with Spring Boot\/Framework.\n- We will adopt best practices for modular design within the constraints of Spring Boot\/Framework to maintain as much flexibility as possible.\n- We will review our decision periodically to ensure it continues to align with project needs and technological advancements.","GenTime":"2024-07-28 22:25:16"}
{"File Name":"testy\/0007-support-node-versions-with-at-least-security-updates.md","Context":"## Context\\nMaking clear which versions are supported and how this will be updated as time passes.\\n\n## Decision\n","Decision":"Only support Node versions with active and security support. Do not support newer, unstable versions.\\nWe can use [endoflife.date](https:\/\/endoflife.date\/nodejs) as a reference. Example: at the moment\\nof this decision, only Node 12, 14 and 16 should be supported.\\n","tokens":20,"id":1048,"Prediction":"## Decision\nWe will implement a clear versioning strategy to indicate supported versions and future updates. This will involve adopting Semantic Versioning (SemVer) to clearly define version changes (major, minor, patch). Supported versions will be documented in the project\u2019s README and official documentation.\n\nWe will also establish an End of Life (EOL) policy for each version, specifying the duration for which each version will receive support (e.g., security patches, bug fixes). A timeline will be created to show the lifecycle of each version.\n\nTo ensure transparency and ease of understanding, we will introduce the following practices:\n1. **Version Numbering**: Adopting Semantic Versioning (e.g., 1.0.0).\n   - **Major** version changes (1.x.x) indicate backward-incompatible changes.\n   - **Minor** version changes (x.1.x) indicate backward-compatible functionality.\n   - **Patch** version changes (x.x.1) indicate backward-compatible bug fixes.\n2. **Documentation**: Including a detailed VERSION_HISTORY.md file in the repository that lists all versions and their respective changes.\n3. **Deprecation Notices**: Announcing deprecations and EOL dates in advance, ideally providing at least 6 months\u2019 notice before a version is no longer supported.\n4. **Automated Updates**: Integrating automated notifications to alert users about new versions and upcoming EOL deadlines.\n\nBy adopting these practices, we aim to provide clear and consistent communication to our users about version support and update timelines.","GenTime":"2024-07-28 22:26:02"}
{"File Name":"testy\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1049,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) process for documenting architectural decisions. The ADRs will be managed using a version-controlled repository (e.g., Git) to ensure traceability and collaboration among team members. Each ADR will follow a standardized template capturing: \n\n1. The title of the decision.\n2. The context and problem statement prompting the decision.\n3. The decision itself.\n4. The consequences, both positive and negative, of the decision.\n5. The alternatives considered and their pros and cons.\n6. The rationale behind the choice.\n\nThis process will help maintain clarity, facilitate knowledge sharing, and support ongoing and future development by providing a structured method for understanding the rationale behind key architectural decisions.","GenTime":"2024-07-28 22:26:04"}
{"File Name":"aws-sync-routes\/0005-uri.md","Context":"## Context\\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\\nThere isn't an obvious fit for this in the official REST API URI specifications.\\n\n## Decision\n","Decision":"The `\/vpcs\/{vpcId}\/route-tables\/{routeTableId}` URI will be used.\\n* `{vpcId}` is the VPC ID.\\n* `{routeTableId}` is the main route table ID\\n","tokens":64,"id":1050,"Prediction":"## Decision\n\nWe have decided to implement a custom API endpoint to handle the synchronization of specified routes from the main route table to custom route tables. This decision addresses the need for functionality that is not directly supported by the official REST API URI specifications.\n\n### Rationale\n\n1. **Functionality Requirement**: Synchronizing routes between the main and custom route tables is a specific need that is not inherently supported by existing REST API specifications.\n\n2. **Limited Information**: The trigger for this synchronization will be based on a log event, which means the information available at the time of the request will be limited. A custom endpoint allows us to design the input parameters and request handling in a way that suits this constraint.\n\n3. **Flexibility**: Creating a custom endpoint gives us the flexibility to define the logic for adding or replacing routes based on the available data and any future requirements that may arise.\n\n4. **Clarity**: By clearly defining a new API endpoint specifically for this purpose, we make the functionality unambiguous for developers and maintainable for the team. Users of the API will know exactly how to synchronize routes by interacting with this endpoint.\n\n### Custom Endpoint Specification\n\n- **Endpoint URL**: `\/api\/sync-route`\n- **HTTP Method**: POST\n- **Request Body**: \n  ```json\n  {\n    \"mainRouteTableId\": \"string\",\n    \"customRouteTableIds\": [\"string\"],\n    \"route\": {\n      \"destinationCidrBlock\": \"string\",\n      \"target\": \"string\"\n    },\n    \"action\": \"add\" | \"replace\"\n  }\n  ```\n- **Response**: \n  ```json\n  {\n    \"status\": \"success\",\n    \"message\": \"Routes synchronized successfully.\",\n    \"synchronizedRoutes\": [\"string\"]\n  }\n  ```\n- **Error Handling**: The API will return appropriate HTTP status codes and error messages in case of failure.\n\n### Implementation Considerations\n\n- **Security**: Ensure that the API has proper authentication and authorization mechanisms to prevent unauthorized access to route tables.\n- **Logging and Monitoring**: Implement detailed logging and monitoring to track the synchronization process and quickly identify any issues.\n- **Validation**: Strict input validation must be performed to ensure that the main route table ID, custom route table IDs, and route details are correct and exist in the system.\n\nThis decision is aimed at providing a clear, maintainable, and functional solution to the problem of route synchronization given the constraints and requirements.","GenTime":"2024-07-28 22:26:12"}
{"File Name":"log4brains\/20210113-distribute-log4brains-as-a-global-npm-package.md","Context":"## Context and Problem Statement\\nLog4brains (`v1.0.0-beta.4`) installation procedure is currently optimized for JS projects and looks like this:\\n- Run `npx init-log4brains`\\n- Which installs locally `@log4brains\/cli` and `@log4brains\/web`\\n- And creates custom entries in `package.json`'s `scripts` section:\\n- `\"log4brains-preview\": \"log4brains-web preview\"`\\n- `\"log4brains-build\": \"log4brains-web build\"`\\n- `\"adr\": \"log4brains adr\"`\\nFor non-JS projects, you have to install manually the packages and the `npx init-log4brains` script does not work.\\nSince Log4brains is intended for all projects, not especially JS ones, we have to make the installation procedure simpler and language-agnostic.\\n## Decision Drivers <!-- optional -->\\n- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\n\n## Decision\n","Decision":"- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\nThe new installation procedure is now language agnostic and will be the following:\\n```bash\\nnpm install -g log4brains\\nlog4brains init\\n```\\nLog4brains will be distributed as a global NPM package named `log4brains`, which provides a global `log4brains` command.\\n- This global package will require the existing `@log4brains\/cli` and `@log4brains\/web` packages\\n- `init-log4brains` will be renamed to `@log4brains\/init` and required as a dependency\\n### Consequences\\nFor a JS project, it is now impossible to pin Log4brains to a specific version.\\nWe may implement a [xojs\/xo](https:\/\/github.com\/xojs\/xo)-like behavior later: \"the CLI will use your local install of XO when available, even when run globally.\"\\n","tokens":224,"id":1065,"Prediction":"## Decision\n\nWe will modify the installation procedure of Log4brains to be language-agnostic and simple for any project type by creating a standalone installation script, `log4brains-init`. Instead of relying on `npx`, which is tied to Node.js and JavaScript projects, `log4brains-init` will be a cross-platform executable script distributed via a package manager that supports various languages (like Homebrew for macOS, Chocolatey for Windows, and apt-get for Linux).\n\n### Key Changes:\n1. **Standalone Installation Script**: Create a standalone executable script called `log4brains-init`, which can be installed via popular package managers or downloaded directly from our website.\n2. **Unified Command**: After installing `log4brains-init`, users can simply run `log4brains-init` in their project directory to initialize Log4brains.\n3. **Language-Agnostic Approach**: The script will detect the type of project (if applicable) and configure Log4brains accordingly, without assuming a JavaScript environment.\n4. **Updating Documentation**: Update the official documentation to guide users through the new installation and initialization processes.\n\n### Benefits:\n- **Simplicity**: One straightforward script to initialize Log4brains, irrespective of the project's language.\n- **Faster Onboarding**: Quicker and easier for users to get started with Log4brains.\n- **Broadened Accessibility**: Makes Log4brains accessible to a wider range of projects, not limiting to just JavaScript-based ones.\n\n### Implementation Steps:\n1. Develop the `log4brains-init` executable script.\n2. Set up distribution channels through package managers.\n3. Update the documentation, tutorials, and references to reflect the new installation procedure.\n4. Ensure rigorous testing across various environments and project types to validate the new process.\n\nThis decision helps to streamline the use of Log4brains in diverse development environments, making it more universal and user-friendly.","GenTime":"2024-07-28 22:27:17"}
{"File Name":"log4brains\/20200925-multi-packages-architecture-in-a-monorepo-with-yarn-and-lerna.md","Context":"## Context and Problem Statement\\nWe have to define the initial overall architecture of the project.\\nFor now, we are sure that we want to provide these features:\\n- Local preview web UI\\n- Static Site Generation from the CI\/CD\\n- CLI to create a new ADR quickly\\nIn the future, we might want to provide these features:\\n- Create\/edit ADRs from the local web UI\\n- VSCode extension to create and maybe edit an ADR from the IDE\\n- Support ADR aggregation from multiple repositories\\n\n## Decision\n","Decision":"Chosen option: \"Multi-packages, monorepo, with Yarn and Lerna\", because\\n- We don't want a monolith because we want the core library\/API to be very well tested and probably developed with DDD and hexagonal architecture. The other packages will just call this core API, they will contain fewer business rules as possible. As we are not so sure about the features we will provide in the future, this is good for extensibility.\\n- Yarn + Lerna seems to be a very good practice used by a lot of other open-source projects to publish npm packages.\\n","tokens":110,"id":1067,"Prediction":"## Decision\n\nBased on the current requirements and considering potential future expansions, we will adopt a modular, extensible, and loosely-coupled architecture. The following decisions have been made:\n\n1. **Microservices Architecture for the Web UI and CLI**:\n   - **Reasoning**: This allows each component (Web UI, CLI, Static Site Generation) to be developed, tested, and deployed independently. It also allows for scalability and better fault isolation. In the future, additional features can be integrated more seamlessly without affecting the core functionalities.\n   - **Implementation**:\n     - **Local Preview Web UI**: Developed as a standalone microservice, fully decoupled from CI\/CD and CLI functionalities.\n     - **CLI Tool**: Developed as a separate microservice to quickly create a new ADR, which can interface with the Web UI and be independent of the preview functionalities.\n     - **Static Site Generation**: Implemented as a microservice integrating with CI\/CD pipelines, triggering site generation upon changes or new ADR additions.\n\n2. **Event-Driven Architecture** for Communication:\n   - **Reasoning**: Event-driven architecture allows seamless integration and communication between microservices without tight coupling. This will enable the CLI, Web UI, and Static Site Generation services to communicate on a need basis, triggering processes such as ADR creation and site updates.\n   - **Implementation**:\n     - Use of an event broker (e.g., RabbitMQ, Kafka) for passing messages and handling events asynchronously.\n\n3. **RESTful APIs** for Future Enhancements:\n   - **Reasoning**: Having RESTful APIs will ensure that future features like creating\/editing ADRs from the web UI and supporting integration with VSCode or other tools can be achieved without significant refactoring.\n   - **Implementation**:\n     - Each microservice will expose a set of RESTful APIs for core functionalities.\n     - Future extensions, such as editing from Web UI or IDE plugins, can interact with these APIs.\n\n4. **Common Data Store for ADRs**:\n   - **Reasoning**: Ensures consistency and a single source of truth for ADRs, which will facilitate easier management, search, and retrieval across different interfaces and services.\n   - **Implementation**:\n     - Utilize a NoSQL or document-based database (e.g., MongoDB) for storing ADRs, scalable and suitable for future needs like aggregation from multiple repositories.\n\n5. **Extensible Frontend Framework** (for Local Web UI):\n   - **Reasoning**: Use a modern, extensible framework (e.g., React or Vue.js) that supports future expansion like creating\/editing ADRs from the UI without significant overhaul.\n   - **Implementation**:\n     - Implement the local web UI using the chosen frontend framework.\n     - Structure the frontend to accommodate future feature additions with minimal disruption.\n\nBy adopting this architectural approach, we ensure that the core functionalities are robust and that future expansions can be seamlessly integrated.","GenTime":"2024-07-28 22:27:29"}
{"File Name":"cnp-design-documentation\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1077,"Prediction":"## Decision\nWe will adopt the practice of creating and maintaining Architectural Decision Records (ADRs) for documenting all significant architectural decisions on this project. This will ensure a standardized and structured format for capturing the context, rationale, and implications of decisions made.\n\n### Rationale\n1. **Traceability**: ADRs will provide a historical record of the architectural decisions, which will help in understanding the evolution of the system and the reasons behind certain choices.\n2. **Consistency**: Using a standardized ADR template ensures that all decisions are documented in a consistent manner, making it easier to review and reference.\n3. **Knowledge Sharing**: ADRs serve as a valuable knowledge repository for new and existing team members, facilitating better onboarding and continuous learning.\n4. **Improved Communication**: Having documented decisions can improve team communication and alignment, especially when changes or challenges arise.\n\n### Implementation Strategy\n1. **Template Selection**: We will use a well-defined ADR template that includes sections such as Context, Decision, Status, Consequences, and Alternatives Considered.\n2. **Tool Integration**: ADRs will be stored and managed in a version-controlled repository, such as a dedicated directory in our project's Git repository, to ensure they are easily accessible and maintainable.\n3. **Decision-Making Process**: During architecture-related discussions and meetings, the responsible team members will document decisions using the ADR template. \n4. **Regular Reviews**: Periodic reviews will be conducted to ensure ADRs remain up-to-date and reflect the current state of the architecture.\n\n### Expected Outcomes\n- A clear and comprehensive record of architectural decisions.\n- Enhanced team understanding and consensus on architectural choices.\n- Simplified training and onboarding process for new team members.\n- Better preparedness for future architectural reviews and audits.","GenTime":"2024-07-28 22:28:15"}
{"File Name":"FindMeFoodTrucks\/Hosting Model.md","Context":"## :dart: Context\\nThe application can be hosted on a private datacenter or on a managed cloud datacenter.\\nChoosing the right hosting model will impact the scalability cost and technology choices for the application.\\n\n## Decision\n","Decision":"The application will be hosted on Microsoft Azure Cloud datacenter.\\n","tokens":43,"id":1089,"Prediction":"## Decision\n\nThe application will be hosted on a managed cloud datacenter.\n\n### Rationale:\n\n1. **Scalability:** A managed cloud datacenter provides on-demand scalability, which is crucial for handling varying loads and future growth without significant changes to the infrastructure.\n\n2. **Cost-Effectiveness:** While there may be a perception that private datacenters are cheaper for large, consistent loads, managed cloud services offer a pay-as-you-go model that can be more economical, especially for startups or scaling businesses. Managed cloud services also reduce CapEx expenditure in favor of OpEx, allowing for more predictable budgeting.\n\n3. **Technology Options:** Managed cloud services offer a plethora of advanced technologies and services (e.g., machine learning, big data analytics, serverless computing) that would otherwise require significant effort and expertise to implement in a private datacenter.\n\n4. **Maintenance and Management:** Managed cloud providers handle hardware maintenance, updates, and security measures, reducing the overhead and complexity for the internal IT team. This allows the team to focus on higher-value tasks such as development and innovation.\n\n5. **Redundancy and Disaster Recovery:** Managed cloud datacenter providers generally offer built-in redundancy and disaster recovery options, safeguarding against data loss and downtime.\n\n6. **Global Reach:** Managed cloud solutions often provide a global infrastructure which can be beneficial for applications targeting international users by reducing latency through geographically dispersed data centers.\n\n### Action Items:\n\n1. **Vendor Selection:** Evaluate and select a managed cloud service provider (e.g., AWS, Google Cloud, Microsoft Azure) based on specific needs, such as compliance requirements, service portfolios, and pricing models.\n   \n2. **Migration Plan:** Develop a detailed plan for migrating existing infrastructure and applications to the chosen cloud platform, including timelines, roles, and responsibilities.\n\n3. **Training:** Ensure that the development and operations teams are adequately trained in the cloud platform's tools and services to maximize the benefits of the cloud environment.\n\n4. **Cost Monitoring:** Implement cost monitoring and management tools to keep track of resource usage and optimize spending on the managed cloud platform.\n\n5. **Security Assessment:** Conduct a comprehensive security assessment to ensure that the chosen cloud platform meets all necessary security requirements and best practices.","GenTime":"2024-07-28 22:29:04"}
{"File Name":"FindMeFoodTrucks\/Messaging choice.md","Context":"## :dart: Context\\nThe Web APIs can be implemented as Synchronous or Asynchronous.\\n\n## Decision\n","Decision":"Considering the fact that the APIs does a datastore look up which can take some time, making the services asynchronous is the recommendation.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/azure.microsoft.com\/mediahandler\/files\/resourcefiles\/api-design\/Azure_API-Design_Guide_eBook.pdf\\n","tokens":20,"id":1091,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to implement the Web APIs using a **Synchronous** approach. The primary reasons for this decision are:\n\n1. **Simplicity**: Synchronous APIs are simpler to design, implement, and debug. They are easier to understand for new developers joining the team.\n   \n2. **Consistency**: For workflows where each step depends on the preceding step, synchronous APIs ensure predictable and consistent execution flow.\n\n3. **Resource Management**: Synchronous operations are easier to manage in terms of resource utilization, including database connections and memory usage, reducing the complexity around concurrency control.\n\n4. **Use Cases Alignment**: The primary use cases for these Web APIs involve real-time client interactions where immediate responses are expected. Synchronous APIs provide an immediate response, fitting these needs well.\n\n5. **Error Handling**: Error handling is more straightforward in synchronous APIs, as errors can be returned immediately in the response, allowing clients to handle exceptions more effectively.\n\nWhile an Asynchronous approach offers benefits in terms of performance and scalability, particularly under high load, the simplicity and clarity of Synchronous APIs align better with our current project requirements and team capabilities. Future enhancements may revisit this decision if we encounter limitations that necessitate the advantages of Asynchronous APIs.\n\nThis decision is aimed at ensuring a balance between efficiency and manageability while meeting our immediate project goals.","GenTime":"2024-07-28 22:29:11"}
{"File Name":"FindMeFoodTrucks\/Architecture Style.md","Context":"## :dart: Context\\nFollowing Architecture styles were considered for the application\\n1. n-tier\\n1. Web-Queue-Worker\\n1. Microservices\\n1. Big data\/compute\\nChoosing the right Architecture style will impact the functional and non-functional efficiencies of the project.\\n\n## Decision\n","Decision":"A simple Microservices based architecture style was implementing CQS pattern will be used for this application. More about CQS pattern can be found here :https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/architecture-styles\/\\n","tokens":58,"id":1094,"Prediction":"## Decision\n\nAfter a thorough evaluation of the various architectural styles, the following decision has been made with respect to the requirements of the project:\n\n**Chosen Architecture Style: Microservices**\n\n### Justification\n\n1. **Scalability:**\n   - Microservices architecture allows for independent scaling of different components depending on their specific load requirements. This ensures better resource utilization and enhanced performance under varying loads.\n\n2. **Flexibility:**\n   - The ability to update, deploy, and scale individual services independently allows for rapid deployment cycles. This leads to faster feature rollouts and continuous improvement without impacting the entire system.\n\n3. **Technology Agnosticism:**\n   - Microservices enable the use of different technologies, languages, and frameworks for different services based on the best fit for each case. This ensures that each service can leverage the optimal technology solution, enhancing overall efficiency.\n\n4. **Resilience:**\n   - The microservices architecture increases the fault tolerance of the system as a whole. If one service fails, the others continue to function, reducing the risk of total system failure.\n\n5. **Team Autonomy:**\n   - This architecture style supports autonomous teams that can develop, test, and deploy their services independently. This enhances productivity and encourages innovation within teams.\n\n### Implementation Considerations\n\n1. **Service Discovery and Load Balancing:**\n   - Implement a robust service discovery mechanism (e.g., Eureka, Consul) and load balancing strategies to handle service communication effectively.\n\n2. **Inter-Service Communication:**\n   - Choose appropriate communication protocols for inter-service communication (e.g., REST, gRPC) based on the use case and performance requirements.\n\n3. **Data Management:**\n   - Decentralize the data management mechanism where each microservice handles its database. However, ensure consistent data management policies across services.\n\n4. **Monitoring and Logging:**\n   - Implement comprehensive monitoring and logging frameworks (e.g., ELK Stack, Prometheus with Grafana) to track the health and performance of each microservice.\n\n5. **Security:**\n   - Ensure secure communication between services by employing standards such as OAuth and mutual TLS. Implement centralized security best practices across the microservices.\n\n6. **Continuous Integration\/Continuous Deployment (CI\/CD):**\n   - Set up a robust CI\/CD pipeline to automate the deployment process, ensuring that each microservice can be tested and deployed independently.\n\nThis microservices-based architecture will ensure that our application is scalable, resilient, and able to meet both current and future demands efficiently.","GenTime":"2024-07-28 22:29:27"}
{"File Name":"FindMeFoodTrucks\/Choice of Compute for Ingestion.md","Context":"## :dart: Context\\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the ingestion sub-system:\\n* Should support scheduling\\n* Should support running in background\\n* Should be able to connect to backend Cosmos DB\\n* Should support .net core framework\\n* Service will only run once a day\\n* Would prefer a managed service\\n* Does not have portability requirements\\nThe following options for compute are considered for this service:\\n* App Services\\n* Functions\\n* Container Instances\\n* Service Fabric\\n* AKS\\nChoosing the right compute model will help optimize the development experience and operations\\n\n## Decision\n","Decision":"The recommended approach is to use Azure Functions considering the following points:\\n* Supports consumption plan (Pay-per-use) which is ideal for sparse usage\\n* Supports .net core framework and is well integrated with Visual Studio development experience\\n* Can leverage cosmos SDK (nuget) to connect back to the data store\\n* Has built-in scheduling capabilities\\n* Is a fully managed PaaS service\\n* No overheads related to cluster management or infrastructure provisioning.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-tree\\n","tokens":135,"id":1095,"Prediction":"## Decision\n\nAfter evaluating the context and requirements for the ingestion sub-system, Azure Functions is chosen as the preferred compute option. Below are the key reasons for this decision:\n\n1. **Scheduling Support**:\n   - Azure Functions has built-in support for scheduling via Time Trigger, making it suitable for tasks that need to run once a day.\n\n2. **Running in Background**:\n   - Functions can easily run as background processes and handle long-running tasks.\n\n3. **Integration with Cosmos DB**:\n   - Azure Functions natively supports binding with Cosmos DB, facilitating seamless connectivity to the backend database.\n\n4. **Support for .NET Core**:\n   - Azure Functions supports .NET Core, aligning with the technology stack requirement.\n\n5. **Managed Service**:\n   - Azure Functions is a fully managed service, ensuring reduced operational overhead for deployment, scaling, and maintenance.\n\n6. **Cost-Efficiency**:\n   - Given that the service will only run once a day, Azure Functions' consumption plan allows for cost-efficient usage, as you only pay for the execution time.\n\n7. **No Portability Requirements**:\n   - Since there is no requirement for portability, using Azure's serverless offering does not introduce any constraints.\n\nIn light of these considerations, Azure Functions stand out as the most appropriate and efficient choice for the compute needs of the ingestion sub-system. This decision will optimize both the development experience and operational workload.","GenTime":"2024-07-28 22:29:31"}
{"File Name":"frontend\/001-spa.md","Context":"## Context\\nManaging playing audio between page loads is impossible. This means that either, we use a pop-up player that plays audio in another window or we build a Single Page Application that doesn't perform page loads.\\nBuilding a single page application will add complexity and other issues to work around like SEO. Pop-up players are clunky, feel dis-jointed from the rest of the site and can be annoying to use in mobile browsers.\\n\n## Decision\n","Decision":"Build a SPA.\\n","tokens":89,"id":1096,"Prediction":"## Decision\n\nWe will implement a Single Page Application (SPA) to manage the playing of audio between navigations within the application. This decision is made after weighing the pros and cons of using a pop-up player versus building an SPA. \n\nDespite the added complexity and potential SEO challenges, an SPA offers a more seamless and cohesive user experience. By avoiding page reloads, the SPA ensures continuous audio playback, which is fundamental for user engagement and experience on our platform. \n\nTo address SEO issues, we will implement server-side rendering (SSR) and use solutions like prerendering and dynamic rendering to ensure our SPA remains SEO-friendly. This strategy will allow us to have the enhanced user experience of an SPA while still maintaining search engine discoverability.","GenTime":"2024-07-28 22:29:33"}
{"File Name":"ea-talk\/0008-define-appropriate-schema-types.md","Context":"## Context\\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how oracle specifically views schemas, but the conversation *may* be relavant to other databases). This conversation has been mostly around how do do our database development, and how do we provide appropriate access to the required data.\\nThrougout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\\nHere are the notes from the original discussion:\\n\n## Decision\n","Decision":"We will use the following definitions for the different types of schemas in our databases:\\n### System Schemas\\nThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.\\n### DBA User Schemas\\nDBA User Schemas will exist for the DBAs to perform necessary functions in our databases.\\nThese schemas will have the highest level of access in our systems, and thus need to be the most careful about credentials and access.\\nDBA User Schemas should follow this naming convention:\\n```\\n{}_DBA\\n```\\nWhere {} is some useful identifier (i.e. `EXAMPLE_DBA`).\\nUsually, we have only one of these per database, `EXAMPLE_DBA`. If more are necessary, they should follow this convention.\\n### Application User Schemas\\nApplication user schemas will exist for each application that needs to access data in our databases.\\nApplication users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nApplication users should not have any object creation permissions (i.e. they should not be able to perform DDL operations).\\nApplication User Schemas should follow this naming convention:\\n```\\n{}_APP\\n```\\nWhere {} is the application name (i.e. `CALENDAR_APP`).\\n### Individual User Schemas\\nIndividual user schemas represent unique individual people that need access to database systems.\\nThese types of schemas are primarly used by developers and people in the organization that perform data analytics functions.\\nIndividual users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nIndividual users should have object creation permissions only for their own user schema.\\nIndividual User Schemas should be named the same as the user's email address. We prefer this to the AD account credentials because it follows a human-readable format, and better allows us to understand who the user is.\\n### Domain Schemas\\nDomain schemas are where the actual objects in the database exist (i.e. tables, views, functions, packages).\\nDomain schemas should have object creation permissions only for their own schema.\\nWhen domain schemas are created, the appropriate roles should also be created to be able to control access to the objects.\\nNote that these might exist in multiple database instances, e.g. custorder might have extract logic in POSP and tables in DSSP.\\nDomain Schemas should be named logically so their purpose can at least be guessed at.\\n### Replicated Schemas\\nReplicated Schmeas are similar to Domain schemas, except they will *only* include data whose system of record is elsewhere. They should not include any objects that aren't represented in the \"parent\" schema.\\nLike Domain schemas, the appropriate roles should also be created to be able to control access to the objects.\\nReplicated Schemas should follow this naming convention:\\n```\\n{}_REPL\\n```\\nWhere {} is the application name (i.e. `CALENDAR_REPL`).\\n","tokens":117,"id":1106,"Prediction":"## Decision\n\n### Adopt Defined Schema Types and Usage Policies for Database Development\n\n**Decision Statement:**\nWe have decided to adopt and implement a standard set of schema types and accompanying usage policies within our database development practices. These schema types and policies will be consistently applied across all projects to ensure clear structure, appropriate data access, and ease of maintenance.\n\n### Schema Types Definitions:\n1. **Development Schema:**\n   - Purpose: Used for development and testing by developers.\n   - Access: Full read\/write access by developers.\n   - Lifetime: Temporary, resets\/discards changes made during the development cycle.\n\n2. **Integration Schema:**\n   - Purpose: Used for integrating different components and ensuring interoperability between modules.\n   - Access: Controlled read\/write access by development leads and integration testers.\n   - Lifetime: Persistent for the duration of the project life-cycle or specific integration phases.\n\n3. **Staging Schema:**\n   - Purpose: Mirror of the production schema for final pre-production testing.\n   - Access: Read-only for the majority, with controlled write access for authorized testing personnel.\n   - Lifetime: Persistent but periodically refreshed from the production schema.\n\n4. **Production Schema:**\n   - Purpose: Used for live operational data.\n   - Access: Strictly controlled, only read\/write access is permissible with appropriate authorization.\n   - Lifetime: Indefinite, governed by organizational data retention policies.\n\n### Usage Policies:\n- **Role-Based Access Control (RBAC):**\n  - Implement RBAC to ensure that access to each schema type is granted based on roles and responsibilities.\n  - Regular audits of access permissions to ensure compliance and security.\n  \n- **Data Masking:**\n  - Apply data masking techniques in non-production schemas to safeguard sensitive information.\n  \n- **Change Management:**\n  - All schema changes should follow a defined change management process including documentation, peer review, and testing before production deployment.\n\n- **Version Control:**\n  - Database schema versions must be managed using a version control system to track changes and facilitate rollbacks if necessary.\n  \n- **Automated Deployment:**\n  - Incorporate automated tools for schema deployment and migration processes to ensure consistency and reduce human error.\n\n### Implementation Plan:\n- **Training & Documentation:**\n  - Conduct training sessions for all stakeholders on the new schema types and usage policies.\n  - Provide comprehensive documentation and guidelines.\n\n- **Monitoring & Reporting:**\n  - Implement tools to monitor schema usage, access patterns, and to generate periodic compliance reports.\n\n- **Review Cycle:**\n  - Regularly review and update schema definitions and policies based on feedback and technological advancements.\n\nBy standardizing our approach to database development using these schema types and policies, we aim to improve data governance, enhance security, and streamline development processes across our organization.","GenTime":"2024-07-28 22:30:01"}
{"File Name":"island.is\/0001-use-nx.md","Context":"## Context and Problem Statement\\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\\n## Decision Drivers\\n- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\n\n## Decision\n","Decision":"- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\nChosen option: \"Nx\", because:\\n- It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n- It's relatively easy to learn with focused documentation.\\n- It has schematics to generate apps, libraries and components that includes all of our tools.\\n- It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n","tokens":87,"id":1117,"Prediction":"## Decision\n\nAfter evaluating several monorepo tools considering our decision drivers, we have decided to adopt Nx as our monorepo management tool. \n\n### Justification:\n1. **Low Complexity and Overhead in Development:**\n   - Nx provides a gentle learning curve with built-in defaults but remains configurable.\n   - It integrates seamlessly with popular technologies like Angular, React, and Node.js, which minimizes the setup required for our projects.\n\n2. **Fit for Our Stack:**\n   - Nx is designed to work well with modern frontend and backend frameworks in our technology stack including Angular, React, Next.js, NestJS, and Express.\n   - It supports TypeScript extensively out of the box, aligning with the languages predominantly used in our projects.\n\n3. **Optimize CI\/CD with Dependency Graphs and\/or Caching:**\n   - Nx\u2019s built-in dependency graph visualizes dependencies between projects, aiding project structure understanding and maintenance.\n   - It supports advanced build caching and distributed task execution, which significantly speeds up CI\/CD processes by avoiding unnecessary rebuilds.\n\n4. **Flexible:**\n   - Nx's extensibility allows us to build custom plugins or use community plugins to tailor it specifically to our needs.\n   - It has strong support for both single and multiple library repositories, providing flexibility as our projects and teams grow.\n\nThis decision aligns with the primary goals of maintaining low complexity while scaling development across multiple projects and teams efficiently. The capabilities of Nx to optimize our CI\/CD processes and manage dependencies effectively were critical factors in this choice.","GenTime":"2024-07-28 22:31:01"}
{"File Name":"island.is\/0012-chart-library.md","Context":"## Context and Problem Statement\\nMultiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\\n## Decision Drivers\\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\n\n## Decision\n","Decision":"- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\nChosen option: \"Recharts\", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\\n### Positive Consequences\\n- We can start implementing charts and graphs as needed in island.is\\n### Negative Consequences\\n- It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.\\n","tokens":79,"id":1119,"Prediction":"## Decision\nThe team has decided to adopt the **Chart.js** library for implementing charts and graphs across all projects within the island.is repository. This decision was guided by several key factors:\n\n1. **Meeting Requirements:** Chart.js fulfills all the requirements for rendering various types of charts and graphs, providing a wide variety of options for customization and interactivity.\n2. **API Quality:** Chart.js has a well-documented and intuitive API that simplifies the process of creating and managing charts, thereby improving developer productivity and reducing the learning curve.\n3. **Pricing:** Chart.js is an open-source library, which means it incurs no licensing costs, making it a cost-effective solution for the organization.\n4. **Bundle Size:** Compared to other libraries, Chart.js has a relatively smaller bundle size, which is advantageous for maintaining overall application performance.\n5. **Typescript Support:** Chart.js offers robust TypeScript definitions and supports strong typing out of the box, ensuring better type safety and integration with existing TypeScript projects.\n\nThis standardized approach ensures a consistent look and feel for chart visualizations across all projects, enhances code maintainability, and leverages the strengths of Chart.js in terms of features, performance, and cost.","GenTime":"2024-07-28 22:31:09"}
{"File Name":"registraties\/001-use-existing-apis.md","Context":"## Context\\nThe Registraties project will need to contain a page that will show data per address (or verblijfsobject) from existing APIs, like [Basisregistraties adressen en gebouwen (BAG)](https:\/\/api.data.amsterdam.nl\/bag\/), [Basisregistratie kadaster (BRK)](https:\/\/api.data.amsterdam.nl\/brk\/) and [Handelsregister (HR)](https:\/\/api.data.amsterdam.nl\/handelsregister). All required data is readily available.\\n\n## Decision\n","Decision":"The project's MVP, which will contain a simple search field with auto-suggest functionality, get its data from existing API endpoints and therefore doesn't need its own back-end setup with API. In the future it might, but for the initial phase, the front-end code is capable of retrieving, formatting and showing the data from [api.data.amsterdam.nl](https:\/\/api.data.amsterdam.nl).\\nThe application will have container components that have sagas injected into them. Each saga is responsible for retrieving data from a single endpoint. If the situation calls for it, a saga can retrieve data from more than one endpoint.\\n","tokens":112,"id":1122,"Prediction":"## Decision\nThe page for displaying data per address (or verblijfsobject) will be developed using a dynamic, client-side rendering approach. This will allow for asynchronous fetching and displaying of data from existing APIs such as BAG, BRK, and HR, ensuring that the page is responsive and provides real-time updates as new data becomes available. Utilizing modern JavaScript frameworks such as React or Vue.js will facilitate the integration with these APIs and provide a seamless user experience. A caching layer will also be implemented to store frequently accessed data, optimizing the performance and reducing the number of API calls.","GenTime":"2024-07-28 22:31:15"}
{"File Name":"registraties\/002-use-styled-components.md","Context":"## Context\\nSASS is used in most of the [Amsterdam](https:\/\/github.com\/Amsterdam) projects. The way SASS is compiled by Webpack, however, has all defined style rules affect the global scope. This can lead to unwanted and unexpected side-effect and will increase maintenance, because each developer working on the project needs to be aware of the rules that have been defined and are scattered throughout the code base. Also, using SASS requires a naming convention to prevent specifity problems.\\nAn alternative to global scope CSS is [styled components](https:\/\/www.styled-components.com\/). This approach is also used by the [Amsterdam reusable component library](https:\/\/amsterdam.github.io\/amsterdam-styled-components), it makes sense to also apply it to projects (like this one) that make use of components from that library.\\n\n## Decision\n","Decision":"SASS will not be used in the Registraties project. Instead, styled components will be used.\\n","tokens":174,"id":1125,"Prediction":"## Decision\n\nWe will transition from using SASS to Styled Components for this project. This decision is based on several key factors:\n\n1. **Scoped Styles**: Styled Components provide scoped CSS that only applies to specific components. This significantly reduces the risk of unintentional global side effects caused by CSS rules.\n\n2. **Consistency**: By adopting Styled Components, we align with the practices used in the Amsterdam reusable component library, promoting consistency across our projects.\n\n3. **Simplified Naming Conventions**: Styled Components encapsulate styles, which minimizes the need for complex naming conventions to manage CSS specificity. This helps in making the code more maintainable and easier to understand.\n\n4. **Developer Experience**: Styled Components offer a more robust development experience with features like automatic critical CSS, theming, and more readable component-based styles.\n\n5. **Maintenance**: The use of Styled Components will help streamline maintenance by reducing the cognitive load on developers. They won't need to be as aware of globally scattered styles, which reduces the risk of CSS conflicts and increases productivity.\n\nWe will initiate the migration process by first addressing new components and gradually refactoring existing components to use Styled Components. This phased approach ensures a smooth transition while minimizing disruption to the ongoing development process.","GenTime":"2024-07-28 22:31:25"}
{"File Name":"simple-android\/001-screen-controllers.md","Context":"## Context\\nWe don\u2019t want to put business logic inside Android framework classes (like an `Activity` or `Fragment`) because those cannot be unit tested. To enable\\na fast feedback loop (i.e. tests that run on the JVM and not Android VM), we separate screens and controllers using\\nthe [MVI architecture](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1) [pattern](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1)\\n.\\n\n## Decision\n","Decision":"Every screen has one controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes back\\nto the screen.\\nUser interactions happening on the screen are abstracted inside data classes of type `UiEvent`. These events flow to the controller in the form of\\nRxJava streams.\\n```kotlin\\n\/\/ Create the UsernameTextChanged event by listening to the EditText\\nRxTextView\\n.textChanges(usernameEditText)\\n.map { text -> UsernameTextChanged(text) }\\n\/\/ Event\\ndata class UsernameTextChanged(text: String) : UiEvent\\n```\\nThe screen sends a single stream of `UiEvent`s to the controller and gets back a transformed stream of UI changes. The flow of data is\\nuni-directional. To merge multiple streams into one, RxJava\u2019s `merge()`  operator is used.\\n```kotlin\\n\/\/ Login screen\\nObservable.merge(usernameChanges(), passwordChanges(), submitClicks())\\n.compose(controller)\\n.takeUntil(screenDestroy)\\n.subscribe { uiChange -> uiChange(this) }\\n```\\nIn the controller, `UiEvent`s are transformed as per the business logic and `UiChange`s are sent back to the screen. The `UiChange` is a simple lambda\\nfunction that takes the screen itself as an argument, which can call a method implemented by the screen interface.\\n```kotlin\\ntypealias Ui = LoginScreen\\ntypealias UiChange = (LoginScreen) -> Unit\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nevents.ofType<UsernameTextChanged>\\n.map { isValidUsername(it.text) }\\n.map { isValid ->\\n{ ui: Ui -> ui.setSubmitButtonEnabled(isValid) } \/\/ UiChange lambda!\\n}\\n}\\n}\\n```\\nWhen the events have to observed across multiple functions in the controller, the stream is shared using `replay()` + `refCount()` so that the UI\\nevents aren't recreated once for every subscription. `replay()` shares a single subscription to the screen by replaying the events to every observer\\nand `refCount()` keeps the subscription alive as long as there is at least one observer.\\n```kotlin\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nval replayedEvents = events.replay().refCount()\\nreturn Observable.merge(\\nenableSubmitButton(replayedEvents),\\nloginUser(replayedEvents))\\n}\\nfun enableSubmitButton(events: Observable<UiEvent>): Observable<UiChange>()\\nfun loginOnSubmit(events: Observable<UiEvent>): Observable<UiChange>()\\n}\\n```\\n![data flow from the Ui to the controller](images\/diagram_screen_controller.png)\\n([diagram source](https:\/\/docs.google.com\/drawings\/d\/1I_VdUM8Pf9O3nOYViqVF6kiyqFaYFD2fHmKRyvwmEl4\/edit?usp=sharing))\\n","tokens":137,"id":1133,"Prediction":"## Decision\nWe will adopt the Model-View-Intent (MVI) architecture pattern for separating business logic from the Android framework classes. This decision ensures that:\n\n1. **Business Logic Isolation**: Business logic is isolated from the `Activity` and `Fragment` classes, enhancing testability.\n2. **Improved Testability**: Adopting MVI allows us to write unit tests that run on the JVM, facilitating a fast feedback loop.\n3. **Maintaining Single Responsibility Principle**: Each component in the architecture will have a single responsibility, making the codebase easier to manage and extend.\n4. **Reactive Programming**: MVI leverages reactive programming principles, making it easier to manage state and events in a consistent and predictable manner.\n\nWe will follow the detailed guidelines and best practices outlined in the MVI architecture reference articles to ensure a robust implementation.","GenTime":"2024-07-28 22:32:04"}
{"File Name":"runbooks-mkdocs\/0003-use-plantuml-for-diagramming.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":1145,"Prediction":"## Context\nThe application is experiencing high latency during peak usage hours, which is impacting the user experience. This latency issue is primarily due to the monolithic architecture of the system, which makes scaling challenging and inefficient. There is a need to improve the system's scalability and performance to handle the traffic more effectively. Additionally, the team has experience with microservices architecture, and there is an opportunity to leverage cloud services to enhance scalability.\n\n## Decision\nThe team has decided to migrate the application from a monolithic architecture to a microservices architecture. This transition will involve decomposing the monolithic application into smaller, independent services that can be scaled horizontally. Each microservice will be designed to handle a specific business capability and will communicate with other services through well-defined APIs. We will also leverage cloud-native technologies, such as container orchestration (e.g., Kubernetes) and managed database services, to further enhance scalability and resilience. By employing microservices, we aim to improve the system's performance and ensure it can handle peak traffic efficiently.","GenTime":"2024-07-28 22:32:48"}
{"File Name":"once-ui\/0002-build-a-high-quality-ui-components-library.md","Context":"## Context\\nMoving towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent api.\\n\n## Decision\n","Decision":"- We will build a library of atomic ui components that will be used throughout the applications.\\n- Developers should be able to easily import only the components they need and use them in their applications.\\n- Components should be well tested and isolated in a way that doesn't break the host application's code.\\n","tokens":40,"id":1151,"Prediction":"## Decision\n\nIn order to ensure consistency, reusability, and maintainability across our suite of applications, we will create a shared library of Angular components. The shared component library will be developed as an Angular Library, which will:\n   \n1. **Contain Shared UI Components:** The library will house UI components that are common across multiple applications. This will include elements such as buttons, form controls, modals, and any other UI elements that fit the common design language.\n\n2. **Design Language Consistency:** We will establish a design system guideline that all components in the library must follow. This ensures that all applications have a consistent look and feel.\n\n3. **Consistent API:** All components will have a consistent and well-documented API to ensure ease of use and integration into the various applications. Components will be designed to be as modular and composable as possible.\n\n4. **Versioning and Distribution:** The shared library will be versioned and distributed via a private npm repository, enabling easy updates and dependency management across different projects.\n\n5. **Testing and Quality Assurance:** Comprehensive unit tests, integration tests, and visual regression tests will be added to the library to ensure high quality and reliability.\n\n6. **Documentation:** Detailed documentation will be provided to inform developers of the correct usage of each component. This includes examples, API specifications, and style guides.\n\n7. **Continuous Integration\/Continuous Deployment (CI\/CD):** A CI\/CD pipeline will be established for the library to automate testing, building, and releasing of updated library versions.\n\nBy centralizing our shared components into a library, we will facilitate faster development, reduce code duplication, and maintain a cohesive user experience across all our applications.","GenTime":"2024-07-28 22:33:07"}
{"File Name":"Head-Start-TTADP\/0006-database.md","Context":"## Context\\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\\n\n## Decision\n","Decision":"While both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN\/GIST indexing accelerating full-text searches.\\n","tokens":90,"id":1167,"Prediction":"## Decision\nAfter evaluating the requirements and features of both MySQL and PostgreSQL, we have decided to use **PostgreSQL** as the primary relational database management system (RDBMS) for the TTA Smarthub platform. \n\nKey reasons for this decision include:\n\n1. **Advanced Feature Set**: PostgreSQL supports a wide range of advanced features such as complex queries, full-text search, and JSON support, which can be beneficial for the structured nature of our data.\n\n2. **Scalability and Performance**: PostgreSQL is known for its robustness and performance especially with complex transactions and large datasets, which aligns with our need for frequent queries and reports.\n\n3. **Compliance and Extensibility**: PostgreSQL conforms to SQL standards and is highly extensible, allowing for custom functions and procedures, which will help us cater to specific business requirements.\n\n4. **Community and Support**: PostgreSQL has a strong open-source community and extensive documentation, ensuring good support and continuous improvements.\n\n5. **Cost Effectiveness**: Being open-source, PostgreSQL reduces the total cost of ownership compared to proprietary alternatives while offering high-quality features.\n\nThis decision ensures that the TTA Smarthub platform will have a highly reliable, efficient, and scalable data management solution suitable for its current and future needs.","GenTime":"2024-07-28 22:33:59"}
{"File Name":"Head-Start-TTADP\/0011-monitoring.md","Context":"## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n\n## Decision\n","Decision":"TTA Smart Hub will utilize New Relic for Monitoring.\\n* Integrates easily with front end and backend code\\n* Track performance metrics and errors\\n* FedRAMP approved\\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\\n","tokens":20,"id":1171,"Prediction":"## Decision\n\nTo address the continuous monitoring needs for the TTA Smart Hub, we will implement a comprehensive monitoring solution leveraging a combination of Prometheus for metrics collection and Grafana for visualization, along with setting up Alertmanager to handle notifications and automated responses to critical incidents.\n\n### Key Components:\n1. **Prometheus**:\n   - **Metrics Collection**: Prometheus will gather and store metrics from various components of the TTA Smart Hub, including application performance, network status, and server health.\n   - **Custom Exporters**: We will develop or configure needed exporters to tailor metrics collection to our specific needs.\n\n2. **Grafana**:\n   - **Visualization**: Grafana will be used to create detailed and customizable dashboards to provide clear and actionable insights into the health and performance of the TTA Smart Hub.\n   - **Historical Analysis**: Grafana will allow us to analyze historical data trends and identify potential issues before they become critical.\n\n3. **Alertmanager**:\n   - **Automated Alerts**: Integration with Prometheus to manage and route alerts to the appropriate response teams, ensuring quick reaction to any dips in performance or detected errors.\n   - **Incident Response**: Configuration of Alertmanager policies to automatically take predefined actions, such as restarting a service, based on specific alert conditions.\n\n4. **Log Management**:\n   - **ELK Stack** (Elasticsearch, Logstash, Kibana) or an equivalent solution will be employed for centralized log management to facilitate in-depth troubleshooting and root cause analysis.\n\n5. **Self-Healing Mechanisms**:\n   - **Automated Remediation**: Implementation of scripts or tools for common issues like clearing cache, restarting services, or reallocating resources upon detecting anomalies.\n\n### Implementation Plan:\n1. **Setup and Configuration**:\n   - Install and configure Prometheus, Grafana, and Alertmanager on our monitoring server.\n   - Develop custom exporters or use existing ones to ensure complete coverage of our monitoring requirements.\n   - Set up Grafana dashboards tailored to different stakeholders (e.g., developers, operations, management).\n\n2. **Integration**:\n   - Integrate Prometheus with various TTA Smart Hub components.\n   - Configure Alertmanager to use communication channels (like email, Slack, etc.) for alerting responsible teams.\n\n3. **Testing and Refinement**:\n   - Conduct extensive testing to ensure metrics are accurate and alerts are firing as expected.\n   - Fine-tune the alerting thresholds and remediation scripts to balance between sensitivity and noise reduction.\n\n4. **Training and Onboarding**:\n   - Train relevant team members on using Grafana dashboards and managing alerts.\n   - Ensure there is adequate documentation to onboard new team members quickly.\n\nThis comprehensive solution will provide continuous monitoring, rapid error detection, and resolution, thereby ensuring optimal uptime for the TTA Smart Hub.","GenTime":"2024-07-28 22:34:21"}
{"File Name":"Head-Start-TTADP\/0007-use-plantuml-for-diagrams.md","Context":"## Context\\nCertain things we need to document, such as our data model and boundary diagrams are\\nbetter represented as diagrams than text. Choosing a single service will make it easier to\\nlearn the system and keep diagrams up to date.\\n\n## Decision\n","Decision":"We will use https:\/\/www.plantuml.com to generate diagrams with the following checks:\\n1. We will not rely on the plantuml.com service to be the sole storage medium for the actual UML text\\n1. We will store each type of diagram in its own markdown file in the \/docs folder\\nPlantUML can also be run locally, in case the plantuml.com service ever goes away. This risk is also why\\nwe must keep the UML source in our source control.\\n","tokens":48,"id":1172,"Prediction":"## Decision\nWe will adopt a single diagramming service to manage all our architectural and design diagrams.\n\n### Justification\n1. **Consistency:** Using a single service ensures that all team members are using the same tool, which promotes consistency in the style and structure of the diagrams.\n2. **Ease of Learning:** New team members will have only one tool to learn, which reduces the onboarding time.\n3. **Simplified Maintenance:** A single service makes it easier to update and maintain diagrams, ensuring that the documentation remains current and accurate.\n4. **Integration:** Having a unified tool might allow better integration with our existing workflow and version control systems.\n5. **Collaborative Features:** Many modern diagramming tools offer real-time collaboration features, which can improve team productivity.\n\n### Tool Evaluation\nA shortlist of potential diagramming tools will be evaluated based on features, ease of use, integration capabilities, cost, and team preferences. A final choice will be made after this evaluation.\n\n### Implementation Plan\n1. **Evaluation Period:** Conduct a 2-week evaluation period where team members can explore shortlisted tools and provide feedback.\n2. **Final Selection:** Based on team feedback and the evaluation criteria, select the diagramming service to be adopted.\n3. **Training:** Organize a training session to ensure all team members are comfortable using the selected tool.\n4. **Migration:** Migrate existing diagrams to the new tool.\n5. **Documentation:** Update the team\u2019s documentation standards to include guidelines for using the new diagramming service.\n\n### Review\nThis decision will be reviewed after 6 months to ensure the selected tool meets the team\u2019s needs and integrates well into the workflow. Adjustments will be made based on any challenges or new requirements that arise during this period.","GenTime":"2024-07-28 22:34:25"}
{"File Name":"Head-Start-TTADP\/0003-configuration-by-environment-variable.md","Context":"## Context\\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\\n\n## Decision\n","Decision":"We will use environment variables to configure applications.\\n","tokens":35,"id":1173,"Prediction":"## Decision\n\nUse environment-specific configuration files to manage different settings for different deployment environments.\n\n### Options Considered\n1. **Single Configuration File with Conditional Logic:** Include all configurations in a single file with conditional logic to choose the appropriate settings based on the environment.\n2. **Environment Variables:** Use environment variables to define configuration settings.\n3. **Environment-Specific Configuration Files:** Create separate configuration files for each environment (e.g., `config.local.json`, `config.production.json`).\n4. **Configuration Service:** Use an external configuration service to retrieve appropriate settings based on the deployment environment.\n\n### Chosen Solution\n**Environment-Specific Configuration Files:**\n\n- **Rationale:** \n  - **Simplicity:** Managing separate files for different environments makes it clear which settings apply to which environment.\n  - **Security:** Keeps sensitive production configurations separate from local development files.\n  - **Flexibility:** Allows easy modification of configurations without altering the application code.\n  - **Version Control:** Configuration files can be part of the codebase and versioned appropriately.\n\n### Implementation Steps\n1. **Create Configuration Files:**\n   - `config.local.json` for local development.\n   - `config.production.json` for production.\n   - Additional files as needed for other environments, such as staging (`config.staging.json`) or testing (`config.test.json`).\n\n2. **Modify the Application to Load Configurations Based on Environment:**\n   - Implement logic in the application startup to load configurations based on an environment variable, e.g., `NODE_ENV` in a Node.js application.\n\n3. **Set Environment Variables:**\n   - Ensure appropriate environment variables are set on the deployment platform (e.g., `NODE_ENV=production` for a production deployment).\n\n4. **Handle Secrets and Sensitive Data:**\n   - Use environment variables or a secret management tool to handle sensitive data like API keys and passwords.\n\n5. **Test Configuration Loading:**\n   - Verify that the application loads the correct configuration based on the environment setting.\n\n### Examples\n- **Node.js:**\n  ```javascript\n  const env = process.env.NODE_ENV || 'local';\n  const config = require(`.\/config.${env}.json`);\n  \n  module.exports = config;\n  ```\n\n- **Python (using `os` and `json` modules):**\n  ```python\n  import os\n  import json\n  \n  env = os.getenv('ENV', 'local')\n  with open(f'config.{env}.json') as config_file:\n      config = json.load(config_file)\n  ```\n\n### Conclusion\nBy using environment-specific configuration files, we can clearly separate configuration settings for different environments, making the application more maintainable, secure, and easy to deploy. This approach will facilitate safer and more efficient transitions between development, staging, and production environments.","GenTime":"2024-07-28 22:34:32"}
{"File Name":"clone_difitalcitizenship\/0002-use-openapi-to-defined-the-api-specs.md","Context":"## Context\\nWe need to define the API specifications of the services we're going to implement.\\n\n## Decision\n","Decision":"We use the [OpenAPI 2.0](https:\/\/swagger.io\/specification\/) specification (aka Swagger spec) as standard for our REST API definitions.\\n","tokens":20,"id":1180,"Prediction":"## Decision\nWe will use the OpenAPI Specification (OAS) version 3.0.3 as the standard for defining our API specifications. This choice is driven by several factors:\n\n1. **Standardization**: OpenAPI is widely used and provides a consistent and standardized way to describe RESTful APIs.\n2. **Tooling Support**: There is extensive tooling support for OpenAPI, including editors, validators, generators for client libraries, and server stubs.\n3. **Documentation**: OpenAPI makes it easier to generate and maintain API documentation. Tools like Swagger UI and Redoc can render interactive documentation directly from the OpenAPI definition.\n4. **Interoperability**: Using OpenAPI will facilitate better integration with other systems and services, as many already support it.\n5. **Automation**: OpenAPI definitions can be used to automate testing, API mocking, and other aspects of the API lifecycle.\n\nWe will store our OpenAPI specifications in a dedicated repository. This repository will be version-controlled, allowing us to track changes and maintain different versions of our API over time. Additionally, all API design and changes will follow a review process to ensure consistency and adherence to our design principles.","GenTime":"2024-07-28 22:34:50"}
{"File Name":"clone_difitalcitizenship\/0003-follow-a-cloud-native-design-principle.md","Context":"## Context\\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\\nDecision criteria will also include:\\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\n\n## Decision\n","Decision":"* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\\n","tokens":227,"id":1182,"Prediction":"### Decision\n\n**Adopt a Multi-Cloud, API-Driven, Serverless Architecture**\n\nGiven the context and the decision criteria outlined, we have decided to adopt a multi-cloud, API-driven, serverless architecture for the Digital Citizenship platform. This decision is guided by the following considerations:\n\n1. **Elasticity and Resilience**:\n   - **Serverless Computing**: By leveraging serverless technologies (e.g., AWS Lambda, Google Cloud Functions, Azure Functions), we can achieve high elasticity and resilience since these services automatically scale up or down based on demand.\n   - **Multi-Cloud Strategy**: Using multiple cloud providers helps ensure resilience against regional or provider-specific outages.\n\n2. **Pay-as-You-Go Pricing**:\n   - **Serverless and Cloud Resources**: Both serverless functions and various cloud services offer pay-as-you-go pricing models, reducing our upfront costs and allowing us to pay only for the resources we actually use.\n\n3. **Exploiting Economies of Scale**:\n   - **Public Cloud Providers**: By utilizing cloud services from leading providers like AWS, Google Cloud, and Azure, we can benefit from their massive economies of scale, which often translate to lower costs and better service levels for us.\n\n4. **Exploiting Falling Costs of Infrastructure Over Time**:\n   - **Cloud Adoption**: As cloud infrastructure costs continue to fall, our decision to leverage cloud-based solutions will allow us to naturally take advantage of these cost reductions.\n\n5. **Quality of Management Tools**:\n   - **Cloud-Native Management and Monitoring**: Public cloud providers offer robust management and monitoring tools (e.g., AWS CloudWatch, Azure Monitor, Google Stackdriver) that support automation, alerting, logging, and insights.\n\n6. **Best of Breed Security**:\n   - **Cloud Security Services**: Cloud providers offer high-quality security tools and services (e.g., AWS Shield, Google Security Command Center, Azure Security Center) that help ensure our application remains secure.\n   - **API Security**: Focusing on an API-driven architecture allows us to implement and enforce rigorous security policies and practices at the API level.\n\n7. **Flexibility and Opportunity Costs**:\n   - **APIs and Microservices**: An API-driven microservices architecture enhances flexibility, allowing us to rapidly develop, deploy, and iterate on individual components without affecting the entire application.\n   - **Serverless and Managed Services**: By leveraging serverless architectures and managed cloud services, our team can focus on core business logic and reduce the opportunity cost associated with managing and maintaining infrastructure.\n\n### Implementation Steps\n\n1. **Define Core API and Microservices Architecture**:\n   - Design the API endpoints and microservices required to fulfill the functional requirements of the Digital Citizenship platform.\n  \n2. **Evaluate and Select Cloud Providers**:\n   - Conduct a detailed evaluation of AWS, Google Cloud, and Azure to determine which provider's serverless and API management services best fit our needs.\n  \n3. **Develop Proof of Concept (PoC)**:\n   - Develop a PoC using serverless functions and API gateways to validate the architecture and demonstrate resilience, elasticity, and other key criteria.\n\n4. **Implement Security Best Practices**:\n   - Implement API security measures, including authentication (e.g., OAuth, JWT) and encryption.\n   - Utilize cloud-native security services to protect data and ensure compliance.\n\n5. **Adopt Cloud Management and Monitoring Tools**:\n   - Integrate cloud-native monitoring and management tools to maintain visibility and control over the platform.\n\n6. **Continuous Integration and Continuous Deployment (CI\/CD)**:\n   - Establish a robust CI\/CD pipeline to automate testing, deployment, and scaling of our services.\n\nBy making these decisions and following the outlined implementation steps, we aim to create a resilient, flexible, and scalable Digital Citizenship platform that meets both our functional and non-functional requirements.","GenTime":"2024-07-28 22:35:03"}
{"File Name":"clone_difitalcitizenship\/0007-choice-of-azure-region.md","Context":"## Context\\nWe must guarantee that the data stored, transferred to and from the cloud services will never leave the borders of the European Union.\\n\n## Decision\n","Decision":"We decide to deploy our services on the [West Europe](https:\/\/azure.microsoft.com\/en-us\/regions\/) region, located in the Netherlands.\\n","tokens":29,"id":1186,"Prediction":"## Decision\n\nWe will use cloud service providers that operate data centers exclusively within the European Union and comply with EU data sovereignty laws. \n\n### Steps to Implement:\n\n1. **Select Cloud Providers:**\n   - Choose cloud service providers with guaranteed data residency within the EU, such as AWS Europe (Frankfurt, Ireland, and Paris regions), Microsoft Azure Europe (France Central, Germany West Central, West Europe), and Google Cloud Europe (Belgium, Netherlands, and Finland regions).\n\n2. **Data Transfer Policies:**\n   - Configure and enforce data transfer policies to ensure that all data handling (storage, processing, and transfer) occurs within the designated EU regions. Restrict access and mobility features that could potentially route data outside the EU.\n\n3. **Compliance Auditing:**\n   - Regularly audit the data storage and transfer mechanisms for compliance with GDPR and other relevant EU data protection regulations. This can be done through scheduled internal audits and independent third-party audits.\n\n4. **Contractual Agreements:**\n   - Include provisions in contracts with cloud service providers mandating compliance with EU data residency and sovereignty requirements. Ensure that data protection clauses are comprehensive and enforceable.\n\n5. **Access Control:**\n   - Implement robust access control mechanisms to ensure that only authorized personnel within the EU can access the data. This includes incorporating multi-factor authentication and geo-restriction policies.\n\n6. **Monitoring and Reporting:**\n   - Set up continuous monitoring systems to track the location of data in real-time and generate reports verifying that data remains within EU boundaries. Evoke automated alerts in case of any policy breach.\n\nBy applying these measures, we can ensure that all data interactions with cloud services fully comply with EU data residency requirements, thereby protecting the data sovereignty and personal information of our customers.","GenTime":"2024-07-28 22:35:19"}
{"File Name":"cdk\/002-component-constuctors.md","Context":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThis project contains a large number of classes, making up the various constructs and patterns. The project is intended to be used as a component library and, therefore, used by a number of people who don't have extensive knowledge of either the CDK or this library. It is therefore important to make the experience of using these classes as intuitive as possible.\\n\n## Decision\n","Decision":"<!-- What is the change that we're proposing and\/or doing? -->\\nConstructors should follow the following rules for consistency.\\n1. The first parameter should be a `scope` of type `GuStack`:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then have a static `id` as it will never change, for example the `Stage` parameter.\\n2. They can also take a `props` object which should be correctly typed:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack, props: object) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then derive `id` from `props` as it will never change, for example `InstanceTypeFor${props.app}`.\\n3. They can also take an `id` of type string and a `props` object which should be correctly typed\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {...}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: any, props: object) {\\n...\\n}\\n}\\n```\\n4. Where all `props` are optional, the `props` object should be optional as a whole\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props?: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n","tokens":95,"id":1193,"Prediction":"## Decision\n\nTo ensure that the component library is intuitive and straightforward for users with varying levels of expertise, we will take the following actions:\n\n1. **Simplify Class Interfaces:** Simplify the interfaces of the classes in the library, making them easy to understand and use. Hide complex configurations behind sensible defaults and provide only the most essential options upfront.\n\n2. **Comprehensive Documentation:** Provide extensive and clear documentation for all classes, methods, and their usages. Includes usage examples, explanations of key concepts, and step-by-step guides.\n\n3. **Consistent Naming Conventions:** Establish and rigorously follow consistent naming conventions across the library to help developers infer functionality from names alone.\n\n4. **Intuitive Error Messages:** Implement user-friendly error messages that are descriptive and provide guidance on how to resolve common issues.\n\n5. **Code Samples and Templates:** Provide ready-to-use code samples and templates that cover common use cases. These will serve as a learning tool as well as a foundation for developers to build upon.\n\n6. **Tooling and IDE Support:** Where possible, leverage tooling and IDE support to provide code completion, inline documentation, and other assistive features.\n\n7. **Community Support and Feedback Loop:** Create channels for community support and feedback, such as a forum, Slack workspace, or GitHub discussions. Actively monitor these channels and use feedback to iterate and improve the library.\n\n8. **Backward Compatibility:** Maintain backward compatibility with existing APIs as much as possible to ensure that users do not need to frequently refactor their code with each library update.\n\nBy implementing these strategies, we aim to enhance the usability and accessibility of the component library, ensuring that it can be effectively leveraged even by those with limited prior knowledge of the CDK or the library itself.","GenTime":"2024-07-28 22:35:39"}
{"File Name":"hee-web-blueprint\/0003-use-github-actions-for-automation.md","Context":"## Context\\nWe should determine the platform we use to automate the build, test and deployment of the HEE National Website Platform.\\n\n## Decision\n","Decision":"Having determined to use Github to manage the source code of the platform, the simplest answer to this question was to look at Github actions. We determined after some investigative work to prove out our ability to deploy to the BloomReach cloud that we should use Github Actions to manage platform automation.\\n","tokens":27,"id":1196,"Prediction":"## Decision\n\nAfter evaluating various options for automating the build, test, and deployment processes of the HEE National Website Platform, we have decided to use **Azure DevOps**. \n\n### Rationale:\n1. **Integration with Existing Infrastructure**: Azure DevOps integrates seamlessly with our existing Azure services and resources, providing a unified environment.\n2. **Scalability**: Azure DevOps offers robust scalability options that align with our future growth plans.\n3. **Comprehensive Features**: The platform provides end-to-end solutions for CI\/CD, including build pipelines, release pipelines, and automated testing.\n4. **Security and Compliance**: Azure DevOps adheres to industry-leading security standards and compliance requirements, which are crucial for our organization.\n5. **Developer Familiarity**: Our development team has prior experience and familiarity with the Azure ecosystem, minimizing the learning curve and enhancing initial productivity.\n6. **Support and Community**: Strong support from Microsoft and an active community of users ensure that we can quickly resolve any issues and adopt best practices.\n7. **Cost**: The pricing model of Azure DevOps is competitive and offers a good balance between cost and functionality.\n\nImplementing Azure DevOps will enable us to streamline our development workflows, enhance collaboration, and improve the overall efficiency of our build, test, and deployment processes.","GenTime":"2024-07-28 22:35:51"}
{"File Name":"hee-web-blueprint\/0008-use-npm-to-host-hee-frontend-framework.md","Context":"## Context\\nWe need to identify how and where we will store packages that describe our front end framework. There are a number of NHS UK and NHS Digital projects that already use NPM as their package repository for front end code.\\n\n## Decision\n","Decision":"We have chosen to store our packages on NPM.\\n","tokens":47,"id":1202,"Prediction":"## Decision\nWe will store our front end framework packages in NPM (Node Package Manager). Given that multiple NHS UK and NHS Digital projects already use NPM as their package repository for front end code, aligning with this choice will allow us to maintain consistency and compatibility with existing NHS projects. Using NPM will also leverage its robust package management features, wide community support, and established best practices for managing and distributing front end code. This decision will facilitate easier integration, dependency management, and collaboration across various teams and projects within the NHS ecosystem.","GenTime":"2024-07-28 22:36:16"}
{"File Name":"hee-web-blueprint\/0013-use-bloomreach-channel-manager.md","Context":"## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n\n## Decision\n","Decision":"Bloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\\n","tokens":141,"id":1205,"Prediction":"## Decision\nWe decided to leverage the native content management capabilities of the Bloomreach platform for managing our document management needs. Additionally, we opted to use Bloomreach's 'Channel' concept as the framework for handling the various sites to be integrated both at the Minimum Viable Product (MVP) stage and for future expansions.\n\n### Rationale:\n1. **Ease of Use:** Bloomreach's native platform provides a user-friendly interface that allows for quicker creation and deployment of new sites. This reduces the learning curve and training requirements for our internal team.\n2. **Component Sharing:** Bloomreach supports the sharing of components across different channels, which streamlines development and ensures consistency in user experience across multiple sites.\n3. **Content Segregation:** The 'Channel' concept supports the ability to segregate content effectively for different sites (e.g., Deenary and Specialty sites), ensuring that content relevant to specific audiences is isolated appropriately.\n4. **Hierarchical Content Sharing:** Bloomreach allows for the sharing of content both up and down the stack. This means that content can be reused within different channels where appropriate, reducing redundancy and ensuring coherence.\n5. **Permissions Model:** The permissions model within Bloomreach can be configured to manage access rights across different channels. This flexibility ensures that only authorized users can access and modify content, adhering to our security policies and compliance requirements.\n\nBy taking this approach, we can make use of the robust and comprehensive features of the Bloomreach platform, thereby eliminating the need for additional integrations that could complicate the architecture and increase maintenance overhead. This decision simplifies our content management strategy while ensuring scalability and flexibility for future growth.","GenTime":"2024-07-28 22:36:26"}
{"File Name":"automate\/adr-2019-01-23.md","Context":"## Context\\nPreviously, we did not have any rules for governing Automate UI route\\nlifecycle. In the past, we likely broke backwards compatibility when\\nre-naming or moving routes.\\nAdditionally, the top nav highlighting in Automate UI was driven off the\\nroot route (e.g. everything under the `Settings` tab must live under the `\/settings`\\nroute or it will not be highlighted properly for pages that live under it).\\nThis means that if you want to move a page to a new tab, it will require a new route.\\n\n## Decision\n","Decision":"Every route that originally linked to a page that is still in use in the webapp\\nmust still work and backwards compatibility must be maintained. Only when the webpage\\nthat the original route linked to is being removed from the app can a route be removed.\\nThis is so that deep linking from our own sources (blog posts, etc.) as well as links\\ncustomers use internally will continue to work until they no longer make sense at all\\nand the original resource no longer exists.\\nTo maintain backwards compatibility, the original route must redirect to the new route\\n(achievable in `app-routing.module.ts`) and an e2e test must be added to verify\\nthat redirection in `deprecated-routes.e2e-spec.ts`. See\\n[this commit](https:\/\/github.com\/chef\/automate\/commit\/38779c5aabb1af8d6c22e4b6e1b07eaf9c8fa06c)\\nfor an example of routes being renamed with proper backwards compatibility, what the e2e\\ntests should look like, and how a page can move between topnav tabs.\\n","tokens":115,"id":1210,"Prediction":"## Decision\n\nTo address the issues with the current Automate UI route lifecycle and ensure backward compatibility, we will adopt the following decisions:\n\n1. **Routing Rules and Governance:**\n   - Establish a set of guidelines and policies for routing changes. This will involve defining clear rules for naming, moving, and deprecating routes.\n   - Introduce a deprecation process for old routes, providing an overlap period where both old and new routes are functional, accompanied by appropriate warnings to users.\n\n2. **Consistent Route Structure:**\n   - Maintain a predictable and hierarchical route structure to ensure that the top navigation highlighting works consistently.\n   - Ensure all child routes under a particular tab (e.g., `\/settings`) follow a consistent pattern to leverage existing highlighting logic.\n\n3. **Backward Compatibility:**\n   - Implement route redirection mechanisms to automatically redirect old routes to new ones wherever possible.\n   - Provide detailed documentation and communication for any route changes, including mapping old routes to new ones for user reference.\n\n4. **Route Management Tooling:**\n   - Incorporate tooling to simplify and automate the management of routes, ensuring compliance with the established governance rules.\n   - Include automated testing to verify that route changes do not negatively impact the top navigation highlighting or overall UI functionality.\n\n5. **Migration Plan:**\n   - Develop a comprehensive migration plan for existing routes that need to be moved or renamed.\n   - Ensure a phased rollout with adequate testing and user feedback at each stage to minimize disruption.\n\nThis approach will help maintain backward compatibility, improve the consistency of the user interface, and provide a structured method for managing route changes in the Automate UI.","GenTime":"2024-07-28 22:36:39"}
{"File Name":"TruBudget\/0004-ressource-level-lifetime.md","Context":"## Context\\nResources have semantically a certain lifetime. That means, after a resources is created it will experience state-transition until it reaches the end of its lifetime (e.g. archival). The lifetimes and state-transitions need to be defined in an abstract way, in order to allow the platform to excel for different use-cases.\\n\n## Decision\n","Decision":"### Lifetime of a single resource\\n```mermaid\\ngraph TD\\nA[Create resource] -->|Auto-assign to creator| B\\nB[State: Open] --> |Do work|B\\nB --> |Assign to somebody who can close| C\\nC[State: Open & Ready to close] --> |Close| D\\nD[Resource closed]\\n```\\nRe-Opening is not supported; it could be discussed as long as the parent is still open (hierarchy must hold). Different usage-scenarios are acommodated via _manual_ steps.\\n#### Example: 4-Eyes Principle\\nScenario: A subproject must only be closed when 2 people A & B (e.g. different departments) checked it.\\nPrereqs: Closing right is given to an administrative person and not to the people signing off\\nApproach:\\n1. Subproject is executed as usual, then for sign-off is assigned to Person A\\n2. Sign-Off Person A enters a Workflow-Item validating the state and assigns Person B\\n3. Sign-Off Person B enters a Workflow-Item validating the state and assigns to administrative personell that has the close-rights\\n4. Administrative person checks whether all required sign-offs are there and then finally closes; otherwise assigns back\\n### Impact to child resources\\nA resource hierarchically describes a node in a tree-like structure. Therefore we need to look at impact of a closed resource to other nodes in the tree.\\nIf a resource has no children, there is no impact on other resources.\\nIf a resource has children (e.g. subprojects \/ workflowitems), **the resource cannot be closed**. All children need to be in state closed before a parent can be closed.\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n","tokens":69,"id":1216,"Prediction":"## Decision\n\nWe will implement a state machine architecture to manage the lifecycle of resources. This architecture will encapsulate the various states a resource can inhabit and the transitions between those states.\n\n### Key Aspects:\n1. **Abstract State Definitions**: Define resource states and allowable state transitions in a highly abstract and configurable manner. This allows for flexibility across different use-cases without being tightly coupled to specific resource types.\n   \n2. **Transition Rules**: Implement strict rules and validations for state transitions. This ensures that resources follow the defined lifecycle consistently and reliably.\n\n3. **Extensible Framework**: Develop the state machine in a modular fashion so that new states or transitions can be added or modified without significant changes to the core platform.\n\n4. **State Metadata**: Incorporate metadata with state definitions to help inform the system\u2019s behavior during transitions, facilitating actions such as notifications, logging, and condition checks.\n\n5. **Resource Archival**: Clearly define the conditions and mechanisms for moving a resource to its final state (e.g., archival), ensuring that this state is immutable and the resource is no longer active.\n\n6. **User Configuration**: Provide an interface for users to define custom lifecycles for their specific use cases while maintaining adherence to the platform's overarching rules and constraints.\n\nBy adopting this state machine approach, the platform will be equipped to handle various resource lifecycles in an organized and scalable manner, thereby enhancing its adaptability and robustness for different application scenarios.","GenTime":"2024-07-28 22:37:03"}
{"File Name":"TruBudget\/0003-project-data-model.md","Context":"## Context\\nWe need to define how to save and query project, subproject and workflow data efficiently.\\n### Definition of terms\\n- Project: Is the root element of the hierarchy. It contains metadata and Subprojects\\n- Subproject: Are a subset of Projects and collect a number of Workflowitems\\n- Workflowitems: Are a number of steps which need to be executed in a certain order\\n- Resources: Projects, Subprojects, Workflowitems are resources. This means they contain permissions, a log and metadata.\\n### Relations\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n### Options\\n1. Each resource has its own stream. Each resource has an array which links to the underlying resources (e.g. A Projects has an array of the ids of its Subrojects). That means if we want to query Subprojects of a specific Project, we need to lookup the Subproject ID's inside the projects to afterswards fetch all the underlying Subproject streams. **Pro:** We are only fetching the data which is necessary, changes inside the resources are storage efficent. **Con:** Resources with a high number of relations result in multiple (maybe hundreds) of queries.\\n2. Each Project has its own stream. All relational resources of the projects are stored inside a JSON and resolved on the API Layer. **Pro:** Only one query necessary to resolve the whole resource tree for one project. **Con:** Large amount of storage needed, since every change inside a single resource would trigger a whole copy of the project tree. Large datastructures have to be kept in memory\\n3. Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource. **Pro:** Easy to query, low storage \/ memory footprint **Con:** None\\n\n## Decision\n","Decision":"Since we need to be able to scale out to larger projects, it is important to be able to execute fast queries and having a low storage \/ memory footprint. The goal is to exploit the Multichain API as much as possible, since we can expect it to be more performant than our API. Option 3 is the option, which covers the required capabilities in the best way.\\n","tokens":438,"id":1225,"Prediction":"## Decision\n\nWe will adopt **Option 3**: Each project will have its own stream, and all relational resources will be organized with multiple key-items within the project stream.\n\n### Reasons for Decision:\n- **Efficiency**: This approach balances the need for efficient querying with storage optimization.\n- **Single Query Resolution**: By organizing the relational resources with multiple key-items, we can resolve specific resources with a single query, reducing the complexity and performance overhead of making multiple queries.\n- **Storage\/Memory Optimization**: This option avoids the large storage requirements and memory footprint that would result from embedding the entire project hierarchy in a single JSON structure, as is the case with Option 2.\n- **Incremental Updates**: Changes to individual resources necessitate only updates to the affected resources, without the need to duplicate the entire project tree. This ensures that updates are storage efficient and can be performed independently, improving maintainability.\n\nAdopting this hierarchical yet modular approach enables the efficient handling of both querying and updates, providing a robust and scalable solution for managing project, subproject, and workflow data.","GenTime":"2024-07-28 22:37:42"}
{"File Name":"TruBudget\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1226,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) system to document all significant architectural decisions made throughout the project lifecycle. The ADRs will be written using Markdown files and stored in a dedicated `decisions` directory within the project repository.\n\n### Rationale\n1. **Consistency**: By having a standardized approach to documenting decisions, we can ensure that all relevant details are captured in a consistent manner.\n2. **Traceability**: ADRs provide a clear historical record of why certain decisions were made, which helps with future maintenance and onboarding of new team members.\n3. **Collaboration**: Storing ADRs in the project repository allows easy access for all team members, promoting better communication and collaboration.\n\n### Details\n1. **Template**: We will use a standardized template for all ADRs to ensure uniformity. The template will include sections such as Context, Decision, Consequences, and Alternatives Considered.\n2. **Version Control**: ADRs will be version controlled using Git, allowing us to track changes and updates over time.\n3. **Review Process**: Each ADR will be reviewed and approved by at least one other team member before being finalized.\n4. **Numbering**: ADRs will be numbered sequentially (e.g., ADR-001, ADR-002) to make referencing easy.\n\nBy adopting this approach, we aim to create a reliable and accessible record of architectural decisions, facilitating better project governance and knowledge sharing.","GenTime":"2024-07-28 22:37:46"}
{"File Name":"jupyter-nbrequirements\/0000-dependencies-management-jupyter-notebooks.md","Context":"## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component\/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n\n## Decision\n","Decision":"* user prospective\\n* reproducibility\\n* traecability\\nThe option selected is 3. because:\\n* enforce reproducibility\\n* enforce traceability between notebook\\n### Positive Consequences <!-- optional -->\\n* Satisfy reproducibility, traecability, shareability.\\n* Notebooks are coupled with dependencies in their metadata.\\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\\n","tokens":140,"id":1228,"Prediction":"### Decision\nWe will use Docker to containerize the Jupyter Notebooks along with their dependencies. Each notebook will be accompanied by a Dockerfile that specifies the exact environment, including all necessary libraries and their versions. This approach ensures that the notebooks are portable, reproducible, and can be easily shared or deployed in any environment that supports Docker.\n\nThe exact steps include:\n1. **Creating a Dockerfile**: For each Jupyter Notebook, a corresponding Dockerfile will be created. This file will specify the base image (e.g., a Python image), the necessary dependencies (both direct and transitive), and any other configuration required to run the notebook.\n   \n2. **Building Docker Images**: With the Dockerfile, we will build a Docker image for each notebook. This image will encapsulate the entire runtime environment.\n\n3. **Storing Docker Images**: These Docker images can be stored in a container registry (e.g., Docker Hub or a private registry), from where they can be pulled and executed on any Docker-compatible system.\n\n4. **Version Control**: Both the Jupyter Notebook and its corresponding Dockerfile will be version-controlled in the same repository. This ensures that changes in the notebook and its dependencies are tracked together.\n\n5. **Documentation and Usage Instructions**: Each repository will include clear documentation on how to build the Docker image and run the notebook within a container. This includes any required setup steps and instructions for non-technical users.\n\nBy following this approach, we ensure that the environment in which the notebook runs is consistent across different users and platforms, greatly enhancing reproducibility, traceability, and security.","GenTime":"2024-07-28 22:37:54"}
{"File Name":"WorkoutLog\/0009_one_time_events_livedata.md","Context":"## Context\\nLiveData events will be pushed again after certain LifeCycle events. Ie. when rotating the device or\\nafter returning to a fragment from another navigation path. Errors or special events like automatic\\nnavigation should only be executed once.\\n\n## Decision\n","Decision":"A new event will be introduced for the ViewModels which has the clear purpose of only occur once.\\n","tokens":49,"id":1231,"Prediction":"## Decision\n\nTo avoid LiveData events being pushed again following certain lifecycle events, adopt the SingleLiveEvent or Event wrapper pattern.\n\n### Implementation Details:\n\n1. **SingleLiveEvent Class:**\n    - Utilize the `SingleLiveEvent` class to wrap LiveData events. This class allows observers to only observe changes once, preventing re-emission of data when lifecycle changes occur, such as device rotations or navigating away and back to a fragment.\n  \n    ```kotlin\n    class SingleLiveEvent<T> : MutableLiveData<T>() {\n        private val pending = AtomicBoolean(false)\n  \n        @MainThread\n        override fun observe(owner: LifecycleOwner, observer: Observer<in T>) {\n            \/\/ Observe the internal MutableLiveData\n            super.observe(owner, Observer { t ->\n                if (pending.compareAndSet(true, false)) {\n                    observer.onChanged(t)\n                }\n            })\n        }\n  \n        @MainThread\n        override fun setValue(t: T?) {\n            pending.set(true)\n            super.setValue(t)\n        }\n  \n        @MainThread\n        fun call() {\n            value = null\n        }\n    }\n    ```\n\n2. **Event Wrapper:**\n    - Alternatively, use an `Event` wrapper class to encapsulate data and expose it with LiveData. This approach will ensure the data is consumed only once.\n  \n    ```kotlin\n    class Event<out T>(private val content: T) {\n\n        private var hasBeenHandled = false\n\n        \/** Returns the content and prevents its use again. *\/\n        fun getContentIfNotHandled(): T? {\n            return if (hasBeenHandled) {\n                null\n            } else {\n                hasBeenHandled = true\n                content\n            }\n        }\n\n        \/** Returns the content, even if it's already been handled. *\/\n        fun peekContent(): T = content\n    }\n    ```\n\n    ```kotlin\n    class MyViewModel : ViewModel() {\n        private val _navigationEvent = MutableLiveData<Event<Unit>>()\n        val navigationEvent: LiveData<Event<Unit>> = _navigationEvent\n\n        fun triggerNavigationEvent() {\n            _navigationEvent.value = Event(Unit)\n        }\n    }\n    ```\n\n3. **Usage Example:**\n    ```kotlin\n    viewModel.navigationEvent.observe(this, Observer { event ->\n        event.getContentIfNotHandled()?.let {\n            \/\/ Navigate to the desired destination or perform the action\n        }\n    })\n    ```\n\n### Rationale\n\n- **Prevents Redundant Events:** Ensures that certain events (e.g., navigations, errors) are not redundantly executed multiple times due to lifecycle changes.\n- **Improves UX:** Prevents unexpected behaviors or users being navigated multiple times in quick succession.\n- **Efficiency:** Reduces unnecessary event handling and processing which is crucial for maintaining responsiveness in UI.\n\n### Trade-offs\n\n- **Complexity:** Introducing these patterns may add a small amount of complexity to the codebase.\n- **Maintenance:** Requires developers to understand and consistently apply the `SingleLiveEvent` or `Event` pattern throughout the app.\n\nThis approach addresses the challenges posed by LiveData lifecycle awareness and ensures that events and errors are executed only once, providing a better user experience and cleaner event handling.","GenTime":"2024-07-28 22:38:05"}
{"File Name":"stamper\/0007-docker.md","Context":"## Context\\nWe need to choose how we will deploy our app.\\n\n## Decision\n","Decision":"We use Docker because it allows us to isolate app with a container and easily deploy to the server with Terraform.\\n","tokens":15,"id":1239,"Prediction":"## Decision\nWe will utilize a cloud-based deployment strategy using an Infrastructure as a Service (IaaS) provider, specifically AWS. The deployment will leverage Amazon EC2 for scalable virtual server instances to ensure flexibility in scaling the application based on demand. We will also use Amazon RDS for the database to ensure high availability and reliability. Additionally, the application will be containerized using Docker to simplify deployments and maintain consistency across different environments. A CI\/CD pipeline will be implemented using AWS CodePipeline and CodeDeploy to automate the deployment process, minimizing manual intervention and reducing the risk of errors during deployment. This strategy provides a balance between scalability, reliability, and operational efficiency, enabling us to handle varying loads and ensure high availability of our application.","GenTime":"2024-07-28 22:38:36"}
{"File Name":"stamper\/0004-bootsptrap-as-frontend-framework.md","Context":"## Context\\nWe need to choose a frontend framework.\\n\n## Decision\n","Decision":"We don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\\n","tokens":12,"id":1240,"Prediction":"## Decision\n\nWe have decided to use **React** as our frontend framework.\n\n### Rationale:\n1. **Popularity and Community Support**: React is one of the most popular frontend frameworks. It has a large community, which means finding solutions to problems, libraries, and tools is easier.\n  \n2. **Component-Based Architecture**: React uses a component-based architecture which allows for reusable UI components. This promotes consistency, easier maintenance, and faster development.\n  \n3. **Performance**: React's virtual DOM implementation provides efficient rendering and updates, making it a fast option for complex and high-performance user interfaces.\n  \n4. **Ecosystem**: The React ecosystem is vast, with many additional libraries available for routing (React Router), state management (Redux, Context API), and other functionalities.\n  \n5. **Learning Curve**: While there might be a learning curve, particularly around JSX and the concepts of state and props, the clear documentation and abundant tutorials make it a manageable challenge.\n  \n6. **Industry Adoption**: Many large and small companies use React, ensuring that skills in React are valuable and widely transferable.\n  \n7. **Integration with Backend**: React can integrate seamlessly with various backend technologies, including REST APIs and GraphQL services.\n\n### Potential Drawbacks:\n1. **High Pace of Change**: The JavaScript ecosystem as a whole, including React, evolves rapidly. This can sometimes require frequent updates and refactoring to stay current.\n2. **Initial Setup**: Setting up a React project can be complex initially when compared with simpler options. However, tools like Create React App mitigate this complexity.\n\nOverall, the benefits of adopting React outweigh the potential drawbacks, making it a suitable choice for our frontend framework.","GenTime":"2024-07-28 22:38:40"}
{"File Name":"stamper\/0003-spring-boot-as-web-framework.md","Context":"## Context\\nWe need to choose a web framework for the app.\\n\n## Decision\n","Decision":"We use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\\n","tokens":15,"id":1243,"Prediction":"## Decision\n\nWe have decided to use the **Django** web framework for the app.\n\n### Reasons for Choosing Django\n1. **Maturity and Stability**: Django is a well-established framework with a large, active community and extensive documentation. Its maturity and stability make it a reliable choice for our project.\n  \n2. **Batteries-Included Philosophy**: Django follows the \"batteries included\" philosophy, meaning it comes with many built-in features, such as an admin panel, authentication, forms, and more. This will speed up development and reduce the need for third-party packages.\n  \n3. **Security**: Django has strong security features out-of-the-box, including protection against common vulnerabilities like SQL injection, cross-site scripting, and cross-site request forgery.\n  \n4. **Scalability**: Django is designed to handle high-traffic sites, making it a suitable choice for our app if we need to scale it in the future.\n  \n5. **Development Speed and Efficiency**: Django\u2019s ORM, templating system, and overall design promote rapid development, which aligns with our project timelines.\n\n6. **REST Framework**: Django's REST framework simplifies the creation of RESTful APIs, which is beneficial if our app needs an API layer.\n\n### Alternatives Considered\n- **Flask**: While Flask is lightweight and offers flexibility, it requires more setup and assembly of third-party packages to achieve the same functionality offered out-of-the-box by Django. This could slow down our development process.\n  \n- **Rails**: Rails (Ruby on Rails) is also a mature and powerful framework but choosing it would require our team to possibly learn Ruby on top of the web framework specifics, which could affect productivity given our team's current proficiency in Python.\n\nBy selecting Django, we aim to leverage its robust features, security benefits, and efficiency to deliver a high-quality web application within our desired timeframe.","GenTime":"2024-07-28 22:38:51"}
{"File Name":"stamper\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1244,"Prediction":"## Decision\nWe will implement an Architectural Decision Records (ADR) system to document all critical architectural decisions made during the project lifecycle. This system will adhere to a standardized format for consistency and ease of comprehension. Each ADR will include context, decision, consequences, and other pertinent details. This practice will ensure transparency, facilitate better communication among team members, and provide a historical record for future reference and audits.","GenTime":"2024-07-28 22:38:53"}
{"File Name":"buy-for-your-school\/0005-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n\n## Decision\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":1246,"Prediction":"## Decision\n\nTo mitigate the risk of inefficient database queries being missed during code reviews and to improve overall application performance, we will implement an automated query performance monitoring and alerting system. Specifically, we will:\n\n1. **Integrate Automated Query Performance Tools:**\n   Use tools such as New Relic, Datadog, or query profiling functions provided by our database (e.g., PostgreSQL's `pg_stat_statements` or MySQL's `slow query log`) to continuously monitor the performance of our database queries.\n\n2. **Set Up Alerting Mechanisms:**\n   Configure alerts for queries that exceed pre-defined performance thresholds (e.g., execution time or resource usage). Alerts should be actionable and provide enough context to diagnose and address the performance issue swiftly.\n\n3. **Implement Continuous Integration Checks:**\n   Add stages in our CI\/CD pipeline where automated tests analyze query performance metrics. Any code changes introducing queries that significantly degrade performance should trigger a warning or block the merge until resolved.\n\n4. **Foster Performance Awareness:**\n   Conduct regular training sessions for developers focused on writing efficient SQL queries and understanding the performance impact of different database operations. This includes deep dives into index usage, query plan analysis, and common pitfalls.\n\n5. **Conduct Performance Reviews:**\n   Schedule regular performance-focused code reviews where existing queries in critical parts of the application are audited for efficiency. This can be part of sprint retrospectives or dedicated performance sprints.\n\n6. **Documentation and Best Practices:**\n   Develop and maintain documentation detailing SQL query best practices, indexing strategies, and guidelines for writing efficient data access layers. Ensure this documentation is readily accessible to all developers.\n\nThese steps aim to build a robust framework for catching inefficient queries early, ensuring ongoing performance monitoring, and cultivating a culture of performance awareness among the team.","GenTime":"2024-07-28 22:39:01"}
{"File Name":"buy-for-your-school\/0013-use-dfe-sign-in-as-auth-provider.md","Context":"## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https:\/\/github.com\/DFE-Digital\/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n\n## Decision\n","Decision":"We are going to use DSI as our single sign-on provider using the OIDC standard.\\n","tokens":126,"id":1252,"Prediction":"## Decision\nWe will proceed with implementing DfE Sign-in (DSI) as the authentication mechanism for the service. Although our initial preference was for a password-less solution, the DfE governance's decision to utilize DSI will be respected and adhered to.\n\n### Rationale:\n1. **Governance Decision:** DfE governance has mandated the use of DSI for this service, ensuring compliance with organizational security and access management policies.\n2. **Security Standards:** DSI is aligned with modern security standards and will provide the necessary authentication features required to restrict access to trusted school buying professionals.\n3. **Integration Readiness:** Implementing DSI will allow for easier integration with other DfE services and potential future requirements, given it is a centrally supported solution.\n4. **User Management Simplification:** By using DSI, we can leverage existing systems for user management, reducing the overhead compared to developing and maintaining a custom or third-party password-less solution.\n\n### Next Steps:\n1. **Technical Evaluation:** Conduct a thorough technical analysis to understand the integration points and necessary changes required to incorporate DSI.\n2. **Implementation Plan:** Develop an implementation plan with timelines, milestones, and responsibilities for integrating DSI into our service.\n3. **Stakeholder Communication:** Coordinate with relevant stakeholders to inform them of the decision and gather any additional requirements or concerns they may have regarding the transition.\n4. **Testing and Validation:** Implement and rigorously test the DSI integration to ensure it meets all necessary security and usability standards before rolling out to production.","GenTime":"2024-07-28 22:39:27"}
{"File Name":"buy-for-your-school\/0006-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n\n## Decision\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":1254,"Prediction":"## Decision\n\nWe have chosen to adopt the `dotenv` gem to manage our environment variables. `dotenv` offers centralized management for environment variables, ensuring that variables are loaded from a `.env` file into the ENV hash when the application starts. This approach helps us avoid direct references to ENV throughout the code and guards against missing environment variables by providing a clear source of truth.\n\nTo address the need for fail-fast behavior and explicit error messages when required environment variables are missing, we will implement a custom configuration wrapper. This wrapper will provide methods to access environment variables that will raise descriptive errors if a variable is not set. For instance, we will have methods like `Config.fetch('FOO')` that can raise an error if `FOO` is not defined, ensuring that our application fails fast with an explicit and understandable message.\n\nRegarding our testing needs, the configuration wrapper will also support functionality to stub environment variables easily within our test suite. This will allow us to simulate different environments without affecting the actual `.env` file or the global ENV hash. We will leverage functionality such as `allow(Config).to receive(:[](VAR_NAME)).and_return(VALUE)` to mock environment variables cleanly and without unintended side effects. \n\nTo summarize, the decision is:\n1. Implement the `dotenv` gem to centrally manage environment variables.\n2. Create a custom configuration wrapper that:\n    - Retrieves environment variables.\n    - Provides fail-fast behavior with explicit error messages when required variables are missing.\n3. Enhance the custom wrapper to support stubbing environment variables in the test suite to ensure isolated and predictable tests.\n\nThis solution aligns with our goals of maintaining supported tools, centralizing environment variable management, and facilitating robust and testable code.","GenTime":"2024-07-28 22:39:32"}
{"File Name":"buy-for-your-school\/0020-use-accessible-autocomplete-for-autocomplete-fields.md","Context":"## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n\n## Decision\n","Decision":"We will use [accessible-autocomplete](https:\/\/github.com\/alphagov\/accessible-autocomplete) to provide the autocomplete capability in our pages.\\nThis package has been chosen because accessibility has been carefully considered when developing the package.\\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\\nand not be jarring to the user.\\n","tokens":26,"id":1255,"Prediction":"## Decision\nWe will implement a client-side autocomplete feature using a combination of existing front-end frameworks and libraries (such as React or Vue.js) along with AJAX requests to query a backend service for suggestions. This approach will ensure a responsive user experience and reduce the load on the server by only querying when necessary.\n\n### Key Considerations\n1. **User Experience**: Ensure the autocomplete suggestions are relevant, quickly rendered, and displayed in a user-friendly manner.\n2. **Performance**: Implement efficient querying mechanisms to fetch only the necessary data and cache the results where possible to minimize server load.\n3. **Scalability**: Design the backend service to handle high volumes of query requests, possibly using a scalable search engine like Elasticsearch.\n4. **Security**: Implement appropriate security measures to validate and sanitize input data to prevent injection attacks and misuse.\n5. **Flexibility**: Ensure that the system is adaptable to different types of fields and data sets which might require autocomplete functionality.\n\n### Steps to Implement\n1. **Requirement Analysis**: Identify the fields that require autocomplete functionality and the type of data they need to suggest.\n2. **Library\/Framework Selection**: Choose a front-end framework compatible with the current tech stack (React, Vue.js, etc.).\n3. **Backend Service Development**: Develop a RESTful service endpoint that the front-end can query for suggestions based on user input.\n4. **UI\/UX Design**: Design the user interface for rendering autocomplete suggestions, ensuring it aligns with overall UI\/UX principles.\n5. **Implementation**: Integrate the front-end autocomplete component with the backend service using AJAX requests.\n6. **Testing**: Perform thorough testing to ensure suggestions are relevant, accurate, and perform well under various conditions.\n\n### Expected Outcomes\n- Improved user experience through faster data entry.\n- Reduced user errors by suggesting valid inputs.\n- Efficient and scalable suggestion querying system.","GenTime":"2024-07-28 22:39:37"}
{"File Name":"buy-for-your-school\/0009-use-simplecov-to-monitor-code-test-coverage.md","Context":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n\n## Decision\n","Decision":"Use Simplecov with RSpec to monitor coverage changes on every test run\\n","tokens":32,"id":1258,"Prediction":"## Decision\n\nImplement an automated testing framework to ensure high test coverage and execute tests consistently.\n\n1. **Continuous Integration (CI) Pipeline**: Set up a CI pipeline using tools like Jenkins, Travis CI, or GitHub Actions. The CI pipeline should automatically run tests on every commit, pull request, and before merging into the main branch to ensure changes do not introduce regressions.\n\n2. **Automated Testing Tools**: Leverage automated testing tools compatible with your technology stack. For instance, use Jest or Mocha for JavaScript, JUnit for Java, or PyTest for Python. These tools provide extensive libraries and frameworks for automated unit, integration, and end-to-end testing.\n\n3. **Code Coverage Reports**: Integrate code coverage tools such as Istanbul (for JavaScript), Jacoco (for Java), or Coverage.py (for Python) within the CI pipeline. These tools will generate code coverage reports that can be analyzed to ensure no critical parts of the codebase remain untested.\n\n4. **Static Analysis**: Use static code analysis tools to catch potential issues before they run. Tools like ESLint for JavaScript, SonarQube, or Checkstyle for Java can greatly aid in maintaining code quality and identifying areas that need more testing.\n\n5. **Regular Review and Maintenance**: Establish protocols for regular review of test cases and coverage reports. Ensure that adding tests is a part of the definition of done for all user stories or tasks.\n\nBy implementing these automated testing strategies within the CI pipeline, the team will maintain high test coverage, reduce the reliance on manual checks, and foster a culture of quality and reliability.","GenTime":"2024-07-28 22:39:49"}
{"File Name":"libelektra\/cryptograhic_key_handling.md","Context":"## Problem\\nThe crypto plugin applies cryptographic operations to Keys and KeySets.\\nIn order to do that it needs keys and initialization vectors (IV).\\nThe problem is how to retrieve or derivate those keys in a safe way and how\\nto pass them on to the underlying crypto libraries (OpenSSL and libgcrypt\\nat the time of writing).\\n\n## Decision\n","Decision":"### General Approach\\nThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.\\nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).\\nThis way we can safely derivate cryptographic keys for symmetric value encryption.\\nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).\\nThe PBKDF2 needs an iteration number and a salt in order to work.\\nThose values will be stored per Key as MetaKey.\\n### Implementation Details\\nDuring the **mount phase** a random master password _r_ is being generated. _r_ is sent to the gpg binary for encryption. The resulting encrypted master password _m_ is stored in the plugin configuration at `config\/masterChallenge`.\\nDuring the **set phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.\\nDuring the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be decrypted. The salt _s(k)_ is read from the encrypted message. By applying the PBKDF2 with _r_ and _s(k)_ the values of _e(k)_ and _i(k)_ are restored. Then the encrypted message can be decrypted.\\n","tokens":73,"id":1270,"Prediction":"## Context\nThe crypto plugin requires keys and initialization vectors (IV) to perform cryptographic operations on Keys and KeySets. Ensuring these keys and IVs are retrieved or derived securely and are safely transmitted to the underlying cryptographic libraries (OpenSSL and libgcrypt) is crucial for maintaining security.\n\n## Decision\n\n1. **Key and IV Storage Location**:\n   - **Secure Environmental Variables**: Keys and IVs will be stored in secure environment variables. This ensures they are not hardcoded or stored in plaintext in configuration files.\n   - **Hardware Security Module (HSM)**: For environments with higher security requirements, keys and IVs will be stored and managed by an HSM, ensuring physical and logical protection.\n\n2. **Key and IV Derivation**:\n   - **Password-Based Key Derivation Function (PBKDF2)**: A key derivation function such as PBKDF2 will be used to derive keys from passwords, ensuring derived keys have sufficient entropy.\n   - **Elliptic Curve Diffie-Hellman (ECDH)**: For key exchange, ECDH will be used to securely derive shared secret keys between parties.\n\n3. **Key and IV Transmission to Crypto Libraries**:\n   - **Secure Memory Management**: Keys and IVs will be loaded into memory securely (e.g., using secure memory allocation functions provided by the operating system or libraries like OpenSSL).\n   - **Encryption in Transit**: If keys must be transmitted across networked systems, Transport Layer Security (TLS) will be used to ensure keys are encrypted in transit.\n\n4. **Key Lifecycle Management**:\n   - **Key Rotation**: Implement a regular key rotation policy to reduce the risk of key compromise.\n   - **Key Revocation**: Establish procedures for revoking keys if they are suspected to be compromised.\n\n5. **Logging and Auditing**:\n   - **Minimal Logging of Sensitive Data**: Avoid logging keys and IVs. Instead, log only non-sensitive identifiers.\n   - **Audit Trails**: Maintain an audit trail of key access and usage to detect unauthorized access or anomalies.\n\n6. **Library Integration**:\n   - **Abstraction Layer**: Create an abstraction layer to handle the interface between the plugin and the underlying crypto libraries (OpenSSL and libgcrypt), allowing flexibility to switch libraries if necessary.\n   - **Consistent API Design**: Ensure the API design for key and IV handling is consistent across different crypto libraries for simplicity and maintainability.\n\n7. **Testing and Validation**:\n   - **Automated Security Testing**: Incorporate automated testing to ensure secure handling of keys and IVs.\n   - **Third-Party Audits**: Periodically undergo third-party security audits to validate the security of key management processes.\n\nBy implementing these measures, the crypto plugin can securely manage keys and IVs, ensuring robust cryptographic operations and protecting sensitive data from unauthorized access or exposure.","GenTime":"2024-07-28 22:40:39"}
{"File Name":"libelektra\/empty_files.md","Context":"## Problem\\nAn empty KeySet is passed to kdbSet(). What is the correct persistent\\nrepresentation?\\n\n## Decision\n","Decision":"Remove files on empty KeySet.\\n","tokens":23,"id":1271,"Prediction":"## Context\nThe problem at hand involves determining the correct way to persist an empty KeySet when it is passed to the `kdbSet()` function. This is critical as the behavior of persisting data correctly impacts the software's reliability, data integrity, and consistency.\n\n## Decision\nWhen an empty KeySet is passed to `kdbSet()`, the correct persistent representation should result in the deletion of any existing data at the specified location. This means that the current state of the KeySet should be fully reflective in the persistent storage.\n\n- If the corresponding file, directory, or data structure exists in the configuration storage backend, it should be removed.\n- This ensures that an empty KeySet is truly represented as \"no data\" at the given path in the persistent storage.\n- This approach avoids ambiguity and potential inconsistencies between the in-memory KeySet and its persisted state.\n\nBy following this decision, the software ensures the integrity of the configuration state, making sure that an empty in-memory representation results in a similar empty or non-existent state in the consistent storage backend. This is aligned with the principle of least surprise, where the lack of keys explicitly means the absence of associated data.","GenTime":"2024-07-28 22:40:42"}
{"File Name":"libelektra\/highlevel_help_message.md","Context":"## Problem\\nWe want to allow to print the help message no matter what errors happened in `kdbOpen` or `kdbGet`.\\n\n## Decision\n","Decision":"Ignore missing `require`d keys (in help mode), but fail for every other error.\\n","tokens":31,"id":1274,"Prediction":"## Decision\n\nImplement a centralized error handling mechanism that ensures the help message is printed regardless of any errors that occur in the `kdbOpen` or `kdbGet` functions. This can be achieved by introducing a global error handler that captures errors and triggers the display of the help message.\n\n### Steps to Implement:\n\n1. **Global Error Handler**: Develop a global error handling function that can intercept and manage errors from both `kdbOpen` and `kdbGet`.\n\n2. **Error Interception**: Modify `kdbOpen` and `kdbGet` to route any errors they encounter to the global error handler.\n\n3. **Help Message Display**: In the global error handler, ensure that any captured error triggers the display of the help message before the program exits or takes other actions.\n\n4. **Consistent Exit Strategy**: Make sure that after the help message is displayed, the application exits gracefully or recovers as needed, depending on the severity of the error.\n\n5. **Unit Testing**: Implement unit tests to simulate various error conditions in `kdbOpen` and `kdbGet` to ensure the help message is consistently displayed.\n\nBy following these steps, users will receive helpful guidance even when critical operations encounter issues, improving the overall user experience.","GenTime":"2024-07-28 22:40:54"}
{"File Name":"libelektra\/spec_metadata.md","Context":"## Problem\\nTo make storage-plugins suitable for `spec` they need to be able to store\\nall the metadata as specified in [METADATA.ini](\/doc\/METADATA.ini).\\nMost file formats do not have support for that.\\nIf metadata is merged from different namespaces, e.g., `spec:` and `user:`,\\nmetadata from one namespace might end up in keys of other namespaces, e.g.,\\nmetadata from `spec:` might end up in `user:`.\\n\n## Decision\n","Decision":"Do not store metadata unrelated to the configuration file structure in any namespace except in `spec:\/`.\\n- Trying to store any other metadata in any other namespace leads to an error.\\nE.g. `kdb set-meta user:\/data metadata_not_suitable_for_storage_plugins something` would fail\\n(validated by `spec` plugin).\\n- Metadata that is designed to be stored by storage plugins to preserve configuration file structure.\\nE.g. `comment` or `order`, might be stored in any namespace.\\nSometimes, the same metadata can be used in several namespaces but with different meanings\\nand ways of serialization, e.g. `type` and `array`:\\n- In `spec:\/` the metadata `array=` (empty value) means \"this is an array\".\\nIf you give it a value e.g. `array=#4` it means \"this is an array with default size X\" (e.g. `#4` = size 5).\\n- In any other namespace `array=` means \"this is an empty array\" and e.g. `array=#4` means \"this is an array with max index #4\".\\n`array=#4` is not stored literally but inferred.\\n- Either the storage plugin does not support arrays, then the metadata will be discarded\\non `kdbSet` but `spec` will keep on adding it for every `kdbGet`.\\n- Or, if the storage plugin supports arrays, the data will be serialized as array\\n(even if the metadata comes from `spec`) and as such available in the next `kdbGet`\\nfrom the storage plugin to be validated by `spec`.\\nUse different storage plugins, or plugins with different configurations,\\nfor the `spec:\/` namespace:\\n- `ni`\\n- TOML with `meta` configuration\\nThe `kdb mount` tool will add the `meta` plugin configuration when mounting a storage plugin to `spec:\/`.\\n- [Spec Expressiveness](spec_expressiveness.md)\\n- [Arrays](array.md)\\n","tokens":103,"id":1278,"Prediction":"## Decision\nTo address the need for storage-plugins to store all metadata as specified in [METADATA.ini], we will implement a two-phase approach:\n\n1. **Enhanced Metadata Handling**:\n   - Extend the storage-plugins to support a more generic and flexible metadata handling mechanism.\n   - Ensure that the storage-plugins can differentiate and encapsulate metadata from various namespaces (e.g., `spec:`, `user:`) separately without causing cross-contamination.\n\n2. **Namespace Isolation**:\n   - Introduce a namespace isolation feature within the storage-plugins.\n   - This isolation will ensure that metadata from different namespaces remains segregated and does not inadvertently mix.\n\n**Implementation Steps**:\n1. **Schema Update**: Update the metadata schema to include namespace identifiers.\n2. **Plugin Update**: Adjust storage-plugins to parse and store metadata with awareness to namespaces.\n3. **Validation**: Implement validation checks to ensure metadata from one namespace does not intrude into another during storage and retrieval processes.\n4. **Backward Compatibility**: Ensure that existing data remains compatible and can be migrated to the new system without loss or corruption of metadata.\n\nBy introducing these changes, we will maintain the integrity of metadata from different namespaces and adhere to the specifications mentioned in [METADATA.ini].","GenTime":"2024-07-28 22:41:12"}
{"File Name":"libelektra\/cmake_spec.md","Context":"## Problem\\nThe compilation variants of plugins blow up the number of plugins.\\nAdditionally there is the concept of default storage + resolver that is\\nneeded for bootstrapping plugins.\\n\n## Decision\n","Decision":"Rejected: keep default plugins as-is\\n","tokens":37,"id":1279,"Prediction":"## Decision\nTo simplify the plugin management and reduce the complexity caused by the numerous compilation variants, we will implement a modular plugin architecture with a focus on configurability and reusability. This decision includes the following key actions:\n\n1. **Modular Plugin System**: Refactor the current infrastructure to support a modular plugin system where each plugin can define its own configuration and dependencies. This will reduce redundancy and make it easier to manage and extend plugins.\n\n2. **Default Storage and Resolver Mechanism**: Implement a standardized default storage and resolver mechanism. This will provide a consistent interface for bootstrapping plugins, ensuring that they can be reliably initialized and integrated into the system without extensive custom configuration.\n\n3. **Configuration Profiles**: Introduce the concept of configuration profiles, allowing users to select different sets of plugins and settings based on their specific needs and environments. This will help manage different compilation variants more effectively.\n\n4. **Dependency Management**: Implement a robust dependency management system to handle inter-plugin dependencies. This will ensure that plugins are loaded in the correct order and their dependencies are available during initialization.\n\n5. **Documentation and Examples**: Provide comprehensive documentation and examples to guide developers on how to use the new modular system, including how to define plugins, set up configurations, and manage dependencies.\n\nBy adopting this decision, we aim to streamline the plugin system, reduce the overhead caused by multiple compilation variants, and provide a more flexible and maintainable solution for both users and developers.","GenTime":"2024-07-28 22:41:17"}
{"File Name":"libelektra\/internal_cache.md","Context":"## Problem\\nWhen doing kdbGet() possible more keys are returned which might be\\nconfusing. When doing a second kdbGet() with a new keyset\\nno keys might be returned, because it is up-to-date.\\nWhen doing kdbSet() a deep duplication is needed.\\nIdea: keep a duplicated keyset internally. Return (non-deep?)\\nduplications exactly of the parts as requested.\\n\n## Decision\n","Decision":"Not yet decided.\\n- [Global Validation](global_validation.md)\\n","tokens":90,"id":1289,"Prediction":"## Decision\n\nWe will implement an internal keyset duplication mechanism within the kdbGet() and kdbSet() functionalities. This decision involves the following specific actions:\n\n1. **Internal Keyset Duplication**: When kdbGet() retrieves keys, it will create and store a deep copy of the retrieved keyset internally.\n2. **Selective Duplication for Requests**: When clients request keys, the system will provide non-deep (shallow) duplicates of the relevant parts of the internally stored keyset, based on the specific request parameters.\n3. **kdbSet() Adjustments**: kdbSet() will operate on the internal deep copy to ensure data modifications do not directly affect the original keyset provided by the client, thereby maintaining data integrity. \n\nBy implementing these steps, we will address the issues of potential confusion caused by varying keysets returned during multiple kdbGet() calls and the need for a deep duplication in kdbSet(). The internal keyset will serve as a consistent reference point, ensuring that the system provides accurate and up-to-date information while reducing the overhead of unnecessary deep duplications.","GenTime":"2024-07-28 22:41:55"}
{"File Name":"libelektra\/store_name.md","Context":"## Problem\\nMemory Consumption in Elektra is quite high as the key names\\nare long and stored twice in memory.\\n\n## Decision\n","Decision":"Only store the unescaped key name, suitable for comparing\/searching\/iterating over name, i.e.:\\n- Remove the escaped name from `struct _Key` and use it only when necessary.\\n- Clarify and reduce [terminology](\/doc\/help\/elektra-glossary.md).\\n- API Changes:\\n- `keyNew (const char*, size_t)`\\n- `keyName` returns the unescaped name\\n- remove `keyUnescapedName`, `keyGetUnescapedNameSize`.\\n- reverse terminology: with \"key name\" we will refer to the unescaped (base) name,\\nthe escaped name will be explicitly called \"escaped key name\".\\n- escaped name will be outside the core for tooling\\n- `keyDup(.., int)` with options to filter which parts are copied\\n(to allow copy of keys where only the key name is copied)\\n","tokens":26,"id":1291,"Prediction":"## Decision\nTo address the elevated memory consumption in Elektra due to long key names being stored twice, the following actions will be taken:\n\n1. **Unicode Compression**: Implement a compression algorithm specifically optimized for key names. By using a more compact encoding, we can significantly reduce the memory footprint.\n\n2. **Shared Memory Design**: Introduce a shared memory approach where the key names are stored in a centralized location and referenced through pointers, rather than duplicating the strings.\n\n3. **Efficient Data Structures**: Adopt more memory-efficient data structures for storing key names, such as Trie or Radix Trees, which inherently reduce redundancy by sharing common prefixes.\n\n4. **On-the-Fly Encoding**: Enable on-the-fly encoding and decoding of key names, where key names are encoded when stored and decoded when accessed, ensuring that they take up less space in memory.\n\n5. **Key Name Hashing**: Create a hashing mechanism that can represent key names as shorter, fixed-length hash values with collision management, saving space by not storing the full key names directly.\n\nThis combined approach should effectively reduce the overall memory consumption of Elektra without compromising the performance or functionality of the system.","GenTime":"2024-07-28 22:42:02"}
{"File Name":"libelektra\/error_handling.md","Context":"## Problem\\nThere are ambiguous cases where the same return value can refer to multiple problems:\\n- name modifications which can be either invalid name or locking the key name\\n- getting values of (non-)binary keys\\n\n## Decision\n","Decision":"- Update documentation in `doc\/dev\/error-*` and link to them in the documentation\\nfor the module `kdb`\\n- Add second channel for getting information about errors\\n- Return error codes directly from functions where failures are expected, e.g. `kdbGet`, `keySetName`\\n- Harmonize return values from all functions and move error reporting to second channel\\n- Binary metadata vs flag #4194\\n","tokens":44,"id":1292,"Prediction":"## Context\nIn our software system, there are several scenarios where the same return value from a function or method can refer to multiple types of problems. This ambiguity creates confusion for both the users and developers, leading to potential misunderstandings and bugs. For example:\n- When modifying names, the same error code might be used to indicate either an invalid name format or that the key name is locked and cannot be modified.\n- When retrieving values, the same return value might be used for both binary and non-binary keys, making it unclear what type of data is being returned.\n\nTo address this problem, we need to implement a clear and unambiguous way to differentiate between these different cases.\n\n## Decision\nTo eliminate ambiguity and improve clarity, the following decisions have been made:\n\n1. **Distinct Error Codes and Return Values**:\n   - Introduce distinct error codes for different error scenarios. Instead of using a single error code for both an invalid name and a locked key name, separate codes will be used (e.g., `ERR_INVALID_NAME` and `ERR_KEY_NAME_LOCKED`).\n   - Use different return values or data types for binary and non-binary keys. For example, introduce a wrapper object that explicitly specifies whether the returned value is binary or non-binary.\n\n2. **Enhanced Documentation and Comments**:\n   - Provide comprehensive documentation for all functions and methods, detailing what each return value and error code represents. This documentation should be easily accessible to all developers.\n   - Include inline comments within the code to explain any complex logic that determines the return value or error code, ensuring that future developers understand the intent.\n\n3. **Consistent Naming Conventions**:\n   - Adopt consistent naming conventions for error codes, return values, and variables to reduce confusion. For example, error codes could follow a pattern like `ERR_<CONTEXT>_<ERROR_TYPE>`.\n   - Ensure all team members are familiar with these conventions and use them consistently across the codebase.\n\n4. **Comprehensive Testing**:\n   - Implement thorough unit tests and integration tests to verify that the correct error codes and return values are being used in all scenarios.\n   - Include edge cases and ambiguous situations in the tests to ensure that the system behaves as expected and returns unambiguous information.\n\nBy implementing these decisions, we aim to make the software more robust, easier to understand, and less prone to errors caused by ambiguity in return values and error codes.","GenTime":"2024-07-28 22:42:07"}
{"File Name":"libelektra\/boolean.md","Context":"## Problem\\nInconsistent use of booleans in various parts of Elektra.\\n\n## Decision\n","Decision":"Only the strings `0` and `1` are allowed in the `KeySet` for `type = boolean`, for both values and defaults.\\nEverything else should lead to errors in checkers (in `kdbSet`).\\nA spec with `type = boolean` without a specified default should be interpreted as `default = 0`.\\nExample for an implementation in C in an application:\\n```c\\nif (k != NULL && strcmp(keyString(k), \"1\") == 0) {\/*true*\/} else {\/*false*\/}\\n```\\nStorage plugins are allowed any representation as suitable, e.g., a JSON plugin might render `1` as `true`.\\nThe type checker plugin should allow\\n- non-presence\\n- the string \"0\"\\n- the string \"1\"\\n","tokens":18,"id":1293,"Prediction":"## Problem\nInconsistent use of booleans in various parts of Elektra.\n\n## Context\nElektra is a configuration management framework that allows users to access configuration settings in a uniform way. Different parts of the Elektra codebase use different representations and conventions for boolean values, which leads to confusion, maintenance difficulties, and potential errors.\n\n## Decision\nStandardize the use of boolean values across the entire Elektra codebase to a single representation and convention. Specifically:\n\n1. **Boolean Data Type**: Use a single boolean data type `bool` consistently throughout the codebase, leveraging a commonly accepted definition provided by a library or language native support.\n\n2. **Boolean Values**: Adopt `true` and `false` as the universal boolean literals. If the language\/library being used supports multiple boolean representations, only use these two.\n\n3. **Configuration Syntax**: Ensure that in configuration files, boolean values are always represented using a standard syntax such as `true` and `false`.\n\n4. **Documentation and Examples**: Update all documentation, examples, and guides to reflect the standardized boolean usage.\n\n5. **Code Refactoring**: Conduct an initial review and refactor existing code to replace inconsistent boolean representations with the standard ones.\n\n6. **Coding Standards**: Update coding standards and perform regular code reviews to ensure adherence to the new boolean conventions.\n\nBy adopting a consistent approach to boolean values, Elektra can minimize confusion, reduce potential errors, and improve maintainability across the entire codebase.","GenTime":"2024-07-28 22:42:11"}
{"File Name":"libelektra\/error_semantics.md","Context":"## Problem\\nWhile we have a classification of errors and warnings, it remains\\nunclear when plugins actually should emit errors and warnings.\\n\n## Decision\n","Decision":"Provide guidelines in the form as tutorials, covering:\\n- prefer errors to warnings\\n- that any not understood metadata (e.g. types), should lead to an error\\n- that wrong specifications, like `kdb meta-set \/tests\/ipaddr\/ipv4 check\/ipaddr ipv8` should be rejected\\n- if the value does not confirm **exactly** to the specified type, an error should be emitted\\n(e.g. only `0` or `1` as boolean)\\n- anything else that is beyond the capabilities of a plugin (not implemented),\\nshould lead to an error\\nViolations against these guidelines can be reported as bug and then either:\\n- the bug gets fixed\\n- the plugin get a worse `infos\/status` but still get shipped with 1.0\\n- the plugin gets removed\\n- [Metadata in Spec Namespace](spec_metadata.md)\\n- [Capabilities](capabilities.md)\\n- [Boolean](boolean.md)\\n","tokens":28,"id":1302,"Prediction":"## Decision\nEstablish a Clear Policy for Emitting Errors and Warnings\n\nIn order to address the ambiguity about when plugins should emit errors and warnings, we will establish a clear policy that defines specific criteria and guidelines. This policy will ensure consistency across plugins and improve the overall reliability and user experience of the system. Here are the key components of the decision:\n\n1. **Error Definition**: \n   Errors should be emitted in situations where the plugin is unable to continue its operation or where data integrity might be compromised. This includes, but is not limited to:\n   - Critical failures that prevent the plugin from initializing or functioning.\n   - Scenarios where the plugin encounters an undefined state or unhandled exceptions.\n   - Security vulnerabilities or breaches detected by the plugin.\n\n2. **Warning Definition**:\n   Warnings should be emitted for non-critical issues that do not prevent the plugin from functioning but might lead to suboptimal performance or other minor problems. This includes, but is not limited to:\n   - Deprecated API usages or features that will be removed in future versions.\n   - Non-critical performance issues.\n   - Minor violations of best practices or coding standards.\n\n3. **Guidelines for Emission**:\n   - **Consistency**: Plugins must follow a consistent format for error and warning messages, including a unique identifier, a brief description, and detailed information about the context.\n   - **Severity Levels**: Define a severity level for each error and warning to help users easily prioritize and address them.\n   - **User Documentation**: Provide clear documentation and guidance on what each error and warning means, and how users can respond to or mitigate these issues.\n   - **Logging and Reporting**: Implement logging mechanisms to capture and report errors and warnings efficiently, facilitating troubleshooting and analysis.\n\n4. **Implementation Plan**:\n   - Review and categorize existing errors and warnings within the system and map them to the newly defined criteria.\n   - Update plugin development guidelines to include the new policy.\n   - Train developers and provide resources to ensure adherence to the new policy.\n   - Continuously monitor and review emitted errors and warnings to refine and improve the policy over time.\n\nBy implementing this policy, we aim to provide a clear and actionable framework for handling errors and warnings in plugins, ultimately leading to a more robust and maintainable software system.","GenTime":"2024-07-28 22:42:46"}
{"File Name":"libelektra\/warning_array.md","Context":"## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n\n## Decision\n","Decision":"The format should be aligned with the correct array notation,\\nstarting with `#0`. The maximum number of warnings will stay at\\n100 entries (`#0` - `#_99`).\\n- [Array](array.md)\\n","tokens":46,"id":1303,"Prediction":"## Decision\nSwitch from the elektra non-conforming array notation (`#00`, `#01`, etc.) to a compliant notation that aligns with the design decision outlined in the array.md documentation. This will involve:\n\n1. Refactoring the code to use the compliant array notation.\n2. Ensuring that the new implementation supports more than 100 entries, thereby removing the current limitation.\n3. Updating any related documentation to reflect the changes in array notation.\n4. Conducting thorough testing to ensure that the new notation works as expected and does not introduce regressions or errors in warning handling.\n\nThis change is crucial for maintaining consistency across the codebase and adhering to established design principles.","GenTime":"2024-07-28 22:42:49"}
{"File Name":"libelektra\/elektra_web.md","Context":"## Problem\\nFor Elektra Web, there needs to be a way to remotely manage instances and groups\\nof instances (clusters). The remote configuration of a single instance is\\nsimple. However, to manage multiple instances, we need to store the information\\nto access the daemons, as well as information about the grouping (clusters) of\\ndaemons.\\n\n## Decision\n","Decision":"Use one cluster daemon (clusterd) to manage all clusters and instances.\\n- [Elektra Web Recursive Structure decision](elektra_web_recursive.md)\\n","tokens":75,"id":1306,"Prediction":"## Decision\n\nTo address the need for remotely managing instances and groups of instances (clusters) in Elektra Web, we will implement a centralized management system using a dedicated configuration server.\n\nThe configuration server will have the following roles and features:\n\n1. **Centralized Storage**: A central repository where all the information about instances and instance groups (clusters) will be stored. This will include the access details for each daemon and the metadata required to define and manage clusters.\n\n2. **REST API**: The server will expose a RESTful API to allow CRUD (Create, Read, Update, Delete) operations on instances and clusters. This will enable external systems or tools to interact programmatically with the management system.\n\n3. **Security**: Implement robust authentication and authorization mechanisms to restrict access to the configuration server. This ensures only authorized users or systems can manage the instance configurations and clusters.\n\n4. **Scalability**: Design the server to handle a large number of instances and clusters efficiently. This may involve using a scalable database backend and appropriate load-balancing strategies.\n\n5. **User Interface**: Develop a web-based user interface that allows administrators to easily manage instances and clusters visually. This interface will provide dashboards, instance grouping functionalities, and detailed instance configuration views.\n\n6. **Extensibility**: Ensure the system is modular and can be extended in the future to incorporate additional functionalities, such as advanced monitoring, reporting, and alerting mechanisms.\n\nBy implementing these features in a centralized configuration server, we will facilitate effective and scalable management of individual instances and clusters in Elektra Web. This solution will ensure that administrators have the necessary tools to manage the entire ecosystem of daemons efficiently and securely.","GenTime":"2024-07-28 22:43:00"}
{"File Name":"libelektra\/holes.md","Context":"## Problem\\nConfig files ideally do not copy any structure if they only want to\\nset a single key.\\n\n## Decision\n","Decision":"Support holes and values for non-leaves in a KeySet if the underlying format allows it.\\nIf the underlying format does not support it and there is also not an obvious\\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\\nvalues in non-leaves can be supported with key names starting with \u00aeelektra.\\n","tokens":23,"id":1307,"Prediction":"## Decision\nAdopt a hierarchical configuration management approach that allows partial configurations to set or override individual keys without needing to replicate the full configuration structure. This can be achieved by implementing a mechanism that merges the partial configurations with the default or base configurations.\n\n### Rationale\n1. **Simplicity:** Users can easily modify or override a specific configuration key without dealing with the complexity of the entire configuration file.\n2. **Maintainability:** Reduces redundancy and makes the configuration management process more maintainable and less error-prone.\n3. **Flexibility:** Allows for modular configurations that can be combined as needed, enabling easier deployment across different environments and scenarios.\n4. **Ease of Use:** Encourages a more user-friendly approach as less experienced users can make necessary changes by specifying only the necessary keys.","GenTime":"2024-07-28 22:43:02"}
{"File Name":"libelektra\/ingroup_removal.md","Context":"## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n\n## Decision\n","Decision":"The `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n","tokens":83,"id":1308,"Prediction":"## Decision\nRemove the `Ingroup:` line from error and warning messages.\n\n### Rationale:\n1. **User Value**: The `Ingroup:` information (whether the error is from `kdb`, `module`, or `plugin`) has been identified as not providing significant value to the end user.\n2. **Verbosity**: Keeping this message increases the verbosity without contributing meaningful information, potentially obfuscating more important parts of the error or warning details.\n3. **Streamline Messaging**: By removing this line, error and warning messages become cleaner and more focused, improving overall readability and user experience.\n\n### Implementation:\n- Update the error and warning message templates within the elektra codebase to omit the `Ingroup:` line.\n- Ensure that any internal logging or debugging tools that utilized this information are also updated accordingly, if necessary.\n\n### Testing:\n- Conduct a review of various error and warning scenarios to ensure that the updated messages are clear and unambiguous without the `Ingroup:` line.\n- Solicit feedback from a subset of end users or testers to confirm that the change improves message clarity without losing important context.\n\n### Considerations:\n- Document the change in the release notes to make end users aware of the less verbose error and warning messages.\n- Monitor user feedback post-deployment to detect any unforeseen issues or requests for additional context.","GenTime":"2024-07-28 22:43:05"}
{"File Name":"libelektra\/reference_counting.md","Context":"## Problem\\n- locking is not reset when ref counting again gets 0 (adding to keyset and\\npop again) #2202\\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\\nfor KeySets (also suggested in #1332)\\n\n## Decision\n","Decision":"- add second counter to Key\\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\\n- return error on reference count overflow\\n- no error on underflow (decrement when zero), instead stay at zero\\n- use fixed sized types (`uint16_t`) for reference counters\\n- increment\/decrement references before\/after passing instances to plugins\\n","tokens":59,"id":1309,"Prediction":"## Decision\n### Lock and Reference Counting Improvements for Key and KeySet in Elektra\n\nAfter thorough consideration of the issues surrounding lock resetting and reference counting in Elektra's KeySets, the following decisions have been made to improve the consistency and functionality of the C++ API:\n\n1. **Reset Locking on Zero Reference Count**: Implement functionality to reset any locks on Key or KeySet objects when their reference count drops to zero. This aims to address the locking issues identified in #2202.\n2. **Unified Reference Counting for KeySet**: Introduce reference counting for `KeySet` objects analogous to `Key` objects. This change will bring a consistent approach to reference management across both entities, as discussed in #1332.\n3. **C++ API Alignment**: Ensure that the C++ API for both `KeySet` and `Key` provides a cohesive experience by implementing unified reference counting semantics.\n\nThese decisions will be implemented in such a manner to ensure backward compatibility and will require comprehensive testing to validate expected behaviors across existing functionalities.","GenTime":"2024-07-28 22:43:08"}
{"File Name":"libelektra\/lookup_every_key.md","Context":"## Problem\\nOn structures like maps or [arrays](array.md) there are different\\npossibilities which keys are looked up in the KeySet and which\\nare simply iterated.\\nWithout any guidelines, applications would provide arbitrary inconsistent\\nbehavior.\\n\n## Decision\n","Decision":"Every key that an application wants to use, must be looked up\\nwith `ksLookup` using a cascading lookup key.\\n- [Arrays](array.md)\\n","tokens":51,"id":1310,"Prediction":"## Decision\nTo achieve consistent behavior across applications when dealing with structures like maps or arrays, we will establish the following guidelines:\n\n1. **Key Lookup vs. Iteration:**\n   - For maps, keys that are inherently part of the business logic and are expected to be accessed frequently should be explicitly looked up in the KeySet.\n   - For arrays, indices should be used primarily to iterate over elements unless specific array positions need frequent and direct access.\n\n2. **Standardizing Key Access:**\n   - Define a clear schema or interface specifying which keys are available for lookup in different data structures.\n   - Use consistent naming conventions for keys across different modules and components to minimize confusion and ensure interoperability.\n\n3. **Documentation and Communication:**\n   - Ensure that the schema or interface definitions are well-documented and communicated to the development team.\n   - Provide examples in the documentation that illustrate proper use cases for key lookup and iteration.\n\n4. **Tool Support:**\n   - Implement tools or utilities that facilitate adherence to these guidelines. For example, helper functions for key lookups in maps and common iteration patterns for arrays can be provided as part of a shared library.\n\n5. **Code Review and Quality Assurance:**\n   - Enforce these guidelines through code reviews. Ensure that developers follow the established practices when adding new features or refactoring existing code.\n\nBy following these guidelines, we aim to reduce arbitrary and inconsistent behavior across applications and improve the robustness and maintainability of the codebase.","GenTime":"2024-07-28 22:43:12"}
{"File Name":"libelektra\/null_pointer_checks.md","Context":"## Problem\\nCurrently all functions do proper argument checking which might degrade\\nperformance.\\n\n## Decision\n","Decision":"Rejected (keep checks) due to time constraints\\n","tokens":17,"id":1317,"Prediction":"## Decision\nTo improve performance, we will implement a selective argument checking strategy. This strategy will involve the following steps:\n1. **Critical Functions:** Continue to check arguments for critical functions where the risk of incorrect usage could lead to severe issues.\n2. **Performance-Sensitive Functions:** Reduce or eliminate argument checking for functions heavily used in performance-sensitive paths, assuming the correctness of inputs based on usage patterns and prior validation steps.\n3. **Internal vs. External APIs:** Maintain rigorous argument checking for external APIs, while internal API argument checking will be minimized as internal code can be trusted more to abide by the expected contracts.\n4. **Testing and Validation:** Enhance unit and integration tests to ensure inputs are validated during the testing phase, reducing the need for runtime argument checks.\n\nBy adopting this selective approach, we aim to strike a balance between maintaining software robustness and optimizing performance.","GenTime":"2024-07-28 22:43:43"}
{"File Name":"libelektra\/capabilities.md","Context":"## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos\/status`, others were not.\\n\n## Decision\n","Decision":"Add `infos\/features\/storage` to document limitations of storage plugins.\\nIdeally, storage plugins should throw an error in `kdbSet` for\\nunrepresentable KeySets.\\nElektra cannot guarantee that any configuration file format can\\nbe mounted anywhere.\\nDevelopers, maintainers and administrators are responsible for what\\nthey mount. They need to test the setup.\\n- [Base Name](base_name.md)\\n","tokens":66,"id":1318,"Prediction":"## Decision\nTo address the problem of representing any KeySet and handling limitations of different storage plugins, we have decided to implement a two-fold solution:\n\n1. **Enhanced Documentation and Status Annotations:** \n   - Ensure that all limitations of storage plugins are thoroughly documented. This includes updating the `infos\/status` field for each storage plugin with detailed information about the restrictions and limitations.\n   - Create a comprehensive compatibility matrix that maps out which types of configurations can be represented by each plugin. This matrix should be easily accessible in the documentation.\n\n2. **Development of a Meta-Plugin Layer:**\n   - Introduce a meta-plugin layer that can dynamically select the appropriate storage plugin based on the structure of the KeySet. This layer will analyze the KeySet's structure and choose the most suitable plugin (e.g., `dump`, `quickdump`, or any other specialized storage plugin) for serialization and deserialization.\n   - The meta-plugin will handle the delegation and fallback mechanisms, ensuring that if the chosen plugin encounters an unsupported structure, another compatible plugin can take over.\n\nThese actions will ensure better flexibility and reliability in representing any KeySet, along with clear communication of each storage plugin's capabilities and limitations.","GenTime":"2024-07-28 22:43:45"}
{"File Name":"libelektra\/error_codes.md","Context":"## Problem\\nThe current error concept has disadvantages in following regards:\\n- A lot of redundant errors\\nAt the moment, each new plugin introduces new error codes which led to about 210+ error codes. Many of those errors\\nare duplicated because developers did not know or search for a similar error which is already present. This concept should\\ngroup similar errors together so that there is one coherent and consistent state again.\\n- Hard to manage specification file\\nSince every developer adds its own error individually, a lot of merge conflicts happen which makes contributing to the codebase\\nunpleasant. Additionally, if you want to reuse any error you have to scrape to the whole file with ~1300+ lines. As there is no\\nsenseful ordering or scheme behind the errors (since they grew by time), it is a hassle to find the correct error code.\\nThe new concept should standardize errors, making it easy to categorize errors from new plugins and avoid merge conflicts.\\n- No senseful way for application developers to use error codes from elektra\\nIf developers of plugins\/ external tools using elektra want to react to errors, they have to be very specific. At the moment there is\\nno possibility to catch all errors easily which force a certain behavior. Eg. if there happens a temporary recoverable error, developers have to\\ncatch for every specific error code rather than a general hierarchical error. The new concept should make it easy to react to errors as they are\\nsensefully grouped together and are hierarchically structured.\\n\n## Decision\n","Decision":"All \"fatal\" errors will be converted to \"errors\" as the distinction is not relevant.\\nUnused errors will be removed from the specification.\\nErrors will be categorized into logical groups with subgroups.\\nEach error will be made up of 5 characters, where the first 2 character indicate the highest level\\nand character 3 to 5 will be used for subgrouping. Errors are prepended with the letter `C` which\\nis the abbreviation for \"Code\".\\n- Permanent errors C01000\\n- Resource C01100\\n- Out of Memory C01110\\n- Installation C01200\\n- Logical C01300\\n- Internal C01310\\n- Interface C01320\\n- Plugin Misbehavior C01330\\n- Conflicting State C02000\\n- Validation C03000\\n- Syntactic C03100\\n- Semantic C03200\\nTo see an explanation of the categories along with a guideline on how to categorize please\\nsee the [Error Codes Guideline](..\/dev\/error-categorization.md)\\n- [Error Message Format](error_message_format.md)\\nShows the new format of the error message\\n- [Error Codes Guideline](..\/dev\/error-categorization.md)\\nShows how to categorize errors\\n","tokens":312,"id":1319,"Prediction":"## Decision\n\nTo address the outlined concerns, we will adopt a hierarchical and standardized error handling approach. This approach will include the following key steps and elements:\n\n1. **Hierarchical Error Grouping**:\n    - Develop a hierarchical error code system where errors are grouped into distinct categories and subcategories. For example, all I\/O related errors could be under a general I\/O error category, with subcategories for file, network, etc.\n    - Define a finite set of top-level error categories to reduce redundancy and simplify error management (e.g., `IOError`, `ValidationError`, `ConfigurationError`).\n\n2. **Error Code Registry**:\n    - Maintain a centralized error code registry with a clear, documented schema. The registry should include unique error codes, their hierarchical category, a short description, and potential causes.\n    - Implement a review process for adding new error codes to ensure consistency and avoid duplication. This should involve a designated team or individual responsible for overseeing error code assignments.\n\n3. **Error Code Guidelines**:\n    - Develop and distribute guidelines for error code definition and usage to all developers. The guidelines should emphasize the importance of reusing existing error codes whenever applicable and should offer best practices for introducing new ones.\n\n4. **Error Specification File Management**:\n    - Break down the giant error specification file into smaller, logically organized files corresponding to the top-level hierarchical categories.\n    - Use a modular inclusion approach in the main specification file to include these smaller files. This modularization will minimize merge conflicts and make it easier to locate specific errors.\n    - Automate the process of integrating these modular files into the main specification file via a build script or pre-commit hook that ensures all error codes are included properly.\n\n5. **Error Handling Framework**:\n    - Implement a robust error handling framework within the system's core libraries which supports catching errors at different hierarchical levels. This framework should allow plugin developers to handle general error categories and specific error codes.\n    - Provide consistent error handling utilities that can catch and propagate errors based on their category, making it straightforward for application developers to implement error handling logic.\n\n6. **Documentation and Training**:\n    - Create comprehensive documentation on the new error handling methodology, including example code snippets, to ease the onboarding transition for developers.\n    - Conduct training sessions or workshops to ensure all developers understand the new error concept and how to adhere to it.\n\nBy following these steps, we aim to reduce redundancy, streamline error management, and facilitate error handling for developers, ultimately leading to a more maintainable and robust system.","GenTime":"2024-07-28 22:43:52"}
{"File Name":"cfo\/0002-use-yarn-install-instead-of-npm-install-for-web.md","Context":"## Context\\nI see found both `yarn.lock` and `package-lock.json` in the repository.\\n\n## Decision\n","Decision":"Sticking to `yarn` for `web` for now.\\n","tokens":23,"id":1320,"Prediction":"## Decision\n\nTo ensure consistency and prevent potential conflicts in dependency management, we have decided to standardize the repository to use only one package manager lockfile. We will use Yarn as the primary package manager. Consequently, we will:\n\n1. **Remove `package-lock.json`:** This file will be deleted from the repository to avoid any confusion and conflicts between npm and Yarn.\n2. **Keep `yarn.lock`:** This file will be the authoritative lockfile for managing dependencies in the project.\n\nAll contributors will be instructed to use Yarn for installing dependencies and managing their development environment. This will be documented in the project's README file and enforced through code reviews and CI\/CD pipelines.","GenTime":"2024-07-28 22:43:55"}
{"File Name":"molgenis-service-armadillo\/0006-use-rdata-format-as-data-input.md","Context":"## Context\\nWe want to make the MOLGENIS \"Armadillo\" service data provider agnostic. There are a couple of reasons why we are doing this\\n- the service is usable for other parties as well\\n- the service can still integrate with MOLGENIS.\\n- the release cycle of the service is data provider independent\\n- the service can be developed by other parties as well\\n\n## Decision\n","Decision":"We implement an endpoint to upload and load RData files in the MOLGENIS \"Armadillo\" service to manage data for the use in DataSHIELD.\\n","tokens":83,"id":1326,"Prediction":"## Decision\nWe have decided to refactor and abstract the MOLGENIS \"Armadillo\" service to be completely data provider agnostic. This will involve:\n\n1. **Modular Design:** \n   - Implement a plugin-based architecture where data provider-specific components can be dynamically added or removed without affecting the core functionalities of the service.\n   - Define clear and consistent interfaces for data providers to ensure seamless integration and interoperability.\n\n2. **Decoupling Data Provider Logic:** \n   - Extract and isolate data provider-specific logic from the core library. This means creating separate modules or services to handle interactions with different data providers.\n   - Utilize dependency injection to allow the core service to interact with various data providers without hard-coded dependencies.\n\n3. **Unified Data Access Layer:** \n   - Introduce an abstraction layer that provides a uniform API for data access and manipulation, regardless of the underlying data provider.\n   - Standardize the communication protocols and data formats used, ensuring that changes in one data provider\u2019s implementation do not adversely impact other parts of the service.\n\n4. **Testing and Validation:** \n   - Implement comprehensive unit and integration tests to ensure that the service behaves correctly with different data providers.\n   - Create mock data provider modules for testing purposes, ensuring that new providers can be easily tested without requiring access to actual data sources.\n\n5. **Documentation and Support:** \n   - Provide detailed documentation and guidelines for third-party developers to integrate their data providers with the MOLGENIS \"Armadillo\" service.\n   - Foster a community-driven approach by encouraging contributions and feedback from external developers and organizations.\n\nBy adopting this approach, we ensure that the MOLGENIS \"Armadillo\" service remains flexible, extensible, and maintainable, while also promoting wider adoption and quicker feature release cycles.","GenTime":"2024-07-28 22:44:06"}
{"File Name":"molgenis-service-armadillo\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":1328,"Prediction":"## Decision\n\nImplement an Architecture Decision Record (ADR) Process\n\n**Summary:**\nTo ensure that all architectural decisions are documented systematically, we will implement an Architecture Decision Record (ADR) process. This process will provide a clear structure for recording the context, consequences, and rationale of each decision, ensuring transparency and aiding future project stakeholders in understanding why certain decisions were made.\n\n**Details:**\n\n1. **Template Adoption:** \n   - Adopt a standardized ADR template. This will include sections such as Title, Status, Context, Decision, and Consequences to maintain consistency across records.\n\n2. **Documentation Repository:**\n   - Create a dedicated repository (could be a folder within the project\u2019s documentation section or a separate repository) where all ADRs will be stored. This makes it easy to access, review, and update records.\n\n3. **Decision-making Protocol:**\n   - Define a protocol for when an ADR should be created. For example, any decision that significantly impacts the system's architecture, design, and functionality should be documented.\n\n4. **Version Control Integration:**\n   - Utilize version control (e.g., Git) to manage changes to ADRs. This helps in tracking the evolution of decisions over time.\n\n5. **Review Process:**\n   - Establish a review process where architectural decisions are periodically reviewed by the architecture team or relevant stakeholders. This step ensures ongoing relevance and accuracy of documented decisions.\n\n6. **Communication:**\n   - Communicate the establishment of the ADR process to the entire project team. Encourage contributions and emphasize the importance of maintaining these records for future project needs.\n\n7. **Tool Support:**\n   - Consider the use of tools or plugins that support ADRs if beneficial for the team's workflow (e.g., Madge for repositories, Markdown editors, etc.).\n\nBy implementing this ADR process, we create a sustainable practice for capturing essential architectural decisions, facilitating better decision-making, and promoting project understanding and continuity.","GenTime":"2024-07-28 22:44:22"}
{"File Name":"dilay\/0003-use-compile-api-to-transfter-class.md","Context":"## Context\\nhttps:\/\/github.com\/microsoft\/TypeScript\/wiki\/Using-the-Compiler-API\\nhttps:\/\/github.com\/dsherret\/ts-morph#readme\\n> TypeScript Compiler API wrapper. Provides an easier way to navigate and manipulate TypeScript and JavaScript code.\\n\n## Decision\n","Decision":"Decision here...\\n","tokens":54,"id":1337,"Prediction":"## Decision\n\nWe have decided to utilize the ts-morph library alongside the TypeScript Compiler API for our project. This decision has been made based on the following considerations:\n\n1. **Ease of Use**: ts-morph offers a higher-level API that simplifies interactions with the TypeScript and JavaScript code, reducing complexity and boilerplate code.\n\n2. **Documentation and Support**: The ts-morph library has comprehensive documentation and a proven track record within the developer community, ensuring better support and resources for our team.\n\n3. **Efficiency**: By leveraging ts-morph, we anticipate increased developer productivity and more efficient code manipulation, which aligns with our project's goals.\n\n4. **Compatibility**: ts-morph is built on top of the TypeScript Compiler API, ensuring that we can still leverage the full capabilities of the underlying API when necessary.\n\nIn conclusion, adopting ts-morph will provide a more streamlined and efficient way to navigate and manipulate our TypeScript and JavaScript codebase, resulting in improved development workflow and maintainability.","GenTime":"2024-07-28 22:44:48"}
{"File Name":"Wikibase\/0009-refactor-hooks-for-testability.md","Context":"## Context\\nCurrently, the `RepoHooks` class remains largely untested due to a combination of two factors:\\n1. The methods in this class are static, and we do not own the contract under which they should be called, as they are\\ndefined as hooks in `extension.json` or as global variables in the entrypoints e.g. [extensions\/Wikibase\/repo\/Wikibase.php:1020](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/7b20d22b3c0bbc37ad23f63e38fadc9b1f2ca057\/repo\/Wikibase.php#L1020), which means we cannot easily refactor the methods to increase testability\\n2. Methods rely heavily on the `WikibaseRepo` singleton and it's store, which make it harder to test, as there is no\\nway to to inject a mock of `WikibaseRepo` without dependency injection.\\nA [RFC for enabling dependency injection](https:\/\/phabricator.wikimedia.org\/T240307) in hooks is currently under way.\\nHowever, an interim solution is needed in order to mitigate the amount of untested logic that exists in that file\\nand other places in the codebase.\\nWhile reviewing this issue, two initial solutions were considered:\\n- Refactor `RepoHooks` into a singleton itself, so that when instantiated, we can inject a Mock of `WikibaseRepo`\\ninstead of using the real deal.\\n- Adopt a pattern used in `WikibaseClient` Which enables us to mock several parts of it (namely the store), and replace\\nthe real store by creating an `overrideStore` method. See in following:\\n- [`client\/tests\/phpunit\/includes\/MockClientStore.php`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/MockClientStore.php)\\n- [`client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php:42`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php#L42)\\nHowever, after a discussion, it was decided to implement a middle ground, that would enable us to gradually refactor\\nhooks, rather than a one time big change.\\n\n## Decision\n","Decision":"It was decided to adopt an existing pattern in Wikibase repo where each hook handler gets its own singleton class and\\nprovides at least four methods:\\n- A constructor to make dependency injection easier.\\n- A public static method to bootstrap an instance from global settings. For consistency, this would typically be named\\n`newFromGlobalSettings`.\\n- (optional) A public static method to get a cached instance of the handler object (rather than instantiate it each time):\\nThis is useful for hooks handlers which are called several times.\\n- A public static method to wire up functionality into the hooks system and should contain little to no logic (as it is\\nhard to test without complete integration or e2e tests).\\n- A public method to perform the actual handler logic in a testable way.\\nThis class should be placed under the `includes\/Hooks` directory. An example of this pattern can be seen in:\\nhttps:\/\/gerrit.wikimedia.org\/r\/c\/mediawiki\/extensions\/Wikibase\/+\/574495\\n","tokens":496,"id":1339,"Prediction":"## Decision\nWe will implement a phased approach to refactor the `RepoHooks` class to improve testability, focusing on incremental changes that support dependency injection without requiring a complete overhaul immediately. This approach offers a compromise that balances the urgency of improving test coverage with the complexity of extensive refactoring efforts. The following steps will be taken:\n\n1. **Intermediate Wrapper Class:** We will create an intermediate non-static wrapper class around the `RepoHooks` static methods. This will allow the `RepoHooks` API to remain the same for now but will enable injection of dependencies for testing purposes.\n\n2. **Dependency Injection for `WikibaseRepo`:** Modify the non-static methods in the wrapper class to accept `WikibaseRepo` as an injectable dependency. This can be configured with default values to maintain compatibility with the existing codebase.\n\n3. **Incremental Refactoring:**\n    - Gradually refactor existing hook methods to use the wrapper class instead of static calls.\n    - Update testing infrastructure to utilize the non-static methods in order to facilitate easier mocking and testing.\n\n4. **Override Method for Stores:** Introduce methods in the wrapper class similar to the `overrideStore` method used in `WikibaseClient`, allowing developers to replace the actual store with a mock for testing purposes.\n\n5. **Maintain Backward Compatibility:** Ensure that the refactored code is backward-compatible with existing hook definitions in `extension.json` and global variables in entry points, to avoid breaking changes during the transition.\n\n6. **Monitoring and Iteration:** Continuously monitor the impact of these changes and iterate based on feedback and evolving requirements, eventually transitioning fully to a more testable, dependency-injected architecture as outlined in the ongoing RFC. \n\nThis strategy aims to make the `RepoHooks` class more test-friendly while minimizing disruption to the current system, thereby achieving improved test coverage and maintainability over time.","GenTime":"2024-07-28 22:44:57"}
{"File Name":"Wikibase\/0000-use-cache-instead-wb_terms-for-data-needed-for-display.md","Context":"## Context\\nWhen an item or a property is displayed in the short form, either as a link, or as a simple text reference, data needed to render this short form are currently loaded from the SQL table (wb_terms). wb_terms is causing several significant issues, and is not possible to be maintained any more in the long run.\\nDecision to use wb_terms, initially introduced as a SQL search index, has been tracked down to [change 176650](https:\/\/gerrit.wikimedia.org\/r\/#\/c\/mediawiki\/extensions\/Wikibase\/+\/176650\/). As discussed there in the code review, and also in https:\/\/phabricator.wikimedia.org\/T74309#798908, it seems there had been no dramatic performance improvements expected, neither noticed after switching to use wb_terms instead of loading the data of the entire item or property.\\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\n\n## Decision\n","Decision":"Wikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\nAs long as using SQL table as a storage of the data used for displaying entities in the short form does not bring significant performance gains, we decide to stop using wb_terms as a data source for this use case.\\nInstead, data of the whole entity is going to be retrieved from storage layer (from the database, or from cached storage that are already in place).\\nIf not efficient enough (e.g. in case of huge-size Wikibase instances like Wikidata ), data needed for display will also be stored in cache, e.g. label of an item in a particular language. That should reduce the amount of computation needed, especially when language fallback needs to be applied, etc.\\n","tokens":330,"id":1341,"Prediction":"## Decision\nWe have decided to discontinue the use of the wb_terms SQL table for loading short form data for items and properties. Instead, we will shift to loading the full entity data, leveraging MediaWiki's existing caching mechanisms, as is already done for lexemes and forms in the WikibaseLexeme extension. \n\nThis decision is motivated by several factors:\n1. **Maintainability**: wb_terms presents significant maintenance challenges and is not feasible to sustain in the long run.\n2. **Minimal Performance Impact**: Historical data and early tests indicate that the performance benefits of wb_terms are negligible. Loading full entity data has not shown significant performance degradation.\n3. **Consistency Across Entity Types**: Aligning the method for loading short form data across all entity types (items, properties, lexemes, forms) reduces complexity and creates a more uniform system architecture.\n4. **Scalability**: The use of MediaWiki's caching mechanisms (e.g., memcached) provides a scalable solution for managing the performance impact of loading full entity data.\n\nBy ensuring that the same approach is used consistently, we simplify the system architecture and improve its maintainability, while leveraging existing caching mechanisms to mitigate performance concerns.","GenTime":"2024-07-28 22:45:02"}
{"File Name":"Wikibase\/0021-federated-properties-source-and-type-dispatching-services.md","Context":"## Context\\nFederated Properties v2 aims to make it possible for users to enable Federated Properties even if their Wikibase already contains data, so that they can choose to use both remote & local Properties to make statements.\\nDispatching by entity type is a widely used mechanism in Wikibase that allows the dispatching service to handle entity ids of different types by delegating to the service implementation defined in the [entity type definitions], thus enabling entity type specific behavior within a single service. With Federated Properties v2 the entity type (\"property\") no longer uniquely identifies the desired service implementation, since local Properties' services need to be handled by database-backed implementations, whereas Federated Properties use API-backed services. In order to work with local and remote Properties, dispatching services need to be aware of the entity's source as well as the type.\\n\n## Decision\n","Decision":"Make all relevant services source and type dispatching.\\n","tokens":172,"id":1346,"Prediction":"## Decision\n\nWe will extend the current dispatching mechanism to include the entity's source information in addition to the entity type. \n\n### Implementation Details:\n\n1. **Extend Entity Identifier:** Modify the entity identifier to include source information (local or remote).\n    - For example, instead of using just `P123` for a property, we will have identifiers like `local:P123` and `remote:P123`.\n\n2. **Update Dispatching Service:** Enhance the dispatching service to accommodate the new identifier structure.\n    - The dispatching logic will first determine the source (local or remote) and then the type (property) to route the request to the appropriate service implementation.\n\n3. **Database-backed vs. API-backed Services:** Clearly delineate between database-backed services for local properties and API-backed services for federated properties.\n    - Ensure both services can seamlessly integrate and interact with the core system.\n\n4. **Backward Compatibility:** Implement a compatibility layer to handle existing data.\n    - For existing Wikibase installations, default all properties to `local` unless specified otherwise.\n\n5. **Configuration and Documentation:** Provide configuration options for the new system along with comprehensive documentation to assist users in transitioning to and utilizing the Federated Properties v2.\n\n### Rationale:\n\n- **Flexibility:** This approach allows a clear distinction and proper routing based on both the property type and data source, ensuring that the correct service implementation is utilized.\n- **Scalability:** It supports the potential addition of more data sources in the future, as the identifier structure can be extended to include new sources.\n- **Backward Compatibility:** It minimizes the disruption for existing users by maintaining compatibility with existing data, making it easier to adopt the new system.\n\nThe decision ensures an effective balance between extending functionalities and maintaining the robustness of existing systems.","GenTime":"2024-07-28 22:45:19"}
{"File Name":"monitoring-doc\/0002-configuration-management.md","Context":"## Context\\nWe have the requirement of adding some resources to the base cloud instances. We currently do\\nthis via the [cloud.conf](https:\/\/github.com\/alphagov\/prometheus-aws-configuration\/blob\/375f34600e373aa0e4c66fcae032ceee361d8c21\/terraform\/modules\/prometheus\/cloud.conf) system. This presents us with some limitations, such as configuration\\nbeing limited to 16kb, duplication in each instance terraform and a lack of fast feedback testing.\\n\n## Decision\n","Decision":"We have decided to move away from cloud.conf as much as possible and instead use it to instantiate\\na masterless puppet agent which will manage the resources.\\n","tokens":109,"id":1372,"Prediction":"## Decision\nWe will transition from using the `cloud.conf` system to a more flexible and scalable approach using cloud-init and configuration management tools like Ansible or Terraform Modules. \n\nThis decision addresses the limitations presented by the `cloud.conf` system, such as:\n\n1. **Configuration Size Limitation**: The 16kb limit is too restrictive for our evolving needs.\n2. **Duplication**: Configuration should be centralized to avoid redundancy across numerous instances.\n3. **Testing and Feedback**: Moving to cloud-init and using tools like Ansible will provide a faster and more robust testing framework, enabling us to achieve better feedback loops.\n\nBy transitioning to this new approach, we anticipate the following benefits:\n\n- **Scalability**: The new system will be able to handle larger and more complex configurations.\n- **Maintainability**: Centralized configurations will reduce duplication and potential inconsistencies.\n- **Speed**: Faster feedback mechanisms will allow us to iterate and troubleshoot configurations more rapidly.\n\nIn summary, this decision prioritizes scalability, maintainability, and speed by leveraging cloud-init and configuration management tools better suited for our resource management and testing requirements.","GenTime":"2024-07-28 22:46:26"}
{"File Name":"monitoring-doc\/0003-use-ecs-for-initial-beta-buildout.md","Context":"## Context\\nExisting self-hosted infrastructure at GDS has been managed in code\\nusing tools like puppet, but in a somewhat ad hoc way with each team\\ndoing things differently, little sharing of code, and much reinvention\\nof wheels.  We would like to learn about other ways of deploying\\ninfrastructure which encourage consistency: in terms of code\\nartifacts, configuration methods, and such like.\\nSystems such as Kubernetes and Amazon ECS are coalescing around Docker\\nas a standard for packaging software and managing configuration.\\n\n## Decision\n","Decision":"We will build our initial prometheus beta in Amazon ECS, and assess\\nhow effective it is.  We will review this decision once we have learnt\\nmore about both prometheus and ECS.\\n","tokens":110,"id":1376,"Prediction":"## Decision\nWe will standardize our deployment process and infrastructure management by adopting Docker as our primary method for packaging software. This choice aligns with industry trends and will help us achieve greater consistency in our infrastructure management.\n\nTo achieve this, we will implement the following measures:\n1. **Containerization with Docker:** All new applications will be containerized using Docker. Existing applications will be gradually migrated to Docker containers. This will standardize application packaging, making it easier to manage dependencies, configurations, and deployments.\n2. **Adopt Kubernetes for Orchestration:** We will use Kubernetes for container orchestration. Kubernetes provides a robust platform for managing containerized applications at scale, supporting various features such as automated rollouts and rollbacks, horizontal scaling, and self-healing capabilities.\n3. **Infrastructure as Code (IaC):** Transition from puppet to a more standardized Infrastructure as Code approach using tools like Terraform or Ansible. This will ensure consistent infrastructure deployments and enable better collaboration across teams.\n4. **Shared Repositories and Best Practices:** Establish shared repositories for Docker images, Kubernetes configurations, and infrastructure code. Develop and document best practices to ensure that all teams adhere to the same standards and methodologies.\n5. **Continuous Integration and Continuous Deployment (CI\/CD):** Implement a CI\/CD pipeline using tools like Jenkins, GitLab CI, or CircleCI to automate the build, testing, and deployment of Docker containers. This will ensure a consistent and repeatable deployment process.\n6. **Training and Support:** Provide training sessions and support resources to assist teams in the transition to Docker, Kubernetes, and new IaC tools. Ensure ongoing support to address any challenges and foster a culture of collaboration and knowledge sharing.\n\nBy adopting Docker and Kubernetes, and standardizing our Infrastructure as Code practices, we aim to achieve greater consistency and efficiency in our infrastructure management. This will reduce redundancy, minimize errors, and streamline the deployment process across all teams.","GenTime":"2024-07-28 22:46:38"}
{"File Name":"wikiindex\/adr-003-testing_library.md","Context":"## Context\\n* We want to write programmer tests to support a TDD workflow.\\n* We want to be able to mock out functions.\\n\n## Decision\n","Decision":"* We will use Midje to test our code.\\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\\n","tokens":30,"id":1387,"Prediction":"## Decision\nWe will adopt the use of a comprehensive testing framework that supports Test-Driven Development (TDD) and allows for the mocking of functions. Specifically, we will use **JUnit** for writing and running tests in a Java environment, alongside **Mockito** for creating mock objects and defining their behavior.\n\n* JUnit will enable us to write repeatable, automated tests and easily integrate them into our build process.\n* Mockito will facilitate the mocking of dependencies to isolate the code under test and simulate behavior for various scenarios.\n\nBy leveraging JUnit and Mockito, we will achieve a robust testing environment that supports our TDD workflow and allows for effective unit testing with mocked dependencies.","GenTime":"2024-07-28 22:47:13"}
{"File Name":"amf\/0003-new-annotation-removal-stage-present-in-all-webapi-pipelines.md","Context":"## Context\\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\\n\n## Decision\n","Decision":"In order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\\n","tokens":61,"id":1393,"Prediction":"## Decision\nTo address the inline emission of references in unresolved models, we decide to implement a multi-step strategy:\n\n1. **Identification of Inline References**: Enhance the parsing mechanism to reliably identify cases where external references are inlined rather than linked. This would involve improving detection logic for elements not present in the base unit references.\n\n2. **Placeholder Creation for Unresolved References**: For inlined references where the target is not found in the base unit references, introduce a placeholder mechanism. This placeholder will act as a marker to allow future resolutions if the target becomes available later, ensuring model integrity without immediate inlining.\n\n3. **Model Emission Adjustment**: Modify the emitting process to recognize these placeholders. Instead of emitting inlined content, the placeholders will be emitted, indicating an unresolved reference. This makes the unresolved state explicit and avoids confusion with truly local elements.\n\n4. **Logging and Reporting**: Implement logging and error reporting to capture instances of reference inlining due to unresolved targets. This helps in debugging and future improvements, providing clear visibility into the frequency and nature of such cases.\n\n5. **Documentation and Guidelines**: Update the documentation to include guidelines for developers on how to handle and interpret these placeholders. This would cover the rationale behind the approach and best practices for handling unresolved references in models.\n\nThis strategy ensures that unresolved references are handled in a way that maintains the integrity and clarity of the model, while providing a clear path for future resolution and minimizing disruption in development workflows.","GenTime":"2024-07-28 22:47:33"}
{"File Name":"amf\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSome other useful links:\\n[https:\/\/github.blog\/2020-08-13-why-write-adrs\/](https:\/\/github.blog\/2020-08-13-why-write-adrs\/)\\n[https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/](https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/)\\n","tokens":16,"id":1397,"Prediction":"## Decision\nWe will use Architectural Decision Records (ADRs) to document the significant architectural decisions made throughout the lifecycle of this project. Each ADR will capture the context, decision, rationale, and consequences of a particular architectural choice, facilitating better communication and transparency among team members. This approach will help ensure that all stakeholders understand and can refer back to why specific decisions were made, streamlining future decision-making and onboarding processes. \n\nADRs will be stored in a version-controlled repository alongside the project's source code to maintain a living documentation that evolves parallel to the project's development.","GenTime":"2024-07-28 22:47:46"}
{"File Name":"GSoC-Braitenberg-Vehicles\/use_javafx_for_animation.md","Context":"Context\\n-------\\nAfter a while writing the project code, I've realized that it needs constant and probably heavyweight rendering.\\nDecision\\n--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n\n## Decision\n","Decision":"--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n","tokens":101,"id":1400,"Prediction":"## Decision\n--------\nContinue utilizing JavaFX\/TornadoFX as the primary framework for constant rendering and animations within the project.\n\nJavaFX\/TornadoFX provides a sufficient and accessible solution for the project's needs, balancing ease of use with acceptable performance for the required rendering tasks. Given the existing constraints and the need to quickly iterate and develop, this choice supports maintaining development momentum without incurring an extensive learning curve or configuration overhead associated with adopting more complex frameworks such as Unity or OpenGL.","GenTime":"2024-07-28 22:47:53"}
{"File Name":"lbh-frontend-react\/0005-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n## Decision\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":1404,"Prediction":"## Decision\n\nWe will adopt ESLint as the linting tool for our project. This decision is based on the following considerations:\n\n1. **Standard Tool**: ESLint is the de facto standard linter for modern JavaScript, ensuring a broad base of community support and extensive documentation.\n2. **TypeScript Support**: ESLint has robust support for TypeScript through various plugins, allowing us to maintain consistent code quality across both JavaScript and TypeScript files.\n3. **Customizable and Extensible**: ESLint provides a high level of customization, enabling us to configure rules and plugins to fit our specific project needs and coding standards.\n4. **Integration**: ESLint integrates well with popular editors and continuous integration (CI) systems, which will help in catching errors early and maintaining code quality effortlessly.\n5. **Conflict Resolution**: For teams migrating from TSLint or other linting tools, ESLint offers compatibility and migration utilities, reducing the friction during transition.\n\nBy implementing ESLint, we aim to improve code consistency, catch potential errors early in the development process, and enhance overall code quality.","GenTime":"2024-07-28 22:48:11"}
{"File Name":"lbh-frontend-react\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n## Decision\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":1408,"Prediction":"## Decision\nWe will adopt [Prettier](https:\/\/prettier.io\/) as the code formatter for our project to ensure consistent code style across the team. This decision is influenced by Prettier's widespread adoption, ease of use, and comprehensive support for various languages in the JavaScript ecosystem. By integrating Prettier, we aim to improve code readability, maintainability, and reduce the cognitive load associated with code style variations. All developers will configure their development environments to utilize Prettier, and we will enforce code style consistency by including Prettier in our continuous integration pipeline.","GenTime":"2024-07-28 22:48:22"}
{"File Name":"lbh-frontend-react\/0010-use-typedoc-to-generate-api-documentation.md","Context":"## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n\n## Decision\n","Decision":"We will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n","tokens":159,"id":1410,"Prediction":"## Decision\n\nWe will adopt **TypeDoc** as our documentation generator for our TypeScript codebase.\n\n### Rationale:\n- TypeDoc leverages TypeScript\u2019s existing type information directly, which eliminates the redundancy and effort involved in maintaining type annotations separately in comments.\n- By generating documentation directly from the code, we ensure that our API docs remain up-to-date with the latest code changes, reducing the chance of inconsistency.\n- TypeDoc integrates well with TypeScript, allowing us to make use of our existing toolchain and ensuring a smoother development workflow.\n- The automatic generation of API documentation will save time and effort, allowing the development team to focus on coding rather than maintaining manual documentation.\n- Using TypeDoc aligns with industry best practices for TypeScript projects, ensuring that our documentation follows a standard format that is familiar to other developers.\n\n### Implementation Plan:\n1. **Install TypeDoc**:\n   ```\n   npm install --save-dev typedoc\n   ```\n\n2. **Configure TypeDoc**:\n   - Create a `typedoc.json` configuration file at the root of the project with the required settings. For example:\n     ```json\n     {\n       \"entryPoints\": [\"src\/index.ts\"],\n       \"out\": \"docs\",\n       \"includeVersion\": true,\n       \"excludePrivate\": true\n     }\n     ```\n\n3. **Add Scripts**:\n   - Add a script to `package.json` to generate documentation easily:\n     ```json\n     \"scripts\": {\n       \"doc\": \"typedoc\"\n     }\n     ```\n\n4. **Integrate with CI\/CD**:\n   - Integrate the documentation generation process into CI\/CD pipelines to ensure that updated documentation is generated and published automatically with each release.\n\n5. **Documentation Review**:\n   - Review the generated documentation for completeness and accuracy, and iterate on comment styles and types as necessary to optimize clarity and usefulness.\n\nThrough these steps, we can ensure our documentation remains in sync with the codebase and provides clear, comprehensive information to our users and developers.","GenTime":"2024-07-28 22:48:31"}
{"File Name":"architecture\/0002-apply-clean-code-guidelines.md","Context":"## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n\n## Decision\n","Decision":"I will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n","tokens":57,"id":1416,"Prediction":"## Decision\n\nTo ensure that anyone can easily understand the code, we will adopt the following practices across all levels of code, groups of code, complete applications, and groups of applications:\n\n1. **Coding Standards:**\n   - Enforce consistent coding standards and style guides (e.g., using tools like ESLint, Prettier for JavaScript or PEP 8 for Python) to maintain uniformity.\n   - Conduct regular code reviews to ensure adherence to these standards.\n\n2. **Documentation:**\n   - Ensure that every function, class, and module has appropriate comments and docstrings explaining their purpose and usage.\n   - Maintain comprehensive README files for repositories, explaining the project structure, setup instructions, and usage.\n   - Use tools like JSDoc (JavaScript), Sphinx (Python), or Doxygen (C++) for generating documentation from code annotations.\n\n3. **Meaningful Naming Conventions:**\n   - Use clear, descriptive names for variables, functions, classes, and modules which convey their purpose and usage.\n   - Avoid abbreviations and acronyms that are not widely understood.\n\n4. **Modular Structure:**\n   - Organize code in small, reusable modules or functions. Each module should have a single responsibility.\n   - Adhere to principles like DRY (Don't Repeat Yourself), KISS (Keep It Simple, Stupid), and SOLID design principles.\n\n5. **Consistent Project Structure:**\n   - Define and adhere to a consistent directory and project structure across similar types of projects.\n   - Utilize templates or boilerplates for starting new projects to maintain this consistency.\n\n6. **Automated Testing:**\n   - Write unit tests, integration tests, and end-to-end tests to ensure code functionality.\n   - Utilize testing frameworks and tools (e.g., Jest for JavaScript, Pytest for Python).\n\n7. **Code Reviews:**\n   - Establish a peer review process where code is regularly reviewed by team members before being merged.\n   - Use pull request templates to ensure reviewers have the necessary context for reviewing the changes.\n\n8. **Continual Learning and Improvement:**\n   - Encourage team members to engage in continual learning and stay updated with best practices.\n   - Conduct regular knowledge-sharing sessions and code walkthroughs within the team.\n\n9. **Refactoring:**\n   - Allocate time for regular refactoring to improve code readability, performance, and maintainability.\n   - Address technical debt proactively.\n\n10. **Tooling and IDE Support:**\n    - Use IDEs and editor configurations that support linting, formatting, and other code quality tools.\n\nBy implementing these decisions, we aim to improve code readability and maintainability, making it easier for any developer to understand and contribute to the codebase effectively.","GenTime":"2024-07-28 22:48:53"}
{"File Name":"architecture\/0003-monitor-condition-and-data-selectors.md","Context":"## Context\\nA lot of Home Assistant integrations use config options like `CONF_MONITORED_CONDITIONS` to allow the user to select which data the integration should expose from the data. This means that the user needs to know what the different data points mean while setting up the integration. While configuring its Lovelace UI, the user has the option to include the entity or not. This means that we allow the user to pick twice.\\n\n## Decision\n","Decision":"Integrations should expose all available data to the backend if that data is fetched in a single API request.\\nIntegrations should only include selector logic if it make sense in the context of interface, like if it would require extra requests. User should not have to read the available documentation and API descriptions to find out which data they want have.\\n```\\nLayer model:\\nFetch            Manage           View\\n-------------      ---------      ------------\\n| Interface | ---> | Core  | ---> | Frontend |\\n-------------      ---------      ------------\\n```\\nIntegrations can set the `entity_registry_enabled_default` property on their entity objects to instruct Home Assistant to disable certain entities by default ([docs](https:\/\/developers.home-assistant.io\/docs\/entity_registry_disabled_by\/#integrations-setting-default-value-of-disabled_by-for-new-entity-registry-entries)).\\n","tokens":90,"id":1417,"Prediction":"## Decision\nRemove the `CONF_MONITORED_CONDITIONS` configuration option from Home Assistant integrations. Instead, default to exposing all available data points by the integration. Users will then utilize the Lovelace UI configuration to include or exclude entities based on their specific needs.\n\n### Rationale\n1. **Simplify Configuration**: Streamline the integration setup process by removing the need for users to understand and select specific data points during the initial configuration.\n2. **Consistency**: Prevent redundancy by avoiding the necessity for users to make similar selections during both integration setup and Lovelace UI configuration.\n3. **Flexibility**: Exposing all available data points by default allows users to have access to all information, making it easier to include or exclude entities as their needs evolve without going back to reconfigure the integration.\n4. **User Experience**: Enhancing the user experience by reducing complexity and minimizing the steps required to fully leverage the integration.\n\nImplementing this decision will align the integration setup process with the more dynamic and user-friendly interface provided by Lovelace UI, thereby creating a more cohesive and seamless overall user experience.","GenTime":"2024-07-28 22:48:57"}
{"File Name":"architecture\/0007-use-static-code-analysis.md","Context":"## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n\n## Decision\n","Decision":"Use Static Code Analysis to find violations of standards.\\n","tokens":45,"id":1418,"Prediction":"## Decision\nWe have decided to strictly adhere to established software development standards and best practices. This includes following coding standards, using standard design patterns, and relying on widely-accepted frameworks and libraries. By doing so, we aim to maximize code readability, maintainability, and quality. Adhering to these standards will also facilitate better collaboration within the team and ensure that the codebase remains robust and scalable for future development efforts.","GenTime":"2024-07-28 22:48:59"}
{"File Name":"architecture\/0013-home-assistant-container.md","Context":"## Context\\nDefine a supported installation method as per [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md).\\n\n## Decision\n","Decision":"This is for running just the Home Assistant Core application on native OCI compatible containerization system. It does not provide the Supervisor experience, and thus does not provide the Supervisor panel and add-ons.\\nThis is a general installation method that is recommended as an alternative to the Home Assistant OS installation method. Due to the shared image with the Home Assistant OS installation method, almost all documentation applies to the Home Assistant Container as well.\\nThe only supported way to run the container is on the host network as root with full privileges.\\n### Supported Containerization system and version\\n- Any Open Container Initiative (OCI) compatible containerization system.\\n### Supported boards\/hardware\/machines\\n- Machines of the following architectures: amd64, i386, armhf, aarch64, armv7\\n### Supported Operating Systems and versions\\n- Running Home Assistant Container is only supported on Linux.\\n- Windows and BSD installations (e.g., macOS and FreeBSD) are not supported.\\n### Additional notes\\nThere is a wide variety of containerization software available. From that perspective, Home Assistant will only actively document the use of Docker.\\n### Required Expertise\\n- **Installation**\\nThis requires the user to have an existing system that can run Docker containers. Installation is either done by running a command from the Docker-cli or via a user interface (Synology, Portainer)\\n* **Start when the system is started:** The user is responsible for configuring the system to start the container when the system is started.\\n* **Run with full network access:** Default installation instructions prescribe net=host to be configured.\\n* **Access USB devices:** It is up to the user to ensure that all devices are correctly passed through to the container.\\n* **Maintaining the Home Assistant installation**\\nIf using the Docker-cli the user needs to manually update the run command. If using a UI the user might be notified of an upgrade or automatically update \u2013 automatically applying updates may result in the system not coming back online. There is no rollback in case the instance does not come online after an update.\\n- **Python upgrades:** Included in the Home Assistant container\\n- **Installing Python dependencies:** Included in the Home Assistant container\\n- **Updating Home Assistant:** Included in the Home Assistant container\\n- **Maintaining the Operating System**\\nSince this is just the core container, all OS responsibilities are with the user.\\n- **Security updates for OS:** Responsibility of the user.\\n- **Maintaining the components required for the Supervisor:** No supervisor, so N\/A\\n**Conclusion:** medium expertise required. Some Docker UIs make it easy to run and update containers. Mapping devices and manually updating Home Assistant will be challenging as they depend per platform.\\n","tokens":41,"id":1430,"Prediction":"## Decision\n\nBased on the context provided and adhering to the guidelines set forth in [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md), the decision is to officially recognize and support the following installation methods for our software:\n\n### Supported Installation Methods:\n\n1. **Home Assistant Operating System**\n   - A full installation that includes the operating system dedicated to running Home Assistant.\n\n2. **Home Assistant Supervised**\n   - A supervised installation on a generic Linux system with the use of Docker.\n\n3. **Home Assistant Container**\n   - Installation using Docker containers for those who want to manage their environments, OS, and updates separately from Home Assistant.\n\n### Rationale:\n\n- **Home Assistant Operating System** allows users to get the benefits of a dedicated appliance, which simplifies the user experience and enhances reliability.\n- **Home Assistant Supervised** offers flexibility for those who already have Linux expertise and desire to maintain and control their host operating system while leveraging Docker for container management.\n- **Home Assistant Container** provides a method ideal for users who prefer using containerized environments due to ease of maintenance, updates, and ecosystem compatibility.\n\n### Non-Supported Methods:\n\n1. Custom or third-party installation scripts and methods not explicitly documented or endorsed by the Home Assistant community.\n2. Unsupported platforms and operating systems that lie outside the scope defined in ADR-0012.\n\n### Implications:\n\n- The development, support, and documentation efforts will be concentrated on the officially supported methods to ensure optimal performance, stability, reliability, and user satisfaction.\n- Users choosing non-supported methods will need to rely on community support and understand that these methods may not have the same level of stability or specific updates as the supported methods.\n\n### Conclusion:\n\nBy defining clear and supported installation methods, we can create a more cohesive and reliable user experience while maintaining the flexibility and adaptability that Home Assistant is known for. This approach also ensures that our resources are utilized effectively in supporting and enhancing these defined methods.","GenTime":"2024-07-28 22:49:43"}
{"File Name":"architecture\/0008-code-owners.md","Context":"## Context\\nWe get contributed a lot of new integrations, new features to integrations and refactors of integrations. The Home Assistant project is honored to receive so many great contributions to our project!\\nUnfortunately, as a contributor, adding oneself as (the, or one of the) code owners of the integration contributed or contributed to, doesn't always happen spontaneously.\\nNot adding oneself as a code owner has drawbacks for the project:\\n- The contributor doesn't \"own\" (in terms of taking responsibility) his code, and thus contribution, in a more formal fashion.\\n- Without being listed as a code owner, our GitHub bot will not notify the contributor, when an issue for the integration is reported, quite possibly affecting his contribution.\\n- Integrations have ended up or may end up with having a single code owner or no code owners at all.\\nAs a result of this:\\n- Bugs are less likely to be resolved in a timely fashion (turn-around time).\\n- Integrations are more prone to break in the future.\\n- Integration with a single code owner:\\n- Do not benefit from multiple code owners being familiar with the integration in terms of code review and general turn-around time.\\n- Become largely unmaintained when the single listed code owner can no longer contribute to the project.\\nWe have quite a few integrations that haven't got multiple code owners or don't have a code owner.\\nDuring the design discussion of this ADR, it also became clear, that the term \"code owner\" has different meanings to our members and contributors. Some interpret it as an honorable mention of contribution; others see it as \"taking responsibility\".\\n\n## Decision\n","Decision":"Code ownership for an integration defined:\\nThe willingness of a contributor to try, at best effort, to maintain the integration. Providing the intention for handling issues, providing bug fixes, or other contributions to the integration one is listed on as a code owner.\\n### Rules\\nIn order to support having (multiple) code owners for integration, to raise the quality and interaction on integration in our codebase, we have a set of rules (exceptions are in the next chapter).\\nFor the following cases, adding oneself as a code owner is required:\\n- When contributing a new integration.\\n- When contributing a new platform to an integration.\\n- When contributing a new feature to an integration.\\n- When contributing a significant refactor or rewrite of an integration.\\nContributions to our integrations, in the above-listed scopes, without having the contributor listed or added as the code owner, is no longer accepted.\\n### Exceptions\\nSome exceptions are in place, to prevent contributors to become demotivated to contribute; and are mainly based around smaller, low-impact contributions.\\nIn the following cases, code ownership may be omitted:\\n- Contributions that solely provides a bug fix(es).\\n- Contributions that only provide additional unit test(s).\\n- Contributions to integrations marked as \"internal\". These integrations are code owned by the Home Assistant core team.\\n- Contributions refactoring across multiple integrations, caused by changes to our core codebase. E.g., due to changes to the used platforms.\\n- Small or low impact contributions to an integration. A currently active code owner for the integration or a Home Assistant code reviewer can decide it may be omitted.\\n- The contributor pro-actively rejects to be listed as a code owner; however, a currently active code owner is willing to accept and take code ownership for the contribution provided by the contributor.\\nCode owner(s) and Home Assistant code reviewers are encouraged to ask a contributor to join an integration code ownership, even when the contribution matches one of the exceptions above.\\n### Withdrawing as a code owner\\nWithdrawing code ownership can happen, and it is sad to see an active code owner leaving the project.\\nA code owner may open up a PR to remove oneself as a code owner. However, this should only be accepted after the last contribution to the integration, made by the contributor, is released in a stable Home Assistant release.\\n","tokens":343,"id":1431,"Prediction":"## Decision\n\nTo ensure the sustained quality and maintainability of our integrations, we will introduce a policy that requires contributors to add themselves as one of the code owners when they contribute new integrations or make significant enhancements to existing ones. This policy aims to foster greater ownership and responsibility among contributors, improve issue resolution times, and maintain the overall health of our integrations.\n\n### Key Points of the Policy:\n\n1. **Mandatory Code Ownership**: When a new integration is contributed or a significant enhancement or refactor is made to an existing integration, the contributor must add themselves as one of the code owners in the CODEOWNERS file.\n   \n2. **Encouraging Multiple Owners**: While a single code owner is acceptable, we strongly encourage contributors to invite other familiar contributors or maintainers to co-own the integration. This distribution of responsibility helps ensure the integration remains robust and is less dependent on a single person\u2019s availability.\n   \n3. **Clear Communication of Responsibilities**: We will clarify the role of a \"code owner\" to the community, emphasizing that it involves taking responsibility for the ongoing maintenance, bug fixes, and overall quality of the code. Being a code owner is both a recognition of contribution and a commitment to uphold the project\u2019s standards.\n   \n4. **Automated Reminders**: Our GitHub bot will be configured to remind contributors to assign themselves as code owners when they submit pull requests for new integrations or major changes. This will act as a gentle nudge to ensure compliance with the new policy.\n   \n5. **Documentation Update**: We will update our contribution guidelines and related documentation to include these new requirements and provide clear instructions on how to add oneself as a code owner in the CODEOWNERS file.\n   \n6. **Review Process**: During the pull request review process, reviewers will verify that the contributor has added themselves as a code owner. Pull requests that do not adhere to this requirement will not be merged until compliance is achieved.\n\nBy adopting this policy, we aim to encourage sustained involvement from contributors, reduce the risk of unmaintained integrations, and improve overall project health and efficiency.","GenTime":"2024-07-28 22:49:48"}
{"File Name":"architecture\/0004-waiting-is-waste.md","Context":"## Context\\nHaving a developer waiting for any process to finish is a stupid thing. It's not only that it's to long for the developer to wait, it also hinders you from fixing issues fast.\\n\n## Decision\n","Decision":"Any processes and workflows need to be very fast and fully automated.\\n","tokens":43,"id":1434,"Prediction":"## Decision\n\nThe decision is to implement asynchronous processing for tasks that are currently synchronous and cause developers to wait. By leveraging asynchronous computing, developers won't have to wait for a process to complete before continuing their work, increasing overall productivity and enabling faster issue resolution.\n\nKey components of the decision involve:\n\n1. **Adopting Asynchronous Frameworks and Libraries:**\n   - Utilize modern asynchronous frameworks or libraries that are compatible with the existing tech stack.\n   - Ensure the chosen frameworks are well-supported and documented to facilitate ease of integration and maintenance.\n\n2. **Task Queuing and Background Processing:**\n   - Implement task queues (e.g., using RabbitMQ, Kafka, or similar) where long-running tasks can be offloaded.\n   - Set up background workers or services to handle these tasks, ensuring they can be processed independently of the main application workflow.\n\n3. **Non-blocking I\/O Operations:**\n   - Adopt non-blocking I\/O operations where applicable to reduce wait times for I\/O-bound processes.\n   - Evaluate sections of the code that involve heavy I\/O operations and refactor them to be asynchronous.\n\n4. **Progress Notifications and Feedback:**\n   - Provide mechanisms for developers to receive real-time updates on the progress of asynchronous tasks.\n   - Implement logging and notification systems to inform developers when tasks are complete or if they encounter issues.\n\n5. **Developer Training and Documentation:**\n   - Provide training sessions to help developers understand the new asynchronous approach and best practices.\n   - Update documentation to reflect the changes, ensuring that all team members can easily refer to new processes and configurations.\n\nBy adopting these measures, the development workflow will become more efficient, allowing developers to focus on high-priority tasks without unnecessary delays.","GenTime":"2024-07-28 22:50:01"}
{"File Name":"architecture\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1436,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) system to document all significant architectural decisions made during the course of the project. This will ensure that all crucial technical and design choices are well-documented for future reference and provide a clear historical context for why certain decisions were made.\n\n### Details of the Decision:\n1. **Format**: We will adopt the MADR (Markdown Architectural Decision Records) template to ensure consistency and clarity in our documentation.\n2. **Storage**: All ADRs will be stored in a dedicated directory within the project repository (`\/docs\/architecture\/decisions`), making them easily accessible to all team members.\n3. **Version Control**: Each ADR will be version-controlled to keep track of changes and updates over time.\n4. **Review Process**: ADRs will be reviewed by the architecture review board (or a designated team) before being finalized to ensure complete and accurate documentation.\n5. **Examples of Documented Decisions**:\n   - Technology stack selection (e.g., languages, frameworks, libraries)\n   - Architecture styles and patterns (e.g., microservices, monolith)\n   - Major third-party integrations\n   - Key performance, scalability, and security considerations\n6. **Ongoing Updates**: ADRs will be updated as necessary when new decisions are made or existing decisions are revisited.\n\nThis systematic approach to recording architectural decisions will support transparency, improve team communication, and provide valuable insights for onboarding new team members.","GenTime":"2024-07-28 22:50:08"}
{"File Name":"ibc-go\/adr-027-ibc-wasm.md","Context":"## Context\\nCurrently in ibc-go light clients are defined as part of the codebase and are implemented as modules under\\n`modules\/light-clients`. Adding support for new light clients or updating an existing light client in the event\\nof a security issue or consensus update is a multi-step process which is both time-consuming and error-prone.\\nIn order to enable new IBC light client implementations it is necessary to modify the codebase of ibc-go (if the light\\nclient is part of its codebase), re-build chains' binaries, pass a governance proposal and validators upgrade their nodes.\\nAnother problem stemming from the above process is that if a chain wants to upgrade its own consensus, it will\\nneed to convince every chain or hub connected to it to upgrade its light client in order to stay connected. Due\\nto the time consuming process required to upgrade a light client, a chain with lots of connections needs to be\\ndisconnected for quite some time after upgrading its consensus, which can be very expensive in terms of time and effort.\\nWe are proposing simplifying this workflow by integrating a Wasm light client module that makes adding support for\\nnew light clients a simple governance-gated transaction. The light client bytecode, written in Wasm-compilable Rust,\\nruns inside a Wasm VM. The Wasm light client submodule exposes a proxy light client interface that routes incoming\\nmessages to the appropriate handler function, inside the Wasm VM for execution.\\nWith the Wasm light client module, anybody can add new IBC light client in the form of Wasm bytecode (provided they are\\nable to submit the governance proposal transaction and that it passes) as well as instantiate clients using any created\\nclient type. This allows any chain to update its own light client in other chains without going through the steps outlined above.\\n\n## Decision\n","Decision":"We decided to implement the Wasm light client module as a light client proxy that will interface with the actual light client\\nuploaded as Wasm bytecode. To enable usage of the Wasm light client module, users need to add it to the list of allowed clients\\nby updating the `AllowedClients` parameter in the 02-client submodule of core IBC.\\n```go\\nparams := clientKeeper.GetParams(ctx)\\nparams.AllowedClients = append(params.AllowedClients, exported.Wasm)\\nclientKeeper.SetParams(ctx, params)\\n```\\nAdding a new light client contract is governance-gated. To upload a new light client users need to submit\\na [governance v1 proposal](https:\/\/docs.cosmos.network\/main\/modules\/gov#proposals) that contains the `sdk.Msg` for storing\\nthe Wasm contract's bytecode. The required message is `MsgStoreCode` and the bytecode is provided in the field `wasm_byte_code`:\\n```proto\\n\/\/ MsgStoreCode defines the request type for the StoreCode rpc.\\nmessage MsgStoreCode {\\n\/\/ signer address\\nstring signer = 1;\\n\/\/ wasm byte code of light client contract. It can be raw or gzip compressed\\nbytes wasm_byte_code = 2;\\n}\\n```\\nThe RPC handler processing `MsgStoreCode` will make sure that the signer of the message matches the address of authority allowed to\\nsubmit this message (which is normally the address of the governance module).\\n```go\\n\/\/ StoreCode defines a rpc handler method for MsgStoreCode\\nfunc (k Keeper) StoreCode(goCtx context.Context, msg *types.MsgStoreCode) (*types.MsgStoreCodeResponse, error) {\\nif k.GetAuthority() != msg.Signer {\\nreturn nil, errorsmod.Wrapf(ibcerrors.ErrUnauthorized, \"expected %s, got %s\", k.GetAuthority(), msg.Signer)\\n}\\nctx := sdk.UnwrapSDKContext(goCtx)\\nchecksum, err := k.storeWasmCode(ctx, msg.WasmByteCode, ibcwasm.GetVM().StoreCode)\\nif err != nil {\\nreturn nil, errorsmod.Wrap(err, \"failed to store wasm bytecode\")\\n}\\nemitStoreWasmCodeEvent(ctx, checksum)\\nreturn &types.MsgStoreCodeResponse{\\nChecksum: checksum,\\n}, nil\\n}\\n```\\nThe contract's bytecode is not stored in state (it is actually unnecessary and wasteful to store it, since\\nthe Wasm VM already stores it and can be queried back, if needed). The checksum is simply the hash of the bytecode\\nof the contract and it is stored in state in an entry with key `checksums` that contains the checksums for the bytecodes that have been stored.\\n### How light client proxy works?\\nThe light client proxy behind the scenes will call a CosmWasm smart contract instance with incoming arguments serialized\\nin JSON format with appropriate environment information. Data returned by the smart contract is deserialized and\\nreturned to the caller.\\nConsider the example of the `VerifyClientMessage` function of `ClientState` interface. Incoming arguments are\\npackaged inside a payload object that is then JSON serialized and passed to `queryContract`, which executes `WasmVm.Query`\\nand returns the slice of bytes returned by the smart contract. This data is deserialized and passed as return argument.\\n```go\\ntype QueryMsg struct {\\nStatus               *StatusMsg               `json:\"status,omitempty\"`\\nExportMetadata       *ExportMetadataMsg       `json:\"export_metadata,omitempty\"`\\nTimestampAtHeight    *TimestampAtHeightMsg    `json:\"timestamp_at_height,omitempty\"`\\nVerifyClientMessage  *VerifyClientMessageMsg  `json:\"verify_client_message,omitempty\"`\\nCheckForMisbehaviour *CheckForMisbehaviourMsg `json:\"check_for_misbehaviour,omitempty\"`\\n}\\ntype verifyClientMessageMsg struct {\\nClientMessage *ClientMessage `json:\"client_message\"`\\n}\\n\/\/ VerifyClientMessage must verify a ClientMessage.\\n\/\/ A ClientMessage could be a Header, Misbehaviour, or batch update.\\n\/\/ It must handle each type of ClientMessage appropriately.\\n\/\/ Calls to CheckForMisbehaviour, UpdateSta\u00e5te, and UpdateStateOnMisbehaviour\\n\/\/ will assume that the content of the ClientMessage has been verified\\n\/\/ and can be trusted. An error should be returned\\n\/\/ if the ClientMessage fails to verify.\\nfunc (cs ClientState) VerifyClientMessage(\\nctx sdk.Context,\\n_ codec.BinaryCodec,\\nclientStore storetypes.KVStore,\\nclientMsg exported.ClientMessage\\n) error {\\nclientMessage, ok := clientMsg.(*ClientMessage)\\nif !ok {\\nreturn errorsmod.Wrapf(ibcerrors.ErrInvalidType, \"expected type: %T, got: %T\", &ClientMessage{}, clientMsg)\\n}\\npayload := QueryMsg{\\nVerifyClientMessage: &VerifyClientMessageMsg{ClientMessage: clientMessage.Data},\\n}\\n_, err := wasmQuery[EmptyResult](ctx, clientStore, &cs, payload)\\nreturn err\\n}\\n```\\n### Global Wasm VM variable\\nThe 08-wasm keeper structure keeps a reference to the Wasm VM instantiated in the keeper constructor function. The keeper uses\\nthe Wasm VM to store the bytecode of light client contracts. However, the Wasm VM is also needed in the 08-wasm implementations of\\nsome of the `ClientState` interface functions to initialise a contract, execute calls on the contract and query the contract. Since\\nthe `ClientState` functions do not have access to the 08-wasm keeper, then it has been decided to keep a global pointer variable that\\npoints to the same instance as the one in the 08-wasm keeper. This global pointer variable is then used in the implementations of\\nthe `ClientState` functions.\\n","tokens":374,"id":1441,"Prediction":"## Decision\n\nWe have decided to implement a Wasm light client module in ibc-go, which will streamline the process of adding, updating, and maintaining light clients across connected chains. The key aspects of this decision are as follows:\n\n1. **Wasm Light Client Implementation**: The new light client will be written in Wasm-compilable Rust code. This approach allows us to leverage the Wasm runtime environment for executing the light client logic securely and efficiently.\n\n2. **Proxy Light Client Interface**: We will introduce a proxy light client interface that will route incoming IBC messages to the appropriate handler functions within the Wasm VM. This ensures that different light clients can be managed consistently within the system.\n\n3. **Governance-Gated Transactions**: Adding or updating a light client will require a governance proposal. This makes the process transparent and ensures community consent for any changes made to the light clients. The deployment of Wasm bytecode will be managed as a governance-gated transaction, simplifying the existing convoluted process.\n\n4. **Simplified Chain Upgrades**: By allowing chains to update their own light client implementations independently of others, the Wasm module reduces the coordination required for consensus updates. This minimizes downtime and disruption for chains undergoing upgrades and helps maintain smooth operations for interconnected networks.\n\n5. **Execution Environment**: The Wasm VM will serve as the execution environment for the light client code. This isolation provides security and flexibility, as it avoids the need to modify the core ibc-go codebase whenever a new light client is introduced or an existing one is updated.\n\n6. **Ease of Integration**: The new approach allows developers to deploy new light clients by submitting Wasm bytecode via governance proposals, making the overall process faster and less error-prone compared to modifying and recompiling the main codebase.\n\nBy adopting the Wasm light client module, we improve the maintainability of ibc-go, enhance security via isolated execution, and significantly reduce the operational overhead and coordination needed for light client updates across connected chains.","GenTime":"2024-07-28 22:50:28"}
{"File Name":"ibc-go\/adr-015-ibc-packet-receiver.md","Context":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n\n## Decision\n","Decision":"`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase channel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistant layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n","tokens":341,"id":1442,"Prediction":"## Decision\n\n1. **Leverage `baseapp.router` for Packet Routing:**\n   - The existing `baseapp.router` will be employed to handle the routing of packets to application modules. This avoids the duplication of routing logic and leverages the existing robust and optimized mechanism present in `baseapp`.\n\n2. **Incorporate Verification in `AnteHandler`:**\n   - Verification steps, which are currently standalone in routing module callbacks, will now be integrated into the `AnteHandler`. This aligns with the existing `AnteHandler`-`Handler` model of the SDK, increasing developer ergonomics by consolidating boilerplate verification code in one place. This should improve maintainability and code clarity.\n\n3. **Introduce `CodeTxBreak` for Atomic Multi-message Transactions:**\n   - A new `CodeType`, `CodeTxBreak`, will be introduced. This code type will signify that a transaction should break execution upon a failure in IBC-related state changes (e.g., token receive failure) without aborting the entire transaction.\n   - By handling errors in this manner, we preserve IBC state modifications even when subsequent application-level changes fail. This prevents scenarios where channels might get stuck due to reverted states, ensuring smoother execution of interrelated IBC operations.\n\n4. **Enhanced Developer Ergonomics and Transaction Reliability:**\n   - These changes collectively aim at both improving the ease of development and increasing the robustness of transaction processing within the IBC ecosystem. Developers will encounter less redundant code and transactions involving multiple messages will be handled more gracefully, preserving important state changes and avoiding channel inconsistencies.\n\nBy taking these steps, we ensure that the IBC routing module adapts well to the existing Cosmos SDK structures, providing a seamless experience both for developers and end-users.","GenTime":"2024-07-28 22:50:33"}
{"File Name":"verify-frontend\/0004-journey-hint-tracking-cookie.md","Context":"## Context\\nThe encrypted `verify-front-journey-hint` cookie was originally created to enable the non-repudiation journey.\\nWhen user selected an IDP, the entity_id was stored in the cookie and if the RP then sent a new Authn request\\nwith a parameter `journey_hint=submission_confirmation` the `\/confirm-your-identity` page was rendered with the\\nIDP from the cookie. The cookie was a simple JSON object:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\"\\n}\\n```\\nIn the early 2018 we introduced a sign-in hint to help users remind what IDP they used previously. The cookie has\\nbeen repurposed and new properties were added to it to track the IDP for each status the user encountered.\\nThe expiry date was also extended to 18 months. For example:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\",\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nFAILED: \"https:\/\/idp-entity-id-1.com\",\\n...\\n}\\n```\\nIf the user has any value in SUCCESS we show the user the sign-in hint for that IDP.\\n\n## Decision\n","Decision":"While implementing a new Pause & Resume functionality, we came across a requirement when we needed to store the status,\\nthe selected IDP and also the RP user has paused with. Rather than creating a new cookie it was decided to re-factor the current\\njourney-hint cookie to support this. In order to know what the latest state was, a new `STATE` object was introduced in the schema.\\nThe `STATE` gets updated with every new Authn response from an IDP. The `ATTEMPT` and `SUCCESS` was kept to keep the cookie backwards\\ncompatible and to help with identifying if there ever was a successful verification. The `entity_id` property got removed and the\\nnon-repudiation journey now uses the `ATTEMPT` value. The cookie schema now looks like this:\\n```\\n{\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nSTATE:  {\\nIDP: \"https:\/\/idp-entity-id.com\",\\nRP: \"https:\/\/rp-entity-id.com\",\\nSTATUS: <SUCCESS | FAILED | FAILED_UPLIFT | CANCEL | PENDING>\\n}\\n}\\n```\\n","tokens":273,"id":1449,"Prediction":"## Decision\nWe have decided to simplify the `verify-front-journey-hint` cookie structure and its usage. The new structure will focus on maintaining the original purpose of the cookie\u2014enabling non-repudiation and facilitating the sign-in hint functionality\u2014while minimizing complexity.\n\n### Simplified Cookie Structure\nThe new cookie structure will be as follows:\n\n```json\n{\n  \"entity_id\": \"https:\/\/idp-entity-id.com\",\n  \"status\": \"SUCCESS\"\n}\n```\n\n### Key Changes:\n1. **Singular Entity ID**: The `entity_id` will continue to store the IDP entity ID, which the user selected.\n2. **Status Tracking**: We will retain a single status that indicates the last successful authentication state\u2014`status: SUCCESS`. This eliminates the need for multiple status tracking within the cookie.\n3. **Expiry Management**: The cookie's expiry date remains set to 18 months, ensuring continued support for the sign-in hint.\n\n### Benefits:\n- **Reduced Complexity**: Simplifying the data structure reduces the risk of errors and enhances maintainability.\n- **Enhanced Performance**: A smaller and simpler cookie saves bandwidth and improves performance.\n- **Focused Functionality**: The decision focuses the cookie's functionality on its primary goals: non-repudiation journey and sign-in hint assistance.\n\n### Implementation Steps:\n1. **Deprecate Old Properties**: Gradually phase out usage of the old properties (`ATTEMPT`, `FAILED`, etc.).\n2. **Cookie Update Logic**: Implement logic to migrate existing cookies to the new structure.\n3. **Backward Compatibility**: Ensure the system can still read old cookies until they are naturally expired.\n\n### Roll-out Strategy:\n- **Incremental Rollout**: Incrementally deploy the new cookie format to ensure system stability and monitor for any issues.\n- **User Communication**: Notify users, if necessary, about any changes to their sign-in experience resulting from this update.\n\nBy adopting this new cookie structure, we aim to streamline the user authentication process and improve system efficiency.","GenTime":"2024-07-28 22:50:53"}
{"File Name":"inner-source_kotlin_spark_template\/20180617-flyway.md","Context":"## Context\\nDevelopers need some tool for DB versioning and safe migrations.\\n\n## Decision\n","Decision":"We will introduce dockerized flyway setup for local development.\\nMigration scripts will also be provided.\\n","tokens":17,"id":1462,"Prediction":"## Decision\nWe will adopt a database migration tool to handle versioning and safe migrations. Specifically, we will use Flyway for this purpose due to its strong community support, ease of use, and seamless integration with our technology stack. Flyway will enable developers to manage schema changes reliably and ensure that each migration is versioned and executed in a controlled manner. This decision will enhance our ability to maintain database schema consistency across different environments and improve the overall reliability of our development and deployment processes.","GenTime":"2024-07-28 22:51:29"}
{"File Name":"inner-source_kotlin_spark_template\/20180525-gradle-kotlin.md","Context":"## Context\\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones.\\nIntellij IDEA has great support for Kotlin based builds.\\n\n## Decision\n","Decision":"We will adopt Kotlin as a Gradle build's definition language.\\n","tokens":32,"id":1463,"Prediction":"## Decision\n\nAdopt Kotlin as the default language for writing Gradle build scripts.\n\n### Rationale\n1. **Readability and Maintainability**: Kotlin-based build scripts offer better readability and are less esoteric compared to Groovy, making it easier for developers to understand and maintain the build configuration.\n   \n2. **IDE Support**: IntelliJ IDEA, a widely used IDE in the developer community, has excellent support for Kotlin. This includes features such as code completion, refactoring, and debugging, which can significantly improve developer productivity and reduce the likelihood of errors in build scripts.\n\n3. **Type Safety**: Kotlin provides enhanced type safety over Groovy. This helps catch potential errors at compile time rather than runtime, resulting in more robust build scripts.\n\n4. **Consistency in Codebase**: If the primary application code is already written in Kotlin, using Kotlin for build scripts ensures consistency across the codebase. This can make onboarding new developers more straightforward and reduce the cognitive load since they need to be proficient with only one language.\n\n5. **Community Adoption and Resources**: There is a growing community and increased availability of resources and examples for Kotlin-based Gradle scripts, making it easier to find support and best practices.\n\nBy standardizing on Kotlin for Gradle build scripts, the team can leverage these advantages to improve the overall development workflow.","GenTime":"2024-07-28 22:51:33"}
{"File Name":"aws_infrastructure\/0005-use-terraform-as-infrastructure-as-code.md","Context":"## Context\\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\\n\n## Decision\n","Decision":"Use Terraform\\n","tokens":21,"id":1464,"Prediction":"## Decision\n\nAdopt \"Infrastructure as Code\" (IaC) for managing AWS resources by using tools like Terraform or AWS CloudFormation.\n\n### Key Points:\n1. **Tool Selection**: \n   - **Terraform**: Platform-agnostic, state management, reusable modules.\n   - **AWS CloudFormation**: Native AWS service, integrated with other AWS services, YAML\/JSON templates.\n\n2. **Version Control**:\n   - Store IaC scripts in a version control system (e.g., Git) to track changes over time.\n   - Implement branching and pull requests to handle updates and reviews.\n\n3. **Automation**:\n   - Integrate IaC with CI\/CD pipelines to ensure automated and consistent deployments.\n   - Use AWS CodePipeline, Jenkins, or other CI\/CD tools for automated workflows.\n\n4. **State Management**:\n   - For Terraform, use remote state storage solutions like AWS S3 with state locking using DynamoDB.\n   - For AWS CloudFormation, make use of stack sets for managing state across multiple accounts\/regions.\n\n5. **Resource Tracking**:\n   - Maintain a centralized repository of IaC scripts for tracking all AWS resources.\n   - Use tagging strategies within IaC scripts for better resource categorization and tracking.\n\n6. **Security**:\n   - Ensure sensitive data (e.g., credentials) are managed securely, using tools like AWS Secrets Manager or Terraform Vault provider.\n   - Implement role-based access control for IaC execution and resource management.\n\n7. **Documentation & Training**:\n   - Provide detailed documentation and training for team members on IaC practices and tools.\n   - Ensure shared understanding of IaC principles and templates.\n\n8. **Compliance & Auditing**:\n   - Regularly review and audit the IaC scripts and AWS resource configurations for compliance against organizational policies.\n   - Use AWS Config, Terraform Cloud, or other auditing tools to monitor changes and ensure compliance.\n\nBy implementing Infrastructure as Code, we can achieve consistent, repeatable, and auditable infrastructure management across our AWS environment.","GenTime":"2024-07-28 22:51:38"}
{"File Name":"branchout\/0002-language.md","Context":"## Context\\nA language should be universal, simple and easily testable\\nOptions\\n* Shell\\n* Go\\n* Java\\n* JavaScript\\nThere should be very few dependencies\\n\n## Decision\n","Decision":"Shell\\n* No dependencies\\n* Installed pretty much everywhere developers are\\n","tokens":36,"id":1472,"Prediction":"## Decision\n\n**Chosen Language: Go**\n\n### Rationale:\n\n1. **Universality:**\n   - **Go (Golang)** is a statically-typed, compiled language that is designed with simplicity and universality in mind.\n   - It is capable of running on multiple platforms with minimal modifications and is well-suited for a wide range of software, including web servers, networking tools, and cloud services.\n\n2. **Simplicity:**\n   - Go has a clean, concise, and easy-to-read syntax which emphasizes simplicity and reduces the overhead associated with maintaining the codebase.\n   - The language avoids features that add complexity, such as complex inheritance, generics, or operator overloading.\n\n3. **Testability:**\n   - Built-in support for testing is a hallmark of Go. It includes a comprehensive testing framework in its standard library, which simplifies the process of writing and running tests.\n   - There is robust support for writing unit tests, benchmarks, and race condition testing, ensuring high code quality and reliability.\n\n4. **Dependencies:**\n   - Go promotes building self-contained binaries with its easy dependency management through `go modules`, which simplifies the management and versioning of dependencies.\n   - It has a static linking approach which results in fewer dependencies during runtime and results in quicker deployments.\n\nGiven these reasons, **Go** aligns well with the stated requirements of universality, simplicity, and testability, as well as minimizing dependencies.","GenTime":"2024-07-28 22:52:12"}
{"File Name":"uniprot-rest-api\/0003-spring-framework.md","Context":"## Context\\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\\nadditional features (than one would otherwise write themselves), etc.\\n\n## Decision\n","Decision":"We have used the [Spring framework](https:\/\/spring.io\/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\\n","tokens":35,"id":1477,"Prediction":"## Decision\nWe have decided to adopt a programming framework for our upcoming project. This decision is motivated by the potential for increased productivity, the benefit of a smaller codebase, enhanced reliability, and the availability of additional features that would be complex and time-consuming to develop from scratch. By leveraging a well-supported and widely-used framework, we aim to expedite our development process and produce a more robust and feature-rich application. The specific framework will be chosen based on a careful evaluation of project requirements, team expertise, and community support.","GenTime":"2024-07-28 22:52:25"}
{"File Name":"evidence-api\/0002-create-two-platform-apis-instead-of-a-single-api.md","Context":"## Context\\nThis application was created to allow services to manage gathered evidence documents in a consistent, safe and centralised way. An early decision point was reached with regards to designing the architecture around this capability.\\nThe basic functionalities known to be required are:\\n- secure storage and management of documents provided by residents and other third parties\\n- tracking of rights and retention over those documents by the council\\n- requests and approval of evidence from third parties\\n- management of evidence requests by officers\\nInfluencing factors:\\n- HackIT's [API Playbook](https:\/\/github.com\/LBHackney-IT\/API-Playbook-v2-beta)\\n- [Clean Architecture](https:\/\/github.com\/madetech\/clean-architecture) principles\\n\n## Decision\n","Decision":"We decided to create two Platform APIs (as defined in the API Playbook):\\n- A [Documents API](https:\/\/github.com\/LBHackney-IT\/documents-api) for the storage and management of documents and claims over them\\n- An [Evidence API](https:\/\/github.com\/LBHackney-IT\/evidence-api) for the management of evidence requests and reviews\\n","tokens":151,"id":1480,"Prediction":"## Decision\n\nBased on the context provided, the following architecture decision has been made to address the required functionalities while complying with the influencing factors:\n\n**Adopt a Microservices Architecture with a Secure API Gateway**\n\n### Rationale:\n1. **Secure Storage and Management of Documents**:\n   - We will utilize a dedicated document storage microservice built on AWS S3, which offers advanced security features like encryption at rest and in transit.\n   - The microservice will ensure role-based access control (RBAC) to manage who can upload, view, and delete documents.\n\n2. **Tracking Rights and Retention**:\n   - A tracking microservice will be responsible for recording and enforcing the rights and retention policies associated with each document, leveraging a metadata database.\n   - This service will utilize cloud-native solutions to automate retention policies and ensure compliance with GDPR and other legal requirements.\n\n3. **Requests and Approval Management**:\n   - A workflow orchestration microservice will handle the requests and approvals of evidence from third parties.\n   - The workflow engine (such as Camunda or Apache Airflow) can ensure that requests are tracked and managed efficiently, and that notifications are sent to the appropriate parties.\n\n4. **Management of Evidence Requests by Officers**:\n   - An officer management microservice will provide the interface and logic for officers to manage their evidence requests.\n   - This service will integrate closely with user authentication services to ensure secure and reliable access for authenticated officers.\n\n### Influencing Factors:\n- **HackIT's API Playbook**: Emphasizes the importance of designing APIs that are consistent, well-documented, and secure. The decision to adopt a microservices architecture aligns well as it allows each service to expose its API, adhering to the API Playbook's standards.\n- **Clean Architecture**: By adopting microservices, we ensure that each service adheres to the principles of Clean Architecture, promoting separation of concerns, scalability, and maintainability.\n\n### Implementation Considerations:\n- **API Gateway**: A secure API gateway (such as AWS API Gateway or Kong) will route requests to the appropriate microservice, ensuring authorization and authentication checks.\n- **Event-Driven Architecture**: Implement an event bus (such as AWS SNS\/SQS or Kafka) to ensure that updates and notifications are propagated between services reliably.\n- **Monitoring and Logging**: Utilize centralized logging and monitoring tools (such as ELK Stack or Prometheus\/Grafana) to keep track of microservices' health and performance.\n\n### Expected Outcomes:\n- **Scalability**: Each service can be independently scaled to meet demand.\n- **Security**: Robust security measures, including encryption, RBAC, and secure APIs, will protect sensitive information.\n- **Maintainability**: Adoption of clean architecture principles ensures that the codebase remains manageable and that changes in one service do not adversely affect others.\n- **Compliance**: Automated management of rights and retention ensures compliance with legal requirements.\n\nThis architectural decision aims to create a robust, secure, and maintainable system that satisfies the application's needs and aligns with the recommended best practices.","GenTime":"2024-07-28 22:52:39"}
{"File Name":"holochain-rust\/0010-write-our-own-dht-implementation.md","Context":"## Context\\nFor the Go based prototype we used IPFS' Kademlia DHT implementations with edits.\\nSince we are switching over to Rust, we can't easily continue using that code base.\\nMore importantly, there are too many Holochain specific additions to a vanilla Kademlia DHT, as well as other possible implementations of achieving entry resilience, routing, our forthcoming World Model, CRDT handling, gossip and our graph properties via linking, so it does not make sense to change existing implementations but instead roll our own.\\n\n## Decision\n","Decision":"We will build our own DHT implementation in Rust from scratch.\\n","tokens":110,"id":1494,"Prediction":"## Decision\nWe will proceed with developing a custom, in-house implementation of a DHT in Rust tailored to meet the specific needs of our project. This custom implementation will allow us to incorporate Holochain-specific features and optimizations for entry resilience, routing, the forthcoming World Model, CRDT handling, gossip, and graph properties via linking. The decision to roll our own DHT is driven by the necessity to have fine-grained control over these functionalities and their integration, which cannot be easily achieved by adapting existing Kademlia DHT implementations or other off-the-shelf solutions.","GenTime":"2024-07-28 22:53:33"}
{"File Name":"holochain-rust\/0003-redux-architecture-pattern.md","Context":"## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n\n## Decision\n","Decision":"The new holochain architecture will follow a redux architecture in order for an agent to have one global state.\\nWe will apply nested state objects which represent a state tree, with sub states for each module.\\nWe use reference counting smart pointers for the sub\\nstates such that it is possible for each module's\\nreducer to decide if the sub state is to be mutated or reused.\\n","tokens":92,"id":1495,"Prediction":"## Decision\nWe have decided to implement a single global state management approach within each agent for the Rust-based rewrite of Holochain. This global state will centralize and simplify interactions between different modules and stateful objects, mitigating the implicit dependencies observed in the Go implementation.\n\nThis decision leverages Rust\u2019s strong safety guarantees and concurrency features to effectively manage state consistency and coherence, even within the distributed context of a peer-to-peer network. By centralizing state management, we aim to reduce the overall complexity, making the system easier to understand, maintain, and evolve.\n\nAdditionally, this approach aligns well with Rust's ownership model, which can help prevent data races and ensure thread safety, critical for a decentralized network. This architectural decision is expected to improve clarity, maintainability, and robustness of the Holochain framework moving forward.","GenTime":"2024-07-28 22:53:35"}
{"File Name":"holochain-rust\/0006-splitting-agent-into-front-house-back-house-or-not.md","Context":"## Context\\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\\n* the agent's keys are by definition part of the front-house\\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\\n\n## Decision\n","Decision":"We decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\\n","tokens":423,"id":1500,"Prediction":"## Decision\n\nWe have decided to adopt a dual-key architecture to differentiate between the two types of agencies: creative (authoring) and network (validation and DHT management).\n\n### Key Details:\n1. **Separate Key Management**:\n   - **Creative Agency**: This part of the system (front-house) will manage the user's private keys and source chain, running on the client machine. This agency is responsible for authoring and providing interfaces for the UI, as well as private key management.\n   - **Network Agency**: This part (back-house) will handle the DHT shard, routing table, metrics, and validations, running on the HoloPorts.\n\n2. **Proxy Authentication**:\n   - The network agency will have its own separate keys to sign packets independently of the creative agency, thus ensuring that the DHT and other network-facing operations can continue even when the user is offline.\n   - For critical operations needing the user's direct authorization (e.g., signing source chain headers), the creative agency can proxy these requests when available.\n\n3. **Modular Separation**:\n   - The creative agency will be responsible for all functions directly interacting with the user, ensuring the security and privacy of the user's keys.\n   - The network agency will handle heavy-lifting network functions and DHT participant roles, using its own keys to maintain operations smoothly and securely.\n\n### Implementation Strategy:\n- Reevaluate the Holochain core structure to support key separation and ensure both agencies can operate concurrently and communicate effectively.\n- Develop necessary protocols for secure, auth-proxy communication between the creative and network agency.\n- Ensure both agencies function optimally in environments where the creative agency might be temporarily offline, leveraging the independent network agency keys.\n\nThis architecture ensures both the security of user keys and the seamless operation of the network, aligning with the principle of separating user\/agent-facing operations from network-facing operations while mitigating the overhead of continuous key communication.","GenTime":"2024-07-28 22:53:55"}
{"File Name":"taxonomy-manager\/001-graph-database.md","Context":"## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n\n## Decision\n","Decision":"* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of \"maintenance only\" since June 2013.\\n### Positive Consequences\\n* PostgreSQL can be clustered, making it highly available\\n* PostgreSQL is broadly available as a managed service\\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\\n### Negative Consequences\\n* We have to design our own schema and code for interacting with the PostgreSQL database\\n","tokens":95,"id":1510,"Prediction":"## Decision\nWe will use a graph database to store and retrieve the SKOS taxonomies. Specifically, we will choose an RDF-native graph database such as Apache Jena TDB, Virtuoso, or Blazegraph.\n\n## Rationale\n1. **Native RDF Storage**: Graph databases natively support RDF data models, eliminating the need for specialized serialization and deserialization mechanisms required by traditional relational databases.\n2. **High Availability\/Fault Tolerance**: Leading RDF graph databases offer features that support high availability and fault tolerance, aligning with our requirements.\n3. **Ease of Use and Learning Curve**: While there is a learning curve associated with graph databases, it is generally outweighed by the benefits of native RDF support and the complexity reduction in querying and maintaining linked data.\n4. **Maintenance Overhead**: By choosing a graph database, the maintenance associated with complex custom serialization\/deserialization code and schema management in RDBMS is reduced.\n5. **Minimizing Vendor Lock-in**: To reduce vendor lock-in, we will prefer open-source solutions or solutions that support standard protocols like SPARQL for querying RDF data, ensuring flexibility and portability of our data.\n\nThis decision allows for efficient, maintainable, and scalable storage of SKOS taxonomies, leveraging the strengths of graph databases in handling complex linked data structures.","GenTime":"2024-07-28 22:54:36"}
{"File Name":"molgenis-frontend\/0003-have-app-create-settings.md","Context":"## Context\\nApps (for example the data-explorer-2 or the biobank-explorer) may need settings data to function properly. As these settings are dynamic (may be changed at run time), and may change between deployments these settings need to be stored somewhere (molgenis database). This poses two problems; 1) how to link the app with the settings data, 2) how to control access to the settings.\\n\n## Decision\n","Decision":"We leave it up to the app to create the settings entity. Each app creates 0 or 1 settings entities. By default everyone can read the settings data. Depending on the app the entity contains 1 or more rows ( in case of multiple rows the app is responsible using the correct row(s) in each context).\\nRejected alternatives:\\n- Add settings via bootstrapped entity in molgenis core; This was deemed as insufficiently flexible due to the dependance on Java knowledge and (possible major version) core release.\\n- Use of app manager; App-manager does not facilitate an automated deployment process. App manager restricts the app architecture and settings structure.\\n- Use of entity meta data: Does not allow for settings per 'entity and app' combination. For instance the dataexplorer needs n settings rows or n tables.\\n- Proxy settings request to external repository; Deemed as hard to manage, app and proxy settings need to be kept in sync, also requires knowledge of proxy (nginx, apache) to configure.\\n","tokens":90,"id":1514,"Prediction":"## Decision\nAfter evaluating the context, we have decided to implement the following solutions:\n\n1. **Linking the App with the Settings Data**: \n    - We will create a dedicated table in the Molgenis database to store the settings data for each app.\n    - Each app will have a unique identifier that will be used as a foreign key in the settings table, ensuring that each set of settings is specifically linked to a particular app.\n    - An API endpoint will be developed to allow dynamic retrieval of the settings data based on the app's unique identifier. This will facilitate easy access and updates to the settings data.\n\n2. **Controlling Access to the Settings**:\n    - We will leverage the Molgenis platform's existing role-based access control (RBAC) capabilities to manage permissions for accessing and modifying settings.\n    - Specific roles will be defined (e.g., Admin, Editor, Viewer) with varying levels of access to the settings data. Only authenticated users with the appropriate roles will be able to view or modify the settings.\n    - An audit log will be implemented to keep track of any changes made to the settings data, ensuring accountability and traceability.\n\nBy implementing these solutions, we aim to ensure that the app settings data is securely linked and accessible while maintaining robust access controls.","GenTime":"2024-07-28 22:54:50"}
{"File Name":"fundraising-application\/005_URL_Schema.md","Context":"## Context\\nWhen we developed the Fundraising application, we did not pay close attention to the URL schema and ended up with three different styles:\\n* \"Slashy\", pseudo-[REST][1]-style URLs like `\/donation\/add`\\n* \"action-sentences\" like `\/apply-for-membership`\\n* a combination of both like `\/contact\/get-in-touch`\\nWe don't do search engine optimization (SEO) in the form of \"meaningful, localized and stable URLs\", as the main traffic to the donation page comes from banners and we don't have much relevant content to that search engines can index.\\n\n## Decision\n","Decision":"We will use the \"action-sentence\" style for URLs in the future. They should follow the pattern `verb-noun` or `verb-preposition-noun`.\\nOur reasoning behind the decision:\\n* They convey more information about what the route does, because we can use all verbs of the English language instead of restricting us to `GET` and `POST`.\\n* REST-style URLs are deceiving because our application has no real API and is not explicitly written with a [RESTful][1] architecture.\\n* We can still have a dedicated REST API in the future, by using the `\/api` route.\\n* The sentence style fits better to our use case architecture, which also read more like sentences.\\nWhenever we change a URL, we decide if we need to create a redirect from the old one to the new in the NGinX configuration. GET support is a good indicator for the need for a redirect. If route is more like a functional \"endpoint\" like `donation\/update`, then we don't need a redirect.\\nIf we need to add i18n information to the URL at some point, we will do it with a \"subdirectory prefix\", e.g. `\/de\/apply-for-membership`, `\/en\/apply-for-membership`. The cons listed at https:\/\/support.google.com\/webmasters\/answer\/182192?hl=en do not outweigh the benefits.\\n","tokens":128,"id":1522,"Prediction":"## Decision\n\nTo standardize the URL schema for the Fundraising application, we will adopt a consistent \"slashy\" REST-style URL structure. This decision aims to improve the maintainability and consistency of the codebase while facilitating a cleaner and more predictable URL pattern for any future enhancements or SEO efforts.\n\n- All URLs will follow the pattern of `\/resource\/action`, e.g., `\/donation\/add`, `\/membership\/apply`, `\/contact\/get-in-touch`.\n- Existing URLs will be refactored and redirected to adhere to the new standard while ensuring backward compatibility for existing bookmarks and external links.\n- New endpoints will conform to this schema from the outset.\n\nThis unified approach will streamline development processes and enhance readability, making it easier for developers to understand and work with the application URLs.","GenTime":"2024-07-28 22:55:30"}
{"File Name":"fundraising-application\/020_PayPal_IPN_Queue.md","Context":"## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n\n## Decision\n","Decision":"* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\nSince this was the first occurrence of the problem, and the Fundraising\\nApplication system is now running well again, we decided against introducing\\nthe IPN queue feature.\\n","tokens":190,"id":1523,"Prediction":"## Decision\n\nWe will implement a queuing system to handle incoming PayPal IPN requests. This queue will temporarily store all incoming IPNs until they are successfully processed by our system. \n\n* Use [message queue service name] (e.g., RabbitMQ, AWS SQS, etc.) to create a robust and scalable queuing system.\n* All incoming IPN requests from PayPal will be placed into this queue the moment they are received.\n* A dedicated processing service will dequeue these IPNs and process them in sequence, ensuring that each request is handled.\n* In the event of a processing failure, the failed request will remain in the queue for retry based on an exponential backoff strategy.\n* Message deduplication mechanisms will be incorporated to prevent duplicate processing of IPNs.\n* Proper logging will be implemented at each step, ensuring full transparency and traceability of the IPN processing workflow.\n* IPNs that fail processing multiple times will trigger an alert to notify the development team for further investigation.\n\nThis approach will ensure that no IPN data is lost, errors can be debugged effectively with the information stored in the queue, and the system will be able to recover from failures more gracefully through automated retries.","GenTime":"2024-07-28 22:55:33"}
{"File Name":"fundraising-application\/003_Validation.md","Context":"## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https:\/\/symfony.com\/doc\/current\/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework\/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n\n## Decision\n","Decision":"For each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https:\/\/github.com\/wmde\/fundraising-frontend-content\/blob\/test\/i18n\/de_DE\/messages\/validations.json) in the [content repository](https:\/\/github.com\/wmde\/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\\n","tokens":225,"id":1532,"Prediction":"## Decision\n\nAfter careful consideration of the context and arguments presented:\n\n1. **Validation Logic Implementation:**\n   - We decided to **write our own validation logic** rather than using an established library. The key reasons for this decision are:\n     - **Independence from External Libraries**: We want to avoid coupling our domain layer to the specific implementation details of any external validation library. This ensures greater flexibility and adaptability over the lifecycle of the project.\n     - **Simplicity of Our Use Cases**: Given that our validation requirements are straightforward, implementing simple custom validations is expected to be more efficient and easier to maintain than introducing the complexities of an external library.\n     - **Unknown Maintenance Cycles of Dependencies**: Dependence on external libraries introduces potential risks regarding their maintenance and longevity, which may lead to added overhead for our team.\n     - **Learning Curve**: Avoiding an external library's API reduces the learning curve for new team members and prevents unnecessary complexity in our development process.\n\n2. **Placement of Validation Logic:**\n   - We decided to place the **validation logic in the use case layer.** This approach offers several benefits:\n     - **Encapsulation and Coherence**: By placing validation in the use case layer, we ensure that all business logic, including validation, resides within a single context. This makes the domain model cleaner and more focused on the business rules.\n     - **Flexibility for Domain Objects**: This avoids forcing the creation of fully formed domain objects at the framework\/presentation layer, allowing for more flexibility in managing the input data and its validation closer to where the business logic is executed.\n     - **Separation of Concerns**: This method adheres to the principle of separation of concerns. The presentation layer is responsible for handling the input and delegating to the use case layer, which is responsible for business logic, including validation.\n\nBy adopting a custom validation approach situated within the use case layer, we aim to achieve a cleaner architecture that is both maintainable and adaptable to future changes.","GenTime":"2024-07-28 22:56:11"}
{"File Name":"opg-data-lpa-codes\/0001-flask-in-aws-lambda-function.md","Context":"## Context\\nFor this project, we would like to package up the API endpoints and logic into a small Flask app in a single lambda function.\\nWe found whilst working on the Documents integration that managing multiple lambda functions quickly became quite hard work,\\nespecially as in that project there was a lot of shared code about the place. In hindsight, we should have refactored the\\nshared code into separate lambda functions, but we never got to it due to time constraints. Also this would just give us\\nmore lambda functions to maintain.\\n#### Why Flask (or any other WSGI app) in a Lambda is a good idea\\n* We can re-use most of the infra setup from the last project so we can get started fast\\n* Easy to run and develop locally, just run like a normal Flask app\\n* Easy to test as it's a normal Flask app\\n* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well established\\n* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flask\\n(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be an\\nanti-pattern?\\n* Maintaining a single lambda function is much easier than maintaining many\\n* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessed\\nby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) so\\nit's a good candidate for experimenting with this, as nothing will need to be reused outside of this app\\n#### Why Flask (or any other WSGI app) in a Lambda is a bad idea\\n* A whole app in a FaaS seems like an anti-pattern (can't find any evidence to support\/contradict this)\\n* Potentially longer cold-start as it has to initialise the whole app not just a single function\\n* [Flask-Lambda](https:\/\/github.com\/sivel\/flask-lambda) is a small package with only 1 contributor, though it is a pretty\\nsimple script that just maps some request variables depending on the source of the request\\n\n## Decision\n","Decision":"* Single lambda function containing small Flask app that provides all endpoints\\n* Flask-Lambda to help easily switch between local dev and AWS\\n","tokens":469,"id":1540,"Prediction":"## Decision\n\nWe have decided to proceed with packaging our API endpoints and logic into a single Flask app deployed as a Lambda function.\n\n#### Justification:\n1. **Speed of Setup and Development**:\n    - We'll be able to leverage much of the infrastructure setup from the previous project, allowing us to get started quickly.\n    - Flask applications are easy to run and develop locally, which will facilitate more rapid development and testing.\n\n2. **Maintainability**:\n    - A single Lambda function will be easier to maintain compared to managing multiple Lambda functions.\n    - In the previous project, managing multiple functions with overlapping shared code proved to be burdensome, and refactoring was not feasible due to time constraints.\n\n3. **Technical Familiarity and Community Support**:\n    - Flask has extensive documentation and a large community, which will help us mitigate potential issues and provide a plethora of reference material.\n    - Our team has substantial experience with Flask, making the development process more efficient.\n\n4. **AWS Chalice as a Precedent**:\n    - AWS Chalice, an official library that effectively serves similar purposes, demonstrates that this approach can align with industry practices. This reduces the concern about whether it constitutes an anti-pattern.\n\n5. **Project Scope**:\n    - The project is small, with well-defined boundaries, and its data and artifacts are solely accessed via its own API. This isolation makes it an ideal candidate for this approach, minimizing risks related to code reuse across different services.\n\n#### Risk Mitigation:\n- **Cold Start Times**: While there might be longer cold-start times due to initializing the whole app, the impact should be manageable given the project's limited scope and expected usage patterns.\n- **Single Contributor Library**: Although Flask-Lambda is maintained by a single contributor, we will perform code reviews and consider contributing fixes or enhancements if necessary. Additionally, the script\u2019s simplicity reduces the risk associated with its singular maintenance.\n\n#### Conclusion:\nPackaging the API endpoints into a Flask app within a single Lambda function is a balanced approach that aligns with our project's requirements for rapid development, ease of maintenance, and familiarity. The potential drawbacks have been considered and deemed manageable within the context of this specific project.","GenTime":"2024-07-28 22:56:49"}
{"File Name":"opg-metrics\/0002-aws-infrastructure.md","Context":"## Context\\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\\n\n## Decision\n","Decision":"To run in a fully managed AWS cloud environment using Terraform to manage it.\\n","tokens":31,"id":1545,"Prediction":"## Decision\nWe will adopt a microservices architecture for the system. This approach aligns well with the need for a lightweight and fully managed system. Each microservice will be independently deployable, easily maintained, and scalable. By utilizing containerization (e.g., Docker) and orchestration tools (e.g., Kubernetes), we can ensure that the system remains lightweight and its deployment is fully managed.\n\nAdditionally, we will integrate a managed cloud service platform (e.g., AWS, GCP, Azure) for infrastructure, which will handle much of the underlying operational complexity. This will allow developers to focus on writing business logic without worrying about the heavy lifting of managing servers and infrastructure.\n\nFor integration purposes, we will employ asynchronous messaging patterns (e.g., using RabbitMQ or Kafka) and well-defined RESTful APIs for synchronous communication. This will facilitate seamless integration between microservices and external systems, ensuring that our solution is highly cohesive, yet loosely coupled.","GenTime":"2024-07-28 22:57:06"}
{"File Name":"origin\/0005-simplify-off-chain-storage.md","Context":"## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n\n## Decision\n","Decision":"We decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\\n","tokens":52,"id":1558,"Prediction":"## Decision\n\nTo resolve the data inconsistency issues between off-chain and on-chain storage, we are implementing the following strategies:\n\n1. **Atomic Transactions:**\n   - Introduce atomic transactions that will ensure either both on-chain and off-chain data are updated concurrently or none of them are. This will leverage blockchain's inherent transactional integrity to ensure consistency across the hybrid storage model.\n\n2. **Commit Log for Off-Chain Data:**\n   - Implement a commit log for off-chain storage. When data is written off-chain, a preliminary entry is made in the commit log. The on-chain reference update will check this log and ensure the on-chain commit only succeeds if the off-chain data write has been confirmed.\n\n3. **Two-phase Commit Protocol:**\n   - Use a two-phase commit protocol to manage the update process. In the first phase, both off-chain and on-chain systems will agree to commit the transaction. In the second phase, the actual commit is performed. If either system cannot comply, a rollback is executed to maintain data consistency.\n\n4. **Consistency Check Routine:**\n   - Develop a consistency check routine that runs periodically. This routine will scan and compare off-chain data against its on-chain references. Any discrepancies will trigger alert mechanisms and attempt automated reconciliation where possible.\n\n5. **Enhanced Error Handling:**\n   - Enhance error handling mechanisms in both off-chain and on-chain systems to detect and respond to inconsistencies more robustly. This includes setting up alerts and fallback procedures when a data inconsistency is detected.\n\n6. **Versioning and Deletion Policy:**\n   - Introduce versioning for the off-chain data, enabling the system to maintain historical states. In case of an inconsistency, the system can roll back to a previously consistent state. Have a robust deletion policy that ensures that references are removed only when both off-chain and on-chain data deletion is confirmed.\n\n7. **Synchronize Timestamps:**\n   - Utilize synchronized timestamps to mark data entries both off-chain and on-chain. This can assist in tracking and reconciling data discrepancies by identifying the exact point of divergence.\n\nBy implementing these strategies, we aim to ensure robust data consistency within the hybrid storage system and prevent potential corruption arising from asynchronous data state changes.","GenTime":"2024-07-28 22:57:57"}
{"File Name":"origin\/0010-more-complete-database-type-and-adjust-migrations.md","Context":"## Context\\nSo far, Origin has been using a development-friendly SQLite database on the backend to store data. While this has been beneficial in the early stages of development, in order to gain adoption and real-world use we should make it easier for anyone using the Origin SDK to get to production as fast as possible.\\n\n## Decision\n","Decision":"A decision has been made to move the Origin SDK to a mode production-ready database. We've decided to go with **PostgreSQL** instead of SQLite, as we've seen that this is the database of choice for many users and setups.\\n","tokens":64,"id":1561,"Prediction":"## Decision\n\nAfter evaluating the current and future needs for the Origin SDK, we have decided to migrate from SQLite to a more robust, production-ready relational database. The chosen database for this transition is PostgreSQL.\n\n**Rationale:**\n\n1. **Scalability:** PostgreSQL offers better scalability options compared to SQLite, catering to anticipated growth and increased data throughput.\n   \n2. **Performance:** PostgreSQL is optimized for complex queries and larger datasets, offering a performance advantage in multi-user environments.\n   \n3. **Advanced Features:** PostgreSQL supports advanced features like concurrency control, better indexing capabilities, and full-text search that are essential for more complex, real-world use cases.\n   \n4. **Security:** PostgreSQL has more comprehensive security features that are critical for production environments, including robust user authentication and data encryption options.\n   \n5. **Community and Support:** PostgreSQL has a large, active community and a wealth of documentation and third-party support, which can aid in smoother deployment and troubleshooting.\n\n**Action Plan:**\n\n1. **Schema Migration:** Design and implement a migration strategy from SQLite schema to PostgreSQL schema, ensuring data integrity and minimal disruption.\n   \n2. **Codebase Updates:** Refactor the existing codebase to replace SQLite-specific queries and database interactions with PostgreSQL-compatible alternatives.\n   \n3. **Deployment:** Set up PostgreSQL instances for development, testing, and production environments, ensuring that best practices for configuration, backup, and disaster recovery are followed.\n   \n4. **Testing:** Implement thorough testing processes to validate the migration and ensure that all functionalities perform as expected with the new database system.\n   \n5. **Documentation:** Update the SDK documentation to guide users through setting up and using PostgreSQL with Origin SDK.\n\n6. **Support:** Provide support to existing users during the transition period, including detailed migration guides and troubleshooting tips.\n\nThis decision is aimed at enhancing the reliability, security, and performance of the Origin SDK as it transitions from early development into widespread adoption and real-world use.","GenTime":"2024-07-28 22:58:08"}
{"File Name":"origin\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1562,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) process using Markdown to document architectural decisions since it is simple, lightweight, and integrates well with our existing version control system (Git).\n\n### Details:\n- **Format**: Each ADR will be a Markdown (*.md) file.\n- **Repository**: ADRs will be stored in a centralized `adr` directory within the project's main repository.\n- **Template**: A standardized Markdown template will be used for consistency. This template will include sections such as:\n  - Title\n  - Date\n  - Status (Proposed, Accepted, Deprecated, etc.)\n  - Context\n  - Decision\n  - Consequences\n- **Versioning**: ADRs will be versioned via the Git system to maintain a historical record of changes.\n- **Review Process**: ADRs will be submitted as pull requests and require approval from designated reviewers (e.g., lead architects or senior engineers) before being merged into the main branch.\n- **Tooling**: Optionally, we may use existing ADR tools (e.g., adr-tools) to automate the creation and management of these records.\n  \nThis approach ensures that architectural decisions are well documented, easily accessible, and version controlled, which will aid in maintaining clarity and consistency throughout the project's lifecycle.","GenTime":"2024-07-28 22:58:11"}
{"File Name":"mediawiki-extensions-WikibaseLexeme\/0003-adr-with-all-team-members.md","Context":"## Context\\nWe use architectural decision records (ADR) on this project.\\nThe project's code base is owned by the team, the team organizes itself (see Journey model) into smaller, short lived \"journey\" sub-units to - amongst others - increase focus. Architecture decisions affect the project for a long time, and will likely soon be faced by developers that were not part of the journey at the time. Consequently their feedback about the architectural decision is inevitable. Additionally, given the intentionally small size of an individual journey's group, the amount of opinions concerning any given ADR could be as small as one or two, should the ADR be voted upon by members of the same journey exclusively. To avoid a flood of ADRs trying to unwrite each other and to increase the standing of ADRs in general and the quality of the individual ADR they should be vetted (RFC) by the entire team.\\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\\n\n## Decision\n","Decision":"We put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.\\n","tokens":214,"id":1563,"Prediction":"## Decision\n\nIn order to maintain a coherent and sustainable architectural framework, ensure inclusivity in architectural decisions, and avoid redundancy or conflict among Architectural Decision Records (ADRs):\n\n1. **Centralized Review Process**: ADRs will undergo a centralized review process involving the entire project team. This ensures that decisions have broad support and consider a diversity of perspectives, mitigating the risk of decisions being made by an overly small or homogeneous group.\n\n2. **Request for Comments (RFC)**: ADR proposals will be shared with the entire team through an RFC process. This allows everyone to provide feedback and ensures that decisions have been thoroughly vetted.\n\n3. **Time-Limited Feedback Loop**: To prevent analysis paralysis and ensure timely decision-making, the RFC period will be limited to a predefined duration, for example, one week. If significant objections or alternative suggestions are raised, an additional focused discussion period may be initiated, but this should not excessively delay the decision-making process.\n\n4. **Clear Decision Criteria**: Decisions will be guided by predefined criteria such as alignment with project goals, long-term maintainability, and technical soundness. These criteria will help ensure that decisions are rational and transparent.\n\n5. **Documentation and Visibility**: All ADRs, along with the feedback received during the RFC process and the final decision-making rationale, will be documented and made accessible to the entire team. This documentation will help future team members understand the context and reasoning behind past decisions, and will reduce the likelihood of redundant or conflicting ADRs.\n\nThis decision sets a structured approach to making architectural decisions that leverages the collective expertise of the entire team while maintaining agility and minimizing disruptions to ongoing work.","GenTime":"2024-07-28 22:58:15"}
{"File Name":"mlmc\/0002-use-mongodb-for-operation.md","Context":"## Context and Problem Statement\\nOperation API need some data storage to be able to list\/commission\/decommission\/launch missiles.\\n## Decision Drivers\\n* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\n\n## Decision\n","Decision":"* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\nDecided to use MongoDB since\\n* The product growths without strict requirements, so entitites are changed very often. In this circumstances schemaless approach is better than predefined schema approach\\n* No need to use RDBMS just to store some random missiless on a stock\\n* I wanted to get some experience in MongoDB\\n### Positive Consequences\\n* Implementation will be faster and easier since schema is going to be changed lot of times\\n### Negative Consequences\\n* n\/a\\n","tokens":48,"id":1574,"Prediction":"## Decision\n\nWe have decided to use a NoSQL database for the Operation API's data storage requirements. Specifically, we will use MongoDB to handle the storage needs for missile operations including listing, commissioning, decommissioning, and launching missiles.\n\n## Rationale\n\n1. **Easy to Use Storage**: MongoDB is known for its ease of use, especially in dynamic development environments. Its document-based model is more intuitive and human-readable, which simplifies the process of storing and retrieving data.\n\n2. **Fast Storage**: MongoDB is designed for high performance and offers fast read and write operations. This aligns with the needs of our API, which requires quick responses for missile operations.\n\n3. **Dynamic and\/or Schemaless Storage**: MongoDB's schemaless nature allows us to handle flexible data models without the need for complex migrations. This adaptability is crucial as the data structure for missile operations may evolve over time.\n\nBy using MongoDB, we can achieve a balance between ease of use, performance, and flexibility, ensuring the Operation API meets its requirements effectively.","GenTime":"2024-07-28 22:59:01"}
{"File Name":"mlmc\/0003-use-rabbit-mq-for-eda.md","Context":"## Context and Problem Statement\\nDifferent services are going to push different kind of messages so there is a need in EDA architecture and some messaging server.\\n## Decision Drivers\\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\n\n## Decision\n","Decision":"* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\nDecided to use RabbitMQ since\\n* It provides exact functionality that is neded for the product (publish-subscribe event messaging)\\n* It is easy to learn and use and I already had some knowledge of how to use it\\n* It has official container on Docker Hub\\n* It seems to be popular and trending these days\\n### Positive Consequences\\n* I will be able to setup EDA messaging in the product.\\n### Negative Consequences\\n* n\/a\\n","tokens":53,"id":1575,"Prediction":"## Decision \n\nAfter careful evaluation of the requirements and decision drivers, we have decided to implement an Event-Driven Architecture (EDA) using **Apache Kafka** as our messaging server. \n\n## Rationale\n\n**Apache Kafka** has been chosen based on the following rationale:\n\n1. **Easy to Use**: Kafka provides a streamlined publish-subscribe model that simplifies the process of message distribution across various services. Its well-documented APIs make it easier to integrate and use.\n\n2. **Easy to Learn**: Kafka has a large community and extensive documentation available, making it relatively simple for our developers to learn and start using. Numerous tutorials and online courses can help to quickly bridge any knowledge gaps.\n\n3. **Possibility to Run from Docker**: Kafka can be easily containerized. Docker images for Kafka are readily available, allowing our team to deploy Kafka in a consistent environment across different stages of development, testing, and production. Running Kafka alongside tools like ZooKeeper in Docker provides a scalable and easily manageable setup.\n\n4. **Scalability**: Kafka is known for its high throughput and scalability, allowing us to handle a large volume of messages efficiently, which is essential for our heterogeneous set of services.\n\n5. **Fault Tolerance**: Kafka\u2019s robust replication and fault tolerance capabilities ensure high availability and reliability of our messaging system, which is critical for maintaining consistent service operation.\n\nBy adopting Apache Kafka, we ensure that our message-driven interactions among different services are efficient, scalable, and reliable while meeting the crucial decision drivers.","GenTime":"2024-07-28 22:59:06"}
{"File Name":"SoundCloudVisualizer\/0004-use-gulp-to-build-source-code.md","Context":"## Context\\nThe application's JS code was loaded via `script` tags in `index.html`. As the project was migrated from Vanilla JS to AngularJS the number of source files that needed to be included grew, and this approach became unwieldy.\\nI looked into how we could bundle our code. At the time of development Grunt and Gulp were the two main task runners available.\\n\n## Decision\n","Decision":"[Gulp](https:\/\/gulpjs.com\/) will be used to bundle client-side code.\\n","tokens":81,"id":1576,"Prediction":"## Decision\n\nWe decided to adopt Gulp as our task runner for bundling the AngularJS code. This decision was based on several key factors:\n\n1. **Modularity and Flexibility**: Gulp's use of streams provides a more modular and flexible approach to defining and executing tasks. This aligns well with our growing codebase and the need for more specialized build processes.\n\n2. **Popularity and Community Support**: At the time, Gulp had a larger and more active community compared to Grunt, ensuring better support, more plugins, and more frequently updated resources.\n\n3. **Efficiency**: Gulp's streaming build system is typically faster than the temporary file-based system used by Grunt. This allows for quicker build times, which is critical as our codebase continues to expand.\n\n4. **Ease of Use**: Gulp's code-over-configuration paradigm made it easier for our development team to customize tasks and pipelines without dealing with complex configuration files.\n\nBy integrating Gulp into our workflow, we intended to streamline our build process, maintain efficiency, and make it easier to manage and deploy our growing number of source files.","GenTime":"2024-07-28 22:59:09"}
{"File Name":"iampeterbanjo.com\/0009-place-tests-next-to-source-files.md","Context":"## Context\\nWhile migrating to Typescript, fixing tests means switching between the source file and their test files. Where the test files are in another folder, the journey is longer. If they were side-by-side then once I had the source or test, finding the related file would be much easier\\n\n## Decision\n","Decision":"Moving test files next to the source code makes it much easier to switch between source and tests. It's also easier to see which test files do not have any unit tests.\\n","tokens":60,"id":1588,"Prediction":"## Decision\nTo streamline the development process during the TypeScript migration, and to enhance ease of navigation between source files and their corresponding test files, we will adopt a new directory structure where test files will be placed side-by-side with their respective source files. This means placing each test file in the same directory as the relevant source file, following a naming convention that clearly distinguishes them.\n\nFor example:\n```\n\/src\n  |-- someModule.ts\n  |-- someModule.test.ts\n```\n\nThis decision aims to minimize context-switching, reduce navigation time, and improve overall developer productivity during and after the migration to TypeScript.","GenTime":"2024-07-28 22:59:47"}
{"File Name":"iampeterbanjo.com\/0012-parallel-database-tests-mongodb-server.md","Context":"## Context\\nJest tests are fast because they can be run in parallel. If we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. There are two ways to decouple tests:\\n- [define databases in tests][defined-test-database]\\n- [randomly create databases for each test][random-test-database]\\n\n## Decision\n","Decision":"In the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the `beforeAll` and `afterAll` hooks because this is more flexible and is less coupled to Jest's idiosyncracies.\\n","tokens":80,"id":1589,"Prediction":"## Decision\n\nWe decided to randomly create databases for each test. This approach ensures complete isolation between tests, eliminating potential race conditions and data conflicts that could arise if multiple tests access and modify the same database concurrently.\n\n### Reasoning\n1. **Isolation of Tests**: By giving each test its own randomized database, we ensure that no test can interfere with another. This is crucial for parallel test execution.\n2. **Avoid Race Conditions**: Shared databases can lead to unpredictable outcomes due to race conditions. Randomly creating databases for each test prevents such inconsistencies.\n3. **Simplicity and Maintainability**: Maintaining separate configuration settings within the tests for defined databases may become cumbersome, especially for large test suites. Automating database creation reduces manual setup and potential configuration errors.\n4. **Reusability**: Using randomized databases can facilitate reusing test scripts in different environments without worrying about conflicts arising from previous test runs.\n\n### Implementation\n- For each test run, generate a unique database identifier.\n- Set up the database with necessary schema and seed data before the test execution.\n- Ensure that the database is properly cleaned up or disposed of after the test completes to avoid resource leakage.\n\nBy adopting this strategy, we can leverage Jest's parallel test execution capabilities fully while maintaining robust and reliable test isolation.\n","GenTime":"2024-07-28 22:59:50"}
{"File Name":"iampeterbanjo.com\/0008-use-ava-test-runner.md","Context":"## Context\\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\\n\n## Decision\n","Decision":"Jest seems like the way to go:\\n- Typescript support\\n- Large community\\n- Familiar\\n","tokens":91,"id":1594,"Prediction":"## Decision\n\nConsidering the issues with [Lab][hapi-lab] and the performance concerns related to Ava as identified in [this review][dodds-jest], I have decided to adopt [Jest][jest] for our testing framework. \n\nJest provides robust support for TypeScript, extensive documentation, and a vibrant community which ensures better plug-and-play capabilities with various tools and plugins. Additionally, Jest's built-in code coverage reporting is reliable, which will address the current misreporting issues we are experiencing with Lab. \n\nThis decision aligns with our needs for accurate code coverage, TypeScript support, and a strong ecosystem for potential extensions and integrations.","GenTime":"2024-07-28 23:00:17"}
{"File Name":"iampeterbanjo.com\/0007-module.exports-vs-export-default.md","Context":"## Context\\nWhen HapiJS plugins are registered they expect an object like this:\\n```JavaScript\\n{\\nname: 'plugin-name',\\nversion: '1.0.0',\\nregister: (server, options) => {}\\n}\\n```\\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\\n```JavaScript\\nimport plugin from '.\/plugin';\\nexport default {\\nplugin,\\n}\\n```\\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\\nSo let's use `module.exports` instead of `export default`.\\n\n## Decision\n","Decision":"Rejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\\n","tokens":202,"id":1596,"Prediction":"## Decision\nWe will use `module.exports` instead of `export default` for our HapiJS plugins. This decision ensures compatibility with HapiJS's CommonJS module system and simplifies the registration process for our plugins.\n\nTo implement this:\n\n1. Replace any `export default` statements with `module.exports`.\n   \n   For example:\n   ```JavaScript\n   module.exports = {\n       name: 'plugin-name',\n       version: '1.0.0',\n       register: (server, options) => {\n           \/\/ Plugin registration logic\n       }\n   };\n   ```\n\n2. When importing and using plugins within your HapiJS application or other CommonJS-based modules, use `require` instead of `import`.\n\n   For example:\n   ```JavaScript\n   const plugin = require('.\/plugin');\n   ```\n\nBy adopting `module.exports`, we ensure that our plugins remain compatible with HapiJS's module expectations, avoid confusion regarding module formats, and maintain consistency across our codebase.","GenTime":"2024-07-28 23:00:26"}
{"File Name":"iampeterbanjo.com\/0011-artist-images-from-spotify.md","Context":"## Context\\nKorin would look much better with artist images. Unfortunately LastFm [removed artist images][lastfm-images-gone] from their API and have replaced it with a placeholder. There are some options to get artist images which are:\\n- [Music story][music-story-api]: The documentation is not easy to understand and the SDK is designed for web browsers. It also uses a custom ID to associate artists, not IMDB. One unique feature is that it has lyrics too.\\n- [Music brainz][music-brainz-api]: Creative commons but has everything except artist images.\\n- [Spotify][spotify-api]: Has artist information including images but uses a custom ID, not IMDB.\\n\n## Decision\n","Decision":"Spotify is the only viable alternative.\\n","tokens":148,"id":1598,"Prediction":"## Decision\nAfter evaluating the available options, we have decided to integrate the Spotify API for obtaining artist images in Korin.\n\n**Rationale**:\n- **Comprehensive Data**: The Spotify API provides not just artist images but also extensive information about artists, which can be beneficial for future features.\n- **Image Quality and Availability**: Spotify maintains a high standard for artist images, which will enhance the visual appeal of Korin.\n- **Integration Feasibility**: Despite using a custom ID system, the Spotify API is well-documented and widely used, making it more accessible for integration.\n\nWhile Music Story also offers artist images and additional features like lyrics, its documentation and API design are tailored more for web browsers, adding complexity to our implementation. Music Brainz, though robust and under a Creative Commons license, lacks the crucial feature of artist images.\n\nTherefore, integrating the Spotify API is the most balanced and future-proof option for our needs.","GenTime":"2024-07-28 23:00:32"}
{"File Name":"iampeterbanjo.com\/0002-blog-engine.md","Context":"## Context\\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\\n### [11ty][4]\\nPros\\n- Easy setup\\n- Predictable static file output\\n- Relatively small codebase\\n- Supporst different template engines\\nCons\\n- The frontmatter parser [didn't work as I expected][7]\\n- Template errors were not reported and would only fail on build\\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\\n### [Gatsby][5]\\nPros\\n- Modern [JAMstack][8] project with React\\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\\n- Opportunity to use GraphQL\\nCons\\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\\n- Developing static files based on API was difficult because of caching and unexpected error handling\\n- Relatively complex project with separate build, develop steps, config, plugins etc\\n### [Hugo][6]\\nPros\\n- Fast and mature static site generator\\n- Small footprint\\nCons\\n- Written in Go - I prefer fullstack JavaScript for this site\\n\n## Decision\n","Decision":"In the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\\n","tokens":300,"id":1599,"Prediction":"## Decision\n\nAfter evaluating the various static site generators, each with its own set of pros and cons, the decision is to proceed with **11ty** for the following reasons:\n\n1. **Simplicity and Predictability**: 11ty offers an easy setup with a predictable static file output, which is crucial for a straightforward blogging site. The relatively small codebase aligns well with our goal of maintaining a lean and manageable project structure.\n\n2. **Flexibility with Templates**: The need to support different template engines is well accommodated by 11ty. This versatility ensures that the website can evolve or include different templating languages as needed.\n\n3. **Readiness for Production**: Although 11ty needs some additional work to make the site production-ready (such as SEO, image optimizations, favicons, etc.), these aspects can be managed through plugins or manual configuration. The predictability of the file output makes this manageable.\n\n**Rejected Alternatives**:\n\n1. **Gatsby**: Despite the modern approach and built-in features for SEO, images, and GraphQL, Gatsby's reliance on client-side JavaScript for static file output and service worker caching introduces unnecessary complexity and unpredictable behavior, making it a less suitable choice.\n\n2. **Hugo**: The speed and maturity of Hugo are commendable. However, Hugo's foundation in Go, as opposed to full-stack JavaScript, does not align with the technological preferences and existing skill set for this project.\n\n### Summary\n11ty is chosen because it offers the best balance of simplicity, flexibility, and control over the static files. Moreover, it aligns well with the preference for a full-stack JavaScript environment, making the development and maintenance easier and more streamlined. The additional production considerations will be addressed separately to ensure a robust and high-quality blog site.","GenTime":"2024-07-28 23:00:36"}
{"File Name":"dogma\/0018-projection-compaction.md","Context":"## Context\\nMany projections produce data that is only required for a limited period of\\ntime.\\nAn application developer needs to consider how long projection data will be\\nretained and ideally implement measures to remove any unnecessary data.\\nOften such data can be removed when some future event occurs. However, in some\\ncases there is no future event that appropriately indicates the data is no\\nlonger required.\\nWe have encountered a genuine example of this when implementing an SQL\\nprojection that inserts a row whenever a user performs a specific action. The\\ndata is queried in order to enforce a \"cool down\" that prevents the user from\\nrepeating that action again within a specific time frame.\\nThe insert is triggered by the occurrence of an event, but the data becomes\\nunnecessary whenever the \"cool down\" time has elapsed.\\nIn this particular use case the \"cool down\" was not part of the business logic,\\nbut rather an API level restriction. Hence, processes\/timeout messages were not\\nthe appropriate solution.\\n\n## Decision\n","Decision":"We have decided to add a `Compact()` method to `ProjectionMessageHandler`.\\nThe implementation of `Compact()` can modify the projection's data by whatever\\nmeans is appropriate such that unnecessary data is removed but the projection\\nstill serves its purpose.\\n","tokens":210,"id":1606,"Prediction":"## Decision\nUtilize a time-based automatic cleanup mechanism for the projection data related to the specific user action.\n\n### Rationale:\nGiven the projection data's temporary relevance, and the absence of a triggering event to signal when data becomes obsolete, a time-based cleanup solution aligns well with the \"cool down\" period's expiration. This approach will ensure that the database remains lightweight and efficient by automatically purging outdated entries, without the need for manual intervention or complex logic.\n\n### Implementation Approach:\n1. **Timestamping Records**: Each row inserted into the SQL projection table will have a timestamp indicating when the user action occurred.\n2. **Scheduled Cleanup Job**: Implement a periodic job (e.g., a cron job or SQL scheduled job) that runs at regular intervals to delete rows where the \"cool down\" period has lapsed based on the timestamp.\n3. **Database Indexing**: Apply proper indexing on the timestamp column to ensure that the cleanup operation runs efficiently without causing significant database load.\n\n### Advantages:\n- **Automated Data Management**: This solution minimizes manual overhead by automating the cleanup process.\n- **Resource Efficiency**: Regularly purging obsolete data helps maintain optimal database performance and storage utilization.\n- **Simplicity**: A time-based approach offers a straightforward and scalable solution without the need for additional business logic or event handling.\n\n### Considerations:\n- **Frequency of Cleanup**: The interval at which the cleanup job runs should be balanced to avoid unnecessary overhead while ensuring timely removal of obsolete data.\n- **Concurrency Management**: Ensuring that the cleanup process does not interfere with ongoing operations, especially during peak load times.\n\nBy implementing this time-based automatic cleanup mechanism, the application can efficiently manage projection data lifecycle and maintain its performance and effectiveness.","GenTime":"2024-07-28 23:01:09"}
{"File Name":"dogma\/0008-location-of-testing-features.md","Context":"## Context\\nWe need to decide whether Dogma's testing features should reside in the `dogma`\\nmodule itself, or a separate `dogmatest` module.\\n### Benefits to separate module\\n1. They can be versioned separately. A `dogma` release can be made without\\nhaving to update the testing tools. This is a _pro_ for the releaser, but\\npossibly a _con_ for the users.\\n1. Users that opt not to test, or not to test using our tools do not need to\\ndownload `dogmatest` at all. This is not a particularly strong argument.\\n1. We can make BC breaking changes to `dogmatest`, without having to make\\na new major release of `dogma`. We would need to document clearly which\\nmajor `dogma` versions are supported by which major `dogmatest` versions.\\n### Benefits to a single module\\n1. The user does not need to determine which `dogmatest` version to use with\\ntheir application; compatible tools are always available right from `dogma`. If\\nwe want to encourage users to test their domain logic using these utilities;\\nthey should be easy to access.\\n1. As mentioned in [#16](github.com\/dogmatiq\/dogma), splitting the testing\\ntools would mean that the example code would also need to be moved elsewhere.\\nHowever, we have since already decided to move the example code to a separate\\nmodule in [ADR-7](0007-location-of-examples.md).\\n\n## Decision\n","Decision":"For the same reasons [ADR-7](0007-location-of-examples.md) we've decided to\\nimplement testing tools in their own module.\\n","tokens":324,"id":1608,"Prediction":"## Decision\nWe have decided to **integrate Dogma's testing features within the `dogma` module itself** rather than creating a separate `dogmatest` module.\n\n### Rationale\n1. **Ease of Use for Users:** Embedding the testing features directly within the `dogma` module ensures that users always have access to compatible testing tools without the need to determine and download `dogmatest`. This approach simplifies the user experience and encourages the use of provided testing utilities.\n\n2. **Alignment with Project Goals:** Since our goal is to encourage users to test their domain logic with the provided utilities, it is crucial that these tools are as accessible as possible. Shipping them within the `dogma` module ensures that there are no barriers to their adoption.\n\n3. **Simplified Version Management for Users:** Keeping the test tools within the `dogma` module allows users to manage a single version of the module, instead of tracking compatibility across multiple modules. This minimizes the complexity in managing dependencies.\n\n4. **Reduced Maintenance Overhead:** Maintaining a single module means there's no need to document and ensure compatibility between `dogma` and `dogmatest` versions. It streamlines the release process and reduces the overhead of separate versioning.\n\n5. **Impact of Example Code Relocation:** While the example code has already been moved to a separate module according to [ADR-7](0007-location-of-examples.md), it does not significantly impact the decision to keep testing features within the `dogma` module. The primary focus remains on ease of use and adoption.\n\nBy making this decision, we ensure that Dogma's testing tools are always at the users' fingertips, fostering better practices in testing without imposing additional effort on version management, thereby providing a seamless development experience.","GenTime":"2024-07-28 23:01:19"}
{"File Name":"dogma\/0011-message-timing-information.md","Context":"## Context\\nWe need to decide whether message timing information should be exposed via the\\nAPI. In this context \"timing information\" refers to important points in time\\nthroughout the lifecycle of a message.\\nThe initial rationale for *not* exposing these timestamps was that any business\\nlogic that depends on time in some way should explicitly include any timing\\ninformation within the message itself. We call such logic \"time-based\" and the\\napproach of including explicit timing information \"modeling time\".\\n\n## Decision\n","Decision":"The sections below focus on each of the message roles, their respective\\ntimestamps of interest, and the decisions made in each case.\\n### Command Messages\\nWe believe the existing requirement that the application \"model time\" is still\\nappropriate for command messages. The time at which the command message is\\ncreated or enqueued is irrelevant; any time information relevant to the domain\\nlogic should be included in the message itself.\\n**We have decided not to expose the command creation time.**\\n### Event Messages\\nThe time at which an event is recorded is a fundamental property of the event\\nitself. Put another way, every event occurs at some time regardless of whether\\nthe domain is time-based.\\nFurthermore, the time at which the event occurs may be relevant to some\\nancillary domain logic that is *triggered* by the event, even if the aggregate\\nthat *produced* the event has no time-based logic.\\nThe inclusion of the \"occurred time\" as a fundamental property of the event is\\nsupported by [Implementing Domain Driven\\nDesign](https:\/\/www.amazon.com\/Implementing-Domain-Driven-Design-Vaughn-Vernon\/dp\/0321834577),\\nChapter 8, in the \"Modeling Events\" section.\\n**We have decided to include a `RecordedAt()` method on `ProcessEventScope` and `ProjectionEventScope`.**\\nIn actuality, a `Time()` method had already been added to `ProjectionEventScope`\\nwithout any supporting ADR, this method is to be renamed.\\n### Timeout Messages\\nThe time at which a timeout message is scheduled to be handled is a fundamental\\nproperty of the timeout concept.\\nBy definition, the use of a timeout message indicates that there is time-based\\nlogic. It seems like an unnecessary imposition to require the application\\ndeveloper to include the scheduled time in the message.\\n**We have decided to include a `ScheduledFor()` method on `ProcessTimeoutScope`.**\\n","tokens":101,"id":1610,"Prediction":"## Decision\n\nAfter thorough consideration, the decision is made to **expose message timing information via the API**.\n\n### Rationale:\n1. **Enhanced Observability and Traceability:**\n   - Exposing timing information will provide better insights into the lifecycle of messages, making it easier to monitor, debug, and trace any issues that arise.\n   \n2. **Simplifies Integration for Clients:**\n   - Clients will benefit from having direct access to timing information, simplifying their implementations and reducing the potential for discrepancies in time-based logic.\n   \n3. **Flexibility for Future Requirements:**\n   - Providing timing information upfront allows for flexibility in meeting future business or regulatory requirements without necessitating changes to the message schema.\n   \n4. **Decouples Business Logic from Infrastructure Concerns:**\n   - Keeping timing information separate from the business message itself ensures a clearer separation of concerns, making the message schema more focused and straightforward.\n   \n5. **Consistency with Common Practices:**\n   - Many modern APIs include timing information in their responses to aid in performance monitoring and analytics, which aligns with industry standards and best practices.\n\n### Implementation Considerations:\n- **Security and Privacy:**\n  - Ensure that timing information does not expose any sensitive data or system internals that could be exploited.\n  \n- **Format and Precision:**\n  - Establish a standardized format and precision level for the timing information (e.g., ISO 8601 for timestamps), ensuring consistency across the API.\n  \n- **Impact on Performance:**\n  - Assess and mitigate any potential performance impacts of collecting and transmitting timing information.\n  \n- **Versioning:**\n  - Introduce these changes in a backward-compatible way, possibly through versioning of the API.\n\n### Next Steps:\n1. Define the specific timing points to be exposed (e.g., message received, processed, and delivered).\n2. Update the API documentation to include details about the timing information.\n3. Implement the necessary changes in the API to collect and return the timing information.\n4. Perform testing to ensure the accuracy and performance of the timing data.\n5. Communicate the changes to all stakeholders and provide guidance on utilizing the new data points.","GenTime":"2024-07-28 23:01:33"}
{"File Name":"dogma\/0015-routing-unrecognized-messages.md","Context":"## Context\\n[ADR-14](0014-apply-historical-events-to-aggregates.md) relaxed the\\nspecification such that `AggregateRoot.ApplyEvent()` implementations were no\\nlonger required to panic with an `UnrecognizedMessage` value when passed an\\nunexpected message type.\\nPrompted by this requirement, we relaxed the requirement for ALL handler\\nmethods, which was likely too broad of a change.\\nSpecifically, unlike when handling a message, the routing methods\\n`AggregateMessageHandler.RouteCommandToInstance()` and\\n`ProcessMessageHandler.RouteEventToInstance()` do not have the option of \"doing\\nnothing\" when passed an unexpected message type.\\n\n## Decision\n","Decision":"Reinstate the hard requirement that the handlers MUST panic with\\n`UnexpectedMessage` when asked to route a message type that was not configured\\nas being consumed by that handler.\\n","tokens":138,"id":1612,"Prediction":"## Decision\n\nWe will narrow the relaxation of the specification change to only affect the `AggregateRoot.ApplyEvent()` method rather than all handler methods. \n\n1. **Scope Reversion**: Revert the broad change that allowed all handler methods to no longer panic on receiving unexpected message types. This change will now only apply to the `AggregateRoot.ApplyEvent()` implementations.\n2. **Specificity in Routing Methods**: Reinforce the requirement for `AggregateMessageHandler.RouteCommandToInstance()` and `ProcessMessageHandler.RouteEventToInstance()` to explicitly handle unexpected message types. Rather than being able to \"do nothing,\" these methods will be required to either:\n   - Log an appropriate error message and return a predefined error.\n   - Implement a predetermined fallback action or a default response mechanism for unexpected message types.\n3. **Documentation Update**: Update the relevant documentation and specifications to reflect this specific and more targeted application of allowed non-panic behavior, and to clearly outline the expected behavior and handling of unexpected message types in routing methods.\n4. **Testing and Validation**: Ensure additional unit tests and validation mechanisms are in place to verify the correct behavior of `AggregateMessageHandler.RouteCommandToInstance()` and `ProcessMessageHandler.RouteEventToInstance()` when handling both expected and unexpected message types.\n\nBy refining the relaxation to apply only to the `AggregateRoot.ApplyEvent()` method, we ensure that other critical components continue to handle unexpected message types in a controlled and predictable manner, maintaining system robustness and reliability.","GenTime":"2024-07-28 23:01:44"}
{"File Name":"dogma\/0004-adr-process.md","Context":"## Context\\nWe need a documented process for proposing, discussing and ultimate accepting or\\nrejecting ADRs via pull requests.\\n\n## Decision\n","Decision":"We have decided to adopt a process that favours rapid changes, at least while\\nthe project is in its infancy. To this end, we will allow ADRs in the `proposed`\\nstatus to be merged to `main` via PRs. The PRs may contain related code changes.\\nWhenever a `proposed` ADR is merged to `main`, a GitHub issue is created to\\ncapture discussion about the ADR. Such issues are given the `adr` label.\\nAny `proposed` ADRs remaining on `main` must be resolved either by approving\\nthe ADR, or by rejecting it and reverting any associated code changes.\\n","tokens":27,"id":1614,"Prediction":"## Decision\n\nWe will implement a documented and structured workflow for Architectural Decision Records (ADRs) using pull requests on our version control system (e.g., GitHub, GitLab). This workflow will consist of the following steps:\n\n1. **Proposal Submission:**\n   - A new ADR will be proposed via a pull request. The proposer must include a detailed ADR document, following a predefined template, that describes the context, problem, considered options, and the proposed decision.\n\n2. **Discussion Period:**\n   - Once the pull request is submitted, a discussion period will commence. During this period, team members will review the ADR, provide feedback, ask questions, and suggest modifications. The proposer is responsible for addressing feedback and iteratively improving the ADR document within the pull request.\n\n3. **Decision Meeting:**\n   - After an adequate discussion period and once consensus seems likely, a decision meeting will be scheduled. Key stakeholders and team members will gather (either physically or virtually) to discuss any final concerns and make a preliminary decision.\n\n4. **Finalization:**\n   - Based on the outcomes of the decision meeting, the proposer will update the ADR document as necessary. The updated document will then go through a final review by the Engineering Lead or designated approver.\n\n5. **Acceptance or Rejection:**\n   - Once the final review is completed without any outstanding issues, the pull request will be either accepted or rejected:\n     - If accepted, the ADR is merged into the main repository, and an entry is added to the project\u2019s ADR index.\n     - If rejected, feedback is documented to explain the decision, and the pull request is closed.\n\n6. **Documentation Update:**\n   - Any related documentation or project artifacts affected by the approved ADR must be updated to reflect the new architectural decision.\n\n7. **Notification:**\n   - A notification (via email, internal messaging, etc.) will be sent to all stakeholders to inform them of the new decision and any next steps.\n\nThis process ensures that all ADRs are evaluated comprehensively, with inputs from all relevant parties, and that decisions are documented systematically and transparently.","GenTime":"2024-07-28 23:01:54"}
{"File Name":"dogma\/0012-identifier-comparison.md","Context":"## Context\\nIdentifiers (the names and keys used to identify applications and handlers) must\\nbe compared by engines to determine if two such entities are to be considered\\nequivalent.\\nThe documentation specifies that such keys must be non-empty UTF-8 strings\\nconsisting of printable characters without whitespace, but it did not previously\\nspecify how such strings would be compared.\\nThese identifiers are either mostly or entirely immutable and generated as part\\nof the source code. They do not need to be parsed and validated from user input.\\n\n## Decision\n","Decision":"In keeping with current behavior, we've decided to specify byte-wise comparison\\nsemantics for identifiers.\\n","tokens":109,"id":1618,"Prediction":"## Decision\nIdentifiers will be compared using a case-sensitive, exact match approach. Given that these identifiers are non-empty UTF-8 strings consisting of printable characters without whitespace, and they are generated as part of the source code (making them either mostly or entirely immutable), the chosen comparison method aligns well with their intended usage.\n\nMoreover, an exact match comparison ensures that there is no ambiguity in identifying applications and handlers, which is crucial for maintaining the integrity and reliability of the software's internal mechanisms. The identifiers' immutability and generation within the source code further justify the decision, as it minimizes the risk of inconsistencies or errors arising from manual inputs or case variations.\n\nThis approach is straightforward and efficient, discouraging potential complications that might occur with alternative comparison methods, such as case-insensitive comparisons or normalization processes. By adhering to this decision, the software can ensure consistent and predictable behavior when dealing with identifiers.","GenTime":"2024-07-28 23:02:07"}
{"File Name":"admin-react-components\/0003-component-bundling.md","Context":"## Context\\nWhen bundling components for usage in consuming applications, should we require consuming applications to have the same dependencies as this repository? Or should exported components be packaged assuming no dependencies?\\n\n## Decision\n","Decision":"We'll package components with their own inline styles. No external dependencies needed.\\n","tokens":38,"id":1631,"Prediction":"## Decision\n\nWe have decided to package exported components assuming no dependencies from the consuming applications. This means that all necessary dependencies will be bundled together with our components. The approach will be to use a bundler tool to package both the components and their requisite dependencies together.\n\n### Rationale:\n\n1. **Ease of Use**: By packaging all necessary dependencies along with the components, we ensure that consumers have a smoother integration experience. They will not need to manage or match the exact versions of dependencies that our components require.\n\n2. **Version Control**: This method reduces the risk of version conflicts or dependency hell in the consuming application. Each component will work with its dedicated version of the dependencies.\n\n3. **Consistency**: Ensuring that each exported component is self-contained leads to more predictable behavior across different consuming applications, regardless of the dependencies they use.\n\n4. **Maintenance**: It simplifies maintenance and support responsibilities as we can ensure all dependencies are compatible with each other within our own package, instead of relying on external applications to manage this.\n\n### Considerations:\n\n- **Potential for Increased Bundle Size**: Bundling dependencies will likely increase the size of our package. However, the trade-offs in ease of use and reliability are deemed worthwhile.\n  \n- **Redundancy**: There might be redundancies in dependencies if consuming applications are already using similar libraries. However, modern bundlers and tree-shaking techniques can help mitigate this issue to some extent.\n\n### Implementation:\n\nWe will incorporate a build process using a bundler like Webpack, Rollup, or similar tools that can bundle both the components and their dependencies. This ensures that when a component is imported into a consuming application, it brings along all necessary dependencies, creating a more seamless integration experience.","GenTime":"2024-07-28 23:02:59"}
{"File Name":"linshare-mobile-android-app\/0011-upload-document.md","Context":"## Context\\nAt this time, we used `android.net.Uri` to extract information and open an `InputStream` to upload.\\nBut, an `Uri` has `UriPermission` protect the data which is represented.\\nThis permission will be revoked by shared application when the received Activity no-longer run.\\nIt is not critical to Android 7, but with Android 9 we always get `Permission Denied` when deliver Uri to Worker to execute.\\n\n## Decision\n","Decision":"- We extract all requirement information of the Document at the time we receive the Intent.\\n- Instead of using Uri directly, we create a temporary file to store the file which Uri represent\\n- We deliver temporary file path to Worker\\n","tokens":94,"id":1643,"Prediction":"## Decision\n\nWe will switch from using `android.net.Uri` to using file paths that point to local files within our app's private storage directory. This solution ensures that the app maintains control over the file permissions and avoids the complications caused by `UriPermission` when delivering URIs to a worker.\n\n### Justification:\n\n1. **Permission Persistence**: Using file paths from the app's private storage ensures that we do not encounter permission issues related to `UriPermission`, which gets revoked when the associated Activity is no longer running. This approach provides persistent access to the files without relying on the context of an activity.\n\n2. **Compatibility**: This method is compatible across different Android versions, addressing the problem introduced in Android 9 where `Permission Denied` errors are encountered due to tightened security measures.\n\n3. **Isolation and Security**: Using app-specific storage ensures that files are isolated from other apps, maintaining security and data integrity.\n\n4. **Performance**: Accessing files directly from private storage can also be more efficient, as there is less overhead compared to managing URIs and permissions.\n\n### Implementation Plan:\n\n1. When a file needs to be uploaded, save it to the app's private storage directory (e.g., `context.getFilesDir()` or `context.getCacheDir()`).\n\n2. Update the worker logic to use file paths instead of URIs to access the files.\n\n3. Ensure proper cleanup of files in the private storage directory to manage space and avoid unnecessary data accumulation.\n\n4. Test the new implementation across different Android versions to validate that the solution works as expected and no longer throws `Permission Denied` errors.\n\n5. Update the documentation and code comments to reflect this change in the approach for handling file uploads.","GenTime":"2024-07-28 23:03:48"}
{"File Name":"linshare-mobile-android-app\/0008-download-with-downloadmanager-service.md","Context":"## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n\n## Decision\n","Decision":"Instead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\\n","tokens":99,"id":1647,"Prediction":"## Decision\n\nWe will use the system's `DownloadManager` service for handling background downloads.\n\n### Rationale:\n\n1. **Built-in Reliability and Stability**: The `DownloadManager` service is specifically designed to handle downloading content in the background. It offers built-in HTTP interactions, handles retries after failures, adapts to connectivity changes, and can resume downloads after system reboots. Leveraging this service ensures a higher level of reliability and stability compared to custom implementations.\n\n2. **Simplified Implementation**: By using `DownloadManager`, we can reduce the complexity of our app's codebase. We don\u2019t need to implement our own background download logic, error handling, or manage network changes. This reduces development time and the potential for bugs.\n\n3. **System-Level Integration**: `DownloadManager` is tightly integrated with the Android system, which means it will likely be optimized for performance and resource usage compared to any third-party or custom solution. \n\n4. **Ease of Monitoring**: Since the `DownloadManager` service provides broadcast intents for download status updates (progress, failure, completion), integrating it into our app allows for straightforward and consistent monitoring of download states. This helps in providing a better user experience through timely and precise notifications.\n\n5. **Future-proofing**: Using a well-supported and maintained system service like `DownloadManager` ensures that our download functionality will be compatible with future versions of the Android platform, thus reducing long-term maintenance effort.\n\nGiven these benefits, the system's `DownloadManager` provides a robust and efficient method to handle background downloads, making it the preferred choice for our application.","GenTime":"2024-07-28 23:04:03"}
{"File Name":"opg-lpa\/0006-modernise-the-code-base.md","Context":"## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https:\/\/pypl.github.io\/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and\/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n\n## Decision\n","Decision":"We have decided to modernise the code base to make it easier to work with and better\\naligned with modern web architecture and standards. This is not a small job, but\\nthe guiding principles we've decided on, shown below, should help us achieve our aims.\\n(\"Modernising the code base\" is not to be confused with \"modernising LPAs\". Here\\nwe're just talking about modernising the code base for the Make an LPA tool.)\\n* **Don't rewrite everything at once**\\nWhere possible, migrate part of an application to a new\\ncomponent and split traffic coming into the domain so that some paths are diverted to that\\ncomponent. This will typically use nginx in dev, but may be done at the AWS level if\\nappropriate (e.g in a load balancer or application gateway).\\nThis is challenging, but means that we don't have to do a \"big bang\" release of the new\\nversion of the tool. Our aim is to gradually replace existing components with new\\nones, which are (hopefully) simpler, future-proofed, more efficient, and don't rely on PHP.\\n* **Use Python for new work**\\nWe considered golang, but don't have the experience in the team to build applications with it.\\nWe felt that learning a new language + frameworks would only reduce our ability to deliver, with\\nminimal benefits: our application is not under heavy load and responds in an\\nacceptable amount of time, so golang's super efficiency isn't essential.\\nWe feel that we could scale horizontally if necessary and have not had any major issues\\nwith capacity in the past.\\n* **Choose containers or lambdas as appropriate**\\nUse a container for components which stay up most of the time, and lambdas for\\n\"bursty\" applications (e.g. background processes like PDF generation, daily statistics aggregation).\\n* **Choose the right lambda for the job**\\nUse \"pure\" lambdas where possible. This is only the case where an application has simple dependencies\\nwhich don't require unusual native libraries outside the\\n[stock AWS Docker images for lambdas](https:\/\/gallery.ecr.aws\/lambda\/python)).\\nIf a component is problematic to run as a pure lambda, use a lambda running a Docker image based\\non one of the stock AWS Docker images for lambdas.\\n* **Choose the right Docker image**\\nWhen using Docker images, prefer the following:\\n* Images based on AWS Lambda images (if writing a component which will run as a Docker lambda).\\n* Images based on Alpine (for other cases).\\n* Images based on a non-Alpine foundation like Ubuntu, but only if an Alpine image is not available.\\n* **Use Flask and gunicorn**\\nUse [Flask](https:\/\/flask.palletsprojects.com\/) for new Python web apps, fronted by\\n[gunicorn](https:\/\/gunicorn.org\/) for the WSGI implementation.\\n* **Use the latest Design System**\\nUse the [Government Design System](https:\/\/design-system.service.gov.uk\/) guidelines for new UI. In\\nparticular, use the\\n[Land Registry's Python implementation of the design system](https:\/\/github.com\/LandRegistry\/govuk-frontend-jinja),\\nwritten as [Jinja2 templates](https:\/\/jinja.palletsprojects.com\/).\\nOur aim should be to utilise it without modification as far as possible, so that we can easily upgrade\\nif it is changed by developers at the Land Registry.\\n* **Migrate legacy code to PHP 8**\\nWhere possible, upgrade PHP applications to PHP 8, when supported by [Laminas](https:\/\/getlaminas.org\/).\\nAt the time of writing, Laminas support for PHP 8 is only partial, so we are stuck with PHP 7 for now,\\nas large parts of our stack are implemented on top of Laminas.\\n* **Specify new APIs with OpenAPI**\\nSpecify new APIs using [OpenAPI](https:\/\/swagger.io\/specification\/). Ideally, use tooling\\nwhich enables an API to be automatically built from an OpenAPI specification, binding to\\ncode only when necessary, to avoid repetitive boilerplate.\\n* **Controlled, incremental releases**\\nProvision new infrastructure behind a feature flag wherever possible. This allows us to\\nwork on new components, moving them into the live environment as they are ready, but hidden\\nfrom the outside world. When ready for delivery, we switch the flag over to make that piece\\nof infrastructure live.\\n* **Follow good practices for web security**\\nBe aware of the [OWASP Top Ten](https:\/\/owasp.org\/www-project-top-ten\/) and code to avoid those\\nissues. Use tools like [Talisman](https:\/\/github.com\/GoogleCloudPlatform\/flask-talisman) to\\nimprove security.\\n* **Be mindful of accessibility**\\nConsider accessibility requirements at every step of the design and coding phases. Aim to\\ncomply with [WCAG 2.1 Level AA](https:\/\/www.w3.org\/WAI\/WCAG22\/quickref\/) as a minimum. While the\\nDesign System helps a lot with this, always bear accessibility in mind when building workflows\\nand custom components it doesn't cover.\\n* **Be properly open source**\\nMake the code base properly open source. While our code is open, there are still barriers to entry\\nfor developers outside the Ministry of Justice, such as the requirement to have access to AWS secrets,\\nS3, postcode API, the Government payment gateway and SendGrid for the system to work correctly. We\\nwill work towards removing these barriers so that onboarding of new developers (internally and\\nexternally) is seamless, and to enable potentially anyone to fully contribute to the project.\\n* **Improve test coverage everywhere**\\nAs we work on the code, be aware of gaps in testing and plug them as they arise. Don't wait for\\nan opportunity to fix everything at once: make refactoring and adding unit tests part of the\\nwork on an issue (unless it's going to take longer than working on the issue!).\\nWhere a whole category of testing is missing, add it (for example, we\\nhave recently implemented the foundations for load testing; see\\n[0004-implement-load-testing](.\/0004-implement-load-testing.md)).\\n* **Automate code quality metrics**\\nIntroduce tools to lint and scan code as we go, to ensure consistent, easy-to-follow code. See\\n[0003-linting-and-scanning](.\/0003-linting-and-scanning.md)) for a starting point.\\n* **Peer review everything**\\nAll commits to the code base must go through peer review before merging. No lone wolf developers.\\n* **Be pragmatic**\\nSee the [pragmatic quick reference](https:\/\/www.ccs.neu.edu\/home\/lieber\/courses\/csg110\/sp08\/Pragmatic%20Quick%20Reference.htm)\\nfor a summary. These are generally good principles for software engineering.\\n","tokens":306,"id":1652,"Prediction":"## Decision\n\n### Migrate the Code Base to Python\n\nGiven the context provided, the decision will be to initiate a phased migration from PHP to Python. This decision has several supporting factors:\n\n1. **Language Trends and Developer Availability**: Python is seeing an upward trend in popularity and is known for its clean syntax and extensive libraries. By transitioning to Python, we stand a better chance of attracting and retaining skilled developers.\n\n2. **Codebase Modernization**: The legacy PHP codebase is aging and difficult to maintain. Migrating to Python allows us to modernize and refactor the code using a more streamlined and maintainable language.\n\n3. **Library and Framework Support**: Python has a robust ecosystem of libraries and frameworks that can help speed up development and adherence to modern software design principles. This is preferable over dealing with rotting PHP libraries that can be more cumbersome to manage.\n\n4. **Design System Update**: Moving to a more modern language and framework will make it easier to adhere to the latest Design System guidelines, given Python's flexibility and extensive front-end integration capabilities.\n\n### Implementation Plan\n\n1. **Assessment Phase**: Conduct a thorough assessment of the legacy codebase to identify critical components, existing dependencies, and integration points.\n\n2. **Pilot Project**: Select a non-critical subset of the application to rewrite in Python. This allows us to build experience and create a reference architecture without impacting the core functionalities.\n\n3. **Module-wise Migration**: Gradually migrate individual modules or services from PHP to Python, ensuring thorough testing and validation at each step to maintain system stability.\n\n4. **Parallel Operation**: For a certain period, run both PHP and Python elements in parallel to ensure a smooth transition and enable rollback if necessary.\n\n5. **Final Transition and Optimization**: Complete the migration and focus on optimizing performance, updating documentation, and training the development team on the new codebase.\n\n### Risk Mitigation\n\n1. **Team Training**: Invest in training current PHP developers in Python to ensure continuity of knowledge and minimize resistance to change.\n\n2. **Incremental Testing**: Implement continuous integration and automated testing throughout the migration process to catch issues early.\n\n3. **Stakeholder Communication**: Maintain transparent communication with stakeholders on progress, risks, and timelines.\n\n### Expected Outcomes\n\n1. **Improved Developer Experience**: A cleaner, more modern codebase that is easier to work with and attracts high-quality development talent.\n\n2. **Enhanced Maintainability**: Reduced technical debt and better alignment with modern software practices.\n\n3. **Design System Compliance**: Easier updates and maintenance of design guidelines, leading to a more consistent and up-to-date user interface.\n\n4. **Future-proofing**: Positioning the application for future growth, making it easier to integrate new technologies and scale as needed.\n\nThis decision aligns with our strategic goals of maintaining a sustainable, high-quality codebase and setting the foundation for future development initiatives.","GenTime":"2024-07-28 23:04:30"}
{"File Name":"opg-lpa\/0002-custom-save-handler-in-service-front.md","Context":"## Context\\nThe service-front component, written in PHP, uses the default Redis save\\nhandler for persisting session data. In certain situations, the\\napplication may request a resource *A* which takes significant time to deliver,\\nsuch as LPA statuses via the Sirius data API. If resource *A*\\nis requested via an Ajax request, it's possible that the client\\nwill request a new resource *B* before *A* is fully processed. If processing for\\n*B* then completes before processing for *A*, the process for *A* can erroneously\\noverwrite session data added by *B*, resulting in loss of session data required\\nby *A*.\\nThis causes particular problems for CSRF tokens, as shown by this typical sequence\\non service-front:\\n1.  dashboard page loads in browser, triggering client-side Ajax request to statuses controller\\n2.  statuses controller reads session data **S** and initiates (slow) request to Sirius\\nAPI to get LPA statuses\\n3.  meanwhile, user goes to replacement-attorney page; the Ajax request is now redundant, as the\\nuser isn't on the dashboard page any more, but the statuses controller doesn't know this\\n4.  replacement-attorney controller reads session data **S**\\n5.  statuses controller continues processing Sirius response, unaware of new data about to be added to\\nsession by replacement-attorney...\\n6.  replacement-attorney adds CSRF data to session, creating **S'**\\n7.  replacement-attorney page renders form with CSRF token, associated with data in **S'**\\n8.  replacement-attorney writes **S'** to session, including CSRF data\\n9.  statuses page finishes processing, unaware of **S'**; it assumes it has\\nthe correct data **S** and writes it to the session, losing the delta between\\n**S** and **S'** (including the CSRF token!)\\n10. user submits form to replacement-attorney controller with CSRF token in the form\\n11. replacement-attorney controller loads again, but retrieves **S** from session (just written by\\nstatuses controller in 9); this doesn't have the CSRF token (which was in **S'**)\\nfor comparison with the value in the form submitted by the user; CSRF validation fails!\\n\n## Decision\n","Decision":"Use a custom save handler to prevent certain Ajax requests from writing data to the session.\\nThis will still use Redis as the storage back-end.\\nThe approach is to send some Ajax requests with a custom `X-SessionReadOnly: true` header,\\nimplying that the controller they invoke should only read from the session and never write to it.\\nThe save handler inspects the header on the incoming request and ignores any requests to write\\nthe session if accompanied by this header.\\nPHP 7+ provides a mechanism to only read from the session, via:\\n```\\nsession_start(array('read_and_close' => true))\\n```\\nHowever, the complexity of the processing in the Laminas stack, which does its own session\\nmanagement, overrides any attempts to call this function. Consequently, the pragmatic\\nsolution is to move down the stack to the lower-level save handler, and implement the read-only\\nbehaviour there for requests we know to be problematic.\\n","tokens":490,"id":1654,"Prediction":"## Decision\n\nTo resolve the issue with session data concurrency and ensure the integrity of CSRF tokens, we will implement a mechanism to prevent session overwrites during concurrent Ajax requests. The decision involves:\n\n1. **Switching from Default Redis Save Handler to a Custom Session Management Solution:**\n   We'll replace the default Redis session handler with a custom solution that supports session locking. This custom handler will queue session writes to ensure that only one write operation is performed at a time, thereby preventing conflicts.\n\n2. **Introduce Session Locking Mechanism:**\n   We'll implement a session locking mechanism where each session is locked before being read and unlocked after being written. This ensures that if a session is being processed by one request, other requests will wait until the session is released. This prevents simultaneous read\/write operations on the session data.\n\n3. **Implement a Proper Mutex or Semaphore:**\n   We will use a Redis-based mutex or semaphore for managing locks on session data. This would involve:\n   - Acquiring a lock before reading\/modifying the session.\n   - Releasing the lock after the session has been safely modified.\n   - Implementing a timeout mechanism to ensure that locks are not held indefinitely in case of failure.\n\n4. **Handling Redundant Requests:**\n   On the client side, we will implement logic to cancel redundant Ajax requests. For instance, if the user navigates to a different page where the current Ajax request is no longer relevant, the client-side script should abort the request. This reduces the load on the server and minimizes potential conflicts.\n\n5. **Upgrade to a More Robust Session Management Library:**\n   Consider upgrading or integrating a more robust session management library that inherently supports safe concurrent access, such as `Symfony Session` with proper Redis integration which already has built-in session locking capabilities.\n\n### Implementation Steps:\n\n1. **Custom Session Handler Development:**\n   Develop and integrate a custom PHP session handler that supports session locks using Redis. It will follow these steps:\n   - Acquire a lock on the session (utilizing Redis SETNX command or using a third-party library like `Predis` or `php-redis-lock`).\n   - Read the session data.\n   - Modify the session data as needed.\n   - Write the session data back to Redis.\n   - Release the lock (using Redis DEL command for the lock key).\n\n2. **Client-Side Request Management:**\n   Modify the JavaScript code to include logic for aborting Ajax requests if the user navigates away from the page before the request completes. Use the `XMLHttpRequest.abort` method to cancel requests.\n\n3. **Testing and Validation:**\n   Thoroughly test the implementation with various scenarios to ensure the integrity of session data and proper CSRF token handling. Test cases should include simultaneous Ajax requests, session data modifications by different parts of the application, and navigation events.\n\nBy implementing these measures, we will ensure that session data corruption due to concurrent Ajax requests is mitigated, and the integrity of security-sensitive data like CSRF tokens is maintained.","GenTime":"2024-07-28 23:04:43"}
{"File Name":"DunkMe\/0001-front-end.md","Context":"#### Context and Problem Statement\\nA front end, client facing technology stack \/ language is needed.\\n#### Considered Options\\n- ASP.NET MVC (C#)\\n- ASP.NET Web Forms (C#)\\n- Angular (with TypeScript)\\n#### Decision Outcome\\nChosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n\n## Decision\n","Decision":"Chosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n","tokens":93,"id":1659,"Prediction":"## Decision\nAfter evaluating the options for the front-end, client-facing technology stack\/language, we have chosen Angular (with TypeScript) as the technology for our project. The decision was based on the following reasons:\n\n1. **Alignment with Trade Me's Technical Stack Trajectory**: Angular is consistent with the direction our technical stack is moving towards. This alignment will make it easier to integrate with existing systems and leverage in-house expertise.\n\n2. **Industry Standard for Greenfield Projects**: Angular is widely considered an industry standard for new projects due to its robust framework, strong community support, and comprehensive ecosystem. This will provide long-term viability and scalability for our application.\n\n3. **Enhanced Productivity and Maintainability**: Angular\u2019s architecture promotes modularity, reusability, and maintainability, which will help in building a well-structured, scalable application. The use of TypeScript adds type safety, catching potential issues at compile-time.\n\n4. **Rich Feature Set**: Angular offers a plethora of built-in functionalities such as two-way data binding, dependency injection, and more, which accelerates development and reduces the need for third-party libraries.\n\n5. **Developer Experience and Support**: Angular has extensive documentation, a vibrant community, and regular updates, which ensure better support and continuous improvements, benefiting our development team.\n\nThus, Angular (with TypeScript) is the selected front-end framework\/technology stack for our client-facing application.","GenTime":"2024-07-28 23:05:03"}
{"File Name":"DunkMe\/0002-data-access-api.md","Context":"#### Context and Problem Statement\\nA technology stack is required for the projects data access application programming interface (API).\\n#### Considered Options\\n- ASP.NET Web API (C#)\\n#### Decision Outcome\\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n\n## Decision\n","Decision":"Chosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","tokens":111,"id":1660,"Prediction":"## Decision\n\nUpon evaluating the requirements for the project's data access API, the decision is to adopt **ASP.NET Web API (C#)** as the technology stack. This decision is made based on the following key factors:\n\n1. **Compatibility with .NET Core 2**: ASP.NET Web API supports targeting .NET Core 2, providing cross-platform capabilities and allowing the application to be developed, tested, and deployed on various operating systems.\n\n2. **Efficiency through Dapper ORM**: By using Dapper as the Object Relational Mapper (ORM), the API can benefit from the lightweight and high-performance data access layer. Dapper significantly reduces the complexity of mapping between database tables and C# objects without the overhead of a full-fledged ORM like Entity Framework.\n\n3. **Skill Reuse and Team Proficiency**: Leveraging ASP.NET Web API alongside Dapper capitalizes on the existing skillsets within the team. This reduces the learning curve, speeds up the development process, and ensures that best practices and established patterns can be maintained and reused.\n\n4. **Unit Testing Facilitation**: Implementing a repository pattern within the ASP.NET Web API facilitates better unit testing. The repository pattern abstracts the data access layer, allowing dependencies to be easily mocked and tested, promoting a robust and reliable codebase.\n\nIn conclusion, adopting ASP.NET Web API (C#) with Dapper ORM and the repository pattern provides an optimal solution that aligns with the project's requirements, promotes efficient and high-quality development, and leverages the team's existing expertise.","GenTime":"2024-07-28 23:05:06"}
{"File Name":"DunkMe\/0003-relational-database.md","Context":"#### Context and Problem Statement\\nA relational database is required to persist the applications data.\\n#### Considered Options\\n- Microsoft SQL\\n- MySQL\\n- PostgreSQL\\n- SQLite\\n#### Decision Outcome\\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n\n## Decision\n","Decision":"Chosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n","tokens":78,"id":1661,"Prediction":"## Decision\n\nTo standardize our technical stack and leverage existing expertise, we have decided to proceed with **Microsoft SQL** as the relational database for persisting the application's data. The specific reasons for this decision are:\n\n1. **Consistency with Existing Stack**: Microsoft SQL is already the widely and almost exclusively used database technology at Trade Me.\n2. **Leverage Existing Expertise**: Our development and operations teams are already proficient in Microsoft SQL, which will minimize ramp-up time and reduce the potential for configuration and maintenance errors.\n3. **Smooth Integration**: Given that other systems and applications in our environment utilize Microsoft SQL, this choice will ensure better integration and data flow between systems.\n4. **Support and Maintenance**: Utilizing a single type of database system simplifies support and maintenance, as well as vendor relations, since we typically deal with Microsoft for our database solutions.\n5. **Performance and Reliability**: Microsoft SQL has proven to be reliable and meets performance expectations in our previous and current applications.\n\nThis decision aligns with our strategic goals of maintaining a cohesive and manageable technical stack, ensuring efficient knowledge transfer and support efficiencies within the organization.","GenTime":"2024-07-28 23:05:10"}
{"File Name":"ODT_OnlineBank\/ARD-0001.md","Context":"## Context\\n*This section describes the forces at play, including technological, political, social, and project local.\\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\\nIt is simply describing facts.*\\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and\\nprocesses. ODT will be initialy focus on Green Field Developent. The Online Bank is a sample app used in the ODT University classes.  The Purpose of theapplication is to provide a\\nsample app as a context for the training courses.  We believe most people are familiar with online\\nbanking, and won't need to spend a lot of time learning the problem domain.\\nThe purpose of this document is to determine the technology stack for the sample application.\\n**NOTE:** While we are initially focused on a Java Web stack, we may elect to develop out an example in the Microsoft Stack.\\nIf you would like to help, please contact us.\\n\n## Decision\n","Decision":"*This section describes our response to these forces. It is stated in full sentences, with active voice.\\n\"We will ...\"*\\nWe will be focusing on the modern web java stack.\\n### UI Technology Stack:\\n- Angular 2\\n- Bootstrap\\n- protractor\\n- jasmine\\n### Server Side Technology Stack:\\n- Java 1.8+\\n- SpringBoot 1.4+\\n- Restful\\n- MySQL\\n- Spring Data JPA (Hibernate)\\n- JUnit 5 \/ Spock\\n### Development Technology Stack:\\n- Local Development\\n- SCM: GIT\\n- Intellij\\n- Favorite Editor\\n- Gradle\\n- Angular Quickstart\\n- Concourse CI (CI\/CD)\\n- Jira\\n- Sonar\\n#### Deployment:\\n- Pivotal Cloud Foundry (Java and Static Build Packs)\\n","tokens":220,"id":1665,"Prediction":"## Decision\nAfter analyzing the context and considering the requirements of the Office of Development Transformation (ODT), the decision is to proceed with the Java Web stack for the initial development of the Online Bank sample application. This decision has been made based on the following justifications:\n\n1. **Familiarity and Ease of Adoption**: The majority of the development team and the intended audience for the training courses are likely to be more familiar with Java and related technologies. This lowers the learning curve and makes the technology more accessible for educational purposes.\n\n2. **Robust Ecosystem**: Java has a robust ecosystem and a plethora of frameworks and libraries which can aid in building a highly scalable and maintainable application. Key tools include Spring Boot for application development, Hibernate for ORM, and Maven or Gradle for build automation.\n\n3. **Integration with DevOps**: Java\u2019s ecosystem supports various DevOps tools and practices efficiently. Continuous integration and continuous delivery (CI\/CD) pipelines can be readily implemented using tools like Jenkins, Git, Docker, and Kubernetes, which are dominant in the Java ecosystem.\n\n4. **Community Support**: Java has a large and active community which can be beneficial in terms of resources, documentation, and troubleshooting. This can facilitate a smoother development process and quicker problem resolution.\n\n5. **Scalability and Performance**: Java-based applications are known for their scalability and performance capabilities, which align with the goals of building a reliable sample application that can be used as a learning tool.\n\n6. **Potential for Future Expansion**: While the focus is initially on the Java Web stack, the architecture and design will be modular and scalable in a way that allows for the introduction of a Microsoft Stack example in the future without major overhauls. This ensures flexibility for future developments.\n\nIn conclusion, while the Java Web stack has been chosen for the initial development, the architecture will be designed with the potential for a future Microsoft Stack implementation in mind. Detailed documentation and modular design principles will be maintained to facilitate any such future transitions.","GenTime":"2024-07-28 23:05:25"}
{"File Name":"govuk-docker\/0004-use-versioned-database-services.md","Context":"## Context\\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\\nMongoDB and Elasticsearch. These have their data persisted in\\n[Docker volumes][volume] defined [for each service][defined-volumes].\\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\\nbegan receiving cryptic errors when trying to run applications that depended\\non MongoDB. This was because they had a volume with data structured for MongoDB\\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\\nWe felt that this upgrade path was more confusing and difficult than it needed\\nbe and wanted to improve it for the future.\\n[volume]: https:\/\/docs.docker.com\/storage\/volumes\/\\n[defined-volumes]: https:\/\/github.com\/alphagov\/govuk-docker\/blob\/cb124a3a1d3353e777d4f777d77f03f93415d415\/docker-compose.yml#L5-L10\\n[mongodb-update]: https:\/\/github.com\/alphagov\/govuk-docker\/pull\/356\\n\n## Decision\n","Decision":"We decided that we would rename database services, and their respective\\nvolumes, to reflect the software version number. For example the `mongo`\\nservice, and namesake volume, have been renamed to `mongo-3.6`.\\nThe precision of the version number is chosen based on the backwards\\ncompatibility between versions of the software. For example to upgrade from\\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\\nhave specified the service and volume with respective to a major version:\\n`elasticsearch-6`.\\n","tokens":238,"id":1666,"Prediction":"## Decision\n\nTo alleviate the issues caused by incompatible data structures when upgrading MongoDB versions within GOV.UK Docker, we have decided to implement a version-specific Docker volume strategy. \n\nSpecifically, for each database service that is likely to undergo version upgrades, such as MongoDB, separate Docker volumes will be defined for each major version. This means instead of having a single `mongo-data` volume, we will have volumes like `mongo-data-2.4` and `mongo-data-3.6`.\n\n### Details of the Decision:\n\n1. **Separate Volumes for Versions:** Each distinct version of a database service (e.g., MongoDB) will have its dedicated Docker volume. This will ensure that data created by one version does not conflict with another version's required data structure.\n\n2. **Clear Volume Naming:** Volumes will be named clearly to reflect the version they correspond to (e.g., `mongo-data-2.4`, `mongo-data-3.6`). This will make it explicit which volume is associated with which database version.\n\n3. **Update Docker Compose Configuration:**\n   - Docker Compose files will be updated to reflect the new volume naming strategy.\n   - When upgrading to a new database version in GOV.UK Docker, the Compose file will link the service to the correctly named volume for that version.\n\n4. **Documentation and User Guidance:** \n   - Comprehensive documentation will be provided to users on how to manage these version-specific volumes.\n   - Instructions will be given on how to migrate data between versions if needed, and how to handle scenarios where reverting to a previous version might be necessary.\n\nBy segregating volumes based on database versions, we aim to minimize cryptic errors and streamline the upgrade process, simplifying database management for developers using GOV.UK Docker. This approach will decouple data dependencies from application configurations and provide clearer upgrade paths in the future.","GenTime":"2024-07-28 23:05:29"}
{"File Name":"govuk-docker\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\nThe [original govuk-docker repo](https:\/\/github.com\/benthorner\/govuk-docker) was written outside of GOV.UK. Major architectural decisions, as well as more minor decisions, were written as [documentation in the repo][DECISIONS], together with their associated user [NEEDS]. While these documents have historical value, they are not being maintained and increasingly differ from the current state of the repo. As part of adopting an ADR approach, we should clearly deprecate these historical documents to avoid confusion.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":127,"id":1669,"Prediction":"## Decision\n\nWe will formally deprecate the historical architectural decision documents within the original govuk-docker repository. This includes both major and minor decisions that were recorded as documentation, along with their associated user needs. \n\nTo implement this:\n\n1. **Deprecation Notice**: Add a deprecation notice at the head of each historical decision document, clearly stating that these documents are no longer maintained and may not reflect the current state of the repository.\n   \n2. **Centralized ADR Directory**: Create a centralized ADR directory in the repository where all new and future architectural decisions will be documented using the ADR format. This will ensure consistency and clarity for all contributors.\n\n3. **Documentation Update**: Update the repository's main README, documentation index, and any other relevant documentation to reference the new ADR directory as the authoritative source for current architectural decisions.\n\n4. **Communication**: Communicate this change to all contributors and maintainers of the repository, ensuring they are aware of the deprecation of the old documents and the new process for recording and accessing architectural decisions.\n\nBy taking these steps, we will maintain the integrity and relevance of our architectural documentation, making it easier for current and future contributors to understand and follow the project's architecture decisions.","GenTime":"2024-07-28 23:05:32"}
{"File Name":"raster-foundry\/adr-0004-static-asset-pipeline.md","Context":"## Context\\nIn selecting a static asset pipeline, there are several factors that are important to us.\\nWe don\u2019t want to lose features that speed up developer feedback, such as a dev server with linting, auto-gen, and browser reload.\\nThe solution needs to support tests run with a single command similar to what we\u2019ve had in previous projects.\\nThe solution needs to be fast.\\nSome team members have expressed dissatisfaction with the stability of the NPM ecosystem, but pretty much everything relies on it to some extent, so the issue is probably best addressed elsewhere (possibly using a lazy NPM cache).\\nIn the past, our team has used Grunt to manage and build static assets. Various team members \/ designers have expressed discontent with the idea of going forward using Grunt, primarily due to inherent factors such as long compile times and configuration complexity.\\nOther teams in Azavea have had good experiences with Webpack. Webpack is not a complete replacement for Grunt, but of the difference in functionality, it is highly likely that NPM tasks will cover it if used in conjunction. Despite scattered complaints of complexity and poor documentation found while researching, I was able to set up pipeline using npm and webpack overnight without too many issues. Significant features which are built in include a dev server capable of supporting hot reloading, very fast feedback on changes, and support for all the tools we commonly use in our applications. In addition to quality of life features, Webpack also gives us a dependency graph which includes static assets and bundle splitting. Combined with NPM, it appears to be a good solution.\\nA final option that we have to consider is Gulp, which has been used on a couple of projects by the Urban Apps team. Where grunt uses configuration files, Gulp uses code. Gulp\u2019s main advantage is speed: streaming operations mean that files never touch disk between chained tasks. Other than the method of configuration and speed of operating, the basic functionality of Gulp and Grunt seem to be fairly aligned.\\n\n## Decision\n","Decision":"Given the good experiences that others in the company have had with Webpack, we\u2019ve decided to run the static asset pipeline using a combination of npm and webpack. Npm will be used to manage dependencies and initiating tasks, while webpack will be used to run the dev server and build static assets.\\n","tokens":408,"id":1680,"Prediction":"## Decision\n\nAfter thorough consideration, we have decided to adopt Webpack in conjunction with NPM as our static asset pipeline solution. This decision is based on the following rationale:\n\n1. **Developer Experience**: Webpack comes with a built-in dev server that supports hot reloading, offering rapid feedback for developers. This aligns well with our requirement for maintaining features that speed up developer feedback, such as linting and auto-generation.\n\n2. **Ease of Setup**: Initial setup of Webpack in combination with NPM was manageable, as evidenced by a successful overnight implementation without encountering significant issues. \n\n3. **Performance**: Webpack has demonstrated efficient compile times and supports advanced features like bundle splitting and static asset dependency graphs, which enhance build performance.\n\n4. **Community and Stability**: While there have been concerns about the complexity and documentation of Webpack, it is extensively used and recommended within the development community. Other teams within our organization have also reported positive experiences, which implies a level of stability and community support that we find reassuring.\n\n5. **Comprehensive Feature Set**: In conjunction with NPM scripts, Webpack covers a wide range of functionality similar to what Grunt offered, but with better performance and more modern features.\n\n6. **Future-Proofing**: Given the rapid evolution of build tools, Webpack offers a robust architecture that is likely to keep pace with future developments. \n\nWhile Gulp does offer significant speed advantages due to its streaming operations, the additional speed does not outweigh the comprehensive feature set and developer experience provided by Webpack and NPM.\n\nThus, we will proceed by configuring our projects using Webpack and NPM, ensuring that the necessary scripts and configurations are in place to facilitate a smooth transition and maintain high productivity for our developers.","GenTime":"2024-07-28 23:06:00"}
{"File Name":"raster-foundry\/adr-0013-spark-execution-environment.md","Context":"## Context\\nPart of the Raster Foundry processing workflow requires a source imagery ingest into a GeoTrellis Catalog. GeoTrellis depends on Apache Spark to parallelize work during an ingest, and that work typically occurs on an Apache Spark enabled compute cluster.\\nPrior iterations of Raster Foundry attempted ingests with an Apache YARN managed compute cluster via Amazon Elastic MapReduce (EMR) on a per-request basis. Unfortunately, that didn't perform well due to the overhead of bootstrapping an EMR cluster.\\nOur goal for the this iteration aims to keep some Spark cluster components active at all times through a combination of spot pricing and usage based cluster auto-scaling. This approach should help minimize cluster bootstrapping durations and keep cost-incurring cluster resources at a minimum.\\n\n## Decision\n","Decision":"Given that we are still early in Raster Foundry's product development, the desire to keep costs low and ship quickly carry significant weight. In order to meet those objectives, but still minimize cluster bootstrapping overhead, the first pass at having Spark enabled resources on standby will consist of a shared (across staging and production) Amazon EMR cluster.\\nThe overall Raster Foundry infrastructure will be split across three Amazon Virtual Private Clouds (VPCs):\\n- Staging (`10.0.0.0\/18`, 16382 IPs)\\n- Spark (`10.0.64.0\/18`, 16382 IPs)\\n- Production (`10.0.128.0\/18`, 16382 IPs)\\nThe `Staging` and `Production` VPCs will be peered with the `Spark` VPC to allow direct private network access, if necessary. Regardless, public facing APIs such as the Amazon EMR's Step API, or the Raster Foundry API can be used to pass information between systems.\\nDespite having thought long and hard about the consequences, I suspect that the section below does not cover all of the issues that lie ahead of us. The hope is that at least these core ideas remain in future iterations of the Spark execution environment:\\n- Shared cluster resources between environments\\n- Isolated environments at the VPC level\\n- Build on top of the Amazon EMR platform\\n- Use spot pricing and auto-scaling aggressively to keep costs low\\n","tokens":162,"id":1681,"Prediction":"## Decision\n\nWe will implement a persistent Spark cluster architecture using Amazon EMR with a focus on leveraging spot instances and auto-scaling to optimize for cost and performance. The Spark cluster will remain partially active at all times to mitigate the overhead observed with cluster bootstrapping in prior iterations. \n\nKey components of this decision include:\n\n1. **Persistent Cluster Setup**:\n   - Maintain a minimum number of always-on instances to serve as the core of the cluster.\n   - Utilize on-demand instances for these core nodes to ensure reliability and availability.\n\n2. **Spot Instances for Worker Nodes**:\n   - Use Amazon EC2 spot instances for the majority of worker nodes to reduce costs.\n   - Implement strategies to handle spot instance interruptions, such as checkpointing and persistent storage.\n\n3. **Auto-Scaling**:\n   - Configure the cluster with auto-scaling policies to dynamically adjust the number of worker nodes based on workload demand.\n   - Use scaling policies that account for both time-based and utilization-based triggers to scale the cluster up or down.\n\n4. **Monitoring and Alerts**:\n   - Implement comprehensive monitoring and alerting for the Spark cluster to ensure visibility into cluster health and performance.\n   - Use Amazon CloudWatch to track key metrics and set up alerts for critical thresholds.\n\n5. **Cost Management**:\n   - Regularly review and optimize the composition of on-demand and spot instances to ensure the balance between cost savings and performance requirements.\n   - Leverage reserved instances where appropriate for predictable workloads to further reduce costs.\n\nBy maintaining a core set of active Spark components and utilizing spot pricing with dynamic scaling, we aim to strike a balance between performance and cost efficiency, thereby improving the overall efficiency of the Raster Foundry processing workflow.","GenTime":"2024-07-28 23:06:04"}
{"File Name":"raster-foundry\/adr-0006-workflow-manager.md","Context":"## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n\n## Decision\n","Decision":"Raster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\\ndatabase and a redis cache.\\n","tokens":353,"id":1688,"Prediction":"## Decision\n\nWe have decided to use Apache Airflow as our tool for managing asynchronous tasks and workflows. \n\n### Rationale:\n1. **Mature and Stable**: Airflow is recognized for its maturity and stability, making it a reliable choice for our requirements.\n2. **Execution Model Fit**: Airflow aligns well with our execution model, offering a flexible and powerful way to manage various types of workflows.\n3. **Written in Python**: Aligns with our team's expertise and existing codebase, facilitating easier integration and maintenance.\n4. **Excellent UI**: Airflow's user interface provides robust capabilities for monitoring and managing workflows, making it easier to track progress, handle retries, and notify personnel as necessary.\n5. **Community Support**: As a well-adopted tool, Airflow benefits from a large community, which translates to better support, more resources, and frequent updates.\n\n### Considerations:\n- **Dependency on Celery**: Airflow requires Celery for the type of workflows we intend to implement. This is acceptable since our team already has familiarity with Celery, and it will not introduce significant additional complexity.\n- **Managing the Scheduler and Cache**: We will need to invest some effort into setting up and managing the scheduler and cache. This is considered manageable given the overall benefits Airflow provides.\n\n### Next Steps:\n- Begin the implementation of Airflow for managing asynchronous tasks and workflows.\n- Provide training and documentation to the team on Airflow usage and best practices.\n- Monitor the system's performance and reliability, iterating as necessary to address any issues that arise during implementation.\n\nBy choosing Airflow, we aim to leverage a well-supported, flexible, and user-friendly tool to manage our diverse workflow and task requirements effectively.","GenTime":"2024-07-28 23:06:24"}
{"File Name":"simple-server\/011-rollup-summary-tables-for-dashboards.md","Context":"## Context\\nEarlier we went with a [materialized view approach](https:\/\/github.com\/simpledotorg\/simple-server\/blob\/master\/doc\/arch\/008-materialized-views-for-dashboards.md) for dashboard reporting data. This approach has worked out well for the current reports but has limitations in terms of scaling and complexity.\\nWe currently have a need for a new set of dashboard reports that rely on similar sets of data, and anticipate more needs for summary data for monthly and quarterly reports. We'd like to try to find a simpler, more stable, and more scalable approach to making that data easily accessible.\\n\n## Decision\n","Decision":"We will create rollup summary tables for to capture the \"most recent blood pressure per patient per period\". These rollup tables will be updated in real-time as blood pressures are captured, removing the need for recurring batch jobs. We will denormalize as much data as we need to to make reporting queries simple and cheap to execute, to prevent the need for any complicated SQL queries or additional data layer caching.\\nOnce the current period has completed, we will treat the data in these rollup tables for that period as immutable. This will let us and our users have more confidence in reports from past periods, as they will not change due to patients moving to different facilities or changes in how we calculate things.\\nWe will focus on the District controlled blood pressure trend report as a first test case for this sort of approach, as it is a key report and one we'd like to gather feedback on as quickly as possible.\\n","tokens":123,"id":1701,"Prediction":"## Decision\n\nWe will transition from the current materialized view approach to a data warehousing solution for our dashboard reporting needs. The new solution will involve the following components:\n\n1. **Data Warehousing**: We will implement a robust data warehousing system, such as Amazon Redshift or Google BigQuery, to store and manage our reporting data.\n2. **ETL Pipelines**: We will develop ETL (Extract, Transform, Load) pipelines to regularly transfer and transform data from our operational databases to the data warehouse. Tools like Apache Airflow or AWS Glue can be used for this purpose.\n3. **Aggregated Tables**: Inside the data warehouse, we'll create aggregated tables that preprocess the data needed for various dashboard reports, ensuring that queries executed by the dashboards are efficient and scalable.\n4. **Data Accessibility**: We will use a business intelligence (BI) tool, such as Looker or Tableau, to create and manage the dashboards, providing easy access to the data warehouse for generating reports.\n\nBy transitioning to a data warehouse, we aim to achieve the following:\n- **Scalability**: A data warehouse can handle larger volumes of data and more complex queries, making it suitable for our growing reporting needs.\n- **Simplicity and Stability**: Having a dedicated data warehousing solution simplifies data management and ensures stability by separating operational and analytical workloads.\n- **Flexibility**: The new approach will allow us to quickly adapt to new reporting requirements and generate summary data for monthly and quarterly reports with greater ease.\n\nThis decision will necessitate a clear migration plan to move from materialized views to the new data warehousing solution, along with training for our team to ensure smooth adoption and effective use of the new system.","GenTime":"2024-07-28 23:07:14"}
{"File Name":"simple-server\/014-region-level-sync.md","Context":"## Context\\n[PRD](https:\/\/docs.google.com\/document\/d\/1Cflct0Y-44IRUVw_5-NptcnNSX1UgAPBiXqoXHq22io\/edit)\\nUsers in large districts reported that the Simple app was running very slow, making the app near-unusable.\\nThe slowdown was caused by the volume of patient data synced to the user\u2019s phone. We realised that the amount of data\\nbeing stored on the device had to be reduced for better long-term performance.\\nCurrently we sync the entire district's records to a user's phone. Some of the large districts have upto 50,000 patients,\\nwhich can amount to 400-500 MB of data. On lower-end phones we noticed the app started slowing down when the DB size grew beyond 250 MB.\\nA district typically has between 1-20 blocks. From trends in IHCI, we found it's uncommon for patients to visit facilities across blocks.\\nPatients that have a BP taken in more than 1 block is around 2%, with the exceptions of:\\nSindhudurg (9.8%), Hoshiarpur (5.3%), Bathinda (3.1%).\\nThis means that we can sync only a block's data to the user's phone and be reasonably confident about finding patients on the app.\\n\n## Decision\n","Decision":"- The server will sync records from the user's block instead of the entire district.\\nSpecifically the following patients will be synced:\\n- patients that registered at a facility in the same block,\\n- patients that are assigned to a facility in the same block, and\\n- patients that have an appointment scheduled at a facility in the same block.\\n- All other sync resources will return records belonging to these patients only.\\n- The sync mechanism should support the ability to adjust the sync radius to any sync region.\\nThis is important in case we need to change the kind of records that are synced to the app in the future.\\nSee the [wiki entry on Region level sync](..\/wiki\/adjusting-sync-boundaries.md) for how it works.\\n### On the app\\n- Users can continue selecting any facility in their district when switching facilities.\\n- Users can continue selecting any facility in their district when scheduling a patient\u2019s next visit or preferred facility.\\n- It is possible that a patient will visit a facility outside their block and their record will not be found on the user\u2019s device. In this case the user should\\n- Scan the patient\u2019s BP passport if they have one.\\n- Register the patient again, as if they were new. Make sure to attach their existing BP passport to the registration.\\n- The duplicate patient records will be merged by the Simple team later.\\n","tokens":282,"id":1708,"Prediction":"## Decision\n\nTo address the issue of the app slowing down due to the large volume of patient data being synced to the user's phone, we will implement block-level data synchronization instead of district-level synchronization. This will significantly reduce the amount of data stored on the device, enhancing app performance, especially on lower-end phones.\n\n### Key Points:\n1. **Block-Level Data Sync:** Modify the sync process to download and store data for only the block in which the user operates, rather than the entire district. Given that the inter-block patient visit rate is low (only 2% on average), this approach should not significantly impact user experience or patient data availability.\n   \n2. **Handling Exceptions:** Special considerations will be made for districts with higher inter-block patient visit rates, such as Sindhudurg (9.8%), Hoshiarpur (5.3%), and Bathinda (3.1%). Users in these districts may need additional support or optional syncing of adjacent blocks to ensure data availability.\n\n3. **Performance Monitoring:** Continuously monitor app performance after implementing block-level sync to ensure that the changes lead to the desired performance improvements. Feedback will be collected from users to assess whether the solution effectively meets their needs without compromising functionality.\n\n4. **User Education:** Provide clear instructions and support to users about the new data sync method. Include guidance on how to access patient records from other blocks if necessary, to minimize confusion and ensure smooth transition.\n\nBy implementing block-level data synchronization, we expect a drastic reduction in the data size stored on user devices, thereby improving app performance and usability across all device types, particularly lower-end phones.","GenTime":"2024-07-28 23:07:37"}
{"File Name":"simple-server\/001-synchronization.md","Context":"## Context\\nNetwork connectivity on phones in rural areas can be low, and\\npatchy. So, the app needs to work offline, as much as possible. The\\nsync mechanism exists to allow sharing records of patients, blood\\npressures, etc across devices.\\nWe need to accommodate cases where patients, and nurses move across\\nfacilities multiple times during a week.\\n\n## Decision\n","Decision":"### Mechanism\\n1. Send records from device to server\\nThe device needs to keep track of records that need to be\\nsynced. These can be new records, or records that have one or more\\nfields updated. These records need to be formatted into the payload\\nschemas as defined in the individual APIs below. The API will not\\ndifferentiate between new and updated records; this is handled by the\\nserver.  These payloads then need to be sent in batches, where each\\nbatch is inherently independent of the previous one. However, it is\\nimportant to _send_ all records that need syncing, before fetching\\nrecords from the server.\\n2. Fetch records from server to device\\nWhen fetching records for the first time, the `first_time` query\\nparameter should be set. The server will then send back a number of\\nrecords as defined by the `number_of_records` query parameter. This is\\nessentially the \\\"batch_size\\\". The first response also includes a\\n`latest_record_timestamp`, that needs to be sent with the next\\nrequest, in order to retrieve more records from the server. If there\\nare no more records to be sent from the server, it returns an empty\\nlist.\\n3. Merge records on device\\nAfter receiving records from the server, the device will match the\\nrecords in the local database using the ID field of the entity, and\\nupdate them to the server's version. If the local record in the\\ndatabase is pending a sync, it will not update it. The merging of\\nrecords will be handled only by the server.\\nThe server may send back the same record multiple times, so updating\\nrecords on the device needs to be idempotent.\\n### Levers\\nPhysical resource constraints such as battery life, or network are not\\nunder our control. In order for the app to adapt well to the\\nconstraints, we need to have knobs on:\\n1. Batch size: how many records to send or fetch\\n2. Sync frequency: how often sending or fetching happens\\nPreferably, these levers are modifiable at run-time, per device.\\n","tokens":77,"id":1712,"Prediction":"## Decision\n\nTo ensure that the application remains functional and reliable in areas with low and patchy network connectivity, the following architectural decisions have been made:\n\n1. **Local Storage and Offline Capability**: Implement a robust local storage mechanism using a database like SQLite or Realm, allowing the app to store patient data, medical records, and other critical information locally on the device. This will ensure the app remains fully functional even in the absence of network connectivity.\n\n2. **Background Sync Mechanism**: Develop a background sync service that periodically attempts to sync data with the central server when network connectivity is detected. The sync process will handle conflict resolution to ensure data consistency across multiple devices and locations.\n\n3. **Incremental Sync**: To minimize data transfer and reduce the load on the network, implement an incremental sync process that only transmits new or updated records since the last sync. This will help in efficiently managing the limited connectivity available in rural areas.\n\n4. **Conflict Resolution Strategy**: Adopt a conflict resolution strategy to handle cases where the same data has been modified on different devices between syncs. This could involve using timestamps, versioning, or user prompts to resolve discrepancies.\n\n5. **Network Quality Detection**: Include a mechanism to detect network quality and adapt the sync behavior accordingly. For example, defer sync attempts or switch to a less frequent sync schedule if the network quality is poor.\n\n6. **User Feedback for Sync Status**: Provide clear and intuitive user feedback regarding the sync status within the app, including indicators for successful sync, in-progress sync, and any errors encountered during the process. This will help users understand and manage the app's connectivity requirements.\n\n7. **Data Prioritization**: Prioritize the syncing of critical data (e.g., patient records, vital signs) over less critical information to ensure that essential information is available on all devices as quickly as possible.\n\nBy incorporating these decisions, the application will be better equipped to handle the challenges posed by low and patchy network connectivity in rural areas while ensuring data availability and consistency across multiple devices and facilities.","GenTime":"2024-07-28 23:07:54"}
{"File Name":"viplab-websocket-api\/0004-use-asymmetric-jwt-signing.md","Context":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n\n## Decision\n","Decision":"* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","tokens":77,"id":1721,"Prediction":"## Decision\nWe will use RSA public\/private key pairs for signing and verifying JSON Web Tokens (JWTs).\n\n## Rationale\n1. **Multi-Tenant Support**: RSA allows us to generate a distinct key pair for each tenant, which ensures that tokens issued for one tenant cannot be validated by another tenant's public key. This fulfills the need for isolated security boundaries between different tenants.\n   \n2. **Security**: RSA is a widely recognized and robust cryptographic algorithm. Using a public\/private key pair provides a high level of security and minimizes the risk of key leakage compared to symmetric keys (HMAC), where the same key is used for both signing and verification.\n\n3. **Scalability**: RSA provides a scalable solution by allowing each tenant to manage their own key pairs. As the number of tenants grows, the system can handle the cryptographic operations efficiently without compromising security.\n\n4. **Compatibility**: RSA keys are widely supported and can be easily integrated with existing systems and libraries for JWT handling. This ensures ease of implementation and interoperability with other systems using JWT for authentication.\n\n## Implications\n1. **Key Management**: Proper key management strategies must be put in place to handle the generation, rotation, and storage of RSA key pairs for each tenant. This includes safeguarding private keys and ensuring public keys are readily available for verification.\n\n2. **Performance**: While RSA is generally more computationally intensive compared to HMAC, modern systems and hardware optimizations can handle this overhead effectively. Performance considerations must be kept in mind, especially in high-traffic scenarios.\n\n3. **Implementation Complexity**: Implementing RSA may introduce additional complexity compared to HMAC, particularly in managing multiple key pairs and ensuring secure storage and distribution of keys. It is essential to allocate resources and expertise to address these challenges.\n\nIn conclusion, the decision to use RSA public\/private key pairs aligns with our requirement for robust multi-tenant support, high security, and future scalability. Implementing proper key management practices will be crucial to the success of this approach.","GenTime":"2024-07-28 23:08:34"}
{"File Name":"external-service-operator\/0004-ports-are-never-stored-as-named-ports.md","Context":"## Context\\nActually it does not make sense to store ports as strings, as those Backends are not in the cluster so they don't have names and can only be referenced by a port number. Nevertheless the Kuberenetes API makes it possible to store ports as string which will not be used at the endpoints generated by the External Service Operator.\\n\n## Decision\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":70,"id":1735,"Prediction":"## Decision\n\nStore ports as integers\n\n### Rationale\n\nGiven that the Backends are not part of the Kubernetes cluster and can only be referenced by a port number, it is more logical to store the ports as integers. This approach ensures consistency with the expected usage at the endpoints generated by the External Service Operator. Storing ports as integers will:\n\n1. Eliminate any confusion or errors that might arise from using string representations of ports.\n2. Align with the natural representation and use of ports as numerical values.\n3. Facilitate validation and range checking, reducing the chance of invalid port values being stored or used.\n4. Simplify code and reduce potential bugs related to string-to-integer conversion.\n\n### Implementation\n\n1. Update the data model to define port attributes as integers.\n2. Refactor any existing code that handles port values to ensure it operates with integers.\n3. Implement validation to confirm that port values fall within the valid range (1-65535).\n4. Test the updated implementation to verify that endpoints generated by the External Service Operator function correctly with integer ports.\n5. Document the change to ensure all team members and relevant stakeholders are aware of the decision and its implications.","GenTime":"2024-07-28 23:09:29"}
{"File Name":"digitalrig-metal-aws\/0007-build-and-deploy-pipelines-separate.md","Context":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n\n## Decision\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":1739,"Prediction":"## Decision\n\nTo address the inefficiencies and inflexibility caused by consolidating the build and deploy stages into one pipeline, the decision is to decouple the build and deploy processes into two separate pipelines. \n\n### Details:\n\n1. **Separate Build Pipeline:**\n   - **Purpose:** This pipeline will focus solely on compiling and packaging the application.\n   - **Trigger:** It will be triggered by changes to the source code or any other necessary build-related files.\n   - **Artifacts:** The build output will be stored as versioned artifacts in a central artifact repository (e.g., Nexus, Artifactory).\n\n2. **Separate Deploy Pipeline:**\n   - **Purpose:** This pipeline will handle the deployment of pre-built artifacts to various environments (e.g., staging, production).\n   - **Trigger:** It can be manually triggered, or based on scheduling policies, or when a new artifact is available.\n   - **Configuration Changes:** The deploy pipeline will allow for configuration changes to be applied without needing a full rebuild of the application.\n\n### Benefits:\n- **Reduced Build Times:** Since configuration changes will not trigger a full build, this will save time and resources.\n- **Enhanced Flexibility:** Changes to configuration files, deployment scripts, and environment-specific settings can be applied independently and more quickly.\n- **Improved Stability:** Isolating the build process ensures that the deploy pipeline is less likely to be affected by build-related issues, leading to more reliable deployments.\n- **Parallelization:** Build and deploy stages can progress independently, facilitating better pipeline parallelization and scheduling.\n\n### Impact and Mitigation:\n- **Initial Setup Effort:** There will be an initial effort required to set up and test the new pipelines. This can be mitigated by creating detailed documentation and automation scripts.\n- **Learning Curve:** Team members will need to familiarize themselves with the new process. Training sessions and detailed guides can help ease this transition.\n\nBy implementing this decision, we aim to streamline our development and deployment workflows, making them more efficient and adaptable to changes.","GenTime":"2024-07-28 23:09:42"}
{"File Name":"digitalrig-metal-aws\/0005-use-aws-rds-aurora-mysql-for-database-persistence.md","Context":"## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL\/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted\/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor\/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny\/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n\n## Decision\n","Decision":"Use AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\\n","tokens":238,"id":1740,"Prediction":"## Decision\n\nWe have decided to use **Amazon Aurora with MySQL compatibility** as the persistence mechanism for Bookit.\n\n### Rationale\n1. **Open Source and Cost-Effective**: MySQL aligns with our preference for open-source solutions, and Amazon Aurora provides a cost-effective pathway within the AWS ecosystem.\n  \n2. **Performance**: Amazon Aurora offers better performance compared to standard MySQL with up-to-the-minute backups. This improvement in performance comes at a minimal additional cost, which is justified even if the immediate performance needs of Bookit are modest. \n\n3. **High Availability**: High availability is a critical requirement for Bookit. Amazon Aurora provides high availability out-of-the-box without the need for complex configurations and maintenance, which simplifies our infrastructure management.\n\n4. **Future Scalability**: While current usage may not necessitate the highest performance, choosing Amazon Aurora with MySQL compatibility offers us the flexibility to scale seamlessly as usage grows over time.\n\n5. **AWS Integration and Ecosystem**: Being an AWS service, Amazon Aurora integrates well with other AWS offerings and leverages the robust AWS security, monitoring, and management capabilities.\n\n### Alternatives Considered\n- **Standard MySQL on AWS RDS**: Rejected due to lesser performance and ease of use compared to Aurora.\n- **PostgreSQL**: Skipped as the Aurora implementation for PostgreSQL was still in Preview, and we required a stable and readily available solution.\n\n### Consequences\n- We will benefit from high performance and high availability provided by Amazon Aurora.\n- Future scalability will be easier, reducing the need for significant migrations or architectural changes as usage grows.\n- A slight increase in cost compared to standard MySQL on RDS is acceptable and justified by the added value provided by Aurora.\n\nThis decision will be revisited if PostgreSQL implementation for Aurora becomes generally available and changes the landscape of our database requirements.","GenTime":"2024-07-28 23:09:46"}
{"File Name":"digitalrig-metal-aws\/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md","Context":"## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n\n## Decision\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":36,"id":1741,"Prediction":"## Decision\n\nWe have decided to switch from Travis CI to GitHub Actions as our CI\/CD solution. GitHub Actions offers a more flexible and robust environment to create various stages of a pipeline, accommodating our specific needs better than Travis. Additionally, GitHub Actions integrates seamlessly with our existing GitHub repositories and provides extensive PaaS capabilities that align with our objectives for a hosted continuous integration\/continuous deployment solution.\n\nKey Factors in Decision:\n1. **Flexibility**: GitHub Actions offers more granular control over the stages and workflows, enabling us to design a pipeline that meets our exact requirements.\n2. **Integration**: GitHub Actions is natively integrated with GitHub, reducing setup complexity and providing smooth collaboration and automation within our existing development workflows.\n3. **Scalability**: GitHub Actions is designed to handle a variety of workloads, allowing us to scale our CI\/CD processes as our project grows.\n4. **Community and Support**: GitHub Actions has a large and active community, along with comprehensive documentation and support from GitHub.\n\nIn summary, moving to GitHub Actions will address the limitations faced with Travis CI and provide us with a powerful, integrated platform for our CI\/CD needs.","GenTime":"2024-07-28 23:09:49"}
{"File Name":"digitalrig-metal-aws\/0006-create-reference-implementation-repository.md","Context":"## Context\\nThe rig defined at [Bookit Infrastructure](https:\/\/github.com\/buildit\/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n\n## Decision\n","Decision":"Create a digitalrig-metal-aws repo (https:\/\/github.com\/buildit\/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording\/concepts.\\n","tokens":162,"id":1742,"Prediction":"## Decision\nTo address the challenges mentioned, we will create a centralized repository dedicated to maintaining the latest and most generic version of the AWS Bare Metal Rig. This new repository will include the full range of component choices to cater to different needs.\n\n### Steps Involved:\n1. **Centralized Repository**: Establish a single repository (`aws-bare-metal-rig`) that will act as the source of truth for the AWS Bare Metal Rig.\n2. **Modular Configuration**: Refactor the current setup to support modular configurations. Each component (Compute, RDS, etc.) will be abstracted into separately manageable modules with clear documentation on usage.\n3. **Feature Flags**: Implement feature flags or a configuration management tool to allow users to easily select different components (e.g., ECS EC2 vs ECS Fargate).\n4. **Branch Management**: Instead of relying on branches for different configurations, we will use directories or configuration files that outline different setups.\n5. **Documentation**: Develop comprehensive documentation and examples for different configurations to aid discoverability and ease of use.\n6. **Version Control**: Use tags and releases to mark stable versions and significant updates, ensuring users can easily identify the latest and most stable configurations.\n\n### Benefits:\n1. **Ease of Discovery**: Users will find it easier to discover and leverage different component choices without navigating through multiple branches.\n2. **Unified Updates**: New features and updates can be managed in a single place, ensuring that all users benefit from the latest improvements without fragmentation.\n3. **Simplified Configuration**: A more modular approach will enable users to tailor their instances to specific needs more efficiently.\n4. **Better Documentation**: Clearer and centralized documentation will improve the overall user experience and reduce the learning curve.\n\n### Implementation Timeline:\n- **Phase 1 (1 Month)**: Setup the centralized repository and migrate existing configurations. Initial documentation to be provided.\n- **Phase 2 (2 Months)**: Refactor for modular components and implement feature flags. Enhance documentation with examples.\n- **Phase 3 (Ongoing)**: Regularly update and maintain the repository, incorporating user feedback and new AWS features.\n\nBy adopting this approach, we aim to streamline the process of instantiating and maintaining instances of the AWS Bare Metal Rig, ensuring consistency and ease of use for all stakeholders.","GenTime":"2024-07-28 23:09:54"}
{"File Name":"digitalrig-metal-aws\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1744,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) process to document key architectural decisions made throughout the project. Each ADR will provide context, the decision made, and the consequences of the decision. We will follow the industry-standard format for ADRs which includes sections for Title, Status, Context, Decision, and Consequences. ADRs will be stored in a dedicated directory within our project's repository and will be version-controlled to ensure traceability and collaborative modification.\n\nOur goal is to ensure that all architectural decisions are well-documented, communicated, and understood across the team, thus aiding future reference and decision-making processes. Each new ADR will be drafted by the engineer responsible for the decision, reviewed by the architecture team and approved in a team meeting before being finalized.\n\nAdditionally, we will conduct periodic reviews of the ADRs to ensure they remain relevant and reflect the current state of the project. This process will help in managing technical debt and aligning the team on the evolving architecture.","GenTime":"2024-07-28 23:10:01"}
{"File Name":"TOSCAna\/0018-cloudfoundry-no-deployment-in-runtime.md","Context":"## Problem\\nMost of the credentials\/information of the environment (like a service address) are only available as soon the application is deployed.\\n\n## Decision\n","Decision":"* Chosen Alternative: creating scripts\\n","tokens":29,"id":1745,"Prediction":"## Decision\nTo handle the late availability of environment-specific credentials and information, we will implement dynamic configuration management in our application.\n\n### Key Actions:\n\n1. **Environment Variables**: \n   - Utilize environment variables for all environment-specific credentials and information.\n   - Ensure these variables can be easily set or updated in the deployment pipeline and during runtime, without requiring application changes or redeployment.\n\n2. **Configuration Files**:\n   - Use external configuration files as a fallback, which can be read by the application at startup or during runtime.\n   - These configuration files will be kept outside of the application's source code and can be updated independently.\n\n3. **Secret Management Tools**:\n   - Integrate with secret management tools (e.g., AWS Secrets Manager, HashiCorp Vault, Azure Key Vault) to securely store and retrieve sensitive information at runtime.\n   - This integration allows the application to access secrets as late as possible and ensures they are always current.\n\n4. **Service Discovery**:\n   - Implement a service discovery mechanism to dynamically obtain service addresses at runtime.\n   - Use tools like Consul, Eureka, or built-in cloud provider solutions to dynamically resolve service endpoints instead of hardcoding them.\n\n5. **Application Initialization**:\n   - Design the application to delay the initialization of components that depend on dynamic configuration until the configuration has been retrieved.\n   - Utilize retry mechanisms to handle temporary unavailability of configuration information.\n\n### Rationale:\n- **Security**: Separating credentials and sensitive information from the application's source code enhances security.\n- **Flexibility**: Dynamic configuration allows for changes in the environment without requiring application redeployment.\n- **Scalability**: This approach supports scaling, as the configuration can be managed centrally and applied consistently across different deployment environments.\n- **Maintainability**: Easier management of configuration changes and credentials through environment variables, configuration files, and secret management tools.\n\n### Implementation Considerations:\n- Ensure proper error handling when retrieving environment-specific information.\n- Implement logging to monitor and troubleshoot configuration access issues.\n- Regularly review and update the configurations and secrets management policies to adhere to the latest security best practices.\n\n### Risks:\n- Potential delays in initialization if the dynamic configuration retrieval fails or is slow.\n- Increased complexity in the deployment pipeline and runtime environment.\n\nBy implementing these measures, our application will be able to obtain required credentials and environment-specific information dynamically and securely, ensuring smooth and flexible deployments.","GenTime":"2024-07-28 23:10:07"}
{"File Name":"CAFE5\/0002-calculate-prior-from-user-input.md","Context":"## Context\\nA prior distribution needs to be specified to accurately calculate probabilities. The prior can\\nbe calculated in many different ways. Hopefully this decision reflects the least surprising\\nresults for the user.\\n\n## Decision\n","Decision":"The prior will be calculated as follows:\\n* if -p specified on command line: calculate a Poisson distribution with the specified lambda\\n* if -f specified on command line: The user has specified a root distribution. Use that. Issue a warning if the user has ALSO specified a Poisson lambda\\n* if -i specified on command line: Estimate a Poisson distribution from the families provided.\\n* Otherwise, use a uniform distribution. Issue a warning as this is not a very reasonable prior.\\n","tokens":40,"id":1747,"Prediction":"## Decision\nWe will use a Bayesian approach with a conjugate prior distribution. Specifically, we will choose a Beta distribution as the prior for binomial probabilities, given its properties of computational convenience and intuitive interpretability. The parameters (\u03b1, \u03b2) for the Beta distribution will be informed by domain-specific knowledge or a data-driven approach such as empirical Bayes, ensuring that the chosen prior reflects realistic expectations and minimizes surprise for the user. This decision balances mathematical rigor with user experience, providing transparent and explainable results.","GenTime":"2024-07-28 23:10:11"}
{"File Name":"CAFE5\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1751,"Prediction":"## Decision\nWe will use the Architectural Decision Record (ADR) format to document all significant architectural decisions made during the project. This format includes the following sections for each decision:\n\n1. **Title**: A short, descriptive title for the decision.\n2. **Context**: The background and rationale for why the decision is being considered.\n3. **Decision**: The actual decision made.\n4. **Consequences**: The expected outcomes of the decision, including any trade-offs or potential drawbacks.\n\nWe will store these records in a designated directory within our project's repository, allowing for easy access and version control. Each ADR document will be named with a sequential number and a concise description of the decision, for example, `001-use-rest-api.md`.\n\nBy adopting this structured approach, we ensure that all architectural decisions are well-documented, transparent, and accessible for future reference. This will also facilitate better communication within the team and support ongoing maintenance and updates to the project.","GenTime":"2024-07-28 23:10:21"}
{"File Name":"contact-frontend\/0003-remove-login-redirection.md","Context":"## Context and Problem Statement\\nWithin contact-frontend, for the standalone pages, two routes into the page exist. The first requires a tax service\\nuser to be logged in, and redirects to login if the service user is not logged in. The second, on a different URL\\nsuffixed with \"-unauthenticated\", does not require login, but serves the same page. After discussion, the PlatUI team\\ndecided that the requirement for a user to be logged in was making a worse experience for the service end user, adding\\nthe requirement of login,when all the same functionality is also available without logging in via a different URL.\\n## Decision Drivers\\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\n\n## Decision\n","Decision":"* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\nChosen option: \"Remove login redirect from all pages and keep existing endpoints for now\", because:\\n* The benefits to end users in terms of accessibility feel significant enough to warrant the change\\n* Information about logged in users will still be persisted to Deskpro but there will not be a redirect for users\\nwho have been logged out in the background\\n* Codebase is significantly simplified for developers working on contact-frontend\\n* However, deleting the \"-unauthenticated\" routes will be a major breaking change across the Platform, and should be\\ndecoupled from this\\n### Positive Consequences\\n* Capturing information about all logged in users, not just the ones who have clicked on a contact-frontend route not\\nending in \"-unauthenticated\"\\n* More accessible experience for tax platform end users who are not forced to log in to report technical problems\\n* Significant simplification of codebase for developers\\n### Negative Consequences\\n* There is a risk that without forcing users to log in to particular implementations of the contact-frontend forms,\\nthere will be less information sent to DCST in certain cases. On the other hand, for certain users, the login\\nrequirement might lock them out of the contact-frontend forms completely, e.g. if they are trying to report a problem\\nwith login\\n","tokens":314,"id":1754,"Prediction":"## Decision\nWe will consolidate the two routes into a single route that does not require login for the standalone pages. Specifically:\n\n1. **Primary URL Access**: We will serve the standalone pages on a primary URL that does not require the user to be logged in.\n2. **User Information**: If a user is logged in, we will attempt to look up and utilize their enrolments to maximize the information persisted to Deskpro agents. This ensures that user details are effectively captured without mandating a login.\n3. **Legacy URLs**: We will redirect any existing traffic from the \"-unauthenticated\" URLs to the new primary URLs.\n4. **Codebase Simplification**: This decision simplifies the codebase by removing the need to maintain both logged-in and non-logged-in versions of the pages.\n\nBy making this change, we aim to:\n\n- Improve user experience by removing unnecessary login steps, thus allowing for quicker and more seamless access to report issues.\n- Ensure we still capture relevant user information when they are logged in, without compromising on functionality.\n- Reduce the complexity of our codebase, expediting the development and testing processes.\n- Make the site more accessible, encouraging more feedback from users, especially those with additional accessibility needs.\n\nThis change will be implemented in the next release cycle, with appropriate user communication to ensure a smooth transition.","GenTime":"2024-07-28 23:10:35"}
{"File Name":"ksch-workflows\/0002-using-spring-boot.md","Context":"## Context\\nWe need [dependency injection](https:\/\/stackoverflow.com\/questions\/130794\/what-is-dependency-injection) in order to build decoupled software units.\\nAlso we need access on a relational database.\\n\n## Decision\n","Decision":"Spring offers a lot of convenience features for those purposes.\\nApache Wicket already provides a Spring integration.\\nWith the help of the [wicket-spring-boot](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot) the convenience feature of Spring Boot can be applied on top of that.\\nThis makes the development faster and easier than using the alternative of dependency injection with [Google Guice](http:\/\/software.danielwatrous.com\/wicket-guice-including-unittests\/).\\nFurther usage of Spring enables potential useage of the various Spring sub-projects for advanced requirements like the integration of other software systems.\\n","tokens":46,"id":1758,"Prediction":"## Decision\nWe will use the Spring Framework for dependency injection and database access.\n\n1. **Spring Framework**: Spring's Dependency Injection (DI) features will help us create loosely coupled software units. By using annotations like `@Autowired` and configuration classes, we can manage dependencies efficiently.\n\n2. **Spring Data JPA**: For relational database access, we will use Spring Data JPA. This provides a repository abstraction over the JPA (Java Persistence API), making it easier to implement data access layers with minimal boilerplate code.\n\n### Justification:\n- **Maintainability**: Using Spring's DI framework enhances the maintainability of the software by allowing various modules to be developed and tested independently.\n- **Scalability**: Spring Data JPA offers robust and scalable solutions for database access, including support for various databases and powerful features like pagination and auditing.\n- **Community and Support**: Both Spring Framework and Spring Data JPA are widely adopted and have rich communities and extensive documentation, ensuring long-term support and availability of resources for troubleshooting and development.\n\n### Implementation Plan:\n1. **Set up Spring Boot**: Initialize a Spring Boot project, which provides a scaffolding for our application with built-in dependency management.\n2. **Configure Dependency Injection**: Use `@Autowired` for injecting dependencies and define beans in a configuration class.\n3. **Integrate Spring Data JPA**: Set up JPA repositories and configure database connectivity properties in the application configuration files.\n4. **Testing**: Implement unit tests to ensure that the components are correctly injected and the database access layers function as expected.\n\nBy following this plan, we will achieve a modular, decoupled architecture with efficient data management.","GenTime":"2024-07-28 23:10:54"}
{"File Name":"tdr-dev-documentation\/0018-govkuk-notify-staging-configuration.md","Context":"## Context\\nIt was decided to set the GovUK Notify Staging service to go live: [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)\\n\n## Decision\n","Decision":"The GovUK Notify team recommended that usually they do not go live with non-production services.\\nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.\\nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.\\nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".\\nFor full details of the \"trial mode\" for GovUK Notify services see here: https:\/\/www.notifications.service.gov.uk\/using-notify\/trial-mode\\n","tokens":44,"id":1767,"Prediction":"## Decision\n\nWe have decided to proceed with setting the GovUK Notify Staging service to go live as per the configuration outlined in ADR [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md). This involves the following actions:\n\n1. **Configuration Deployment**: Deploy the configuration changes necessary for the GovUK Notify Staging environment, ensuring they match the settings detailed in the ADR.\n  \n2. **Testing**: Perform thorough testing in the staging environment to ensure that notifications are sent and received correctly. This includes unit tests, integration tests, and end-to-end tests to verify functionality.\n\n3. **Monitoring Setup**: Set up monitoring and alerting to track the performance and health of the GovUK Notify Staging service. This will help quickly identify any issues that arise post-deployment.\n\n4. **Access Control**: Review and adjust access control measures to ensure that only authorized personnel can make changes to the staging service and monitor its performance.\n\n5. **Documentation**: Update documentation to reflect the go-live status of the GovUK Notify Staging service, including any new procedures, configurations, or troubleshooting guidelines.\n\n6. **Stakeholder Notification**: Notify relevant stakeholders and team members about the go-live decision and provide details on the expected impact and benefits of this change.\n\nBy taking these steps, we ensure a smooth transition for the GovUK Notify Staging service to become fully operational, supporting our ongoing development and testing objectives.","GenTime":"2024-07-28 23:11:27"}
{"File Name":"tdr-dev-documentation\/0015-govuk-notify-staging-configuration.md","Context":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n\n## Decision\n","Decision":"The \"staging\" GovUK Notify service is to be live to allow for user testing on this environment.\\nAlso, it will mirror the TDR production environment as closely as possible.\\n","tokens":126,"id":1784,"Prediction":"## Decision\n\nThe decision has been made to configure each TDR environment (development, test, staging, production) with its own separate GovUK Notify service. This approach addresses the limitation that GovUK Notify does not natively support multiple environments within a single service.\n\nIn detail:\n- Each environment will have its own dedicated GovUK Notify service instance.\n- The services will be configured in \"trial mode\" for non-production environments (such as development and testing) to leverage the sending limits and restricted email recipient functionality, reducing the risk of unintended email sends during development and testing phases.\n- The production environment will be configured in \"live mode\" to ensure that all emails can be sent without restrictions.\n\nThis segregation allows us to mirror the environment separation within our Keycloak setup and ensure that environment-specific configuration and service reliability are maintained, while still leveraging GovUK Notify\u2019s functionality and features effectively.","GenTime":"2024-07-28 23:12:31"}
{"File Name":"tdr-dev-documentation\/0017-govuk-notify-keycloak-message.md","Context":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\n\n## Decision\n","Decision":"It was decided to pass the entire Keycloak message into the GovUK Notify template via the `keycloakMessage` personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.\\nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.\\nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.\\n","tokens":33,"id":1786,"Prediction":"## Decision\n\n### Chosen Email Provider: GovUK Notify\nWe have decided to use GovUK Notify as the email provider for Keycloak. This decision is influenced by the following factors:\n\n1. **Compliance and Security**: GovUK Notify adheres to stringent UK government security standards. This ensures that the email communications from our Keycloak system remain secure and compliant with governmental regulations.\n\n2. **Cost-Effectiveness**: GovUK Notify is a cost-effective solution for sending emails, offering competitive pricing which aligns with our budgetary constraints.\n\n3. **API Integration**: GovUK Notify offers a reliable and well-documented API which can be seamlessly integrated with Keycloak. This will facilitate smooth email communications from the Keycloak system, including user verification, password resets, and other notifications.\n\n4. **Reliability and Scalability**: With proven reliability and scalability, GovUK Notify can handle the volume of emails we anticipate sending without any degradation in service quality.\n\n5. **Support and Maintenance**: Utilizing a service supported by the UK government ensures ongoing maintenance, support, and improvements, reducing the operational overhead for our team.\n\nThis decision is aligned with our objectives to provide a trustworthy, efficient, and cost-effective email communication system for all Keycloak-related functionalities. \n\nImplementation will proceed by configuring Keycloak to utilize the GovUK Notify service as detailed in the associated documentation. This will involve updating Keycloak\u2019s email settings, testing the integration, and monitoring the system to ensure expected performance and reliability.","GenTime":"2024-07-28 23:12:39"}
{"File Name":"NorthwindCore\/0001-record-architecture-decision.md","Context":"## Context\\nAs the project is an example of a clean code and best practices of software development, it is necessary to save all architectural decisions in one place.\\n\n## Decision\n","Decision":"For all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","tokens":33,"id":1792,"Prediction":"## Decision\nTo ensure that all architectural decisions are documented and easily accessible, we have decided to implement an Architectural Decision Record (ADR) process. This involves:\n\n1. **Centralized Repository**: Establishing a centralized repository (e.g., within the project's main GitHub repository) dedicated to storing all ADRs.\n2. **Standard Template**: Using a consistent and standardized template for each ADR to ensure uniformity and comprehensiveness in documentation.\n3. **Version Control**: Leveraging version control systems (e.g., Git) to track changes and updates to ADRs, providing a clear history and rationale for architectural decisions.\n4. **Review Process**: Implementing a review and approval process for all new ADRs, ensuring that decisions are peer-reviewed and validated.\n5. **Accessibility**: Making the ADR repository easily accessible to all team members and stakeholders to promote transparency and shared understanding.\n\nThis approach will maintain the integrity of the architectural decisions and uphold the principles of clean code and best practices in software development.","GenTime":"2024-07-28 23:13:00"}
{"File Name":"fixcity\/0003-sys-use-modular-monolith.md","Context":"## Context\\n- Constraints\\n- Greenfield project\\n- Quality attributes\\n- Expected fast load\\n\n## Decision\n","Decision":"We will not separate components in to separate deployment units,\\nwe will use modular monolith approach with single database.\\n","tokens":20,"id":1797,"Prediction":"## Decision\nGiven the context of a greenfield project with constraints emphasizing quality attributes and an expected fast load time, we have decided to implement a microservices architecture using a combination of performant backend technologies (e.g., Go or Node.js) and a highly optimized frontend framework (e.g., React or Angular).\n\n### Reasoning:\n1. **Microservices Architecture**: This will allow us to develop and deploy independent components, enhancing maintainability and scalability. Each microservice can be optimized individually for performance.\n2. **Performant Backend Technologies**: Using efficient programming languages like Go or Node.js for backend services will help reduce response times and handle high levels of concurrent requests efficiently.\n3. **Optimized Frontend Framework**: Utilizing modern frontend frameworks such as React or Angular will enable us to build fast-loading and highly responsive user interfaces. These frameworks also offer advanced tooling for performance optimization.\n4. **Load Testing and Monitoring**: We will integrate load testing tools and monitoring services to continuously track performance metrics and ensure that the service meets the expected fast load times throughout the development lifecycle.\n\nBy combining these approaches, we aim to meet the project's quality attributes and deliver a robust, high-performing software system.","GenTime":"2024-07-28 23:13:14"}
{"File Name":"hmpps-interventions-docs\/0005-for-release-prioritise-consistency-over-availability.md","Context":"## Context\\n1. We have a fixed deadline\\n1. We will have our first users week beginning 7 June\u00a02021\\n1. For our Delius integration, we rely on an intricate set of reference data\\n1. We are uncertain if there are any issues with the reference data for all possible combinations of offenders\\n1. We rely on these integrations to set up \"container\" data in nDelius. If they fail, we need to recover before providers book appointments\\nWe want to ensure when updates fail, we have a way to resume that retries the interaction.\\nHowever, **we do not have retry mechanisms in place yet**.\\nWe could add an admin task\/script to retry a certain event (we use Spring Application Events that don't\\ncreate side effects, so we could retrigger those).\\nWe feel this would create an overhead that would be too much administrative burden together with\\nthe anticipated noise of most users starting the service at the same time.\\n\n## Decision\n","Decision":"Due to the uncertainties and lack of convenient retry mechanism,\\nwe favour **consistency over availabilty** in the short term.\\n","tokens":201,"id":1799,"Prediction":"## Decision\n\nTo address the requirement of ensuring data consistency and recoverability of failed updates without adding administrative overhead, we will implement an automated retry mechanism within our system.\n\n### Implementation Details:\n1. **Use Spring Retry Library:**\n   We will incorporate the Spring Retry library to enable automatic retries of failed operations within our integration with nDelius. This allows for transparent retries without the need for manual intervention.\n   \n2. **Retry Configuration:**\n   a. Configure the number of retries: Set a reasonable limit to avoid endless loops (e.g., a maximum of 3 attempts).\n   b. Backoff policy: Implement an exponential backoff strategy to progressively increase the wait time between retries to reduce the load on the system and external services.\n\n3. **Error Handling:**\n   a. Log all retry attempts and their outcomes to facilitate monitoring and debugging.\n   b. If the maximum retry limit is reached and the operation still fails, log this as a critical issue and notify the appropriate team for immediate investigation.\n\n4. **Monitoring and Alerts:**\n   Implement monitoring and alerting mechanisms to detect and notify of retry failures beyond the maximum attempts, allowing for proactive resolution.\n\n5. **Fallback Mechanism:**\n   In scenarios where retry still fails, implement fallback logic to ensure that the system can continue operating, such as queuing the failed operation for later manual review and intervention.\n\nBy implementing this automated retry mechanism, we aim to minimize the administrative burden and enhance system reliability, thereby supporting a successful onboarding of our first users in early June 2021.","GenTime":"2024-07-28 23:13:20"}
{"File Name":"experimenter\/0009-nimbus-web-sdk-architecture.md","Context":"## Context and Problem Statement\\nThis is part of a body of work necessary to support the use of Nimbus within web applications.\\nThe current Nimbus SDK is written in such a way that it supports client-oriented experimentation \u2014 experiments are downloaded, evaluated, and stored on the client, and a feature store is exposed with the experiment branches applied.\\nIn previous decisions (not in this repository), we've already decided that in order to support web experimentation the Nimbus SDK will need to be updated to be stateless, to support a more statically defined set of helper methods, and to have additional support for Python.\\nUltimately, the problem we're trying to solve can be boiled down to one question \u2014 how can we update the Nimbus SDK to support web applications while continuing to support the existing clients?\\nAs an example of what the Cirrus API might look like, we can likely expect endpoints to perform the following:\\n* Enroll a user into available experiment(s)\\n* This would return the enrolled experiments as well as the feature values given the enrollments\\n* Fetch the default feature values\\n* Fetch the feature manifest\\n* Fetch a specific feature value given enrolled experiments\\n## Decision Drivers\\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\n\n## Decision\n","Decision":"* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\nWe have decided to move forward with option number 2, \"Cargo features, one library\".\\nThis option, like the others mentioned, meets the key decision drivers.\\nWe believe using this option will be the most maintainable long-term, despite the added complexity of using cargo features.\\nIn addition, implementing this option has a similarly short timeline and amount of work necessary as compared to the \"Cargo features, separate library\" option, without the additional overhead of more complex typing.\\n### Positive Consequences\\n* Meets all decision drivers.\\n* Small amount of work necessary with a very limited amount of code churn.\\n### Negative Consequences\\n* It will be difficult to draw boundaries between Cirrus and Nimbus code.\\n* We could run into unexpected issues with UniFFI, as multiple UDL files in a single crate has not yet been tested.\\n2. [Cargo Features](https:\/\/doc.rust-lang.org\/cargo\/reference\/features.html)\\n2. Examples\\n1. [Multi-module typing example](https:\/\/github.com\/jeddai\/application-services\/pull\/1\/files#diff-02305e8e02a7900352e67be1fd2eef0b5a7c7cf91f4cc3e4559668c124d34e88R11-R19)\\n","tokens":307,"id":1819,"Prediction":"## Decision\n\nTo support the use of Nimbus within web applications while continuing to support existing clients, we will implement the following architectural decisions for the Nimbus SDK:\n\n1. **Stateless Architecture for Nimbus SDK**:\n   - The SDK will be updated to adopt a stateless design. This ensures that no client-side state is stored, enabling better scalability and ease of integration with web applications.\n\n2. **Core Logic Reuse**:\n   - The core logic of the existing Nimbus SDK, which is currently written in Rust, will be re-used for the Nimbus web service, also referred to as Cirrus. This leverages the existing robust architecture and minimizes rework, ensuring consistency across different platforms.\n\n3. **UniFFI for Python Support**:\n   - We will integrate Python support via UniFFI, a tool that facilitates building cross-platform APIs in Rust. This will enable the Nimbus SDK to expose its functionality to Python applications, thereby supporting broader use cases within web applications.\n\n4. **API Endpoints Implementation**:\n   - We will implement the following endpoints in the Nimbus web service (Cirrus) to cater to web application needs:\n      - **Enroll a user into available experiment(s)**: This endpoint will handle the logic of enrolling users into experiments and will return both the enrolled experiments and the associated feature values.\n      - **Fetch default feature values**: This endpoint will retrieve the default feature values as defined in the Nimbus configuration.\n      - **Fetch the feature manifest**: This endpoint will provide the feature manifest, which includes detailed information about all available features.\n      - **Fetch a specific feature value given enrolled experiments**: This endpoint will return the value of a specific feature based on the user's enrolled experiments.\n\nBy making these decisions, we will maintain compatibility with the current Nimbus clients while extending functionality to support web applications more effectively. This approach ensures that the core strength of the existing SDK is preserved, and the additional requirements for web integration are met in a structured and efficient manner.","GenTime":"2024-07-28 23:14:32"}
{"File Name":"experimenter\/0007-doc-hub-url.md","Context":"## Context and Problem Statement\\nWe'd like to have a permanent URL for the experimenter docs hub that is:\\n- Memorable\\n- Stable\\n- Ideally, part of the rest of our data information architecture\\n\n## Decision\n","Decision":"We will use a custom domain for now (experimenter.info) until a decision is made on the general organization of data.mozilla.org in the future, at which time we will update the URL to fit in with the rest of our data documentation.\\n","tokens":45,"id":1823,"Prediction":"## Decision\n\nWe will establish the permanent URL for the experimenter documentation hub as `https:\/\/docs.ourdatahub.com\/experimenter`. This decision is based on the following criteria:\n\n1. **Memorable**: The URL is concise and easy to remember for the users.\n2. **Stable**: It leverages our primary domain (`ourdatahub.com`), ensuring it remains stable and under our control.\n3. **Aligned with Data Information Architecture**: Integrating the URL within our existing domain aligns with our overall information architecture, making it easier for users to navigate between different sections of our documentation and services.\n   \nBy adopting this URL structure, we enhance the user experience while maintaining consistency and control over our documentation resources.","GenTime":"2024-07-28 23:14:48"}
{"File Name":"cloud_controller_ng\/0003-switching-to-request-specs-for-controllers.md","Context":"Context\\n-------\\nThere are two approaches to unit testing controllers, neither of which is very useful: If we mock most of the\\ndependencies of the controller, our tests become tightly coupled to the implementation and do not give us a\\nlot of confidence. If we make them more integration-style tests, we end up duplicating lots of tests between the\\ncontroller and the request specs.\\nAdditionally, writing controller specs is a discouraged pattern:\\n> Controller specs can be used to describe the behaviour of Rails controllers. As of version 3.5, however, controller specs are discouraged in favour of request specs (which also focus largely on controllers, but capture other critical aspects of application behaviour as well). Controller specs will continue to be supported until at least version 4.0 (see the release notes for details).\\nFrom [RSpec\u200a\u2014\u200aController or Request Specs?][]\\n**Warning**: Request specs in the `cloud_controller_ng` codebase are not actually Rspec\/Rails request specs (which would be [declared with `type: :request`](https:\/\/relishapp.com\/rspec\/rspec-rails\/docs\/request-specs\/request-spec)), but use [Rack Test](https:\/\/github.com\/rack-test\/rack-test).  Rack Test has similar looking helper methods for making requests (e.g. `get`, `post`, etc), but these take different arguments than the equivalent Rspec methods.\\nDecision\\n--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n\n## Decision\n","Decision":"--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n","tokens":403,"id":1829,"Prediction":"## Decision\n\nWe will exclusively use request specs for testing controllers going forward. This decision aligns with current best practices and ensures more meaningful and reliable tests for our controller behavior.\n\n### Actions to be Taken:\n\n1. **Transition Existing Controller Specs**:\n   - Identify and transition all existing controller specs into request specs to minimize redundancy and maximize coverage.\n   - Ensure that all transitioned tests effectively use `Rack Test` and adapt any differences in method signatures or behavior between controller specs and request specs.\n\n2. **Adjust Spec Helpers**:\n   - Update current controller spec helpers to function appropriately as request spec helpers. This means eliminating direct mocks of the CC app and replicating the request environment as closely as possible.\n\n3. **Delete Deprecated Specs**:\n   - Once confirmation of the successful transition of tests is in place, delete the old controller specs to avoid any confusion and maintain a clean codebase.\n\n### Expected Benefits:\n\n- **Reduced Duplication**:\n   - By focusing only on request specs, testers will avoid duplicative effort between controller and request specs, ensuring a more streamlined testing process.\n   \n- **Enhanced Confidence in Tests**:\n   - Request specs provide better end-to-end coverage of the controller functionality, catching issues that unit-level controller tests might miss due to their highly mocked environment.\n\n- **Alignment with Best Practices**:\n   - As controller specs are currently discouraged and may see reduced support in future frameworks, transitioning to request specs prepares the codebase for future updates and maintains adherence to recommended practices.\n\n### Potential Challenges:\n\n- **Initial Transition Overhead**:\n   - The change will require an initial investment in transitioning existing tests and updating helper methods, which may take time and resources.\n   \n- **Learning Curve**:\n   - Developers may need time to adapt to the differences in writing and managing request specs compared to controller specs, especially around the nuances of `Rack Test`.\n\nBy committing to these actions, we aim to maintain a modern, maintainable, and effective testing strategy that enhances the overall quality and reliability of our software.","GenTime":"2024-07-28 23:15:02"}
{"File Name":"sdbmss\/0002-build-crud-and-search.md","Context":"## Context\\nThe Schoenberg Institute won three-year NEH grant to rebuild the existing Schoenberg Database of Manuscripts to\\nreplace the then current SDBM which was written in ColdFusion, lacked critical data model features and user-based editing.\\nThe new application needed a more flexible, user contribution, complex search and an up-to-date technology stack.\\n\n## Decision\n","Decision":"The following technologies were select for the following reasons.\\n- Blacklight, which uses Solr, was chosen to provide complex, configurable search, and uses technology common in the library, and for which long term product support could be expected. Blacklight was also used for bookmarking.\\n- Rails is required by Blacklight\\n- MySQL was selected as it was the database of choice within library technology services, and supported by sysadmin staff\\n- Solr is required by Blacklight\\n- Delayed Job (URL) was chosen as for orchestrating background jobs, namely sending updates from the database to the Solr index\\n- Sunspot was chosen for the Rails-Solr interaction\\n- AngularJS was chosen to support complex, dynamic page interactions\\n- cancancan was selected to provide tiered, role-based access for users with different permission levels\\n","tokens":72,"id":1837,"Prediction":"## Decision\n\nAfter thorough evaluation of the current needs and future scalability requirements, the following decisions have been made for the rebuilding of Schoenberg Database of Manuscripts (SDBM):\n\n1. **Technology Stack**: \n   - **Front-End**: We will use React.js, due to its component-based architecture, virtual DOM for efficient rendering, and strong community support.\n   - **Back-End**: We will adopt Node.js with Express.js for the backend to ensure a non-blocking, event-driven architecture that can handle numerous simultaneous connections efficiently.\n   - **Database**: We will use PostgreSQL, given its powerful querying capabilities, support for complex data models, and ACID compliance to ensure data integrity.\n   - **Search Capabilities**: Elasticsearch will be utilized to deliver powerful full-text search capabilities and handle complex search queries efficiently.\n   - **Authentication and User Management**: We will integrate Auth0 for a secure and robust user authentication mechanism, including user-based editing features.\n   - **Hosting**: We will deploy the application on AWS for its scalable infrastructure, extensive services, and compliance with security best practices.\n\n2. **Data Model Enhancements**: \n   - The data model will be redesigned to support flexible user contributions and complex relationships within the catalog of manuscripts.\n   - Schemas will be defined to support additional metadata fields, richer hierarchical relationships, and controlled vocabularies to enhance discoverability and data richness.\n\n3. **User Contributions**:\n   - Implement a moderation system for user contributions to ensure data quality. All user submissions will undergo a review process before being published.\n   - Build user-friendly interfaces to encourage contributions from scholars and the wider community. This may include guided forms, inline validation, and tooltips for assistance.\n\n4. **Search and Discovery**:\n   - Develop advanced search functionalities, including Boolean operators, faceted search, and filters by various metadata fields to help users find relevant manuscripts quickly.\n   - Implement a responsive and intuitive search interface to provide a seamless user experience across devices.\n\n5. **Legacy Data Migration**:\n   - Develop a robust data migration plan to transfer existing data from the ColdFusion application to the new PostgreSQL database, ensuring no data loss.\n   - Create scripts to clean and transform legacy data into the new data model format.\n\n6. **Testing and Quality Assurance**:\n   - Adopt a test-driven development (TDD) approach to ensure code quality and reliability.\n   - Implement continuous integration and continuous deployment (CI\/CD) pipelines to automate testing and deployment processes.\n\n7. **Community and Collaboration**:\n   - Facilitate a feedback mechanism for continuous improvement based on user experience and community suggestions.\n   - Engage with domain experts and user community regularly to ensure the application meets evolving scholarly needs.\n\nBy adopting this modern technology stack and focusing on flexible data models and user contributions, the new SDBM will be well-equipped to serve the scholarly community for years to come.","GenTime":"2024-07-28 23:15:28"}
{"File Name":"sdbmss\/0008-rdf-sparql-support.md","Context":"## Context\\nAs part of participation in the [Mapping Manuscript Migrations (MMM)][mmm] Linked Data project, the SDBM needed to export its data in RDF format for aggregation in a unified set of data from the project's three contributing organizations.\\n[mmm]: http:\/\/mappingmanuscriptmigrations.org \"Mapping Manuscript Migrations project site\"\\nAt the time the export was built a single unified data model had not been agreed upon.\\n\n## Decision\n","Decision":"The project decided to build a SPARQL endpoint built on Apache Jena. Since there was no target data model, the SDBM was exported to a custom namespace using a simple, direct mapping from SDBM model attributes to RDF. RabbitMQ messenger queue and a listener interface (`interface.rb`) to push updates from the SDBM to Jena. Also a simple SPARQL query interface was added for SDBM users.\\n","tokens":94,"id":1839,"Prediction":"## Decision\nAfter careful consideration of the project's requirements and the current lack of a unified data model, the decision has been made to implement a flexible and extensible RDF export pipeline. This pipeline will have the following key characteristics:\n\n1. **Interim Data Model**: Utilize an interim, extensible data model that adheres to common RDF standards while remaining adaptable to changes.\n2. **Version Control**: Implement version control on the exported RDF data to manage changes and iterations efficiently as the unified data model evolves.\n3. **Metadata Mapping**: Develop a robust metadata mapping mechanism to ensure as much consistency as possible with other contributing organizations.\n4. **Validation and Testing**: Incorporate validation and testing procedures to ensure data integrity and compliance with RDF standards.\n5. **Stakeholder Communication**: Maintain open communication channels with project stakeholders to promptly adapt to emerging consensus on the unified data model.\n\nThis decision aims to balance the current necessity for RDF export with the flexibility needed to accommodate future changes, facilitating seamless integration into the Mapping Manuscript Migrations project.","GenTime":"2024-07-28 23:15:35"}
{"File Name":"LogLady\/0006-use-electron-updater-autoupdate.md","Context":"## Context\\nThere are multiple packages available for handling auto-update on Electron apps, and one was already installed in the project but not used - which package should the project use?\\n\n## Decision\n","Decision":"We will use [electron-updater](https:\/\/github.com\/electron-userland\/electron-builder) as the package for auto-update.\\nThe package already installed in the project was replaced. It was decided earlier to use it instead of electron-updater because support for platforms was limited - but with new versions that has been added.\\nElectron-updater is the most popular package for auto-updating Electron apps, with more weekly downloads on npm and more dependents than the package that was already installed.\\nOverall, electron-updater seems like a more reliable package for handling auto-update.\\n","tokens":36,"id":1852,"Prediction":"## Decision\n\nAfter an evaluation of the current needs and the options available, we have decided to proceed with the package that was already installed in the project but not yet used. The reasoning for this decision includes: \n\n1. **Ease of Integration:** Since the package is already installed, integration with the current codebase should be more seamless compared to introducing a new package.\n2. **Consistency:** There may have been initial considerations why this package was chosen for installation. Leveraging existing choices can maintain consistency within the project's dependencies.\n3. **Minimize Risk:** Introducing a new package brings in additional risks related to compatibility and stability. Using the already installed package reduces these risks.\n4. **Cost-Efficiency:** Using the installed package saves time and effort that would otherwise be spent on evaluating, installing, and integrating a new package.\n\nWe will proceed with a detailed assessment and implementation plan for incorporating the existing package into our auto-update functionality.\n","GenTime":"2024-07-28 23:16:17"}
{"File Name":"LogLady\/0003-implement-redux.md","Context":"## Context\\nState handling in React becomes very complicated when you have too many states\\n\n## Decision\n","Decision":"We implement Redux to handle state\\n","tokens":17,"id":1854,"Prediction":"## Decision\nTo address the complexity of state handling in React due to having too many states, we have decided to integrate a state management library. After evaluating several options, we have chosen to use Redux for global state management. Redux will allow us to maintain a single source of truth for our application's state, making it easier to manage and debug state transitions. This decision also includes implementing Redux Toolkit, which provides a more concise and less error-prone way to manage actions and reducers, and integrating Redux DevTools for better state inspection and debugging.\n\nAdditionally, for local component states that do not necessitate global access, we will continue to use React's built-in `useState` and `useReducer` hooks. This approach strikes a balance between leveraging Redux for complex, shared state management while keeping simpler, isolated state management within component boundaries.","GenTime":"2024-07-28 23:16:21"}
{"File Name":"LogLady\/0004-implement-styled-components.md","Context":"## Context\\nComplicated with uses of different css rules\\n\n## Decision\n","Decision":"Only uses styled-components for design\\n","tokens":12,"id":1855,"Prediction":"## Decision\nTo address the complexity arising from the use of different CSS rules, we have decided to standardize our CSS by adopting a CSS-in-JS solution such as Styled Components or Emotion. This approach will help in managing styles more effectively by:\n\n1. Enabling scoped styling to avoid conflicts and ensure consistent application of styles.\n2. Allowing the use of JavaScript to dynamically compose and manage styles, making it easier to maintain and update.\n3. Providing better integration with our existing React components, resulting in a more declarative and modular styling architecture.\n\nThe adoption of CSS-in-JS will involve refactoring existing styles incrementally to ensure smooth transition and minimal disruption to the current setup.","GenTime":"2024-07-28 23:16:23"}
{"File Name":"content-data-api\/adr-007-etl-publishing-api-content-store.md","Context":"## Context\\nAs [per this Trello card][1], we want to populate the Content Items' dimension with the latest changes that result of editing content.\\n\n## Decision\n","Decision":"Addressing the ETL process for Content Items this way:\\n1. The first time, we load all the content items via [publishing-api][3]. We retrieve all the content-ids and paths of all the content items that are live.\\n2. For each Content Item we will get the JSON from the [Content Store][2], and we will extract the attributes that we need to fulfil the immediate needs. We will also persist all the JSON to be able to extract other attributes in the future.\\n3. On a daily basis, we will be listening to [publishing-api][3] events, in the same way that [Rummager][4] or [Email Alert Service][5] do. Once we receive a change for the content item, we will automatically update the content items dimension with the new approach.\\n### Benefits:\\n1. This is more aligned with GOV.UK architecture.\\n1. This is very light and efficient. It also embrace simple code as the ETL process for Content Items is almost trivial.\\n","tokens":33,"id":1857,"Prediction":"## Decision\n\nWe will implement a process to automatically populate the Content Items' dimension with the latest edits.\n\n### Details:\n1. **Auto-Sync Mechanism**: Develop an auto-sync mechanism that listens to changes in content items and updates the Content Items' dimension accordingly.\n2. **Event-Driven Approach**: Utilize an event-driven architecture where content updates trigger events that are processed by a dedicated service responsible for keeping the dimension data consistent.\n3. **Data Consistency Checks**: Implement periodic consistency checks to ensure that the Content Items' dimension accurately reflects the latest content changes.\n4. **Monitoring and Logging**: Add comprehensive monitoring and logging to track updates and troubleshoot any discrepancies in real-time.\n5. **Versioning System**: Consider adding a versioning system to keep historical records of content changes, enabling rollback capabilities if needed.\n\n### Rationale:\n- **Efficiency**: Automatically updating the Content Items' dimension ensures that the data is always up-to-date with minimal manual intervention.\n- **Scalability**: An event-driven model scales well as it can handle a high volume of content edits without significant performance degradation.\n- **Reliability**: Consistency checks and comprehensive logging ensure the reliability of the data, and the versioning system provides an extra layer of data integrity.","GenTime":"2024-07-28 23:16:29"}
{"File Name":"content-data-api\/adr-006-track-metrics-via-time-dimension.md","Context":"## Context\\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\\n\n## Decision\n","Decision":"Build a data warehouse(*) that maintains a copy of the information of the transactional systems.\\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\\n","tokens":80,"id":1863,"Prediction":"## Decision\n\nWe will implement a centralized data warehouse that will consolidate data from the various disparate sources into a unified repository. This data warehouse will store both current and historical data, enabling comprehensive analytical reporting and performance monitoring of the publishing workflow within GOV.UK.\n\nKey steps involved in this decision include:\n\n1. **Data Integration:** We will employ ETL (Extract, Transform, Load) processes to regularly extract data from various applications, transform it into a consistent format, and load it into the data warehouse.\n\n2. **Data Storage:** The data warehouse will be architected to store large volumes of detailed transactional data, providing a historical archive as well as current operational data. This will support complex queries and analytical processing.\n\n3. **Analytical Tools:** We will deploy business intelligence (BI) tools on top of the data warehouse to enable the creation of analytical reports and performance indicators. These tools will help stakeholders gain insights into the GOV.UK publishing workflow and make data-driven decisions.\n\n4. **Scalability and Performance:** The chosen data warehouse solution will be scalable to accommodate growing data volumes and provide high performance for data queries and reporting.\n\n5. **Security and Compliance:** The data warehouse will adhere to strict security and data governance policies to ensure the privacy and integrity of data. Access controls and auditing mechanisms will be put in place to comply with regulatory requirements.\n\nBy implementing a centralized data warehouse, we will achieve integrated data analysis capabilities, support enhanced decision-making processes, and improve the overall efficiency of the publishing workflow within GOV.UK.","GenTime":"2024-07-28 23:16:50"}
{"File Name":"content-data-api\/adr-009-track-metrics-by-basepath.md","Context":"## Context\\nWe started tracking performance and quality metrics by `content_id`, but after\\nthe first 3 months it was clear that the user needs to track metrics at the\\nbase_path level in Guides and Travel Advice.\\n\n## Decision\n","Decision":"Track metrics at the base_path level\\n","tokens":46,"id":1866,"Prediction":"## Decision\n\nWe have decided to shift our performance and quality metrics tracking from `content_id` to `base_path` for Guides and Travel Advice. This change aims to better align with user needs and improve the relevance of our metrics. \n\n### Steps to implement:\n1. **Data Schema Update**: Modify the existing data schema to include `base_path` as a primary key for metric tracking.\n2. **Data Migration**: Migrate historical data from `content_id` to `base_path` to ensure continuity and comparability of metrics.\n3. **Update Tracking Systems**: Adjust the configuration of all tracking and analytics systems to collect and report metrics based on `base_path`.\n4. **User Education**: Communicate the changes to users and provide documentation or training on how to interpret the new metrics.\n5. **Monitor and Validate**: Post-implementation, closely monitor the system to ensure accurate tracking and address any issues promptly.\n\n### Justification:\n- **Better Alignment with User Needs**: Users primarily interact with content via base paths, making this the more logical unit for tracking.\n- **Improved Decision Making**: Tracking by `base_path` will provide more granular and actionable insights for Guides and Travel Advice.\n- **Historical Metrics Preservation**: Migrating historical data ensures that users can still access and analyze past performance trends.\n\nBy making this adjustment, we expect to provide our users with more relevant and useful metrics, thereby enhancing their ability to monitor and improve content performance and quality effectively.","GenTime":"2024-07-28 23:17:05"}
{"File Name":"rfcs\/0000-template.md","Context":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n\n## Decision\n","Decision":"[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":60,"id":1879,"Prediction":"## Decision\n\n[decision]: #decision\n\nGiven the forces described in the context section, a decision has been made to adopt a microservices architecture for the following reasons:\n\n1. **Scalability:** Microservices will allow us to scale individual components independently, which is essential given the expected growth and variability in user demand.\n   \n2. **Technology Diversification:** The microservices approach will enable teams to choose the best technology stack for each service, promoting flexibility and leveraging the team's expertise.\n\n3. **Team Autonomy:** Each team will be responsible for a specific service, which aligns well with our social structure of cross-functional teams, encouraging ownership and faster development cycles.\n\n4. **Risk Mitigation:** By decomposing the system into smaller, manageable services, we reduce the risk of a single point of failure. This aligns with our project's risk management strategy.\n\n5. **Maintainability:** Smaller, well-defined services will be easier to maintain and troubleshoot, which will be beneficial for our long-term maintenance strategy.\n\nWhile adopting a microservices architecture introduces certain complexities such as service orchestration, inter-service communication, and deployment, the benefits of scalability, flexibility, and maintainability outweigh these challenges given our current context.\n\nA detailed implementation plan will follow, taking into account the necessary infrastructure changes, team training requirements, and a phased rollout strategy to minimize disruption.","GenTime":"2024-07-28 23:17:35"}
{"File Name":"toc-poc\/1577574698834_architecture_choice_for_template_generation.md","Context":"## Context\\nThe POC assumes generation of the table of contents based on the content presented on the page. Data need to be fetched from somewhere, and then presented on the view in some way. Data model won't be simple data structure, as it can be multi-leveled list, which might also contain some behavior, to make it easier to prepare for rendering. It's not going to be simple CRUD application, even if model will not contain much behavior.\\n\n## Decision\n","Decision":"Use ports and adapters architecture style.\\n","tokens":94,"id":1883,"Prediction":"## Decision\n\n### Architecture Style\nAdopt a Model-View-Controller (MVC) architecture for the POC. Given the complexity of the data model and the advanced rendering functionalities, the MVC pattern will segregate concerns effectively, ensuring a clear separation between the data model (M), the presentation layer (V), and the control logic (C).\n\n### Data Model\nDesign a flexible, extensible multi-leveled data model that encapsulates both structure and behavior. Utilize classes or objects to represent the hierarchical nature of the table of contents (ToC). Each node in this model can represent a section, with children representing subsections, which will facilitate easy manipulation and rendering. Embed behavior in the model to support tasks such as traversing nodes and generating nested lists.\n\n### Data Fetching Strategy\nImplement data fetching through a repository pattern which will abstract the source of the data, allowing for scalability and potential integration with different data sources such as APIs, databases, or static files.\n\n### View Component\nDesign the view component to be flexible and capable of rendering multi-leveled list structures. Enhance usability by utilizing lazy loading, collapsing\/expanding sections, and supporting dynamic updates as the underlying data changes.\n\n### Technologies\n- **Backend**: Utilize a robust backend framework (such as Django, Spring, or Express.js) that can manage the complexity of the data model and behavior.\n- **Frontend**: Leverage a modern frontend library or framework (such as React, Angular, or Vue.js) to build dynamic and responsive UI components for the table of contents.\n\n### Integration and Testing\n- **Unit Tests**: Create comprehensive tests for individual components of the data model to ensure data integrity and correct behavior.\n- **Integration Tests**: Develop integration tests to ensure data fetching and view rendering work seamlessly together.\n- **User Experience Tests**: Implement usability tests to ensure the generated table of contents meets user expectations and requirements.\n\n### Documentation\nMaintain clear and detailed documentation for the data model, including its structure and behavior, as well as for each component's role in the MVC architecture. This will facilitate future maintenance and potential extensions of the POC.\n\nBy making these decisions, the architecture will provide a robust foundation to handle the complex requirements, ensure maintainability, and facilitate future enhancements.","GenTime":"2024-07-28 23:17:54"}
{"File Name":"aspan-server\/0003-replacing-ramda-with-lodash.md","Context":"## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n\n## Decision\n","Decision":"Decision here...\\n","tokens":22,"id":1890,"Prediction":"## Decision\nWe will adopt arrow functions as the preferred syntax for writing functions in our JavaScript codebase. This decision is based on their ability to reduce visual noise and enhance code readability. Arrow functions also provide consistent behavior for the `this` keyword, which can lead to fewer errors and more predictable code. Going forward, all new functions should be written using arrow function syntax unless a specific use case requires traditional function expressions. Existing code may be refactored to use arrow functions where applicable and beneficial.","GenTime":"2024-07-28 23:18:14"}
{"File Name":"aspan-server\/0004-info-graphql-import.md","Context":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n\n## Decision\n","Decision":"Decision here...\\n","tokens":20,"id":1893,"Prediction":"## Decision\nWe have decided to use GraphQL SDL (Schema Definition Language) to define, import, and export our schema modules. This approach allows us to break down our GraphQL schema into smaller, manageable components, promoting modularity and improving maintainability.\n\nWe will implement the following decisions:\n1. **Module Definition:** Each module will have its own SDL file where we define the schema types, queries, mutations, and subscriptions relevant to that module.\n2. **Import Syntax:** We'll adopt the commonly used import syntax for importing types and schema definitions from other modules. This will help us to share and reuse common types and definitions across different modules.\n3. **Single Schema Stitching:** We will use a schema stitching or federation approach to combine these individual modules into a single schema that can be served by our GraphQL server.\n4. **Tooling:** For schema stitching, we will use tools like `graphql-tools` or Apollo's schema federation which are widely supported and have a strong community backing.\n\nThis decision aims to enhance our GraphQL development workflow by ensuring clear separation of concerns, improving code organization, and facilitating module reusability.","GenTime":"2024-07-28 23:18:22"}
{"File Name":"talktofrank-beta\/0003-site-wide-cms-configuration.md","Context":"## Context\\nCertain elements on the Talk To Frank site need to be loaded regardless of the user entry point,\\ne.g. the current user story involves adding a warning message bar that appears on all pages.\\nThe CMS needs to be updated to support the addition of site wide 'settings'.\\n\n## Decision\n","Decision":"A new 'site settings' content model will be created.  A 'Sitewide' item will then be created to contain all sitewide settings.\\nThis would then allow for the following (future) setup,\\nContent items of type 'site settings'\\n- Sitewide\\n- Homepage - any settings that should override the sitewide values on the homepage.\\n- Drugs A-Z - any settings that should override the sitewide values on the Drugs A-Z page.\\nReferences:\\n- https:\/\/www.contentful.com\/r\/knowledgebase\/dynamic-microcopy\/\\n- https:\/\/www.contentfulcommunity.com\/t\/is-it-common-to-have-a-settings-content-model\/924\/2\\n","tokens":59,"id":1894,"Prediction":"## Decision\nTo ensure that certain elements, such as a warning message bar, are consistently loaded across all pages on the Talk To Frank site, we will implement a global settings feature within the CMS. This will involve the following actions:\n\n1. **Enhancement of CMS Schema:** We will extend the current CMS schema to include a new 'global settings' section. This section will be configured to store site-wide settings such as the content and styling of the warning message bar.\n\n2. **Development of Global Settings Interface:** A user-friendly interface will be created within the CMS to allow administrators to easily update and manage these global settings. This interface will be integrated seamlessly into the existing CMS dashboard.\n\n3. **Modification of Frontend Template:** We will update the frontend templates and scripts to check for these global settings during the rendering process and ensure they are implemented across all entry points of the website. For instance, the logic to render the warning message bar will be incorporated at a base template level to guarantee its presence on every page.\n\n4. **Testing and Validation:** Rigorous testing will be conducted to validate that the global settings feature works as expected and does not negatively impact the website\u2019s performance or user experience. This will include unit tests, integration tests, and user acceptance testing.\n\n5. **Documentation:** Comprehensive documentation will be created to outline the new global settings feature, including how to use the CMS interface to manage these settings and any changes made to the frontend template structure.\n\nBy implementing these changes, we ensure that necessary elements like the warning message bar are consistently displayed to users irrespective of their entry point on the site, thereby improving site functionality and user experience.","GenTime":"2024-07-28 23:18:25"}
{"File Name":"drt-v2\/0013-use-lihaoyi-s-autowire.md","Context":"## Context\\nWe've got a Single Page app, it needs to talk to the server. Our use of scala and scalajs means we can use [lihaoyi's autowire\\nmacros](https:\/\/github.com\/lihaoyi\/autowire)\\nAlthough this is essentially a 0 on the [Richardson maturity model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nit has huge benefits in terms of speed of change. We also (at the moment) only have the one client of the SPA so we can afford the tight coupling.\\nIt doesn't preclude moving toward something more restful, as we can just add routes when we recognise a need.\\n\n## Decision\n","Decision":"Use autowire for now.\\n","tokens":147,"id":1907,"Prediction":"## Decision\nWe will utilize lihaoyi's autowire macros for server-client communication in our Single Page App built with Scala and Scala.js. Despite being a level 0 on the Richardson Maturity Model, this approach is favored due to its significant benefits in terms of speed of iteration and development. Given that our application currently has only one client (the SPA), the tight coupling is acceptable and manageable. \n\nWe will also ensure our architecture remains flexible, allowing for the potential addition of more RESTful routes in the future as new requirements or clients emerge. This decision balances the current need for rapid development cycles with the potential for future scalability and evolution.","GenTime":"2024-07-28 23:19:11"}
{"File Name":"drt-v2\/0006-physical-deployment-one-jvm-per-port.md","Context":"## Context\\nDRTv1 tried to normalise all ports into a single model. We frequently had requests for port specific features\\nor structures. Also it\\n\n## Decision\n","Decision":"Use a jvm per airport\\n","tokens":33,"id":1912,"Prediction":"## Decision\nGiven the frequent requests for port-specific features and structures, we have decided to move away from a single, normalized port model. Instead, we will implement discrete models for each port type. This will allow us to accommodate the unique requirements and features specific to each port, enhancing the flexibility and functionality of the system. By tailoring the models to specific port characteristics, we aim to improve maintainability and scalability, ultimately providing a more robust and adaptable solution for our users.","GenTime":"2024-07-28 23:19:24"}
{"File Name":"SiebenApp\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1922,"Prediction":"## Decision\nTo effectively record architectural decisions made throughout the project, we will adopt the practice of creating Architectural Decision Records (ADRs). Each ADR will document the decision context, the decision itself, the rationale behind it, and potential consequences. This practice will provide a clear and structured way to capture the reasoning behind key decisions, ensuring future maintainability and clarity. The ADRs will be stored in a dedicated directory within the project's repository, ensuring easy access and consistent documentation.","GenTime":"2024-07-28 23:19:48"}
{"File Name":"green_log\/0003-decouple-generation-and-handling.md","Context":"## Context\\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\\n\n## Decision\n","Decision":"De-couple generation of log message\/entries from how they are handled.\\n* A `logger` object provides an API that can be used to generate log entries.\\n* Log \"entries\" are strongly typed structures.\\n* Log entry \"handlers\" provide a simple, consistent interface.\\n```mermaid\\nsequenceDiagram\\nApp ->> Logger:       info(\"Message\")\\nLogger ->> Handler:   <<(entry)\\n```\\n","tokens":35,"id":1924,"Prediction":"## Decision\n\nWe will adopt a centralized, flexible logging framework that provides a unified API for log generation across all applications. This framework will be capable of integrating with various logging backends and formats. \n\n### Specific Decisions:\n\n1. **Unified Logging API**: All applications will utilize a common logging library that implements a standardized API for creating log entries.\n   \n2. **Support for Multiple Backends**: The chosen logging framework will support multiple backends for log storage, including file systems, databases, and cloud-based solutions (e.g., ELK stack, Splunk).\n\n3. **Modularity and Extensibility**: The framework should be modular, allowing for easy extension and customization to integrate new logging mechanisms, filters, and formatters without altering the core API.\n\n4. **Log Levels and Filtering**: The framework will natively support multiple log levels (e.g., DEBUG, INFO, WARN, ERROR) and allow for configurable filtering policies to control log verbosity and filtering based on dynamic criteria.\n\n5. **Formatted Output**: The logging framework will support various formats including plain text, JSON, XML, and others as needed, and should allow format customization through configuration files or runtime parameters.\n\n6. **Asynchronous Logging**: To minimize performance overhead, the framework should provide options for asynchronous logging, ensuring that log generation does not block application execution.\n\n7. **Central Configuration Management**: Logging configurations (such as log levels, backends, and formatting rules) will be centrally managed, allowing changes to be propagated across all applications seamlessly.\n\n8. **Cross-Cutting Concerns**: The framework will include mechanisms for logging cross-cutting concerns such as request tracing, error tracking, and performance metrics.\n\n9. **Monitoring and Alerting Integration**: The logging system will have integration points with monitoring and alerting systems to ensure key events and errors can trigger notifications and incident responses.\n\nBy implementing a flexible and consistent logging framework with these capabilities, we aim to ensure that logs from all applications are generated, processed, and utilized effectively and consistently. This will enhance our ability to monitor, debug, and analyze the behavior of our applications in a unified manner.","GenTime":"2024-07-28 23:19:57"}
{"File Name":"green_log\/0006-use-lock-free-io.md","Context":"## Context\\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\\nBut also, we want logging to perform well.\\n\n## Decision\n","Decision":"_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\\n- avoid using of mutexes to serialise output\\n- perform atomic writes to `IO` streams (using `<<`)\\n","tokens":37,"id":1927,"Prediction":"## Decision\nWe will implement a thread-safe logging mechanism using a dedicated logging thread and a concurrent queue.\n\n1. **Choice of Logger Implementation**:\n   We will use an asynchronous logging library, such as `log4j` for Java or `logrus` for Go, that supports appending log entries to a queue. If such a library is not available for the programming language in use, we will create a custom logging solution employing the following architecture:\n\n2. **Thread-Safe Queue**:\n   We will utilize a thread-safe queue to store log messages. Each log entry will be encapsulated as an object that contains the message, timestamp, and log level.\n\n3. **Dedicated Logging Thread**:\n   We will dedicate a single background thread to dequeue log entries and write them to the desired sink (file, STDOUT). This will ensure that log entries are processed in the order they were added, avoiding interleaving.\n\n4. **Backpressure Handling**:\n   In scenarios where the queue is getting filled quickly, we will implement a backpressure strategy to handle overflow situations, such as dropping the oldest log messages or applying rate-limiting techniques.\n\n5. **Flush Mechanism**:\n   The logging thread will periodically flush the log messages from the queue to the designated sink to minimize the loss of log entries on application crashes. The frequency of this flush operation will be configurable to balance performance and reliability.\n\n6. **Configuration Options**:\n   The logging system will be designed to be highly configurable, allowing users to choose between various sinks (file, STDOUT), set log levels, configure file rotation policies, and more.\n\nBy implementing these measures, we will ensure that our logging solution performs efficiently and avoids interleaving log entries, maintaining clarity and consistency in the log output.","GenTime":"2024-07-28 23:20:06"}
{"File Name":"site-stitcher\/001-generate-static-pages.md","Context":"## Context\\nI see so many thing broken on the web today, it is not only website speed, it is also\\nhow many tools we have accepted to use to solve a simple problem. Especially in the JS\\nworld I feel the healthy \"How much do I need?\" question is asked rarely. We also teach\\nnewcomers that it is ok to load 1GB of npm modules before we can actually build an empty page.\\nIn this repo I try to do the simplest thing possible to generate my website.\\nI am looking for dependencies that are as tiny as possible, some I just need to not use and\\nwrite the needed code itself, this also makes me aware how much of a dependency I would have needed\\nonly.\\nAlso the site that I want to ship shall have all a11y, lighthouse, etc. scores as high as possible\\ntherefore the site shall degrade gracefully be accessible and very light weight.\\nI also want the simplest hosting setup and infrastructure possible, due to using github it might\\nbe github-pages, which is just static pages.\\n\n## Decision\n","Decision":"With static pages the above can be achieved. Easily.\\n","tokens":222,"id":1931,"Prediction":"## Decision\n\nBased on the context provided and the objectives of ensuring a lightweight, accessible, and minimalistic website, the following decisions are made:\n\n1. **Minimal Dependencies**: Only essential dependencies will be used. Libraries and frameworks that add significant overhead or complexity (such as large JavaScript frameworks) will be avoided. Where feasible, custom code will be written to handle specific functionality.\n\n2. **Static Site Generation**: We will use a static site generator that produces optimized HTML, CSS, and JavaScript files. Static site generators like Jekyll, Hugo, or Eleventy will be considered due to their ability to produce lightweight sites and their compatibility with GitHub Pages.\n\n3. **Accessibility First**: Accessibility (a11y) will be a priority from the start. We will use semantic HTML and ARIA roles to ensure the site is navigable by screen readers. Regular audits using tools like Lighthouse will be conducted to ensure high accessibility scores.\n\n4. **Performance Optimization**: The site will be optimized for speed by minimizing the use of large assets and leveraging techniques such as lazy loading, image optimization, and minimizing JavaScript. Tools like Lighthouse will be used to monitor and improve performance.\n\n5. **Static Hosting via GitHub Pages**: The site will be hosted using GitHub Pages for simplicity. Static assets will be served directly from GitHub Pages, leveraging its built-in CDN for fast global delivery.\n\n6. **Progressive Enhancement**: The website will be designed to degrade gracefully, ensuring that essential content and functionality are accessible even if JavaScript is disabled or the user is on a slow connection.\n\nBy adhering to these decisions, we can ensure that the website remains lightweight, accessible, and easy to maintain, meeting the high standards set for performance and usability.","GenTime":"2024-07-28 23:20:23"}
{"File Name":"tendermint\/adr-051-double-signing-risk-reduction.md","Context":"## Context\\nTo provide a risk reduction method for double signing incidents mistakenly executed by validators\\n- Validators often mistakenly run duplicated validators to cause double-signing incident\\n- This proposed feature is to reduce the risk of mistaken double-signing incident by checking recent N blocks before voting begins\\n- When we think of such serious impact on double-signing incident, it is very reasonable to have multiple risk reduction algorithm built in node daemon\\n\n## Decision\n","Decision":"We would like to suggest a double signing risk reduction method.\\n- Methodology : query recent consensus results to find out whether node's consensus key is used on consensus recently or not\\n- When to check\\n- When the state machine starts `ConsensusReactor` after fully synced\\n- When the node is validator ( with privValidator )\\n- When `cs.config.DoubleSignCheckHeight > 0`\\n- How to check\\n1. When a validator is transformed from syncing status to fully synced status, the state machine check recent N blocks (`latest_height - double_sign_check_height`) to find out whether there exists consensus votes using the validator's consensus key\\n2. If there exists votes from the validator's consensus key, exit state machine program\\n- Configuration\\n- We would like to suggest by introducing `double_sign_check_height` parameter in `config.toml` and cli, how many blocks state machine looks back to check votes\\n- <span v-pre>`double_sign_check_height = {{ .Consensus.DoubleSignCheckHeight }}`<\/span> in `config.toml`\\n- `tendermint node --consensus.double_sign_check_height` in cli\\n- State machine ignore checking procedure when `double_sign_check_height == 0`\\n","tokens":85,"id":1944,"Prediction":"## Decision\n\nTo mitigate the risk of double-signing incidents by validators, we will implement a feature in the node daemon that checks the last N blocks before a vote is cast. This check will be integrated into the voting algorithm and will ensure that a validator does not sign twice for the same block height inadvertently.\n\n### Key Components of the Decision:\n1. **Recent Blocks Voting Check**:\n    - The node daemon will maintain a record of the most recent N blocks.\n    - Before a vote is cast, the node daemon will check this record to ensure the validator has not already voted for the same block height in the recent history.\n\n2. **Consensus Algorithm Update**:\n    - The consensus algorithm will be updated to include this pre-vote validation step.\n    - This move ensures that votes are only cast after confirming there's no risk of double-signing.\n\n3. **Redundant Validator Detection**:\n    - Implement a detection mechanism within the node daemon to identify and warn if duplicated validators are running.\n    - The system will flag potential double-signature attempts and halt the voting process for those instances.\n\n4. **Auditing and Logging**:\n    - Implement robust logging to keep an audit trail of all votes cast.\n    - Ensure that any double-signing attempts are logged with high visibility.\n   \n5. **Risk Reduction Algorithms**:\n    - Develop and incorporate multiple risk reduction algorithms to cater to various potential failure modes.\n    - These algorithms will cover different aspects of validation, ensuring comprehensive protection against accidental double-signing.\n\nBy incorporating these changes, we aim to significantly reduce the probability of validator nodes mistakenly double-signing blocks, thereby enhancing the integrity and reliability of the blockchain network.","GenTime":"2024-07-28 23:21:05"}
{"File Name":"tendermint\/adr-063-privval-grpc.md","Context":"## Context\\nValidators use remote signers to help secure their keys. This system is Tendermint's recommended way to secure validators, but the path to integration with Tendermint's private validator client is plagued with custom protocols.\\nTendermint uses its own custom secure connection protocol (`SecretConnection`) and a raw tcp\/unix socket connection protocol. The secure connection protocol until recently was exposed to man in the middle attacks and can take longer to integrate if not using Golang. The raw tcp connection protocol is less custom, but has been causing minute issues with users.\\nMigrating Tendermint's private validator client to a widely adopted protocol, gRPC, will ease the current maintenance and integration burden experienced with the current protocol.\\n\n## Decision\n","Decision":"After discussing with multiple stake holders, [gRPC](https:\/\/grpc.io\/) was decided on to replace the current private validator protocol. gRPC is a widely adopted protocol in the micro-service and cloud infrastructure world. gRPC uses [protocol-buffers](https:\/\/developers.google.com\/protocol-buffers) to describe its services, providing a language agnostic implementation. Tendermint uses protobuf for on disk and over the wire encoding already making the integration with gRPC simpler.\\n","tokens":147,"id":1949,"Prediction":"## Decision\nMigrate Tendermint's private validator client to use gRPC instead of the current custom protocols.\n\nThis decision aims to address the issues with the existing `SecretConnection` and raw TCP\/Unix socket connection protocols by adopting gRPC, a widely used and well-supported framework for remote procedure calls.\n\n### Reasons for Decision:\n1. **Security**: gRPC provides built-in support for secure communication through TLS, reducing the risk of man-in-the-middle attacks and enhancing the overall security of the validator connections.\n   \n2. **Maintainability**: By using gRPC, a widely adopted and well-documented protocol, the ongoing maintenance burden will be significantly reduced. Debugging and support will be easier due to the availability of extensive resources and community support.\n\n3. **Integration Ease**: gRPC's language-agnostic nature simplifies the integration process, allowing developers to use multiple programming languages without being tied to Golang. This facilitates easier and quicker integration with Tendermint's private validator client.\n\n4. **Interoperability**: gRPC ensures better compatibility and interoperability with other systems and tools, enabling a more seamless ecosystem.\n\n5. **Performance**: gRPC is efficient and designed for high performance, offering features like HTTP\/2 for transport, which provides multiplexing and improved latency over traditional HTTP\/1.x used by raw TCP connections.\n\n### Implementation Plan:\n1. **Design Phase**:\n   - Define the gRPC service and method definitions required for the validator-client communication.\n   - Ensure that security measures (e.g., TLS) are adequately planned and implemented.\n\n2. **Development Phase**:\n   - Implement the gRPC server and client components for the validator.\n   - Replace existing `SecretConnection` and raw TCP\/Unix socket usages with gRPC calls.\n\n3. **Testing Phase**:\n   - Thoroughly test the gRPC implementation for security, performance, and correctness.\n   - Ensure comprehensive unit and integration testing to catch any regressions or compatibility issues.\n\n4. **Deployment Phase**:\n   - Gradually roll out the gRPC-based validator client to assess stability before a full deployment.\n   - Provide documentation and support for users migrating to the new protocol.\n\n5. **Transition Support**:\n   - Maintain backward compatibility with the existing protocols for a defined period to give users time to migrate.\n   - Provide detailed migration guides and support channels to assist users during the transition phase.\n\nBy migrating to gRPC, Tendermint will benefit from enhanced security, easier integration, and reduced maintenance efforts, ultimately leading to a more robust and reliable validator infrastructure.","GenTime":"2024-07-28 23:21:35"}
{"File Name":"tendermint\/adr-012-peer-transport.md","Context":"## Context\\nOne of the more apparent problems with the current architecture in the p2p\\npackage is that there is no clear separation of concerns between different\\ncomponents. Most notably the `Switch` is currently doing physical connection\\nhandling. An artifact is the dependency of the Switch on\\n`[config.P2PConfig`](https:\/\/github.com\/tendermint\/tendermint\/blob\/05a76fb517f50da27b4bfcdc7b4cf185fc61eff6\/config\/config.go#L272-L339).\\nAddresses:\\n- [#2046](https:\/\/github.com\/tendermint\/tendermint\/issues\/2046)\\n- [#2047](https:\/\/github.com\/tendermint\/tendermint\/issues\/2047)\\nFirst iteraton in [#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)\\n\n## Decision\n","Decision":"Transport concerns will be handled by a new component (`PeerTransport`) which\\nwill provide Peers at its boundary to the caller. In turn `Switch` will use\\nthis new component accept new `Peer`s and dial them based on `NetAddress`.\\n### PeerTransport\\nResponsible for emitting and connecting to Peers. The implementation of `Peer`\\nis left to the transport, which implies that the chosen transport dictates the\\ncharacteristics of the implementation handed back to the `Switch`. Each\\ntransport implementation is responsible to filter establishing peers specific\\nto its domain, for the default multiplexed implementation the following will\\napply:\\n- connections from our own node\\n- handshake fails\\n- upgrade to secret connection fails\\n- prevent duplicate ip\\n- prevent duplicate id\\n- nodeinfo incompatibility\\n```go\\n\/\/ PeerTransport proxies incoming and outgoing peer connections.\\ntype PeerTransport interface {\\n\/\/ Accept returns a newly connected Peer.\\nAccept() (Peer, error)\\n\/\/ Dial connects to a Peer.\\nDial(NetAddress) (Peer, error)\\n}\\n\/\/ EXAMPLE OF DEFAULT IMPLEMENTATION\\n\/\/ multiplexTransport accepts tcp connections and upgrades to multiplexted\\n\/\/ peers.\\ntype multiplexTransport struct {\\nlistener net.Listener\\nacceptc chan accept\\nclosec  <-chan struct{}\\nlistenc <-chan struct{}\\ndialTimeout      time.Duration\\nhandshakeTimeout time.Duration\\nnodeAddr         NetAddress\\nnodeInfo         NodeInfo\\nnodeKey          NodeKey\\n\/\/ TODO(xla): Remove when MConnection is refactored into mPeer.\\nmConfig conn.MConnConfig\\n}\\nvar _ PeerTransport = (*multiplexTransport)(nil)\\n\/\/ NewMTransport returns network connected multiplexed peers.\\nfunc NewMTransport(\\nnodeAddr NetAddress,\\nnodeInfo NodeInfo,\\nnodeKey NodeKey,\\n) *multiplexTransport\\n```\\n### Switch\\nFrom now the Switch will depend on a fully setup `PeerTransport` to\\nretrieve\/reach out to its peers. As the more low-level concerns are pushed to\\nthe transport, we can omit passing the `config.P2PConfig` to the Switch.\\n```go\\nfunc NewSwitch(transport PeerTransport, opts ...SwitchOption) *Switch\\n```\\n","tokens":179,"id":1956,"Prediction":"## Decision\nWe will refactor the p2p package to create a clear separation of concerns between different components by extracting the physical connection handling responsibilities away from the `Switch` component. Specifically:\n\n1. **Extract Physical Connection Handling:** Create a new dedicated component for physical connection handling, tentatively named `ConnectionManager`.\n2. **Refactor `Switch`:** Modify the `Switch` component to remove the direct dependency on `config.P2PConfig` and delegate connection handling tasks to the `ConnectionManager`.\n3. **Modularize Configuration:** Introduce a new configuration module that can manage configurations for different components separately, thereby ensuring that the `Switch` does not depend directly on `P2PConfig`.\n\n### Steps:\n\n1. **Create `ConnectionManager`:**\n   - Develop a new module, `ConnectionManager`, responsible for establishing, maintaining, and closing physical connections.\n   - Decouple connection-related methods and properties from the `Switch` and refactor them into the `ConnectionManager`.\n   - Ensure the `ConnectionManager` interfaces with necessary configuration parameters, but through a modular approach to configuration.\n\n2. **Modify `Switch`:**\n   - Refactor the `Switch` to use the `ConnectionManager` for any tasks related to physical connections.\n   - Remove direct dependencies on `P2PConfig` within the `Switch`.\n   - The `Switch` should now focus on its core responsibilities, such as routing messages and managing peer states, and delegate connection tasks.\n\n3. **Modular Configuration Handling:**\n   - Introduce a layered configuration structure that separates configurations for different components like `Switch` and `ConnectionManager`.\n   - Implement configuration loaders that can fetch and provide relevant parts of configuration files to appropriate components.\n   - Ensure backwards compatibility and provide migration scripts or guidelines if necessary.\n\nThis decision will enhance the maintainability, testability, and extensibility of the p2p package by ensuring that each component has a well-defined scope of responsibility. The isolation of concerns will also facilitate easier debugging and future feature additions.","GenTime":"2024-07-28 23:21:51"}
{"File Name":"tendermint\/adr-014-secp-malleability.md","Context":"## Context\\nSecp256k1 has two layers of malleability.\\nThe signer has a random nonce, and thus can produce many different valid signatures.\\nThis ADR is not concerned with that.\\nThe second layer of malleability basically allows one who is given a signature\\nto produce exactly one more valid signature for the same message from the same public key.\\n(They don't even have to know the message!)\\nThe math behind this will be explained in the subsequent section.\\nNote that in many downstream applications, signatures will appear in a transaction, and therefore in the tx hash.\\nThis means that if someone broadcasts a transaction with secp256k1 signature, the signature can be altered into the other form by anyone in the p2p network.\\nThus the tx hash will change, and this altered tx hash may be committed instead.\\nThis breaks the assumption that you can broadcast a valid transaction and just wait for its hash to be included on chain.\\nOne example is if you are broadcasting a tx in cosmos,\\nand you wait for it to appear on chain before incrementing your sequence number.\\nYou may never increment your sequence number if a different tx hash got committed.\\nRemoving this second layer of signature malleability concerns could ease downstream development.\\n### ECDSA context\\nSecp256k1 is ECDSA over a particular curve.\\nThe signature is of the form `(r, s)`, where `s` is a field element.\\n(The particular field is the `Z_n`, where the elliptic curve has order `n`)\\nHowever `(r, -s)` is also another valid solution.\\nNote that anyone can negate a group element, and therefore can get this second signature.\\n\n## Decision\n","Decision":"We can just distinguish a canonical form for the ECDSA signatures.\\nThen we require that all ECDSA signatures be in the form which we defined as canonical.\\nWe reject signatures in non-canonical form.\\nA canonical form is rather easy to define and check.\\nIt would just be the smaller of the two values for `s`, defined lexicographically.\\nThis is a simple check, instead of checking if `s < n`, instead check `s <= (n - 1)\/2`.\\nAn example of another cryptosystem using this\\nis the parity definition here https:\/\/github.com\/zkcrypto\/pairing\/pull\/30#issuecomment-372910663.\\nThis is the same solution Ethereum has chosen for solving secp malleability.\\n","tokens":356,"id":1957,"Prediction":"## Decision\nTo address the concerns of second-layer malleability in Secp256k1 signatures, we will adopt the following strategies:\n\n1. **Signature Standardization**:\n   - Implement signature normalization where the `s` component of the signature is standardized. Choose `s` such that it lies in the lower half of the field: `s <= n\/2`. This means for any given signature `(r, s)`, if `s` > `n\/2`, it will be converted to `(r, n - s)`, thus ensuring a unique representation.\n   \n2. **Transaction Validation Adjustment**:\n   - Modify the transaction validation logic to recognize only the standardized form of signatures. This way, even if the original signature is malleable, it will always get converted back to its standardized form before validation. Therefore, downstream applications and nodes don\u2019t have to handle multiple forms of the same logical message.\n\n3. **Tooling and Documentation Updates**:\n   - Update the existing cryptographic libraries and tools to support this new standardization approach. Ensure that the documentation clearly specifies the new signature format and its importance in maintaining the integrity of the transaction hashes.\n\n4. **Backwards Compatibility and Network Upgrade Path**:\n   - Plan for a phased network upgrade to allow all participants time to adjust their systems. During this period, nodes should accept both forms of signatures but convert them to the standardized format internally.\n   - Set a future network upgrade block height where non-standard signatures will be rejected to enforce consistency across the network.\n\nBy taking these steps, we can significantly reduce the malleability concerns that currently affect Secp256k1 and ensure that transaction hashes remain consistent and reliable across the network. This will simplify downstream development and improve overall network robustness.","GenTime":"2024-07-28 23:21:55"}
{"File Name":"tendermint\/adr-015-crypto-encoding.md","Context":"## Context\\nWe must standardize our method for encoding public keys and signatures on chain.\\nCurrently we amino encode the public keys and signatures.\\nThe reason we are using amino here is primarily due to ease of support in\\nparsing for other languages.\\nWe don't need its upgradability properties in cryptosystems, as a change in\\nthe crypto that requires adapting the encoding, likely warrants being deemed\\na new cryptosystem.\\n(I.e. using new public parameters)\\n\n## Decision\n","Decision":"### Public keys\\nFor public keys, we will continue to use amino encoding on the canonical\\nrepresentation of the pubkey.\\n(Canonical as defined by the cryptosystem itself)\\nThis has two significant drawbacks.\\nAmino encoding is less space-efficient, due to requiring support for upgradability.\\nAmino encoding support requires forking protobuf and adding this new interface support\\noption in the language of choice.\\nThe reason for continuing to use amino however is that people can create code\\nmore easily in languages that already have an up to date amino library.\\nIt is possible that this will change in the future, if it is deemed that\\nrequiring amino for interacting with Tendermint cryptography is unnecessary.\\nThe arguments for space efficiency here are refuted on the basis that there are\\nfar more egregious wastages of space in the SDK.\\nThe space requirement of the public keys doesn't cause many problems beyond\\nincreasing the space attached to each validator \/ account.\\nThe alternative to using amino here would be for us to create an enum type.\\nSwitching to just an enum type is worthy of investigation post-launch.\\nFor reference, part of amino encoding interfaces is basically a 4 byte enum\\ntype definition.\\nEnum types would just change that 4 bytes to be a variant, and it would remove\\nthe protobuf overhead, but it would be hard to integrate into the existing API.\\n### Signatures\\nSignatures should be switched to be `[]byte`.\\nSpatial efficiency in the signatures is quite important,\\nas it directly affects the gas cost of every transaction,\\nand the throughput of the chain.\\nSignatures don't need to encode what type they are for (unlike public keys)\\nsince public keys must already be known.\\nTherefore we can validate the signature without needing to encode its type.\\nWhen placed in state, signatures will still be amino encoded, but it will be the\\nprimitive type `[]byte` getting encoded.\\n#### Ed25519\\nUse the canonical representation for signatures.\\n#### Secp256k1\\nThere isn't a clear canonical representation here.\\nSignatures have two elements `r,s`.\\nThese bytes are encoded as `r || s`, where `r` and `s` are both exactly\\n32 bytes long, encoded big-endian.\\nThis is basically Ethereum's encoding, but without the leading recovery bit.\\n","tokens":100,"id":1962,"Prediction":"## Decision\n\nWe have decided to migrate from amino encoding to Protocol Buffers (protobuf) for encoding public keys and signatures on the blockchain. \n\n### Rationale:\n\n1. **Enhanced Performance**: Protobuf provides better serialization speed and reduced message size compared to amino, which will improve overall performance.\n   \n2. **Language Support**: Protobuf is widely supported across numerous programming languages, ensuring ease of parsing and compatibility which was mentioned as the fundamental reason behind opting for amino initially.\n\n3. **Sustainability & Maintenance**: Protobuf is actively maintained and continuously improved by Google and the open-source community, ensuring long-term support.\n\n4. **Schema Evolution**: Even though upgradability of the cryptosystem was not a primary requirement, Protocol Buffers offer schema evolution capabilities, providing future flexibility.\n\n5. **Adoption Trend**: Protobuf is becoming an industry standard for data interchange, increasing the likelihood of community expertise, documentation, and toolset availability.\n\n### Implementation Plan:\n\n1. **Protobuf Schema Design**: Design a Protobuf schema for encoding the public keys and signatures.\n2. **Transition**: Implement the transition gradually, ensuring backward compatibility during the migration phase.\n3. **Validation**: Implement thorough testing to validate that the new encoding scheme doesn't affect the integrity and security of the cryptosystem.\n4. **Documentation and Training**: Update the documentation and provide training to ensure all stakeholders are familiar with the new system.\n5. **Monitoring and Feedback**: Monitor the new system after deployment and gather feedback for further optimizations if needed.\n\nBy moving to Protocol Buffers, we can maintain compatibility with multiple languages, ensure high performance, and align with industry standards while addressing our current and foreseeable future needs in cryptographic encoding.","GenTime":"2024-07-28 23:22:06"}
{"File Name":"tendermint\/adr-061-p2p-refactor-scope.md","Context":"## Context\\nThe `p2p` package responsible for peer-to-peer networking is rather old and has a number of weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilites, poor performance, custom protocols, and incorrect behavior. A refactor has been discussed for several years ([#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)).\\nInformal Systems are also building a Rust implementation of Tendermint, [Tendermint-rs](https:\/\/github.com\/informalsystems\/tendermint-rs), and plan to implement P2P networking support over the next year. As part of this work, they have requested adopting e.g. [QUIC](https:\/\/datatracker.ietf.org\/doc\/draft-ietf-quic-transport\/) as a transport protocol instead of implementing the custom application-level `MConnection` stream multiplexing protocol that Tendermint currently uses.\\nThis ADR summarizes recent discussion with stakeholders on the scope of a P2P refactor. Specific designs and implementations will be submitted as separate ADRs.\\n\n## Decision\n","Decision":"The P2P stack will be refactored and improved iteratively, in several phases:\\n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.\\n* **Phase 2:** additional transports and incremental protocol improvements.\\n* **Phase 3:** disruptive protocol changes.\\nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.\\n","tokens":227,"id":1979,"Prediction":"## Decision\n\nWe have decided to proceed with a comprehensive refactor of the `p2p` package. This refactor aims to address the existing weaknesses such as tight coupling, leaky abstractions, lack of tests, DoS vulnerabilities, poor performance, custom protocols, and incorrect behavior. \n\nAs part of this refactor, we will:\n\n1. **Adopt QUIC as the Transport Protocol**: We will replace the custom application-level `MConnection` stream multiplexing protocol with QUIC. This transition leverages QUIC's modern, secure, and performant transport capabilities, addressing several of the current weaknesses in our P2P stack.\n\n2. **Implement Modular and Testable Components**: Refactor the `p2p` package to improve modularity and encapsulation, ensuring components are loosely coupled and easily testable. This will involve defining clear interfaces and abstractions to replace the current tightly coupled design.\n\n3. **Improve Security**: Address DoS vulnerabilities by implementing robust mitigation strategies such as rate limiting, connection policing, and enhanced security protocols available in QUIC.\n\n4. **Boost Performance**: Optimize performance by leveraging QUIC\u2019s efficiency and modern congestion control algorithms. This aims to reduce latency and improve throughput in the P2P network.\n\n5. **Remove Deprecated Features**: Eliminate outdated and incorrect functionalities that are causing incorrect behavior and clutter within the package.\n\n6. **Collaborate with Stakeholders**: Work closely with Informal Systems and other stakeholders to ensure compatibility and timely integration of their work on the Rust implementation of Tendermint.\n\nDetailed designs and specific implementation strategies will be documented and reviewed in subsequent ADRs to ensure all technical decisions are thoroughly considered and vetted by the community.\n\nThis decision sets the course for a more robust, secure, and performant P2P networking foundation for Tendermint, aligning with the long-term goals and needs of the ecosystem.","GenTime":"2024-07-28 23:23:19"}
{"File Name":"tendermint\/adr-073-libp2p.md","Context":"## Context\\nAs part of the 0.35 development cycle, the Tendermint team completed\\nthe first phase of the work described in ADRs 61 and 62, which included a\\nlarge scale refactoring of the reactors and the p2p message\\nrouting. This replaced the switch and many of the other legacy\\ncomponents without breaking protocol or network-level\\ninteroperability and left the legacy connection\/socket handling code.\\nFollowing the release, the team has reexamined the state of the code\\nand the design, as well as Tendermint's requirements. The notes\\nfrom that process are available in the [P2P Roadmap\\nRFC][rfc].\\nThis ADR supersedes the decisions made in ADRs 60 and 61, but\\nbuilds on the completed portions of this work. Previously, the\\nboundaries of peer management, message handling, and the higher level\\nbusiness logic (e.g., \"the reactors\") were intermingled, and core\\nelements of the p2p system were responsible for the orchestration of\\nhigher-level business logic. Refactoring the legacy components\\nmade it more obvious that this entanglement of responsibilities\\nhad outsized influence on the entire implementation, making\\nit difficult to iterate within the current abstractions.\\nIt would not be viable to maintain interoperability with legacy\\nsystems while also achieving many of our broader objectives.\\nLibP2P is a thoroughly-specified implementation of a peer-to-peer\\nnetworking stack, designed specifically for systems such as\\nours. Adopting LibP2P as the basis of Tendermint will allow the\\nTendermint team to focus more of their time on other differentiating\\naspects of the system, and make it possible for the ecosystem as a\\nwhole to take advantage of tooling and efforts of the LibP2P\\nplatform.\\n\n## Decision\n","Decision":"Tendermint will adopt libp2p during the 0.37 development cycle,\\nreplacing the bespoke Tendermint P2P stack. This will remove the\\n`Endpoint`, `Transport`, `Connection`, and `PeerManager` abstractions\\nand leave the reactors, `p2p.Router` and `p2p.Channel`\\nabstractions.\\nLibP2P may obviate the need for a dedicated peer exchange (PEX)\\nreactor, which would also in turn obviate the need for a dedicated\\nseed mode. If this is the case, then all of this functionality would\\nbe removed.\\nIf it turns out (based on the advice of Protocol Labs) that it makes\\nsense to maintain separate pubsub or gossipsub topics\\nper-message-type, then the `Router` abstraction could also\\nbe entirely subsumed.\\n","tokens":387,"id":1982,"Prediction":"## Decision\n\nThe Tendermint team has decided to adopt LibP2P as the foundational peer-to-peer networking stack for the Tendermint system. This change aims to address the challenges and limitations identified during the refactoring and reexamination process following ADRs 61 and 62. \n\nBy integrating LibP2P, Tendermint will be able to:\n\n1. **Decouple Peer Management and Business Logic**: Separating these concerns will simplify the architecture, allowing for clearer abstractions and more efficient iteration on system features and performance enhancements.\n   \n2. **Leverage Existing Ecosystem**: Utilizing LibP2P will enable Tendermint to benefit from the existing tools, libraries, and community support that come with the LibP2P ecosystem, thus reducing the maintenance burden and accelerating development.\n\n3. **Ensure Future Compatibility**: Although this transition involves moving away from certain legacy components, the adoption of a widely-used and thoroughly-specified stack like LibP2P will enhance long-term interoperability and allow for more robust and flexible networking capabilities.\n\n4. **Focus on Core Differentiators**: With the networking stack handled by LibP2P, the Tendermint team can redirect effort and resources towards enhancing other unique aspects of the Tendermint system, thereby delivering more value to its users and stakeholders.\n\nThis strategic decision will guide subsequent development tasks, refactorings, and enhancements, and is expected to form the basis for upcoming Tendermint releases, replacing and augmenting the groundwork laid by ADRs 60, 61, and 62.","GenTime":"2024-07-28 23:23:26"}
{"File Name":"tendermint\/adr-003-abci-app-rpc.md","Context":"## Context\\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\\nClients need to talk to Tendermint for proofs, unless we burden all app devs\\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\\nserver than two, but granted it makes querying a bit more kludgy since it needs\\nto be passed as a `Query`. Also, **having a very standard rpc interface means\\nthe light-client can work with all apps and handle proofs**. The only\\napp-specific logic is decoding the binary data to a more readable form (eg.\\njson). This is a huge advantage for code-reuse and standardization.\\n\n## Decision\n","Decision":"We dont expose an RPC server on any of our ABCI-apps.\\n","tokens":161,"id":1985,"Prediction":"## Decision\n\nImplement an RPC-server in the ABCI-server that acts as a proxy to Tendermint.\n\n### Rationale\n\n1. **Centralization of Exposures**: By having a single RPC-server handling communications, the complexity of securing two different servers (ABCI-server and Tendermint) is reduced. Focusing on securing a single point of communication could minimize security risks and simplify maintenance tasks.\n\n2. **Standardization of Interface**: Using a very standard RPC interface allows the light-client to operate consistently across all applications, which simplifies the implementation of secure proofs for clients. The light-client can focus on handling standard RPC procedures, which enhances code-reuse and reduces the learning curve for developers working on various applications.\n\n3. **Application-specific Logic Simplification**: Developers will not have to manage Tendermint-specific functionalities manually in their application, such as exposure of proof details. This offloads the burden to a centralized RPC-server in the ABCI-server, allowing application developers to focus on application-specific logic and data decoding.\n\n4. **Enhanced Client-side Consistency**: Ensuring that all applications expose a consistent interface means that clients and tools built to interact with the blockchain can be made more robust and versatile. The consistency reduces the chances of bugs or misinterpretations due to varied implementations in different applications.\n\nThe chosen approach ensures a balance between standardization, security, and simplicity without significantly compromising the ability to query or interact with the application effectively.","GenTime":"2024-07-28 23:23:33"}
{"File Name":"visit-plannr\/0002-dynamodb-composite-primary-key-design.md","Context":"## Context\\n\"[In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.](https:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/HowItWorks.CoreComponents.html)\"\\nThere are two types of primary key in dynamodb.\\nThe first kind is having just a partition key. The partition key is a hash and determines where on physical storage the item is placed. The partition key must be unique.\\nThe second kind is a composite primary key. It consists of a partition key and a sort key. The partition key stays the same but doesn't need to be unique in isolation. Rather the sort key\/ partition key pair must be unique.\\nIn a real system this would probably push towards StreamName as the partition key: so that events that logically live together physically live together. And Event Number in the stream as the sort key. So that the order of items as they are stored on physical media matches the order they are likely to be read.\\nThis introduces unwanted complexity at this time in the code for tracking event numbers.\\n\n## Decision\n","Decision":"For now instead of an Event Number as the sort key we will introduce a UUID EventId. And use StreamName as the HASH key. StreamNames will need to be unique anyway.\\n","tokens":260,"id":1990,"Prediction":"## Decision\n\n**Use a Simple Primary Key (Partition Key Only) Strategy**\n\nGiven the current complexity introduced by managing event numbers in a composite primary key, we will adopt a simpler approach by using a simple primary key (partition key only) strategy for our DynamoDB tables. The partition key will be based on a unique identifier that ensures items are properly distributed and uniquely identifiable within the table without needing additional sort keys.\n\n**Rationale:**\n\n1. **Simplification of Implementation**: \n   - Using a partition key only simplifies the schema design and initial implementation. There's no need to manage or track additional sort keys, thus reducing complexity in the codebase.\n\n2. **Unique Identification**: \n   - By ensuring that the partition key is unique, we can still maintain the uniqueness requirement for each item in the DynamoDB table.\n\n3. **Scalability**:\n   - While using only a partition key, we need to ensure that the partition key is designed to distribute the load evenly across the partitions to avoid hot spots.\n\n4. **Future Flexibility**:\n   - This design allows us to quickly iterate and adapt without being locked into managing the order of items via sort keys at this stage of development.\n\n5. **Cost and Performance**:\n   - Simpler key structure can lead to more predictable performance and lower operational overhead.\n\nShould the need arise to organize data with more intricate querying and sorting requirements, we can evaluate the introduction of a composite primary key structure or secondary indexes in future iterations.\n\n**Action Items:**\n\n1. Define the partition key schema to ensure uniqueness and even distribution.\n2. Implement the necessary database operations using the simple primary key strategy.\n3. Document any potential constraints or limitations inherent in this approach for future reference.\n4. Monitor performance and re-evaluate the key strategy periodically to ensure it meets the evolving needs of the application.","GenTime":"2024-07-28 23:23:50"}
{"File Name":"handbook\/0003-css.md","Context":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n\n## Decision\n","Decision":"- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","tokens":126,"id":1992,"Prediction":"## Decision\nWe will use a CSS-in-JS library, specifically styled-components, to manage the styling of our websites and web applications. Styled-components allows us to write actual CSS syntax within our JavaScript files, which will enable us to integrate closely with our design system and achieve the desired performance and safety.\n\n### Justification\n1. **Performance**:\n   - Styled-components employ optimizations such as automatic critical CSS and code splitting, which can dramatically reduce the amount of unused CSS.\n   - This results in smaller file sizes and faster load times, contributing to high-performance web applications.\n\n2. **Design System Integration**:\n   - By using a CSS-in-JS library, we can easily share design system constants between our JavaScript logic and CSS styles.\n   - Styled-components support theming, allowing us to create a consistent design system across different parts of our application.\n\n3. **Type-Safe**:\n   - Using TypeScript along with styled-components allows us to enjoy type checks, which helps in catching errors during compile time.\n   - This is particularly useful when refactoring, as TypeScript will alert us of any type mismatches or issues, adding an extra layer of safety.\n\n4. **Component Reusability and Encapsulation**:\n   - Styled-components allow us to build styled components that internally manage their styles, preventing external overrides.\n   - This ensures that our reusable components do not accept arbitrary styles or classes, aligning with our requirement to make components closed.\n\n5. **Responsive Props with Atomic Layout Components**:\n   - Styled-components provide a way to manage responsive design through props.\n   - We can create atomic layout components that adjust styles based on the props provided, facilitating the creation of flexible and responsive designs easily.\n\n### Additional Considerations\n- We will train the team in styled-components and ensure everyone is familiar with the structure and best practices for using this library.\n- We will create a style guide and documentation for our design system to ensure consistency and ease of use across our team.\n- We will periodically review our design system and the utilization of styled-components to ensure that we are meeting our performance goals and maintaining a high level of code quality.\n\nOverall, adopting styled-components aligns well with our decision drivers and will provide a robust solution for styling our websites and web applications in a performant and safe manner.","GenTime":"2024-07-28 23:23:59"}
{"File Name":"handbook\/0009-naming-files-and-directories.md","Context":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n\n## Decision\n","Decision":"* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","tokens":93,"id":1996,"Prediction":"## Decision\n\nWe will standardize the naming conventions for files and directories within the monorepo to ensure consistency and ease of navigation. The following conventions will be adopted:\n\n1. **File Naming Conventions:**\n   - Use `kebab-case` for all file names (e.g., `user-profile.component.ts`, `order-management.service.ts`).\n   - Extension-specific files will use the following conventions:\n     - TypeScript files: `.ts`\n     - HTML files: `.html`\n     - Stylesheets: `.scss` or `.css`\n     - JSON files: `.json`\n   \n2. **Directory Naming Conventions:**\n   - Use `kebab-case` for all directory names (e.g., `user-profile`, `order-management`).\n   - Group related files into folders by feature or domain, e.g., placing all user-related components, services, and models within a `user-profile` directory.\n   - Separate directories by purpose, using the following structure as an example:\n     ```\n     src\/\n       app\/\n         user-profile\/\n           components\/\n           services\/\n           models\/\n         order-management\/\n           components\/\n           services\/\n           models\/\n       assets\/\n       environments\/\n     ```\n\n3. **Naming Conventions for Specific Types:**\n   - Components: `[component-name].component.ts` (e.g., `user-profile.component.ts`)\n   - Services: `[service-name].service.ts` (e.g., `user-auth.service.ts`)\n   - Models: `[model-name].model.ts` (e.g., `user.model.ts`)\n   - Utility functions: `[utility-name].util.ts` (e.g., `date-formatter.util.ts`)\n\n4. **Consistent Naming Across NX Generators:**\n   - Ensure all generated code adheres to these naming conventions by customizing NX generator defaults where applicable.\n\nBy implementing these conventions, we aim to provide a consistent, intuitive, and maintainable structure for the codebase that supports easier navigation and understanding for all team members.","GenTime":"2024-07-28 23:24:19"}
{"File Name":"super-eks\/0007-backup.md","Context":"## Context and Problem Statement\\nIn the current setup of super eks there is no backup solution installed and configured. A backup solution can be helpful for some resources that are not managed by IaC.\\n\n## Decision\n","Decision":"### Positive Consequences\\nUsers get a state of the art backup technology included with super eks, enabling them to restore a cluster after a\\ncatastropic failure.\\n### Negative Consequences\\nUsers have to think about how the backup needs to be setup and distinguish between resources that have been set up by\\nIaC and manually set up resources (e.g., secrets).\\n","tokens":41,"id":2034,"Prediction":"## Decision\n\nWe will implement a backup solution for the super eks setup.\n\n## Reasons for the Decision\n\n1. **Data Safety:** A backup solution ensures that critical data is not lost in case of unforeseen events such as system failures, human errors, or malicious attacks. This is particularly important for resources that are not managed by Infrastructure as Code (IaC) and hence might not be easily recoverable.\n\n2. **Minimize Downtime:** With a backup solution in place, recovery of lost data can be quick, minimizing downtime and improving the reliability and availability of the system.\n\n3. **Compliance and Legal Requirements:** Depending on the industry and region, there can be legal requirements to maintain backups of certain types of data. Implementing a backup solution will help in ensuring compliance with these regulations.\n\n4. **Operational Efficiency:** A well-configured backup solution can automate the backup and recovery processes, freeing up valuable time for the team to focus on more strategic activities.\n\n## Backup Solution Selection Criteria\n\nGiven the context, the backup solution should meet the following criteria:\n\n1. **Compatibility:** Must be compatible with the current EKS setup and any additional AWS services used.\n2. **Ease of Use:** Should be easy to deploy, configure, and maintain.\n3. **Automated:** Capable of scheduled and automated backups with minimal manual intervention.\n4. **Cost-Effective:** Should fit within the allotted budget without compromising on essential features.\n5. **Scalability:** Can scale as the application and associated data grow.\n6. **Security:** Proper encryption and security features to protect backup data.\n7. **Recovery Time Objective (RTO) and Recovery Point Objective (RPO):** Must meet the defined RTO and RPO for critical resources.\n\n## Implementation Plan\n\n1. **Evaluate Backup Solutions:** Review and evaluate different backup solutions such as AWS Backup, Velero, or other cloud-native tools based on the set criteria.\n2. **Pilot Implementation:** Start with a pilot implementation to test the chosen solution in a controlled environment.\n3. **Configuration:** Configure the backup schedules, retention policies, and recovery options.\n4. **Testing:** Conduct regular testing of the backup and recovery process to ensure that it works as expected.\n5. **Documentation:** Document the backup and recovery procedures.\n6. **Monitoring and Maintenance:** Implement monitoring to track the status of backups and perform regular maintenance.\n\nBy implementing a robust backup solution, we ensure data integrity, improve system reliability, and align with best practices for data management.","GenTime":"2024-07-28 23:26:23"}
{"File Name":"ReportMI-service-manual\/0018-submission-state-machine.md","Context":"## Context\\nAs outlined in [ADR-0016][adr-0016], the Data Submission Service will use\\n\"submissions\" to describe something that a supplier has submitted to us in order\\nto complete a \"task\".\\nSubmissions will exist in a state machine that outlines what is happening with\\nthem.\\nCurrently, we expect there to be 6 states:\\n* **pending** - a blank submission that is awaiting data\\n* **processing** - a submission where data is being processed\\n(eg a file is being ingested or data is being validated)\\n* **validation_failed** - the submitted data has failed the validation process\\nand needs to be corrected\\n* **validation_passed** - the submitted data has passed the validation process,\\nand the supplier now needs to review the results\\n* **supplier_rejected** - the supplier has reviewed the result of the data\\nprocessing and has rejected it (eg they have realised that their data needs to\\nbe amended)\\n* **supplier_accepted** - the supplier has reviewed the result of the data\\nprocessing and are happy that it is accurate\\nOther states may added in future to cover approval processes, and fixing\\nmistakes in returns.\\n\n## Decision\n","Decision":"The system will model the 6 states highlighted above.\\nSubmissions containing data will proceed through the states from `pending` to\\n`processing` to either `validation_failed` or `validation_passed`. If the\\nvalidation has passed, the supplier can reject the submission (move to\\n`supplier_rejected`) or accept the submission (move to `supplier_accepted`).\\nA 'no business' submission will proceed straight to `supplier_accepted` once\\nthe supplier has confirmed they wish to make this submission.\\nThis is shown in the following diagram.\\n![Submission state machine diagram](..\/diagrams\/0018-submission-states.jpg)\\n","tokens":250,"id":2039,"Prediction":"## Decision\n\n**We will implement a finite state machine (FSM) to manage the lifecycle of submissions in the Data Submission Service.**\n\nGiven the outlined states and transitions, the state machine will follow these definitions:\n\n1. **States**:\n    - **pending**: Awaiting data entry by the supplier.\n    - **processing**: Actively handling the data, whether ingesting, processing, or validating.\n    - **validation_failed**: Data has failed validation checks and needs correction from the supplier.\n    - **validation_passed**: Data has passed validation checks and is waiting for supplier review.\n    - **supplier_rejected**: Supplier has reviewed the processed data and rejected it, requiring amendments.\n    - **supplier_accepted**: Supplier has reviewed the processed data and approved it.\n\n2. **Transitions**:\n    - From **pending** to **processing**: When data is submitted by the supplier.\n    - From **processing** to **validation_failed**: If the data fails validation checks.\n    - From **processing** to **validation_passed**: If the data passes validation checks.\n    - From **validation_passed** to **supplier_rejected**: If the supplier reviews the data and finds it needs amendments.\n    - From **validation_passed** to **supplier_accepted**: If the supplier reviews the data and finds it accurate.\n\n3. **Extensibility**:\n    - The state machine will be designed to allow the addition of new states, such as those needed for future approval processes and corrections for mistakes in returns, without significant redesign.\n\nBy implementing this FSM, we aim to ensure a clear, maintainable, and extensible process for managing submissions through their lifecycle stages, thereby improving the robustness and clarity of the Data Submission Service.","GenTime":"2024-07-28 23:26:40"}
{"File Name":"ReportMI-service-manual\/0011-use-auth0-for-temporary-user-authentication.md","Context":"## Context\\nOur user facing applications will need to authenticate users.\\nIn the long-term, we want to use the standard user sign-on services in CCS for\\neach appropriate user group.\\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\\nFor CCS staff it is their G-Suite account.\\nUsing a single authentication service will reduce administration overheads\\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\\nusers it will also mean they don't have to remember yet another username and\\npassword combination.\\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\\nwill take time. For now, we want a simple sign-in service that we can integrate\\nwith quickly, to enable us to test and iterate the application.\\n\n## Decision\n","Decision":"We will use [Auth0][service-auth0] for authentication on a temporary basis.\\nAuth0 is an authentication service that uses the OAuth protocol. It provides\\nand simple integration path, and is free for several thousand active users.\\nWe acknowledge that we are incurring technical debt by not integrating\\nimmediately with SRS and G-Suite, however we believe that this will let us move\\nmore quickly during development.\\nThis decision will also allow us to gain a better understanding of the user\\nneeds around authentication before a decision around the final authentication\\nsolution is made.\\nWe will replace Auth0 with the proper authentication service later in the\\ndevelopment process.\\n","tokens":179,"id":2041,"Prediction":"## Decision\n\nWe will implement a temporary user authentication service using a third-party provider like Auth0 or Firebase Authentication for fast integration and ease of use. This solution will serve as an interim measure until we can fully integrate with the Supplier Registration Service (SRS) for suppliers and the CCS G-Suite for CCS staff.\n\n### Justification:\n\n1. **Speed and Efficiency**: Third-party authentication services provide pre-built, scalable solutions that allow for quick integration, enabling us to focus on developing and iterating other parts of our application.\n\n2. **User Experience**: By leveraging an established authentication service, we can minimize the effort users need to sign in, which aligns with our long-term goal of minimizing multiple sign-on processes.\n\n3. **Resource Management**: This approach reduces the initial overhead on our development team, allowing them to prioritize core application features while providing a reliable, tested authentication mechanism.\n\n4. **Scalability**: Third-party providers like Auth0 or Firebase handle the complexities of authentication, providing a range of features including user management, password recovery, and multi-factor authentication, which we can utilize as needed.\n\n### Next Steps:\n\n1. **Implementation**: Integrate the chosen third-party authentication service into our user-facing applications.\n2. **Migration Plan**: Develop a strategy for transitioning from the temporary authentication service to SRS and CCS G-Suite as soon as those integrations are feasible.\n3. **User Communication**: Clearly communicate to users that this is a temporary measure and inform them of upcoming changes regarding the authentication process.\n4. **Monitoring and Feedback**: Actively monitor user feedback and authentication issues to ensure a smooth user experience until full integration with SRS and CCS G-Suite is achieved.","GenTime":"2024-07-28 23:26:45"}
{"File Name":"ReportMI-service-manual\/0019-store-data-validation-rules-in-json-files.md","Context":"## Context\\nEach framework has a specific description of what data must be submitted in the\\nmonthly MI return. This description outlines what fields should be submitted,\\nwhat data types are accepted and any validation rules which should be applied.\\nAt the moment, these rules are encapsulated in the Excel template which\\nsuppliers download, using drop-downs, tool-tips and sometimes macros.\\nA separate copy of these rules is configured for each framework in MISO, which\\nthe service uses to validate templates have been filed in correctly when\\nuploaded.\\n### Longer term approach\\nIn future, we want a single source for the rules for framework data submissions\\nwhich we can use for various tasks including:\\n- generating submission templates in different formats (eg Excel, ODS, CSV)\\n- validating submitted returns\\n- documenting any APIs\\nThese rules should ideally be stored in a central place and made available to\\nany services which require them via an API. This will allow new services to\\nshare the ruleset and description.\\nThe high-level principle is that there should be an artefact that describes the\\nrules that need to be applied.\\nThe artefacts need to be version controlled so they can be changed during the\\nlifetime of the framework. Services may need to access and used outdated\\nversions of the rules.\\n### Interim approach\\nFor the initial waves of on-boarding, we expect only a small number of\\nframeworks to be submitting monthly returns.\\nWhile we develop our approach, and learn more about the varied rules for each\\nframework, we will use an interim approach for storing the rules.\\nWe will create a JSON file for each framework which outlines the data structure\\nrequired and the validation rules for each field. This JSON file will be stored\\nin a GitHub repository, and used to generate templates and validate submitted\\nfiles.\\nEventually, we will develop an API to provide access to this information.\\n\n## Decision\n","Decision":"We will create a JSON file for each framework and store it in a GitHub\\nrepository.\\nThis JSON file will describe each field, it's data type and associated\\nvalidation rules.\\nWe will migrate these files to a longer-term solution once we have a better\\nunderstanding of the required rules for each framework.\\n","tokens":391,"id":2045,"Prediction":"## Decision\n\n1. **Interim JSON-Based Storage**: In the initial phase, we will store the rules for each framework in JSON files. Each JSON file will detail the data structure requirements and the validation rules for each field. These JSON files will be manually created and maintained in a GitHub repository. This approach allows us to rapidly iterate and adapt to the specific needs of each framework while we are in the learning phase and onboarding a small number of frameworks.\n\n2. **Version Control**: These JSON files will be version-controlled using GitHub. This will enable us to manage changes over time and ensure that we can revert to or reference older versions of the rules as needed. This is crucial for the long-term maintainability and accuracy of the data validation process.\n\n3. **API Development**: Concurrently with the interim solution, development will begin on creating a centralized API that will serve as the single source of truth for all framework data submission rules. This API will be responsible for:\n   - Generating submission templates in various formats (Excel, ODS, CSV).\n   - Validating the submitted returns against the defined rules.\n   - Documenting and providing access to these rules for any services requiring them.\n\n4. **Documentation and Communication**: Clear documentation will be created to describe the structure and use of the JSON files. This documentation will be made available within the GitHub repository to guide developers and other stakeholders in understanding and using the rules effectively.\n\n5. **Gradual Transition**: Over time, as the API matures, we will gradually transition from the JSON\/GitHub-based interim solution to using the centralized API for all rule-related tasks. This transition will be planned and staged to avoid any disruptions in the validation and submission processes.\n\nBy taking these steps, we aim to create a structured and adaptable approach to data validation that can evolve to meet future needs while ensuring immediate functionality through an interim solution.","GenTime":"2024-07-28 23:26:55"}
{"File Name":"ReportMI-service-manual\/0014-use-gov-uk-notify-for-sending-notifications.md","Context":"## Context\\nThe Data Submission Service will need to send notifications to suppliers at\\nvarious stages during the submission process.\\nThe notifications will, for example, include:\\n- Telling a supplier they have a task to complete\\n- Telling a supplier that a deadline is approaching\\n- Telling a supplier that their task is overdue\\n- Providing a receipt for a submission\\nInitially, the notifications will be email, but may also need to be SMS based in\\nfuture.\\nTo reduce the need for each organisation or service team to build it's own\\ninfrastructure for sending email and SMS based messages, the Government Digital\\nService (GDS) has built [GOV.UK Notify][service-notify].\\nGOV.UK Notify can send notifications either via an API call, or by uploading a\\nCSV to the website. Sending email is free.\\nThe service is already used in various parts of CCS.\\n\n## Decision\n","Decision":"We will use GOV.UK notify to send notifications for Data Submission Service.\\nFor MVP we may use the CSV upload function to send notifications, but this will\\nbe replaced by automated API calls later in development.\\n","tokens":188,"id":2050,"Prediction":"## Decision\nWe will integrate the Data Submission Service with GOV.UK Notify for all notification needs.\n\n### Rationale:\n1. **Existing Infrastructure:** GOV.UK Notify is an established service already used within various parts of CCS, ensuring reliability and familiarity.\n2. **Cost Efficiency:** Sending emails through GOV.UK Notify is free, which helps in cost savings.\n3. **Flexibility for Future Needs:** The service supports emails currently and extends easily to SMS in the future if required.\n4. **Implementation Ease:** GOV.UK Notify provides API calls, simplifying the integration process with our existing system infrastructure.\n5. **Scalability:** Using a centralized service like GOV.UK Notify allows easy scaling as the volume of notifications increases.\n6. **Support and Maintenance:** Reduced need for additional support and maintenance overhead by leveraging an existing, centrally managed service.\n\nIn conclusion, using GOV.UK Notify aligns with our goals of efficiency, cost-effectiveness, and future-proofing the Data Submission Service.","GenTime":"2024-07-28 23:27:13"}
{"File Name":"ReportMI-service-manual\/0006-use-terraform-to-create-and-document-infrastructure.md","Context":"## Context\\nRunning a service like the Data Submission Service requires infrastructure of\\nvarious kinds.\\nIt's good practice to manage this infrastructure using code (Infrastructure as\\nCode), as this allows the infrastructure to be version-controlled and managed\\nlike the rest of the application code.\\nThere are various mechanisms for doing this. Each of the main cloud providers\\nhave their own solutions to manage infrastructure in code, for example Amazon\\nWeb Services (AWS) has CloudFormation, Microsoft Azure has Resource Manager etc.\\nHowever each of these are specific to the individual cloud provider.\\nIt would also be possible to do this manually by running scripts against the\\ncloud provider API. However, this would take a significant amount of time and\\nwould take effort to make it work across more than one cloud provider.\\nThere are tools available, like Terraform, which allow you to define\\ninfrastructure as code, in a standard way which can then be applied against more\\nthan one cloud provider. The tool handles the differences between providers.\\n\n## Decision\n","Decision":"We will use Terraform to create the infrastructure for the Data Submission\\nService.\\n","tokens":208,"id":2062,"Prediction":"## Decision\nWe will use Terraform for managing our infrastructure as code.\n\n## Rationale\n1. **Cross-Cloud Compatibility**: Terraform allows us to work with multiple cloud providers, ensuring that our infrastructure code is not locked into a single vendor. This provides the flexibility to switch or expand our infrastructure to other cloud providers in the future without significant rework.\n\n2. **Standardization**: Using Terraform provides a standard and consistent way to define and manage infrastructure across different environments and teams. This minimizes discrepancies and reduces the potential for errors.\n\n3. **Version Control**: Infrastructure as code defined using Terraform can be stored, versioned, and reviewed using our existing version control systems. This aligns well with our existing development workflows and provides a history of changes for auditing purposes.\n\n4. **Community and Support**: Terraform has a strong community and extensive documentation, which will help us adopt and implement best practices more easily. Additionally, it provides regular updates and support for a wide range of providers and services.\n\n5. **Efficiency**: Terraform automates the creation, modification, and deletion of infrastructure, reducing the manual effort and time required to manage our infrastructure. Its plan and apply steps allow us to preview changes before applying them, reducing the risk of unexpected issues.\n\nBy adopting Terraform, we enable more flexible, efficient, and maintainable infrastructure management, while also aligning with best practices in Infrastructure as Code.","GenTime":"2024-07-28 23:27:56"}
{"File Name":"application-services\/0004-early-startup-experiments.md","Context":"## Context and Problem Statement\\nAs an experimenter, I would like to run experiments early on a user's first run of the application. However, the experiment data is only available on the second run. We would like to have that experiment data available before the user's first run.\\nFor more information: https:\/\/docs.google.com\/document\/d\/1Qw36_7G6XyHvJZdM-Hxh4nqYZyCsYajG0L5mO33Yd5M\/edit\\n## Decision Drivers\\n* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\n\n## Decision\n","Decision":"* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\nNone of the options were feasible, so for now we are sticking with option **(A) Do Nothing** until there are experiments planned that are expected to run on early startup on the first run, then we will revaluate our options.\\nThe **(B) Bundle Experiment data with app on release** option was rejected mainly due to difficulty in disabling experiments and pausing enrollments. This can create a negative user experience as it prevents us from disabling any problematic experiments. Additionally, it ties experiment creation with application release cycles.\\nThe **(C) Retrieve Experiment data on first run, and deal with delay** option was rejected due to the fact it changes the Nimbus SDK will no longer be idempotent,and the possibility of introducing undesirable UI.\\n","tokens":170,"id":2067,"Prediction":"## Decision\n\nImplement a server-side initialization process that pre-fetches the experiment data before the user's first run of the application.\n\n### Rationale:\n- **Availability of experiments early on the first run:** By pre-fetching the experiment data from the server during the application installation or first launch process, we ensure that this data is available as soon as the user opens the app for the first time.\n  \n- **No impact on experimentation data analysis:** This approach ensures the integrity of the experimentation data by making sure that all users who are supposed to be in the experiment have the necessary data from the moment they start using the application.\n  \n- **Flexibility in creating experiments:** The server-side initialization allows experiment configurations to be updated and managed centrally, providing flexibility to create and modify experiments without needing to release new versions of the application.\n  \n- **Ability to quickly disable experiments:** With server-side management, experiments can be disabled instantly by updating the server configurations, reducing the risk and impact of any issues found with an experiment.\n  \n- **Simplicity of releases:** This decision minimizes client-side changes and dependencies, simplifying the release process as it offloads the complexity to server-side logic.\n  \n- **Mobile's expectations of Nimbus:** Ensuring the SDK remains idempotent aligns with maintaining a consistent and predictable initialization flow even when experiments are involved.\n\n### Implementation Plan:\n1. **Server-Side Setup**: Ensure that the server is capable of serving the experiment data required for the application.\n   \n2. **Initialization Process**: Modify the application's initial startup sequence to include a step where it fetches the necessary experiment data from the server.\n   \n3. **Caching Mechanism**: Implement a caching mechanism to store the fetched experiment data locally, ensuring it is readily available on subsequent runs.\n   \n4. **Error Handling**: Incorporate error handling to manage scenarios where the server-side fetch fails, ensuring the application remains functional.\n   \n5. **App Configuration**: Ensure that there is a fallback mechanism so the application can operate with default settings if the experiment data cannot be pre-fetched or if an experiment is disabled or updated.\n\n### Potential Risks:\n- **Dependency on Server Availability**: The initial application experience may depend on server availability, which requires robust infrastructure and fallbacks.\n- **Increased Complexity for Initialization**: Adds complexity to the application's initialization process, necessitating thorough testing and validation.\n\nBy moving the experiment data fetch to the server side and initializing it before the user's first run, we can ensure a seamless integration of experiments from the onset without compromising on flexibility or simplicity in the application release process.","GenTime":"2024-07-28 23:28:17"}
{"File Name":"ng-kaart\/0005-links-in-tabel-tonen.md","Context":"## Context\\nURL's worden gestyled zodat deze clickable zijn in het Identify paneel. We willen graag dat ze ook bruikbaar zijn in de tabel.\\nUit de lagen.json heb ik afgeleid dat sommige van deze links veldtype URL hebben en andere veldtype String.\\nWe willen deze URL's graag op dynamische wijze tonen in de tabel (niet gewoon de URL zelf, maar ook niet telkens gewoon LINK).\\nEen groot deel kan getoond worden op basis van de waarde in een ander veld bv dossiernummer bij AV.\\nVoor andere lagen mag er een vaste string getoond worden.\\nEn voor sommige lagen een combinatie van 2 velden als label.\\n\n## Decision\n","Decision":"Momenteel detecteren we links automatisch door naar de start van de string of naar veldtype te kijken. Als het met http of https begint of\\nveldtype 'url' heeft beelden we het af als een link.\\nWe houden voor de tabel vanaf nu rekening met 'html' veld. Indien ingevuld, dan gebruiken we de inhoud van dat html veld om de weergave af te beelden.\\nWe staan toe dat er tokens instaan om bepaalde velden dynamisch\\nin te vullen. Bvb:\\n{ \"isBasisVeld\": true, \"label\": \"Rapport\", \"naam\": \"rapport_url_1\", \"veldType\": \"url\", \"html\": \"{{periode_1_begin}} - {{periode_1_einde}}\" },\\nIndien 'html' leeg is, nemen we de waarde in 'label'.\\nMet het veld \"constante\" moet ook rekening gehouden worden, dat moet soms genomen worden ipv de waarde van het veld. Bvb:\\n{\\n\"isBasisVeld\": true,\\n\"label\": \"Open in Werf\",\\n\"naam\": \"externeurl\",\\n\"veldType\": \"string\",\\n\"html\": \"{werfid}\",\\n\"constante\": \"https:\/\/{domain.name}\/werf\/schermen\/werf\/{werfid};werf=werf%2Fapi%2Fwerf%2F{werfid}\"\\n},\\n","tokens":172,"id":2074,"Prediction":"## Decision\n\nWe hebben besloten om de logica voor het dynamisch tonen van URL's in de tabel als volgt te implementeren:\n\n1. **Detectie van URL Veldtype:**\n   Beide veldtypen (URL en String) dienen te worden herkend en correct ge\u00efnterpreteerd als potenti\u00eble hyperlinks. Dit betekent dat onze logica moet kunnen onderscheiden wanneer een veld een daadwerkelijke URL bevat, zelfs als het veldtype String is.\n\n2. **Basis Voorwaarden:**\n   a. Voor URL-veldtype: De waarde zal standaard als een klikbare link worden weergegeven.\n   b. Voor String-veldtype: Een validatie zal worden uitgevoerd om te controleren of de waarde een URL bevat. Indien voldoet, zal deze als klikbare link worden weergegeven.\n\n3. **Dynamische Labels:**\n   a. Indien er een corresponderende datasetregel bestaat waarbij een ander veld een label definieert (zoals dossiernummer), dan zal deze waarde als de tekst van de hyperlink worden gebruikt.\n   b. Voor lagen die een vaste string moeten tonen, zal de logica een configuratie bevatten waar deze string is gedefinieerd. Dit kan worden ingesteld via een configuratiebestand of direct binnen de JSON-definitie.\n   c. Voor lagen die een combinatie van twee velden als label nodig hebben, zal de logica een template-strategie gebruiken waarbij de twee veldwaarden worden gecombineerd volgens een vooraf gedefinieerde syntaxis (bijv. [Veld1]-[Veld2]).\n\n4. **Configuratiebestand Uitebreiding:**\n   De huidige lagen.json zal worden uitgebreid om deze configuraties te ondersteunen. Dat wil zeggen, elke laag of URL-veld kan worden bijgewerkt met eigenschappen zoals:\n   ```json\n   {\n     \"labelField\": \"dossiernummer\",\n     \"fixedString\": \"Gedefinieerde Tekst\",\n     \"compositeLabel\": [\"veld1\", \"veld2\"]\n   }\n   ```\n\n5. **Fallback Logica:**\n   Indien geen specifieke configuratie bestaat voor een URL of veldtype combinatie, zal de default weergave \"LINK\" zijn, zodat gebruikers op zijn minst een indicatie hebben dat er een hyperlink beschikbaar is.\n\nDoor deze aanpak krijgen we de volgende voordelen:\n- Betere gebruikservaring door dynamische en contextueel accurate labels.\n- Eenduidige configuratie en makkelijke uitbreiding voor nieuwe lagen of datasetregels.\n- Flexibiliteit om snel aanpassingen te maken in de toekomst via configuratiebestanden.\n\nDeze wijziging zal worden gedocumenteerd en gedistribueerd aan het hele ontwikkelteam, zodat er consistentie en begrip is rond deze nieuwe functionaliteiten.","GenTime":"2024-07-28 23:28:52"}
{"File Name":"ToPS\/0001-extra-test-class-for-junit-test-with-SWI-Prolog-involved.md","Context":"## Context and Problem Statement\\nTests including Prolog queries with SWI Prolog can not be executed with CircleCI because SWI Prolog is not\\nrunning at the executing system.\\n\n## Decision\n","Decision":"Chosen option: extra test class to reduce the effort and to still enable testing SWI Prolog locally. For this the extra class is\\nannotated with @Ignore. All other tests run with CircleCI. These tests includes the core functionalities of the developed tool.\\n","tokens":38,"id":2080,"Prediction":"## Decision\n\nWe will integrate SWI Prolog into our CircleCI configuration to ensure that tests involving Prolog queries can be executed seamlessly.\n\n1. **Use a Docker Image with SWI Prolog:**\n   - We will modify our CircleCI configuration to use a Docker image that has SWI Prolog installed.\n   - This approach will ensure that the CircleCI virtual environment has access to Prolog during the execution of tests.\n\n2. **Custom Docker Image:**\n   - If no suitable Docker image is available, we will create a custom Docker image that includes SWI Prolog along with the necessary dependencies for our project.\n   - Publish this image to a Docker registry for reuse and version control.\n\n3. **CircleCI Configuration Update:**\n   - Update the CircleCI configuration file (`.circleci\/config.yml`) to specify the Docker image.\n   - Ensure steps are included to install any additional dependencies, configure the environment, and run the tests.\n\n4. **Testing the Integration:**\n   - Once the configuration is updated, we will run multiple build-and-test iterations to ensure that Prolog-based queries are being processed correctly in the CI environment.\n   - Review test outcomes and make necessary adjustments to the configuration to handle any issues or edge cases.\n\n5. **Documentation:**\n   - Document the changes in the project's README and any relevant development documentation to inform other developers about the update and how the CI environment is configured.\n\nThis decision will allow us to maintain the integrity of our test suite by ensuring all components, including SWI Prolog, are available during the CI process.","GenTime":"2024-07-28 23:29:09"}
{"File Name":"portfolio\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\nEven though this project is mainly a 1 engineer side-project, recording architectural decisions allows using the records as a reminder of the \"state of mind\" we were in when making a decision, as well as the motivations behind it.\\nAdditionally, it's a good engineering practice that would be nice to try out, and this blog is a great \"guinea pig\" for this approach.\\nSee this article on [GitHub's engineering blog](https:\/\/github.blog\/2020-08-13-why-write-adrs\/) for some addition reasons why they could be a good practice.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.\\n","tokens":135,"id":2091,"Prediction":"## Decision\n\nWe will adopt the practice of writing Architectural Decision Records (ADRs) for this project.\n\n### Details:\n\n1. **Template and Format**: We will use a standardized template for ADRs to ensure consistency and clarity. The template will include sections such as Context, Decision, Status, Consequences, and Alternatives.\n\n2. **Storage**: ADRs will be stored in a dedicated directory within the project's repository (e.g., a `docs\/adr` directory). This ensures easy access and version control.\n\n3. **Frequency**: ADRs will be created for all significant architectural decisions. Even if only one engineer is working on the project, each major decision should get an ADR entry.\n\n4. **Review and Update**: ADRs will be reviewed periodically, and updated if the decisions they document are revisited or if the context changes significantly.\n\n### Rationale:\n\n- **Documentation**: ADRs serve as a living documentation of the architectural choices and the rationale behind them. They provide clarity and historical context for future reference.\n  \n- **Transparency and Clarity**: Even though this is a one-engineer project, having a clear record of decisions helps ensure that the engineer (and any future collaborators) have a clear understanding of why certain decisions were made.\n\n- **Good Practice**: Maintaining ADRs is a best practice in software engineering. It encourages thoughtful decision-making and can be a valuable habit to develop for larger, collaborative projects.\n\n- **Learning and Improvement**: This approach allows experimentation with ADRs in a low-risk environment. It serves as a practical exercise in documenting and reflecting on architecture decisions.\n\n### Implementation Steps:\n\n1. **Select an ADR Template**: Choose a template that is simple yet comprehensive. The Markdown format is preferred for ease of writing and version control.\n2. **Create ADR Directory**: Set up a `docs\/adr` directory in the project repository.\n3. **Document Initial ADRs**: Begin documenting the architectural decisions made so far in the project.\n4. **Integrate ADRs into Workflow**: Make writing ADRs a part of the workflow for any significant architectural change or decision.\n5. **Set Review Schedule**: Establish a regular schedule for reviewing and updating ADRs.\n\nOverall, adopting ADRs will contribute to better project management, improved documentation, and enhanced personal and professional development in software architecture practices.","GenTime":"2024-07-28 23:29:53"}
{"File Name":"claim-additional-payments-for-teaching\/0004-deployment-on-heroku.md","Context":"## Context\\nDepartment for Education have a Cloud Infrastructure Program based on Azure that\\nthey would like digital services to use. Access to Azure is heavily restricted\\nfor production, and slightly restricted for lower environments.\\nWe need to be able to work quickly, particularly in the early stages of this\\nproject.\\nWe need to be able to deploy prototypes and experimental features and versions\\nof the service for user research.\\n\n## Decision\n","Decision":"We will use Heroku to deploy the application.\\nWe will use Heroku's pipeline feature to run CI and deploy the application.\\n","tokens":84,"id":2093,"Prediction":"## Decision\n\nTo address the constraints and needs identified in the context, the following decision has been made:\n\n1. **Use Azure for Production:** We will adhere to the Department for Education's Cloud Infrastructure Program and utilize Azure for all production deployments, ensuring compliance with the security and access management protocols established by the department.\n\n2. **Utilize Azure Dev\/Test Subscriptions:** For lower environments like development, testing, and staging, we will leverage Azure Dev\/Test subscriptions to take advantage of the slightly less restrictive controls. This will allow the team to work more efficiently and maintain a closer environment parity with production.\n\n3. **Lightweight CI\/CD Pipeline:** Implement a lightweight Continuous Integration\/Continuous Deployment (CI\/CD) pipeline that allows for rapid deployment of prototypes and experimental features. This will enable us to conduct frequent user research and iterate quickly based on feedback.\n\n4. **Local Development:** Encourage local development environments where possible to accelerate the early stages of development. This approach will help circumvent some of the restrictions imposed on cloud environments and ensure rapid prototyping.\n\n5. **Azure Services for Rapid Prototyping:** Make use of Azure services and tools that support rapid prototyping and experimentation, such as Azure DevOps, Azure Functions, and Azure App Services. These tools will help facilitate quick deployments and scaling of experimental features.\n\n6. **Compliance and Security Reviews:** Ensure that all deployments, even those in lower environments and for prototyping, undergo necessary compliance and security reviews to align with the Department for Education's policies.\n\n7. **Documentation and Knowledge Sharing:** Maintain thorough documentation and establish a knowledge-sharing culture within the team to ensure a clear understanding of infrastructure usage, deployment processes, and any encountered challenges or solutions.\n\nThis decision aims to balance the need for speed and agility in the early stages of the project with the compliance and security requirements of the Department for Education's cloud policies.","GenTime":"2024-07-28 23:30:00"}
{"File Name":"claim-additional-payments-for-teaching\/0009-capture-teacher-reference-number.md","Context":"## Context\\nA claimant\u2019s eligibility is, in part, determined by their qualifications. We\\nwant to be able to validate that a claimant\u2019s qualifications match those of the\\neligibility criteria.\\n\n## Decision\n","Decision":"To aid DfE in the process of validating a claimant\u2019s qualifications, we will\\ncollect the claimant\u2019s \u2018Teacher Reference Number\u2019 or TRN.\\nWith the TRN, DfE can use the Database of Qualified Teachers\\n([DQT](https:\/\/teacherservices.education.gov.uk\/SelfService\/Login)) to validate\\na claimant\u2019s qualifications.\\n","tokens":42,"id":2098,"Prediction":"## Decision\n\nTo validate that a claimant\u2019s qualifications match the eligibility criteria, we will implement a rule-based validation system. This system will consist of the following components:\n\n1. **Eligibility Criteria Engine**: This engine will house a set of predefined rules corresponding to the eligibility criteria. These rules will be defined by domain experts and can be updated as the eligibility criteria evolve over time. \n\n2. **Claimant Qualification Parser**: This component will parse and standardize the claimant's submitted qualifications into a structured format that can be evaluated against the rules in the Eligibility Criteria Engine.\n\n3. **Validation Module**: This module will compare the standardized claimant qualifications against the rules in the Eligibility Criteria Engine. It will return a validation result indicating whether the claimant meets the eligibility criteria, including detailed feedback on any non-compliance for transparency and further action.\n\n4. **User Interface for Rule Management**: To ensure the system remains adaptable, we will create an interface that allows domain experts to add, update, or remove eligibility rules without requiring deep technical knowledge. This tool will help keep the rules current and relevant.\n\n5. **Logging and Audit Trail**: The system will include logging to track validation attempts, rule changes, and claimant data evaluations to ensure accountability and ease of troubleshooting.\n\n### Justification\n\n- **Modular Design**: By separating the validation logic from the qualifications parsing, we ensure flexibility and maintainability. Changes to eligibility criteria rules can be implemented without altering the claimant qualification parsing logic.\n  \n- **Ease of Updates**: A user-friendly interface for managing rules allows non-technical users to keep the system up to date with the latest eligibility requirements efficiently.\n\n- **Transparency**: Providing detailed feedback on validation results helps claimants understand the reasons behind their eligibility status, promoting transparency and trust in the process.\n\n- **Accountability**: Logging and auditing mechanisms ensure that the validation process is transparent and traceable, helping in resolving disputes and ensuring compliance with regulatory requirements.\n\nThis rule-based validation approach will ensure that our system is robust, flexible, and capable of evolving with changing eligibility criteria.","GenTime":"2024-07-28 23:30:13"}
{"File Name":"claim-additional-payments-for-teaching\/0003-use-rubocop-for-linting.md","Context":"## Context\\nWe need to lint our Ruby code, the team has previous experience working with\\nRuboCop.\\n\n## Decision\n","Decision":"We will use RuboCop for linting our Ruby code\\n","tokens":24,"id":2101,"Prediction":"## Decision\n\nWe will adopt RuboCop as the linting tool for our Ruby codebase. Given the team's previous experience and familiarity with RuboCop, it will enable a smoother integration into our development process and ensure consistent code quality and style across the project. Furthermore, RuboCop's extensive configuration options and community support make it a suitable choice for our linting requirements.","GenTime":"2024-07-28 23:30:21"}
{"File Name":"eq-author-app\/0002-routing-mk2.md","Context":"## Context\\nRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.\\nAs these conditions can be any boolean expression we need a more complex routing structure to handle\\n\n## Decision\n","Decision":"Implement a new version of the routing in the API that can handle nested expressions.\\nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2\\n### Terminology\\n- Routing - all rules etc for a page with a fallback destination when no rules match.\\n- Rule - a series of expressions that when evaluated to true will go to the destination specified.\\n- Expression - Either a binary expression or expression group\\n- Expression Group - A group of BinaryExpressions combined as either `and` or `or`.\\n- Binary Expression - The base item with a left hand side, condtion and right hand side.\\n- Left hand side - The item being compared against (e.g. a question)\\n- Condition - A way of comparing left and right e.g. `=`, `>`, `>=`, `includes`, `not_includes`\\n- Right hand side - The value being compared against a left.\\n### Example\\nOn page 1, when the user enters 5 go to page 2, when they select red or white go to page 3, otherwise go to page 4.\\n```\\npage: {\\ntitle: \"Page 1\",\\nanswers: [\\n{ id: \"answer1\", type: NUMBER, label: \"Answer\" }\\n],\\nrouting: {\\nrules: [\\n{\\nexpressionGroup: {\\noperator: \"AND\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer1\",\\ntype: CURRENCY\\n}\\n},\\ncondition: \"Equals\",\\nright: {\\nnumber: 5\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page2\"\\n}\\n}\\n},\\n{\\nexpressionGroup: {\\noperator: \"OR\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer2\",\\ntype: RADIO\\n}\\n},\\ncondition: \"OneOf\",\\nright: {\\noptions: [\\n{\\nvalue: \"red\"\\n},\\n{\\nvalue: \"white\"\\n}\\n]\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page3\"\\n}\\n}\\n}\\n],\\nelse: {\\npage: {\\nid: \"Page4\"\\n}\\n}\\n}\\n}\\n```\\n","tokens":51,"id":2111,"Prediction":"## Decision\n\nWe have decided to implement a rule-based routing engine to handle the complex routing structure required for survey navigation. This engine will allow the definition of conditional paths based on boolean expressions derived from survey responses. Here are the key elements of the decision:\n\n1. **Rule Definition Syntax**: We will design a user-friendly rule definition syntax that can support complex boolean expressions. This syntax should allow the combination of multiple conditions using logical operators (AND, OR, NOT).\n\n2. **Routing Engine Architecture**: \n    - The routing engine will be developed as a modular component of the survey system.\n    - It will evaluate the conditions at runtime and determine the next page or action based on the results.\n\n3. **Condition Evaluation**:\n    - The engine will utilize a parser to interpret the boolean expressions.\n    - We will support standard comparison operators (e.g., =, !=, <, >, <=, >=) and logical operators to form compound expressions.\n\n4. **Integration Points**:\n    - The routing logic will be integrated with the survey form submission process.\n    - Hooks will be provided to tap into the routing workflow, allowing for custom extensions and modifications by other system components or plugins.\n\n5. **Testing and Validation**:\n    - We will implement comprehensive unit tests to ensure the correctness of the rule parsing and evaluation.\n    - User acceptance testing will be conducted to verify the usability and reliability of the routing functionality.\n\n6. **Documentation and User Interface**:\n    - Detailed documentation will be provided to guide users in creating and managing routing rules.\n    - A user interface for rule management will be developed, including a visual editor for easy creation of boolean expressions.\n\nThis approach ensures a flexible, extensible, and user-friendly routing system, providing the necessary complexity handling for survey navigation based on respondent answers.","GenTime":"2024-07-28 23:30:46"}
{"File Name":"james\/0005-distributed-task-termination-ackowledgement.md","Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n\n## Decision\n","Decision":"* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n","tokens":54,"id":2121,"Prediction":"## Decision\n\nWe will implement a distributed task manager using a message queue-based publish-subscribe system to manage task execution states and termination events across the cluster. Specifically, we will leverage a robust message broker such as Apache Kafka or RabbitMQ to facilitate communication between nodes.\n\n1. **Task Distribution:**\n   - Each task submitted to the task manager will be published to a central task queue.\n   - Worker nodes in the cluster will subscribe to this queue, pull tasks as they become available, and execute them.\n   - Each task will carry metadata that includes task ID, submission timestamp, and requirements for task execution.\n\n2. **Monitoring and Termination Notification:**\n   - Each node will publish a heartbeat signal to a health-check topic to indicate it's alive and process-capable.\n   - Upon task completion or failure, the worker node will publish a termination event to a dedicated termination topic.\n   - All nodes (including the one that will handle client notifications) will subscribe to this termination topic.\n\n3. **Client Notification:**\n   - Clients blocking on task completion notification can register a callback or subscribe to a specific client response queue.\n   - The node handling the termination event will publish a notification to the corresponding client's response queue.\n\n4. **Redundancy and Failover:**\n   - In case a node fails, other nodes will detect the lack of heartbeat signals and reassign tasks of a failed node, ensuring no task remains in an undetermined state.\n   - A persistent state store (using distributed storage like Apache Kafka log files or a shared database) will ensure that tasks progress and termination states are not lost during node failures.\n\nBy utilizing this distributed, decoupled, and scalable approach to task management and event notification, we ensure that tasks are efficiently distributed across the cluster, and clients are reliably informed of task status changes.","GenTime":"2024-07-28 23:31:24"}
{"File Name":"james\/0015-objectstorage-blobid-list.md","Context":"## Context\\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\\nHowever writing in Object storage:\\n- Takes time\\n- Is billed by most cloud providers\\nThus choosing a right strategy to avoid writing blob twice is desirable.\\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\\n\n## Decision\n","Decision":"Rely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":166,"id":2122,"Prediction":"## Decision\n\n### Implement Deduplication Mechanism for Blob Storage ###\n\nGiven the context that the mail traffic is heavily duplicated and the existing ObjectStorage can incur significant cost and performance penalties due to redundant writes, the following decisions are made:\n\n1. **Introduce a Hash-Based Deduplication Layer:**\n   - Implement a hashing mechanism to generate unique identifiers for each blob (using cryptographic hash functions such as SHA-256).\n   - Before writing a blob, compute its hash and check whether the blob with the same hash already exists in the ObjectStorage.\n   - If the hash exists, skip writing the blob and use the already stored reference.\n\n2. **Cache Recent Hashes:**\n   - Maintain an in-memory cache of recently stored blob hashes to quickly determine the presence of already stored blobs, therefore reducing the number of expensive `exist` calls to ObjectStorage.\n\n3. **Optimize `Exist` Checks:**\n   - Even though the `exist` method in ObjectStorage (OpenStack Swift) is not efficient, optimize its usage by batching `exist` checks where feasible or implementing asynchronous checking to minimize performance impact on the main application.\n\n4. **Fallback Mechanism:**\n   - In cases where hash collisions occur (though rare with cryptographic hashes), implement a fallback mechanism to handle such scenarios, ensuring data integrity by differentiating blobs with valid differentiators (such as metadata).\n\n5. **Logging and Monitoring:**\n   - Implement detailed logging and monitoring to track the deduplication performance and savings, to provide insights for further optimization.\n\nBy adopting a deduplication strategy based on hashing and optimizing the `exist` checks, we can significantly reduce redundant writes, thereby improving performance and lowering costs.","GenTime":"2024-07-28 23:31:29"}
{"File Name":"james\/0002-make-taskmanager-distributed.md","Context":"## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n\n## Decision\n","Decision":"Create a distribution-aware implementation of `TaskManager`.\\n","tokens":102,"id":2127,"Prediction":"## Decision\n\nWe will introduce a centralized `TaskManager` that operates at the cluster level. This new `TaskManager` will manage the orchestration, scheduling, execution, and monitoring of tasks across all nodes in the cluster. The centralized `TaskManager` will provide a unified view and control over all tasks within the cluster, allowing us to distribute workloads and manage tasks efficiently.\n\nTo achieve this, we will:\n\n1. **Central Task Repository**: Design and implement a central repository where task metadata and states can be stored and accessed by any node in the cluster.\n\n2. **Task Scheduling and Execution**: Develop a mechanism where tasks can be scheduled on any node within the cluster based on resource availability and load balancing considerations. Tasks scheduled from any node will be queued into this central repository and picked up by the most appropriate node.\n\n3. **Task Orchestration and Monitoring**: Enhance the `TaskManager` to support centralized monitoring and orchestration capabilities. This component will be responsible for querying the central repository to monitor task states, reassign failed tasks, and provide visibility into task progress across the entire cluster.\n\n4. **APIs for Task Management**: Expose APIs that allow listing, accessing, and managing tasks across the cluster, which can be used by tools or users to interact with tasks seamlessly.\n\n5. **Fault Tolerance**: Ensure the system is fault-tolerant by replicating the central repository data across multiple nodes and implementing failover mechanisms.\n\nBy centralizing the `TaskManager` and making it aware of all tasks in the cluster, we will achieve an organized and efficient way to handle tasks, improve resource utilization, and enhance overall system reliability and observability.","GenTime":"2024-07-28 23:31:46"}
{"File Name":"james\/0042-james-cli-based-on-webadmin.md","Context":"## Context\\nJames servers offer a command-line interface in order to interact with the server. However, it relies on the JMX protocol, which is known to be insecure. The JMX server embedded in Apache James, also used by the command line client is exposed to a java de-serialization issue according to [NVD-CVE-2017-12628 Detail](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628), and thus can be used to execute arbitrary commands.\\nBesides, the current CLI interface is also not optimal for users. It places actions in front of entities with contiguous syntax, making it harder for the user to remember the command (for example, which entity the GET action command can interact with). If we design to place the entity first and the outgoing actions can interact with that entity afterward, the user will easily imagine what he\/she can do with each entity. This creates an intuitive interface that is easier to remember.\\nWebadmin APIs use HTTP protocol, which is more secure than JMX protocol to interact with James servers.\\nWebadmin command-line interface is an upcoming replacement for the outdated, security-vulnerable JMX command-line interface.\\n\n## Decision\n","Decision":"We decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.\\n* What libraries will we use?\\n* http client: ***Feign library***. We used it as an http client in other parts of James so we continue to use it.\\n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to the follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.\\n* How will we limit breaking changes this new CLI will cause?\\n* Work on a wrapper to adapt the old CLI API.\\n* Where will we locate this cli code?\\n* server\/protocols\/webadmin-cli\\n* Write a man page.\\n* Picocli generates beautiful documentation for our CLI (HTML, PDF and Unix man pages).\\n* We decided to adopt a more modern, modular CLI syntax:\\n```\\n$ .\/james-cli [OPTION] ENTITY ACTION {ARGUMENT}\\n```\\nwhere\\nOPTION: optional parameter when running the command line,\\nENTITY: represents the entity to perform action on,\\nACTION: name of the action to perform,\\nARGUMENT: arguments needed for the action.\\n#### Examples\\nAdd a domain to the domain list.\\n```\\n$ .\/james-cli --url http:\/\/127.0.0.1:9999 domain create domainNameToBeCreated\\n```\\nIn above command-line\\nOPTION: --url http:\/\/127.0.0.1:9999\\nENTITY: domain\\nACTION: create\\nARGUMENT: domainNameToBeCreated\\n","tokens":244,"id":2133,"Prediction":"## Decision\n\nWe will replace the current JMX-based command-line interface (CLI) with a new CLI that interacts with the James servers using Webadmin APIs over HTTP.\n\n### Key Points:\n1. **Security Improvement**:\n   - Transitioning to Webadmin APIs over HTTP mitigates the security vulnerabilities associated with the JMX protocol, specifically addressing the known java de-serialization issue as outlined in [NVD-CVE-2017-12628](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628).\n   - HTTP is a more secure protocol compared to JMX and provides a more robust mechanism to handle server interactions securely.\n\n2. **Usability Enhancement**:\n   - We will redesign the CLI to place entities first followed by actions, making the interface more intuitive.\n   - This pattern will make it easier for users to remember commands and understand the actions that can be performed on each entity.\n\n3. **Adoption of Webadmin APIs**:\n   - Utilize the existing Webadmin APIs to build the new CLI, ensuring that the command line tool remains aligned with ongoing improvements and updates to the James server's RESTful API interfaces.\n\n4. **Implementation Strategy**:\n   - Develop a transitional period during which both the JMX and Webadmin-based CLIs are available, allowing users to gradually move to the new interface.\n   - Provide comprehensive documentation and migration guides to support users in transitioning to the new CLI.\n\n5. **Future-proofing**:\n   - Ensuring that the CLI is extensible in its design so that future enhancements and additional features can be easily integrated.\n   - Aim to achieve high alignment with modern CLI design standards to ensure long-term usability and maintainability.","GenTime":"2024-07-28 23:32:12"}
{"File Name":"james\/0013-precompute-jmap-preview.md","Context":"## Context\\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\\n\n## Decision\n","Decision":"We should pre-compute message preview.\\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\\nWe should have a Cassandra and memory implementation.\\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore\\nis idempotent and the task can be run in live without any concurrency problem.\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":75,"id":2134,"Prediction":"## Decision\n\nTo enhance performance and reduce processing overhead, we will implement server-side caching for the JMAP message preview property.\n\n### Details:\n1. **Cache Implementation**:\n    - A server-side caching layer will be introduced to store the preview property of JMAP messages.\n    - The cache will store the first 256 characters of meaningful text for each message.\n    \n2. **Cache Updates**:\n    - Previews will be generated and cached at the time of message receipt.\n    - Any changes to the message contents (such as edits or updates) will trigger an update to the cached preview.\n\n3. **Cache Expiration**:\n    - An appropriate expiration policy will be established to ensure that outdated previews do not persist indefinitely in the cache. This could involve time-based expiration and\/or validation on message access.\n\n4. **Retrieval Optimization**:\n    - When a message listing is requested, the system will first check the cache for the preview of each message.\n    - If a preview is available in the cache, it will be served directly from the cache.\n    - If a preview is not available or is deemed expired, the process of retrieving the full message body, parsing it using MIME parsers, and extracting meaningful text will be performed, after which the new preview will be stored in the cache.\n\n5. **Scale Considerations**:\n    - The caching system will be designed to efficiently handle the volume of previews expected based on user activity.\n    - Potential use of distributed caching solutions to handle high scalability requirements.\n\nBy implementing caching for the JMAP message preview property, we can significantly reduce the processing load and improve the response time for message listings in JMAP clients. This change aims to enhance user experience by providing quicker access to message previews while lowering the system\u2019s computational burden.","GenTime":"2024-07-28 23:32:16"}
{"File Name":"james\/0004-distributed-tasks-listing.md","Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\\n\n## Decision\n","Decision":"* Read a Cassandra projection to get all `Task`s and their `Status`\\n","tokens":32,"id":2140,"Prediction":"## Decision\nWe will implement a centralized task registry to manage the listing of all tasks running on the cluster. This registry will collect and maintain the status and metadata of each task from individual nodes in real-time. Nodes in the cluster will be responsible for notifying the centralized registry whenever a new task is started or an existing task is terminated. The registry will be exposed via an API that allows clients to query the current state of all tasks running across the cluster.\n\n### Rationale\n1. **Consistency**: A centralized registry ensures a single source of truth for all task data, improving consistency and reducing the complexity of querying multiple nodes directly.\n2. **Scalability**: The centralized approach allows individual nodes to focus on task execution rather than maintaining and serving lists of tasks.\n3. **Simplicity**: Clients interact with a single endpoint to retrieve task information, simplifying the implementation and usage of task listing functionality.\n4. **Real-time Updates**: The registry model supports real-time updates, providing accurate and timely task information while reducing latency and overhead on individual nodes.\n\n### Implementation Considerations\n- **Fault Tolerance**: Implement redundancy and failover mechanisms for the central registry to ensure high availability.\n- **Performance**: Optimize the registry's data storage and retrieval mechanisms to handle high volumes of task data efficiently.\n- **Security**: Secure the registry API to prevent unauthorized access and ensure data integrity.\n\n### Alternative Considered\nA decentralized task listing where each node maintains and serves its task list was considered but was found to increase complexity and reduce consistency, making it less favorable compared to a centralized solution.","GenTime":"2024-07-28 23:32:36"}
{"File Name":"james\/0025-cassandra-blob-store-cache.md","Context":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n\n## Decision\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2145,"Prediction":"## Decision\n\nTo address the varying sizes and access patterns of the data being stored and to reduce latency, we will implement a hybrid storage strategy utilizing both Object Storage and Cassandra based on the type and access pattern of the data:\n\n1. **Mailbox Message Headers**:\n   - **Storage Medium**: Cassandra\n   - **Reasoning**: Headers are small in size and require frequent access. Storing them in Cassandra, which has a low read latency (~4ms), will improve performance for operations such as IMAP FETCH and JMAP message listing.\n\n2. **Mailbox Message Bodies**:\n   - **Storage Medium**: Object Storage\n   - **Reasoning**: Bodies can vary in size from small to large and are accessed infrequently. Object Storage is well-suited for storing large blobs efficiently.\n\n3. **DeletedMessageVault Message Headers**:\n   - **Storage Medium**: Cassandra\n   - **Reasoning**: Although headers are small and access is infrequent, using Cassandra ensures a consistent and low-latency retrieval process.\n\n4. **DeletedMessageVault Message Bodies**:\n   - **Storage Medium**: Object Storage\n   - **Reasoning**: Similar to Mailbox message bodies, the storage of these bodies in Object Storage is appropriate given their potential size and infrequent access pattern.\n\nBy implementing this hybrid approach, we aim to optimize the storage performance for different types of data, ensuring faster access times where needed and efficient storage handling for larger data blobs.","GenTime":"2024-07-28 23:32:56"}
{"File Name":"james\/0040-quality-levels-definitions.md","Context":"## Context\\nWe hereby define as an artifact compiled artifact that external people consumes. This includes:\\n- libraries\\n- Mail servers\\n- Extensions for James Mail Servers\\n- Command line tools\\nWe designate as a feature an optional, opt-in behaviour of a James server that can be configured by\\nuser willing to rely on it.\\nJames as a project delivers several artifacts, and features. In order for project users to better\\nunderstand the underlying quality of the artifact they use, as well as the level of risk associated,\\nwe need to better define some quality levels.\\n\n## Decision\n","Decision":"For a given artifact or feature, by **mature** we mean that:\\n- *interfaces* in components need a contract test suite\\n- *interfaces* have several implementations\\n- *implementation* of these interfaces need to pass this contract test suite which provides unit tests\\n- Decent integration tests coverage is needed\\n- Performance tests need to be conducted out\\n- Quality Assurance with external clients needs to be conducted out\\n- known existing production deployments\/usages\\n- usable documentation\\nThis is the maximum quality level delivered by the James project. Users should feel confident using these\\nartifacts or features.\\nBy **experimental** we designate an artifact or feature not matching yet the above requirements. However some\\nactive contributors are willing to raise the quality level of this component, and eventually make it\\nmature. Or at least are willing to support users.\\nUsers should have low expectations regarding experimental artifacts or features. They are encouraged to contribute to them\\nin order to raise its quality.\\nBy **unsupported** we mean that an artifact or feature do not match most of the *mature* quality conditions. Active\\ncontributors do not feel confident delivering support for it. This artifact or feature might be deprecated and\\nremoved from future James releases. Users are strongly encouraged to contribute to the artifact development.\\n","tokens":118,"id":2150,"Prediction":"## Decision\n\nWe will introduce a structured quality classification for the various artifacts and features provided by the James project. This classification will inform users about the maturity, stability, and risk associated with each artifact or feature, allowing them to make more informed decisions based on their specific requirements and tolerance for risk.\n\n### Quality Levels\n\n1. **Experimental**\n   - **Description:** New or significantly changed artifacts and features that are undergoing active development and stabilization. These are not recommended for use in production environments as they may contain bugs or incomplete functionalities.\n   - **Who should use it:** Developers and early adopters interested in contributing to the project by testing and providing feedback.\n\n2. **Beta**\n   - **Description:** Artifacts and features that have progressed beyond the experimental phase but are still subject to significant change. These are near completion but may still contain minor issues.\n   - **Who should use it:** Users who need to evaluate or integrate upcoming features and can tolerate some instability or minor issues.\n\n3. **Stable**\n   - **Description:** Artifacts and features that are considered reliable and have been thoroughly tested. They are static or change minimally to address bugs or security vulnerabilities.\n   - **Who should use it:** General users looking for dependable functionality with minimal risk in their production environments.\n\n4. **Deprecated**\n   - **Description:** Artifacts and features that are being phased out and will be removed in future releases. Users are advised to transition to alternatives.\n   - **Who should use it:** Users maintaining existing systems that depend on these artifacts and features should plan for migration.\n\n### Implementation Plan\n\n1. **Labeling Artifacts and Features:**\n   - We will classify each current artifact and feature into one of the quality levels above.\n   - Documentation and release notes will explicitly state the quality level of each artifact and feature.\n\n2. **Communication:**\n   - Ensure project communication (website, blogs, user guides) clearly explains the quality levels and their implications.\n   - Regularly update the community and stakeholders about changes in quality levels for any artifact or feature.\n\n3. **Review and Feedback Mechanism:**\n   - Establish a process for ongoing review of artifacts and features, allowing for reclassification as needed based on user feedback and internal assessments.\n   - Provide channels for users to report issues and experiences to help improve the quality classification.\n\n4. **Adoption and Compliance:**\n   - Integrate quality level definitions into the development lifecycle, ensuring new artifacts and features are assessed and assigned an appropriate quality level as part of the release process.\n   - Encourage compliance through automated checks and developer training.\n\nBy implementing this structured quality classification system, we aim to enhance user confidence, improve the transparency of our artifacts and features, and facilitate better decision-making for both developers and end users.","GenTime":"2024-07-28 23:33:18"}
{"File Name":"stentor\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2166,"Prediction":"## Decision\nWe have decided to implement an Architectural Decision Record (ADR) process for documenting the architectural decisions made throughout the project. This involves:\n\n1. **Template Creation**: Developing a standardized template for recording decisions, covering aspects such as context, decision, alternatives considered, consequences, and related decisions.\n\n2. **Repository**: Setting up a dedicated repository or section within our project documentation where all ADRs will be stored and managed. This will ensure easy access and reference for all team members.\n\n3. **Decision-making Meetings**: Conducting regular meetings dedicated to discussing and documenting architectural decisions. These meetings will include key stakeholders and subject matter experts.\n\n4. **Approval Workflow**: Establishing a workflow for reviewing and approving ADRs. This ensures that each decision is vetted and agreed upon by the necessary parties before implementation.\n\n5. **Versioning and Tracking**: Implementing version control for ADRs to track changes and updates over time. Each ADR will have a unique identifier and version history.\n\n6. **Integration with Project Management Tools**: Integrating ADR documentation with our project management and collaboration tools to ensure visibility and alignment with ongoing project activities.\n\n7. **Training and Awareness**: Providing training and resources to the team on the importance of ADRs and how to effectively document and utilize them.\n\nBy adopting this process, we aim to enhance transparency, facilitate better decision-making, and maintain a historical record of architectural decisions that can be referenced throughout the project lifecycle.","GenTime":"2024-07-28 23:34:26"}
{"File Name":"opg-data\/0005-content-structure.md","Context":"## Context\\nA consistent and well-defined document specification is required so that we may develop an API contract\\n\n## Decision\n","Decision":"Our structure closely follows the [JSON-API](https:\/\/jsonapi.org\/format\/#document-structure) document structure\\n### Document Root Level\\nAt the root level is always a JSON object.\\nThis **MUST** contain at least one of the following root-level members:\\n* data: the document's \"primary data\" resource object\\n* A single resource object is represented by a JSON object\\n* A collection or resource objects is represented by an array of objects\\n* errors: an array of error objects\\n#### Single resource object\\n```json\\n{\\n\"data\": {},\\n\"meta\": {}\\n}\\n```\\n#### Collection of resource objects\\n```json\\n{\\n\"data\": [\\n{...},\\n{...}\\n],\\n\"errors\": [],\\n\"meta\": {},\\n\"links\": {}\\n}\\n```\\nJSON-API states\\n> The members data and errors MUST NOT coexist in the same document.\\n`opg-data` standard is that if a data resource OBJECT is returned, then there **MUST be no** error member.\\nHowever there ARE certain circumstances where an array of errors **MAY** be returned alongside a data resource COLLECTION. See [0007-error-handling-and-status-codes.md#errors-in-20x](0007-error-handling-and-status-codes.md#errors-in-20x)\\nThe root JSON object **MAY** also contain the following root-level members:\\n* meta: a meta object that contains non-standard meta-information\\n* links: a links object related to the primary data (typically used for pagination links if the data returned is a collection)\\n### The Resource Object\\nSee [JSON-AP](https:\/\/jsonapi.org\/format\/#document-resource-objects)\\nNamely:\\n* As a minimum, every resource object **MUST** contain:\\n* an id member\\n* a type member. The values of the id and type members **MUST** be strings. For consistency and the avoidance of confusion, types **MUST** use PLURAL. eg \"articles\", \"people\"\\n* an array of attributes (even if empty)\\n* IF the \"data\" top level member is present, then there **MUST** be a `links` array, containing as it's minimum, a `self` member with a URL which **MUST** be callable at the API\\n* A resource object's data attributes is presented in an array named \"attributes\"\\n* A resource object's links is presented in an object named \"links\"\\n* A resource object's relationships is presented in an array named \"relationships\"\\n* A resource object's attributes and its relationships are collectively called its \"fields\"\\n```json\\n{\\n\"data\": [\\n{\\n\"type\": \"articles\",\\n\"id\": \"1\",\\n\"attributes\": {\\n\"title\": \"My First Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/1\",\\n\"next\": \"https:\/\/api.example.com\/articles\/2\"\\n},\\n\"relationships\": {\\n...\\n}\\n},\\n{\\n\"type\": \"articles\",\\n\"id\": \"2\",\\n\"attributes\": {\\n\"title\": \"Second Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/2\",\\n\"prev\": \"https:\/\/api.example.com\/articles\/1\"\\n},\\n\"relationships\": {\\n...\\n}\\n}\\n],\\n\"links\": {},\\n\"meta\": {}\\n}\\n```\\n#### Relationships\\nRelationships will be defined in a separate ADR\\n### Meta\\nSee [JSON-API](https:\/\/jsonapi.org\/format\/#document-meta)\\n### Errors\\nErrors responses will be defined in a separate ADR\\n","tokens":21,"id":2181,"Prediction":"## Decision\nWe will adopt the OpenAPI Specification (OAS) for defining our API contracts. \n\n### Justification:\n1. **Consistency and Clarity**: The OpenAPI Specification is a widely accepted standard for API documentation, offering a clear and structured format for defining endpoints, request\/response schemas, and other essential API details.\n   \n2. **Tooling and Automation**: With a well-defined OAS contract, we can leverage various tools for API development, testing, documentation generation, and client SDK generation. This will accelerate development and integration cycles.\n   \n3. **Interoperability**: Using OAS ensures that our API can be easily understood and consumed by clients from different technologies and platforms, promoting broader adoption and smoother integration.\n\n4. **Future-proofing**: Adopting a standardized specification like OAS facilitates easier updates and maintenance as the API evolves over time.\n\n5. **Collaboration**: The clear, machine-readable format of OAS enables better collaboration between developers, stakeholders, and third-party users, ensuring everyone has a consistent understanding of the API capabilities and usage.\n\n### Implementation Steps:\n1. **Define API Endpoints**: Outline all the necessary endpoints and their respective operations.\n2. **Schema Definitions**: Describe request parameters, response structures, and error messages using JSON Schema.\n3. **Documentation**: Utilize tools like Swagger UI to generate interactive API documentation from the OAS file.\n4. **Version Control**: Maintain the OAS file in the project's version control system to track changes and facilitate team collaboration.\n5. **Integration and Testing**: Leverage tools like Postman or custom scripts to automate testing and validation against the OAS contract.\n\nBy adopting the OpenAPI Specification, we ensure our API is robust, well-documented, and easy to use for both internal and external developers.","GenTime":"2024-07-28 23:35:14"}
{"File Name":"opg-data\/0009-api-domain-structure.md","Context":"## Context\\nWe need to establish a domain structure which\\n* Is product agnostic\\n* Is consistent across the opg-data service\\n\n## Decision\n","Decision":"We will adopt the pattern:\\n[pull-request-id].[account-stage].[microservice-domain].api.opg.service.justive.gov.uk\\nWhere [pull-request-id] is for ephemeral dev environments.\\nWhere [account-stage] is the stage in our path to live\/accounts i.e. dev,pre and empty for prod services.\\nWhere [microservice-domain] is the scoped domain of the microservice\/integration in in question.\\n* hey\\n### examples\\n#### root:\\n* https:\/\/api.opg.service.justice.gov.uk\\n#### integration:\\n* https:\/\/deputy-reporting.api.opg.service.justice.gov.uk\\n#### environments per integration:\\n* https:\/\/pre.deputy-reporting.api.opg.service.justice.gov.uk\\n* https:\/\/dev.deputy-reporting.api.opg.service.justice.gov.uk\\n#### pr raised on an environment per integration:\\n* https:\/\/pr-1234.dev.deputy-reporting.api.opg.service.justice.gov.uk\\n","tokens":28,"id":2183,"Prediction":"## Decision\nWe will adopt a standardized domain-driven design (DDD) approach to establish a product-agnostic, consistent domain structure across the opg-data service. This includes:\n\n1. **Universal Language & Terminology**: Define and implement a ubiquitous language across the service to ensure that terms and concepts are understood by all stakeholders in the same way.\n\n2. **Modular Domain Structure**: Design the domain in a modular fashion, using bounded contexts to divide the domain model into smaller, independent, and cohesive parts.\n\n3. **Microservices Architecture**: Implement the domain structure as a collection of microservices, each responsible for a specific bounded context, allowing for better scalability and maintainability.\n\n4. **Shared Kernel**: Identify and implement a shared kernel for common domain elements that are essential and common across multiple services, ensuring reusability without redundant implementation.\n\n5. **API Gateway**: Use an API Gateway to expose and manage the domain services, providing a single entry point and ensuring consistent access patterns across the opg-data service.\n\n6. **Documentation & Guidelines**: Create thorough documentation and clear design guidelines to ensure consistency and understanding throughout the development and maintenance processes.\n\nBy adopting these principles and approaches, we ensure the domain structure remains product-agnostic and consistent, thus facilitating easier adaptation and maintenance.","GenTime":"2024-07-28 23:35:22"}
{"File Name":"opg-data\/0008-hateoas.md","Context":"## Context\\nWe wish to build a RESTful API, and to be technically RESTful, we must implement HATEOAS. This gets us to stage 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n\n## Decision\n","Decision":"Our API is to be as RESTful as it can be.\\nWe recognise that we must implement Hypermedia as the Engine of Application State.\\nThe content structure already discussed in [0005-content-structure.md](0005-content-structure.md) implements links within our resource object... this ADR is to explicitly state our aim is to implement HATEOAS.\\nFurthermore, for every endpoint on the API we will implement an OPTIONS verb, returning every action possible at that endpoint.\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","tokens":88,"id":2184,"Prediction":"## Context\nWe wish to build a RESTful API, and to be technically RESTful, we must implement HATEOAS. This gets us to stage 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\n\n## Decision\nWe will implement HATEOAS (Hypermedia as the Engine of Application State) in our RESTful API. This means that every resource representation returned by our API will include hypermedia links that enable clients to navigate the application's state dynamically. This decision will elevate our API to level 3 of the Richardson Maturity Model, ensuring greater flexibility, discoverability, and adherence to RESTful principles.\n\n### Justification:\n1. **REST Compliance**: Achieving full REST compliance enhances the self-descriptive nature of our API, making it more intuitive and easier to consume.\n2. **Client Simplicity**: By incorporating hypermedia links, the client does not need to know the structure of the API beforehand, reducing client-side complexity.\n3. **Scalability and Evolution**: HATEOAS enables easier evolution and scaling of the API as changes can be decoupled from the client logic, ensuring backward compatibility and smoother transitions.\n4. **Enhanced Navigation**: Clients get navigable links that guide them through available actions and related resources, improving developer experience and productivity.\n\n### Actions:\n1. **Design**: Define a standardized approach to embedding hypermedia links in resource representations.\n2. **Implementation**: Update all resource endpoints to include appropriate HATEOAS links.\n3. **Documentation**: Clearly document the hypermedia-driven interactions to assist client developers.\n4. **Testing**: Rigorously test to ensure that the links are dynamic and correctly guide the client through the application state.\n\n### Potential Challenges:\n1. **Initial Overhead**: Implementing HATEOAS might introduce additional development overhead initially.\n2. **Client Adaptation**: Clients will need to be designed or updated to take full advantage of the hypermedia-driven navigation.\n\n### Mitigation Strategies:\n1. **Incremental Adoption**: Start with key resources and incrementally introduce HATEOAS across the API.\n2. **Education and Support**: Provide detailed guides, examples, and support to client developers to facilitate smooth adoption.","GenTime":"2024-07-28 23:35:27"}
{"File Name":"macondo\/0003-remote-repositories-management.md","Context":"## Context\\nRight now this cli tool comes with a builtin set of commands that is hardcoded\\nto a gist I own somewhere. This is not ideal.\\nWe want this tool to easily access \"repositories of commands\" that are either\\nremote or local; as well as a way to easily add\/remove repositories.\\nSome ideas of repositories:\\n- HTTP urls pointing to yaml manifests with commands definitions\\n- Github repositories\\n- Local files or directories\\n\n## Decision\n","Decision":"- We won't have built-in commands or repositories\\n- We will support three types of repositories:\\n- Remote http resources pointing to yaml files\\n- Local manifest yaml files\\n- Local directories containing .mcd files\\nBecause http resources are now supported, we could make it so that the\\nrepositories with commands has CI\/CD via Jenkins, which generates a repo and\\npublishes it to artifactory.\\nThe macondo file will be simplified to only include, for now, a top-level\\n`repositories` array, which would point to the repositories.\\nA macondo update command will be added that refreshes the list of commands\\nprovided by the repositories. This is only done for http repos, whose\\ncontents could change anytime.\\nA new set of commands to be implemented:\\n- `macondo repo add` to add repositories\\n- `macondo repo remove` to remove them\\n- `macondo repo update` to update them\\n- `macondo repo list` to list them\\n","tokens":92,"id":2191,"Prediction":"## Decision\n\nWe have decided to implement a flexible and extensible repository system for the CLI tool to support the addition and removal of command repositories. This system will accommodate various types of repositories, including remote URLs, GitHub repositories, and local files\/directories.\n\n### Key Components\n\n1. **Repository Interface**: Define a common interface for all repository types. This interface will include methods for listing commands, fetching command definitions, and updating repositories.\n\n2. **Repository Types**:\n    - **HTTP URLs**: Support HTTP(S) endpoints that provide YAML manifests with command definitions.\n    - **GitHub Repositories**: Enable pulling command definitions from specified branches or tags in GitHub repositories.\n    - **Local Files\/Directories**: Allow the inclusion of local files or directories containing command definition YAML files.\n\n3. **Repository Management**:\n    - **Configuration File**: Introduce a configuration file (e.g., `repositories.yaml`) to maintain a list of repositories. This file can be updated to add or remove repositories.\n    - **CLI Commands**: Provide CLI commands to add, list, and remove repositories easily.\n\n4. **Caching and Updating**:\n    - Implement a caching mechanism to store fetched command definitions locally and reduce the need for repeated fetching.\n    - Provide a command to manually refresh the repository cache.\n\n### Implementation Plan\n\n1. **Define Repository Interface**: Create an abstract class or interface that standardizes the operations required for any repository.\n2. **Implement Repository Types**: Develop classes that implement the repository interface for:\n    - HTTP URL repositories.\n    - GitHub repositories.\n    - Local files\/directories.\n3. **Update Configuration**: Develop functionality to read and update the `repositories.yaml` configuration file.\n4. **CLI Commands**: Add commands `repo-add`, `repo-list`, and `repo-remove` to manage repositories through the CLI tool.\n5. **Caching Mechanism**: Design and implement a caching strategy to store fetched command definitions locally.\n\n### Rationale\n\n- **Flexibility**: This approach supports multiple types of repositories, making it easier to extend and support new repository types in the future.\n- **Manageability**: By using a configuration file and providing CLI commands, users can easily manage their repositories.\n- **Scalability**: The system can efficiently handle a large number of commands by fetching only when necessary and caching results.\n\nThis decision will significantly improve the usability and maintainability of the CLI tool, equipping it with a robust and flexible command repository system.","GenTime":"2024-07-28 23:35:54"}
{"File Name":"macondo\/0007-automatic-aligning-of-user-host-user-for-better-interoperability.md","Context":"## Context\\nA well constructed macondo command should be easy to run directly (without\\nmacondo) by just executing it. This is hard to achieve unless the user in the\\ncontainer resembles as much as possible the host running it.\\nOne case where this is important is modifying files in the host. This is\\nachieved by mounting a volume into the running container. However, the files\\nwritten from the docker container are owned by the docker user, i.e. the user id\\nand group id of the file is that of whatever docker user happened to write it.\\nSo if the container runs with the root user, which is unfortunately common, then\\nthe files written to the host also are owned by root, making them innacessible\\nto the host user.\\nPotential ideas:\\n- A flag that enables user alignment and mounts HOME into docker's HOME.\\n- Customize home when running in OSX to be \/Users\/bla instead of \/home\/ble\\n- Even if mounting the whole home, it should be easy to mount current PWD into\\nsomething else \/mnt\/blablabla and use tha as working directory\\n\n## Decision\n","Decision":"Add a mechanism to align the user\/group of the host system with that of the docker container.\\n","tokens":237,"id":2193,"Prediction":"## Decision\n\nWe will implement the following solution to address the issue of user alignment between the host and the container:\n\n1. **User Alignment Flag**: Introduce a flag (e.g., `--user-align`) that, when enabled, aligns the user IDs (UIDs) and group IDs (GIDs) between the host and the container. This ensures that any files written from within the container will have the same ownership attributes as the host user.\n\n2. **Mount HOME Directory**: Modify the behavior of the container to optionally mount the host's HOME directory into the container's HOME directory. This can be controlled using another flag (e.g., `--mount-home`). For macOS users, map the host's HOME directory (typically `\/Users\/username` on macOS) to a suitable directory within the container (like `\/Users\/username`), instead of the default `\/home\/username`.\n\n3. **Mount Current Working Directory**: Allow users to mount the current working directory (PWD) of the host to a specified directory within the container. This can be handled with a flag (e.g., `--mount-pwd <container-directory>`). This will facilitate context-specific operations without the need for reconfiguring the entire home directory.\n\n4. **Non-Root User Default**: By default, discourage running containers as the root user unless explicitly required. Instead, use a non-root user (aligned with the host's UID and GID) to prevent the problem of root-owned files on the host filesystem.\n\n5. **Working Directory Option**: Enable specification of a working directory within the container using an option (e.g., `--work-dir <container-directory>`). This directory can be different from the mounted home or PWD, ensuring flexibility in execution and working context.\n\nThese changes will closely align the user environment inside the container with that of the host, thereby enhancing ease of use and reducing file permission issues when interacting with files on the host system.","GenTime":"2024-07-28 23:36:01"}
{"File Name":"connecting-to-services\/0015-add-info-page.md","Context":"## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n\n## Decision\n","Decision":"The application will provide an info page.\\n","tokens":55,"id":2198,"Prediction":"## Decision\nTo aid in debugging and ensure clarity on the deployed version and current time, the application will include two key features:\n\n1. **Version Display**: The deployed version of the application will be clearly displayed within the application's user interface. This can typically be seen in the footer, header, or an \"About\" section. Moreover, the version information will also be included in the application's logs and diagnostic outputs.\n\n2. **UTC Time Display**: The current UTC time will also be made easily accessible within the application's user interface, which can be useful for debugging issues related to date and time. This time will be displayed in a consistent and easily readable format, and it will also be logged alongside other relevant information.\n\nBy implementing these features, developers, and support personnel will have a clearer picture of what version is being used and the current system time in UTC, thereby facilitating more effective debugging and troubleshooting.","GenTime":"2024-07-28 23:36:16"}
{"File Name":"connecting-to-services\/0003-use-postcodes-io-for-postcode-lookup.md","Context":"## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat\/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat\/lon that can be used to query against.\\n\n## Decision\n","Decision":"[Postcodes.io](https:\/\/postcodes.io\/) provides both an out-code and a postcode\\nlookup. The results of which return latitude and longitude. The service is\\nfree, [supported](https:\/\/postcodes.io\/about),\\n[monitored](https:\/\/status.ideal-postcodes.co.uk\/) and contains the full set of\\nactive postcodes for the UK. Supplied by the\\n[ONS](https:\/\/data.gov.uk\/dataset\/national-statistics-postcode-lookup-uk).\\n","tokens":68,"id":2204,"Prediction":"## Decision\n\nThe application will integrate a geocoding service to convert postcodes into lat\/lon coordinates. This service will be used to resolve the user-provided out-code or postcode to a lat\/lon format which can then be utilized to find the nearest services.\n\n### Rationale\n\n1. **Accuracy**: Geocoding services provide accurate conversion of postcodes to geographical coordinates, which will enable precise service searches.\n2. **Scalability**: As the application scales and receives a higher volume of searches, leveraging an external geocoding service will help maintain performance and reliability.\n3. **Maintainability**: Using a third-party service simplifies the codebase by offloading the complexity of geocoding, making the application easier to maintain.\n4. **Flexibility**: Integrating a geocoding service allows for potential future support of additional geographical search formats without significant changes to the core application logic.\n\n### Implementation Plan\n\n1. **Research and Select Geocoding Service**: Evaluate and select a suitable geocoding service (e.g., Google Maps Geocoding API, OpenCage Geocoding API, Mapbox Geocoding API) considering factors such as cost, performance, and integration complexity.\n2. **Integrate Geocoding API**:\n   - Implement the necessary API calls to the selected geocoding service within the application.\n   - Handle the response from the geocoding service to extract the lat\/lon coordinates.\n3. **Update Search Logic**:\n   - Modify the search logic to use the lat\/lon coordinates obtained from the geocoding service to find the nearest services.\n4. **Error Handling**:\n   - Implement robust error handling to manage potential API call failures or invalid postcodes.\n5. **Testing**:\n   - Conduct thorough testing to ensure that postcodes are correctly resolved to coordinates and that service searches return accurate results.\n6. **Deployment**:\n   - Deploy the updates to production and monitor for any issues or performance impacts.\n\nBy following this plan, the application will be able to accurately and efficiently resolve postcodes to lat\/lon coordinates, thereby enhancing the service search functionality.","GenTime":"2024-07-28 23:36:36"}
{"File Name":"connecting-to-services\/0008-add-basic-authentication-to-test-environments.md","Context":"## Context\\nEvery environment the application is available in, be that development, review,\\nstaging or public are openly available to anybody. There is no access control\\nat all.  This isn't suitable for a site that could be misinterpreted as the\\n'real' version either by people or by search engines and other bots. There\\nneeds to be some form of hurdle to overcome in order to prompt people to\\nconsider whether this is the correct site. There should also be a barrier to\\nprevent access by bots.\\n\n## Decision\n","Decision":"We have decided to use basic authentication on the service in all environments\\napart from the public facing one. This will only be applicable to the\\nenvironments hosted within the Rancher environment. The username and password\\nwill not be secret and will be included within the `README` of the application.\\n","tokens":107,"id":2207,"Prediction":"## Decision\n\nImplement access control measures for all non-public environments (development, review, staging) to restrict unauthorized access and prevent misinterpretation as the 'real' version.\n\n1. **Authentication**:\n   - Use basic authentication (username and password) for accessing non-public environments.\n   - Integrate with the existing authentication system if available, or use a simple but reliable method (e.g., HTTP Basic Auth).\n\n2. **Authorization**:\n   - Only grant access to the necessary team members and stakeholders who need to work in these environments.\n   - Maintain an access control list and regularly review it to ensure that only authorized personnel have access. \n\n3. **Robots Exclusion Protocol**:\n   - Add `robots.txt` files to all non-public environments to disallow crawling and indexing by search engines and bots.\n   - Example:\n     ```\n     User-agent: *\n     Disallow: \/\n     ```\n\n4. **Environment Labels and Notices**:\n   - Clearly label and display notices on all pages in non-public environments to inform users that the environment they are accessing is for development\/review\/staging purposes and not the live production site.\n   - Use a conspicuous banner or watermark indicating the environment status.\n\n5. **IP Whitelisting** (Optional, for added security):\n   - Restrict access to non-public environments to specific IP addresses or ranges, such as those coming from corporate VPNs or office networks.\n\nBy implementing these measures, we establish a controlled, secure access mechanism that mitigates risks related to unauthorized access, phishing, and inadvertent information dissemination.","GenTime":"2024-07-28 23:36:48"}
{"File Name":"fare-platform\/0002-permission-crud-operations-on-records-config.md","Context":"## Context\\nInitially a record could be made by anyone, but the problem is that that record was only some metadata without file attached on it.\\n\n## Decision\n","Decision":"To avoid the situation where a record is created without file, we put `deny_all` permission in fare\/records\/config.py that denies to all the possibilities to create, update and delete this kind of records.\\n","tokens":30,"id":2215,"Prediction":"## Decision\n\nTo ensure that every record created within the system contains all necessary information and associated files, we will implement the following changes:\n\n1. **Mandatory File Attachment**: A new rule will be established in the record creation process. Every new record must have at least one file attached to it. This means users will not be able to save or submit a record until they have attached the required file(s).\n\n2. **Role-Based Access Control**: We will introduce role-based access control (RBAC) to manage who can create records. Roles will include varying levels of permissions, such as Admin, Editor, and Viewer. Only users with appropriate roles (e.g., Admin and Editor) will have the capability to create new records, ensuring that these records are always accompanied by the necessary files.\n\n3. **Form Validation**: Enhance the form used for record creation by implementing validation checks. The form will alert users if they attempt to submit a record without attaching a file, prompting them to complete the required fields before proceeding.\n\n4. **Guidelines and Training**: Provide users with clear guidelines and training on the new process for creating records, emphasizing the importance of attaching relevant files.\n\nBy making file attachment mandatory and restricting the ability to create records to specific roles, we aim to improve the quality and completeness of the data stored, thereby enhancing the overall utility and reliability of the system.","GenTime":"2024-07-28 23:37:11"}
{"File Name":"decodeweb\/0003-store-configuration-into-local-storage.md","Context":"## Context\\nTo provide the required functionality for the pilot we need to store some\\nconfiguration in a persistent way while just operating within a browser\\nenvironment. This state could be written to a backend database, or we could\\njust store into localstorage.\\n\n## Decision\n","Decision":"We will store the user's configuration into localstorage protected by a PIN\\ncode.\\n### Proposed storage structure\\n```json\\n{\\n\"1234\": {\\n\"keypair\": {},\\n\"blindSignature\": {},\\n\"devices\": {\\n\"abc123\": {\\n\"longitude\": 12.2,\\n\"latitude\": 52.4,\\n\"exposure\": \"INDOOR\",\\n\"communities\": {\\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\\n\"credential\": {},\\n\"blindproofCredential\": {}\\n}\\n}\\n}\\n}\\n}\\n}\\n```\\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\\n","tokens":52,"id":2221,"Prediction":"## Decision\nWe have decided to use the browser's localStorage for storing configuration state during the pilot phase.\n\n### Rationale:\n1. **Simplicity**: Using localStorage simplifies the architecture. We avoid the need to set up and manage a backend database, which reduces initial setup time and potential points of failure.\n  \n2. **Performance**: Reading from localStorage is faster than performing server round-trips. This will lead to a more responsive user experience.\n  \n3. **Persistence**: LocalStorage persists data even after the browser is closed, making it suitable for storing configuration data that must be retained across sessions.\n  \n4. **Scope**: The pilot phase requires a quick, lightweight solution. LocalStorage meets the immediacy and scale needs without the overhead of secure server communication and database management.\n  \n5. **Security Considerations**: Since this is a pilot, the potential risks associated with storing configuration data in localStorage (such as lack of encryption) are deemed acceptable for this phase. However, this will be reassessed if the pilot moves into production, where a secure backend solution may be warranted.\n\n### Constraints:\n- LocalStorage has a size limit (typically around 5MB), which should be sufficient for the pilot's configuration data but is a constraint to be aware of.\n- LocalStorage is subject to the Same-Origin Policy, meaning data stored is specific to the given domain.\n\n### Implementation:\n- Configuration state will be written and read from localStorage using standard JavaScript APIs (e.g., `localStorage.setItem()`, `localStorage.getItem()`).\n- Ensure that the configuration data is serialized to JSON for storage and properly parsed when retrieved.\n\n### Re-evaluation:\n- The use of localStorage will be re-evaluated at the end of the pilot phase. If the application is to be scaled or if the data requiring storage becomes more sensitive or voluminous, transitioning to a backend database may be necessary.\n\nThis decision provides a practical and efficient way to store configuration data for the pilot, while allowing flexibility to scale or adapt the approach based on the outcomes and feedback received during the pilot phase.","GenTime":"2024-07-28 23:37:33"}
{"File Name":"cf-k8s-networking\/0007-maintain-generated-istio.md","Context":"## Context \ud83e\udd14\\nCf-k8s-networking was designed to be integrated with\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/). The Istio\\ninstallation used to be maintained by\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/), but the networking\\nteam needed to be able to easily make changes to [Istio](https:\/\/istio.io\/)\\nconfiguration to enable more networking features for [Cloud\\nFoundry](https:\/\/www.cloudfoundry.org\/).\\n\n## Decision\n","Decision":"We decided to move the scripts to build Istio configuration, and maintain a\\ngenerated Istio configuration within the cf-k8s-networking repository.\\nThe build scripts and `ytt` overlays for Istio live in this repo (links removed\\nas they are no longer relevant or accurate). **UPDATE** This configuration has\\nmoved as a result of [ADR\\n017](.\/0017-moving-istio-configuration-out-of-this-repo.md).\\n","tokens":133,"id":2236,"Prediction":"## Decision \ud83d\udcdc\nThe management of the Istio installation should be transitioned from the cf-for-k8s team to the cf-k8s-networking team. This will provide the networking team with full control over the Istio configuration, enabling them to implement and maintain the required networking features for Cloud Foundry more efficiently.\n\n### Justifications\n- **Responsiveness**: The cf-k8s-networking team can implement changes and improvements to Istio configurations more swiftly as they directly address related requests and issues.\n- **Specialization**: The cf-k8s-networking team has specialized knowledge and focus on networking, positioning them better to optimize the Istio configuration for Cloud Foundry's needs.\n- **Independence**: This reduces dependencies and coordination effort with the cf-for-k8s team, streamlining the workflow and reducing potential bottlenecks.\n\n### Implementation Steps\n1. **Knowledge Transfer**: Initiate a comprehensive handover process between cf-for-k8s and cf-k8s-networking teams to ensure seamless transition and continuity of expertise.\n2. **Resource Allocation**: Allocate necessary resources and permissions for the cf-k8s-networking team to manage the Istio installation effectively.\n3. **Documentation Update**: Revise and update all relevant documentation to reflect the changes in responsibility and processes.\n4. **Monitoring and Maintenance**: Establish monitoring and maintenance protocols under the cf-k8s-networking team\u2019s purview to ensure reliability and performance.\n5. **Stakeholder Communication**: Inform all relevant stakeholders about the transition plan, timeline, and points of contact for any Istio-related issues or enhancements.\n\nBy implementing this decision, we will enhance the agility, efficiency, and effectiveness of our networking infrastructure within the Cloud Foundry ecosystem on Kubernetes.","GenTime":"2024-07-28 23:38:20"}
{"File Name":"cf-k8s-networking\/0016-job-for-upgrading-istio-sidecars-on-workloads.md","Context":"## Context\\nIstio's service mesh capabilites are facilitated via sidecars injected into\\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\\nto the version of Istio that injects them.\\nTypically when new versions of Istio are released, new versions of the sidecars\\nare released as well. Istio has been good so far about supporting older versions\\nof sidecars that were deployed before Istio was upgraded, but it is still\\n[documented best practice](https:\/\/istio.io\/latest\/docs\/setup\/upgrade\/) to roll\\nall the pods after an Istio upgrade.\\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\\nother bash scripts. This limits our options considerably. See this [Slack\\nthread](https:\/\/cloudfoundry.slack.com\/archives\/CH9LF6V1P\/p1592521879117400) on\\nthat constraint.\\n\n## Decision\n","Decision":"We will use the kubernetes\\n[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resource\\nto run the kubectl command needed to roll workload pods, after waiting for the\\nnew Istio control plane to be up and healthy.\\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\\nneeded to list resources in the `istio-system` namespace, and restart resources\\nin the configured workload namespace. We will also build and maintain a\\ncontainer image that contains the Job's logic.\\nAll istio components will be tagged with their Istio version so that the job can\\npositively determine that the correct version of control plane components are\\nalive and healthy. We will also name the job according to it's Istio version, so\\nthat we can take advantage of `Jobs` inherent immutability in cases where a\\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\\nagain will not cause it to rerun, preventing workloads from rolling\\nunnecessarily). Subsequent jobs will clean up previous ones.\\n","tokens":224,"id":2240,"Prediction":"## Decision\n\nGiven the constraints and requirements:\n\n1. **Automated Sidecar Rollout Strategy:** Implement an automated strategy for rolling out new sidecars seamlessly during an Istio upgrade within the same `kapp deploy` operation. This ensures compliance with the cluster operators' expectations of having a single-step deployment process.\n\n2. **Leverage Istio's In-Place Upgrade Capabilities:** Utilize Istio's support for in-place upgrades of sidecars to maintain compatibility with older versions initially while starting to roll out new sidecar versions incrementally.\n\n3. **Ensure Backward Compatibility:** Ensure that the new Istio release deployed maintains backward compatibility with the older sidecars currently running. This will provide a seamless transition period where both new and old sidecars can coexist without service disruption.\n\n4. **Monitor and Log Upgrade Process:** Implement stringent monitoring and logging during the deployment process to catch any discrepancies or issues early. This allows for quick rollback or troubleshooting if necessary.\n\n5. **Clear Communication and Documentation:** Provide clear communication, both in release notes and internal documentation, about the upgrade process, the potential impacts, and the steps taken to ensure a seamless rollout to inform all stakeholders.\n\nBy adhering to these decisions, we can achieve a seamless, single-step deployment process that aligns with the constraints and expectations described.","GenTime":"2024-07-28 23:38:36"}
{"File Name":"cf-k8s-networking\/0006-rewrite-http-liveness-readiness-probes-for-healthchecks.md","Context":"## Context\\nWith Istio auto mTLS enabled in `STRICT` mode, [http liveness and readiness\\nprobes](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/configure-liveness-readiness-startup-probes\/) no longer work because the `kubelet`, which makes the http requests, does not have Istio issued certificates.\\nIstio [supports rewriting `http` probes](https:\/\/istio.io\/docs\/ops\/configuration\/mesh\/app-health-check\/#enable-globally-via-install-option) during the sidecar injection process.\\n#### Figure 1\\n_Liveness probe flow when Istio mTLS is disabled or `PERMISSIVE`. Probe `GET` request regularly travels through the Envoy sidecar to the app._\\n![No mTLS\/PERMISSIVE mTLS mode liveness probe diagram](..\/assets\/liveness-probe-adr-1.png)\\n#### Figure 2\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe is not rewritten. Probe `GET` request fails at the Envoy sidecar because it does not include the correct certificates._\\n![STRICT mTLS liveness probe diagram with no probe rewrite](..\/assets\/liveness-probe-adr-2.png)\\n#### Figure 3\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe **is rewritten by Istio**. Probe `GET` request bypasses the sidecar and goes through the Istio `pilot-agent` instead. The `pilot-agent` is configured to direct the request to the app._\\n![STRICT mTLS liveness probe diagram with probe rewrite](..\/assets\/liveness-probe-adr-3.png)\\n\n## Decision\n","Decision":"We have decided to install Istio with\\n`--set values.sidecarInjectorWebhook.rewriteAppHTTPProbe=true`\\nThis will rewrite the liveness and readiness probes on any app pods when injecting a sidecar into the app pod. Any namespaces that have the label `istio-injection=enabled` will have their liveness and readiness probes rewritten, as shown in Figure 3.\\n","tokens":360,"id":2243,"Prediction":"## Decision\n\nEnable Istio's HTTP probe rewrite feature when operating in `STRICT` mTLS mode.\n\n### Justification:\n- **Issue with Probes:** Without the rewrite feature, HTTP liveness and readiness probes will fail because the `kubelet` does not possess Istio-issued certificates required for communication through the Envoy sidecar.\n- **Operational Integrity:** Ensuring that Kubernetes probes work correctly is critical for the consistency and health of the application deployments. Failed probes could lead to unnecessary restarts or disruptions in service.\n- **Solution via Rewrite:** By enabling Istio's HTTP probe rewrite, we can bypass the Envoy sidecar for these specific probe requests, directing them through the Istio `pilot-agent`. This ensures that the probes reach the application without requiring the `kubelet` to have the certificates.\n- **Maintaining Security Posture:** This decision allows us to maintain `STRICT` mTLS mode for secure communication between services, while also ensuring that the health checks are not obstructed.\n- **Implementation:** During the sidecar injection, Istio can be configured to rewrite HTTP probes such that the request bypasses the Envoy sidecar, addressing the issue outlined in Figure 2 and ensuring proper routing as shown in Figure 3.\n\nBy integrating this probe rewrite feature, we maintain a secure setup while ensuring operational stability and reliability of our services.\n","GenTime":"2024-07-28 23:38:49"}
{"File Name":"cf-k8s-networking\/0008-implement-workarounds-for-capi-and-log-cache-to-unblock-global-strict-mtls.md","Context":"## Context\\nWe need to turn on STRICT mTLS for all components on the mesh. However, some\\ncomponents are currently incompatible with this mode.\\nCAPI is incompatible because it uses an init container to run migrations. This\\ninit container comes up before the sidecar, so it is unable to establish an mTLS\\nconnection with the capi database. This causes the init container to fail and\\nprevents capi from coming up. See [this\\nissue](https:\/\/github.com\/cloudfoundry\/capi-k8s-release\/issues\/12) in capi.\\nLog-cache is incompatible because it is configured to establish its own tls\\nconnection, which is incompatible with the mTLS the sidecars are attempting to\\nestablish.\\n\n## Decision\n","Decision":"We have provided configuration workarounds in the form of Policies, that were\\nplaced in the cf-for-k8s repo to be owned by the respective teams that manage\\nthe troublesome components.\\n[Pull Request](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/pull\/35)\\n","tokens":153,"id":2249,"Prediction":"To address the challenges presented by enabling STRICT mTLS for all components in the mesh, the following decisions have been made:\n\n1. **Component-specific mTLS Exemptions**:\n   - **CAPI**: Temporarily exempt the CAPI initialization containers from STRICT mTLS. Create a custom policy that allows the init container to connect to the database using a different form of secure communication until the CAPI sidecar comes up and can establish proper mTLS communication.\n   - **Log-cache**: Similarly, exempt Log-cache from STRICT mTLS and manage its own TLS configurations until a more seamless integration with the mTLS sidecars can be achieved.\n\n2. **Phased mTLS Rollout**:\n   - Roll out STRICT mTLS in phases, initially enabling it only for components that are fully compatible. Begin with non-critical components and incrementally enable STRICT mTLS for other services as they become compatible.\n   - Continuously monitor and resolve compatibility issues, ensuring that each phased rollout does not disrupt the overall system functionality.\n\n3. **Compatibility Roadmap**:\n   - Engage with the maintainers of incompatible components (e.g., CAPI, Log-cache) to contribute to or wait for upgrades that will make them mTLS compatible.\n   - Track and prioritize these upgrades in the development and deployment pipelines, ensuring that the changes are tested and validated in staging environments before production rollout.\n\n4. **Automated Testing and Validation**:\n   - Implement automated tests to verify that STRICT mTLS configurations work as designed and that exemptions do not weaken the overall security posture.\n   - Ensure that these tests cover all components, taking particular care to validate the behavior of those initially exempted under the custom policies.\n\n5. **Documentation and Communication**:\n   - Document the exemptions, phased rollout plan, and identified compatibility issues alongside the roadmap toward full STRICT mTLS compliance.\n   - Regularly communicate with all relevant stakeholders, including development, operations, and security teams, to keep them updated on progress and potential impacts on their workstreams.\n\nThis approach balances progress towards a secure environment with the practical need to maintain system stability and functionality during the transition period.\n\n---\n\nBy following these decisions, we ensure a secure and seamless transition to STRICT mTLS for all compatible components while managing exceptions and incompatibilities in a controlled and planned manner.","GenTime":"2024-07-28 23:39:14"}
{"File Name":"cf-k8s-networking\/0013-rename-master-branch.md","Context":"## Context\\nA [proposal](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/topic\/75070528#9059) was\\nmade on the cf-dev mailing list to rename our `master` branch to `main` to make\\nthe CF community a more welcoming and inclusive environment.\\n\n## Decision\n","Decision":"We are going to rename our `master` branch to `develop`. `develop` was chosen\\ninstead of `main` by team consensus because it better describes the use of the\\nbranch.\\n","tokens":63,"id":2251,"Prediction":"## Decision\nWe have decided to proceed with the renaming of the `master` branch to `main` across all our repositories. This change aligns with our goal of fostering a more welcoming and inclusive community. The transition plan includes updating all relevant project documentation, scripts, and CI\/CD configurations to reflect the new branch name. We will also provide a transition period during which both branch names will be accessible to ensure a smooth migration for all contributors.","GenTime":"2024-07-28 23:39:22"}
{"File Name":"princeton_ansible\/0002-group-vars.md","Context":"## Context\\nThere are a lot of different ways to set variables in Ansible and we should have\\na consistent way to do it in all playbooks for each group.\\nWe want to make sure that there's a smaller vault.yml file per group, instead of\\na gigantic vault.yml file for everything. This is because one huge vault.yml\\nfile causes merge conflicts in multiple Github Pull Requests.\\n\n## Decision\n","Decision":"Unique group variables will be placed in `\/group_vars\/<groupname>`\\nEncrypted group variables will be placed in `\/group_vars\/<groupname>\/vault.yml`\\nShared group variables will be placed in `\/group_vars\/all\/`\\n","tokens":82,"id":2255,"Prediction":"## Decision\nWe will adopt a consistent variable setting strategy in all Ansible playbooks by creating smaller, more focused `vault.yml` files for each group. Each group will maintain its own dedicated `vault.yml` file to encapsulate its specific variables. This approach will prevent the centralization of all variables into a single massive `vault.yml` file, thereby reducing the likelihood of merge conflicts in GitHub Pull Requests.\n\n### Implementation Details:\n1. **Group-Specific `vault.yml` Files**: Create a `vault.yml` file for each group within their respective directories. For example:\n   ```\n   group_vars\/\n   \u251c\u2500\u2500 group1\/\n   \u2502   \u2514\u2500\u2500 vault.yml\n   \u251c\u2500\u2500 group2\/\n   \u2502   \u2514\u2500\u2500 vault.yml\n   \u2514\u2500\u2500 group3\/\n       \u2514\u2500\u2500 vault.yml\n   ```\n\n2. **Naming Conventions**: Follow a consistent naming convention for `vault.yml` files across all groups.\n\n3. **Scope Limitation**: The `vault.yml` file for each group should only include variables pertinent to that group. Common variables used across multiple groups should be placed in a shared `common.yml` file.\n\n4. **Documentation and Training**: Update documentation to reflect these changes and provide training for team members to ensure consistent practice.\n\n5. **Review Process**: Implement a review process to check for consistent use of these small `vault.yml` files during code reviews.\n\nBy adopting this strategy, we aim to ensure easier management of variables, reduce merge conflicts, and maintain a cleaner, more modular codebase.","GenTime":"2024-07-28 23:39:35"}
{"File Name":"princeton_ansible\/0004-key-scope","Context":"## Context\\nIn many cases we can generate keys or tokens as-needed. We may also be able to create keys with varying permissions.\\nWe'd like to reduce our vulnerability in the case of one of these keys becoming compromised, either via a compromised box or via a software exploit to application code.\\n\n## Decision\n","Decision":"Keys should be minted on a per-feature, per-application basis.\\n","tokens":61,"id":2256,"Prediction":"## Decision\nWe have decided to implement a policy of generating short-lived, limited-privilege keys and tokens to enhance security. These keys will have restricted permissions tailored to their specific use-cases and will have a short expiration time to limit the window of opportunity for misuse in the event of a compromise. The use of automated key rotation mechanisms will be incorporated to ensure keys\/tokens are updated regularly without manual intervention. This approach will mitigate the risk associated with long-lived keys and reduce the impact if a key or token is compromised. Additionally, a robust logging and monitoring system will be employed to detect and respond to suspicious activities promptly.","GenTime":"2024-07-28 23:39:37"}
{"File Name":"sre-adrs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made for our team.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as discussed in our original proposal: https:\/\/docs.google.com\/document\/d\/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU\/edit#\\n","tokens":16,"id":2261,"Prediction":"## Decision\nWe will use an Architectural Decision Record (ADR) template to document all significant architectural decisions. Each decision will include the following information:\n\n1. **Title**: A short, clear title for the decision.\n2. **Context**: A detailed description of the situation that necessitates the decision.\n3. **Decision**: The decision that has been made, including rationale and any alternatives considered.\n4. **Status**: The current status of the decision, such as proposed, accepted, or deprecated.\n5. **Consequences**: The implications of the decision, both positive and negative.\n\nThe ADR documents will be stored in a dedicated directory within our project repository, named `\/docs\/arch-decisions`, allowing easy access and version control. The team will review and update these records regularly to ensure they remain current and relevant.","GenTime":"2024-07-28 23:39:58"}
{"File Name":"wordpress-template\/0003-theme-namespace-naming-convention.md","Context":"## Context\\nWe need to pick a namespace for themes, and renaming can be tedious.\\nOnly one theme is ever active at a time, so there's no risk in naming collisions except when dealing with child themes.\\n\n## Decision\n","Decision":"We'll use the `\\Theme` namespace for every theme we make except for child themes which will have the namespace `\\ChildTheme`.\\n","tokens":45,"id":2262,"Prediction":"## Decision\nWe will adopt a naming convention for themes that minimizes the need for renaming and reduces the potential for collisions, especially when dealing with child themes. The following guidelines will be implemented:\n\n1. **Parent Themes**: Namespace parent themes with a clear, descriptive, and concise name. Use a standard format such as `ThemeName` (e.g., `CorporateTheme`, `BlogTheme`).\n\n2. **Child Themes**: Child themes will inherit and extend the namespace of their parent themes. The naming convention for child themes will follow the format `ParentThemeName.ChildThemeName` (e.g., `CorporateTheme.Marketplace`).\n\n3. **Active Theme Identification**: Maintain a single active theme at a time within the system's configuration settings to avoid any ambiguity. The active theme will be referenced directly by its namespace.\n\n4. **Versioning**: If necessary, include versioning information as part of the namespace to differentiate between different iterations of a theme. The format will be `ThemeName_vX` for parent themes and `ParentThemeName_vX.ChildThemeName` for child themes (e.g., `CorporateTheme_v1`, `CorporateTheme_v1.Marketplace`).\n\nThis approach ensures that namespaces are organized, collisions are minimized, especially with child themes, and renaming is simplified.","GenTime":"2024-07-28 23:40:01"}
{"File Name":"spin-archetype-base\/0002-unified-terminology.md","Context":"## Context\\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\\n\n## Decision\n","Decision":"A `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\\n","tokens":34,"id":2270,"Prediction":"## Decision\n\nTo address the inconsistency in naming conventions within the codebase, we will establish and enforce a clear and comprehensive naming convention guideline. This guideline will cover aspects such as:\n\n1. **Variable Names**: Use descriptive and meaningful names that reflect their purpose or the data they hold. Avoid single-character names except for loop counters or very short-lived variables in small scopes.\n\n2. **Function and Method Names**: Use verbs or verb phrases to indicate the action or behavior (e.g., `calculateTotal`, `fetchUserData`). Methods\/functions performing similar actions should have consistent prefixes (e.g., `getUserName`, `getUserEmail`).\n\n3. **Class and Object Names**: Use nouns or noun phrases that accurately describe the entity (e.g., `Invoice`, `UserProfile`).\n\n4. **Constants**: Use all uppercase letters with underscores to separate words (e.g., `MAX_CONNECTIONS`, `DEFAULT_TIMEOUT`).\n\n5. **Consistency**: Ensure consistent naming across different parts of the application to prevent confusion and reduce cognitive load. This includes following the same conventions for similar entities, methods, and variables.\n\n6. **Documentation and Review**: Document the naming conventions in a shared and easily accessible manner. Additionally, incorporate naming convention checks into code review processes to enforce these guidelines.\n\nBy implementing these naming conventions, we aim to improve code readability, maintainability, and reduce the likelihood of errors arising from ambiguous or inconsistent naming.","GenTime":"2024-07-28 23:40:31"}
{"File Name":"where-away\/0004-test-with-jest.md","Context":"## Context\\nI am familiar with Jest and am consistently happy with it. Also it's pretty\\npopular and widely used so there's good support.\\n\n## Decision\n","Decision":"Use Jest for testing.\\n","tokens":31,"id":2273,"Prediction":"## Decision\nSince Jest is a testing framework that you are familiar with and have had positive experiences with, and because it is popular and widely supported in the community, we will select Jest as our testing framework for the project. This decision leverages your existing expertise, ensures good community support, and aligns with industry standards, thereby increasing the efficiency of our testing process and the overall reliability of our codebase.","GenTime":"2024-07-28 23:40:41"}
{"File Name":"where-away\/0009-use-xml-for-bookmarks-format.md","Context":"## Context\\nOptions:\\n- custom format, custom parser\\n- that's a lot of work\\n- json\\n- awkward format for depicting a tree\\n- yaml\\n- significant whitespace makes this more error-prone, and the bookmarks file\\nis likely to be edited frequently\\n- xml\\n- classic\\n- shows trees well\\n- less error prone to edit\\n- not supported natively in node\\n- neither parsing nor schema validation\\n- schema is simple enough that I could hand-roll a validator\\n- see discussion on parsing -- it needs a library: https:\/\/stackoverflow.com\/questions\/11398419\/trying-to-use-the-domparser-with-node-js\\n- top two promising libraries:\\n- jsdom, 32 dependencies https:\/\/www.npmjs.com\/package\/jsdom\\n- xmldom, 0 dependencies https:\/\/www.npmjs.com\/package\/xmldom\\n\n## Decision\n","Decision":"Use XML for the bookmarks format. Use\\n[xmldom](https:\/\/www.npmjs.com\/package\/xmldom) to parse, and hand-roll the\\nvalidation.\\n","tokens":180,"id":2274,"Prediction":"## Decision\n\nWe have decided to use XML for representing the tree structure in our bookmarks file.\n\n### Rationale\n- XML is a classic and well-established format for depicting tree structures.\n- It reduces the likelihood of errors in manual edits compared to YAML, given that the significant whitespace in YAML can make it more error-prone.\n- Although XML is not supported natively in Node.js, we can hand-roll a validator due to the simplicity of our schema.\n- By choosing XML, we benefit from its structured format, which aligns with our need to depict tree structures clearly and effectively.\n\n### Supporting Libraries\nTo handle XML parsing and manipulation, we will integrate the `xmldom` library, which has 0 dependencies, thereby minimizing the risk of bloat and potential conflicts in our application dependencies. This also simplifies maintenance and enhances performance.","GenTime":"2024-07-28 23:40:44"}
{"File Name":"where-away\/0011-render-buttons-instead-of-anchor-elements-for-link-nodes.md","Context":"## Context\\n- semantically, an html element that takes you to a new web page when you click\\nit is an anchor -- `<a>`.\\n- I'd like tab navigation and the Enter key to work for navigating between\\nbookmarks.\\n- buttons can be accessed via Tab and activated via Enter\\n- not so with an anchor tag. See\\nhttps:\/\/stackoverflow.com\/questions\/41476292\/how-to-give-focus-to-anchor-tag-without-href,\\netc.\\n\n## Decision\n","Decision":"Model the links as `<button>` elements to make the interaction more normal.\\n","tokens":100,"id":2280,"Prediction":"## Decision\nAfter considering the context and the need for optimal keyboard navigation between bookmarks while maintaining semantic correctness, we have decided to use `<button>` elements for bookmarks instead of `<a>` tags.\n\n### Rationale:\n1. **Accessibility:** Using `<button>` elements ensures that bookmarks can be accessed via Tab and activated via the Enter key, providing a better user experience for keyboard navigation, which is crucial for accessibility.\n2. **Semantic Considerations:** While `<a>` tags are semantically used for navigation, buttons offer the functionality required to ensure accessible navigation, given the constraints of tabbing and activation.\n3. **Implementation Simplicity:** Implementing buttons for this purpose will be more straightforward than adding additional event listeners or handling focus management for `<a>` tags.\n\n### Implementation Notes:\n- Ensure the buttons are styled appropriately to visually indicate that they function as navigational elements.\n- Consider adding aria-labels or other ARIA properties to enhance accessibility and provide context to screen readers.\n- Ensure the route changes appropriately when a button is activated to maintain expected navigation behavior.","GenTime":"2024-07-28 23:41:02"}
{"File Name":"stac-ml-aoi-extension\/0004-multiple-label-items.md","Context":"## Context\\nShould each `ml-aoi` Item be able to bring in multiple labels?\\nThis would be a useful feature for training multi-class classifiers.\\nOne can imagine having a label STAC item for buildings and separate STAC item for fields.\\nSTAC Items Links object is an array, so many label items could be linked to from a single `ml-aoi` STAC Item.\\n#### Limiting to single label link\\nLimiting to single label link however is appealing because the label item metadata could be copied over to `ml-aoi` Item.\\nThis would remove the need to follow the link for the label item during processing.\\nIn practice this would make each `ml-aoi` Item also a `label` Item, allowing for its re-use by tooling that understands `label`.\\nIf multi-class label dataset would be required there would have to be a mechanical pre-processing step of combining\\nexisting labels into a single STAC `label` item. This could mean either union of GeoJSON FeatureCollections per item or\\na configuration of a more complex STAC `label` Item that links to multiple label assets.\\n#### Allowing multiple labels\\nThe main appeal of consuming multi-label `ml-aoi` items is that it would allow referencing multiple label sources,\\nsome which could be external, without the need for pre-processing and thus minimizing data duplication.\\nIf multiple labels were to be allowed the `ml-aoi` the pre-processing step above would be pushed into `ml-aoi` consumer.\\nThe consumer would need appropriate metadata in order to decipher how the label structure.\\nThis would require either crawling the full catalog or some kind of meta-label structure that combines the metadata\\nfrom all the included labels into a single structure that could be interpreted by the consumer.\\n\n## Decision\n","Decision":"`ml-aoi` Items should be limited to linking to only a single label item.\\nRequiring the consumer to interpret multiple label items pushed unreasonable complexity on the user.\\nAdditionally combining labels likely requires series of processing and validation steps.\\nEach one of those would likely require judgment calls and exceptions.\\nFor instance when combining building and fields label datasets the user should check that no building and field polygons overlap.\\nIt is not realistic to expect all possible requirements of that process to be expressed by a simple metadata structure.\\nTherefore it is better to explicitly require the label combination as a separate process done by the user.\\nThe resulting label catalog can capture that design and iteration required for that process anyway.\\n","tokens":375,"id":2286,"Prediction":"## Decision\n\nWe decide to **allow multiple labels** for each `ml-aoi` (Machine Learning Area of Interest) Item.\n\n### Rationale\n\n1. **Flexibility and Versatility**:\n    - Allowing multiple labels enables a richer and more versatile use of the dataset for various machine learning tasks, particularly for training multi-class classifiers.\n    - Users can seamlessly incorporate various label sources, including external ones, without additional preprocessing steps, making the dataset more adaptable to different use cases.\n\n2. **Avoiding Duplication and Redundancy**:\n    - By linking to multiple labels rather than embedding all label metadata within the `ml-aoi` items, we avoid data duplication and maintain a cleaner, more modular dataset structure.\n\n3. **Simplifying Data Management and Updating**:\n    - Changes to label datasets \u2013 such as updates or additions \u2013 can be managed independently and simply linked to the `ml-aoi` items without requiring modifications to the `ml-aoi` metadata itself.\n\n4. **Consumer Responsibility**:\n    - Although this approach shifts some preprocessing responsibilities to the `ml-aoi` consumer, it provides the flexibility for consumers to handle and interpret the labels in ways that best suit their application requirements.\n    - Consumers can develop and use standardized mechanisms or tools to aggregate and decode the label structures, leveraging the unified metadata provided.\n\n5. **Enhanced Collaboration Potential**:\n    - Allowing multiple labels supports collaboration where different parties might contribute different types of labels for the same area of interest. This promotes a more collaborative, integrative approach to dataset enrichment.\n\n### Implementation Considerations\n\n- **Metadata Structure**:\n    - Ensure that the `ml-aoi` Item contains sufficient metadata to describe the structure and sources of the linked label items, facilitating easier interpretation and processing by consumers.\n    - Develop a clear, standardized way of referring to and differentiating between multiple label sources within the `ml-aoi` metadata.\n\n- **Consumer Support**:\n    - Provide documentation and examples illustrating how to handle multiple labels associated with `ml-aoi` items.\n    - Consider developing or recommending tools or libraries that assist in the aggregation and preprocessing of label data from multiple sources for end-users.\n\n- **Performance and Scalability**:\n    - Assess the potential performance impacts of crawling and interpreting multiple label sources at runtime. Optimize the metadata format and indexing to minimize processing overhead.\n\nBy allowing multiple labels for each `ml-aoi` item, we aim to maximize the flexibility, usability, and collaborative potential of the dataset, supporting a broader range of machine learning applications.","GenTime":"2024-07-28 23:41:26"}
{"File Name":"texas-holdem-code-challenge\/0008-use-typescript.md","Context":"## Context\\nThe approach I have chosen (see docs\/model.md and docs\/algorithm.md) leads to\\nmany small unit-testable components and a main function that ties them together.\\nThe options I can see for the main function are:\\n1. Write it with unit tests, heavily mocking the dependencies\\n2. Use a type system to check that the plumbing matches up, don't unit test the\\nmain function\\n3. Don't test the main function, don't use types. My guess is I'll make some\\nmistake along the way and have to debug it.\\nIn all cases I'll run integration tests, but without (1) or (2) I won't know\\nwhere bugs are when the integration test fails.\\nMy guess is that (2) will be more efficient than (1), and either of them will be\\nmore efficient than (3).\\nFor JavaScript static typing, I'm familiar with TypeScript.\\n\n## Decision\n","Decision":"Use TypeScript.\\n","tokens":193,"id":2290,"Prediction":"## Decision\nBased on the given context, the decision is to use option **2.** By leveraging TypeScript, a type system, to ensure that the main function's plumbing matches up correctly, we can effectively reduce the likelihood of integration bugs and improve development efficiency.\n\n**Rationale:**\n- TypeScript will help catch type mismatches and other common errors at compile time, reducing the need for extensive debugging during runtime.\n- Type safety provides a robust correctness guarantee that complements integration tests effectively.\n- While heavily mocking dependencies in unit tests (option 1) can provide detailed verification, it also introduces complexity and maintenance overhead. \n- Option 3 is not viable due to the higher risk of making errors and the inefficiency in debugging them when they occur.\n\nTherefore, the chosen approach balances thoroughness and efficiency by combining static type checking with integration testing, maintaining high reliability without overly complicated unit mock setups.","GenTime":"2024-07-28 23:41:37"}
{"File Name":"texas-holdem-code-challenge\/0002-use-node-js.md","Context":"## Context\\n- The problem instructions state \"C++ or node.js are preferred\".\\n- I am most familiar with node.js out of those two.\\n- I am completing this problem in my spare time and have a limited amount of\\ntime to spend on it: it's convenient to work faster if possible.\\n\n## Decision\n","Decision":"Use Node.js to complete this problem.\\n","tokens":67,"id":2294,"Prediction":"## Decision\n\nGiven the context provided:\n\n1. **Familiarity with Node.js**: Since you are most familiar with Node.js, it ensures that development can proceed more quickly and with fewer potential issues related to unfamiliarity with the language or ecosystem.\n\n2. **Preference in Instructions**: The problem instructions indicate a preference for either C++ or Node.js, meaning both are acceptable solutions. Choosing Node.js aligns with the preferences outlined.\n\n3. **Limited Time Availability**: Given your limited spare time to work on this problem, choosing a language you are comfortable with will maximize efficiency and reduce development time.\n\n### Decision: Use Node.js for this project\n\nBy leveraging your expertise in Node.js, you will be able to work more efficiently and effectively, aligning with your limited time availability and fulfilling the problem's language preferences.","GenTime":"2024-07-28 23:41:47"}
{"File Name":"texas-holdem-code-challenge\/0009-use-typescript-3-4-5.md","Context":"## Context\\nThe TypeScript plugin for ESLint requires TypeScript below 3.5.0. The next earlier\\nversion is 3.4.5 (from ~ 1 month ago)\\nWarning from ESLint:\\n```\\n=============\\nWARNING: You are currently running a version of TypeScript which is not officially supported by typescript-estree.\\nYou may find that it works just fine, or you may not.\\nSUPPORTED TYPESCRIPT VERSIONS: >=3.2.1 <3.5.0\\nYOUR TYPESCRIPT VERSION: 3.5.1\\nPlease only submit bug reports when using the officially supported version.\\n=============\\n```\\nFurther, there are no cutting edge TypeScript features I need for this.\\n\n## Decision\n","Decision":"Use TypeScript 3.4.5\\n","tokens":158,"id":2296,"Prediction":"## Decision\n\nDowngrade TypeScript to version 3.4.5 in order to be compatible with the TypeScript plugin for ESLint and avoid the warning messages. Since no cutting-edge TypeScript features are needed, this ensures stability and compatibility with the existing ESLint tooling.\n\n### Actions to Implement:\n1. Update the project's `package.json` to specify TypeScript version 3.4.5:\n    ```json\n    \"devDependencies\": {\n        \"typescript\": \"3.4.5\",\n        \/\/ ... other dependencies\n    }\n    ```\n\n2. Run `npm install` or `yarn install` to apply the changes.\n\n3. Verify that the ESLint setup is functioning correctly without the warning.\n\nBy making this change, we ensure that the development workflow remains stable and we maintain compatibility with the toolsets in use.","GenTime":"2024-07-28 23:41:53"}
{"File Name":"skunkworks\/state-management.md","Context":"## Context\\nTypes of State:\\n- Local State\\n- e.g. React `useState` Hook\\n- Shared State\\n- e.g. React Context, Redux\\n- Remote State (i.e. API)\\n- e.g. React Context, Redux\\nShared State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked`\\nRemote State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked` \/ `@tanstack\/react-query`\\n\n## Decision\n","Decision":"- Local State: React `useState` Hook\\n- Shared State: `recoil`\\n- Remote State: `@tanstack\/react-query`\\n","tokens":208,"id":2298,"Prediction":"## Decision\n\nBased on the provided context and considering the trade-offs associated with each option for managing shared and remote state, the following decision has been made:\n\n### Shared State:\n- **Decision:** Use `constate` for managing shared state.\n- **Rationale:**\n  - Compared to `redux` with `redux-observable`, `constate` offers less boilerplate and simpler integration.\n  - Unlike React Context, `constate` mitigates issues related to \"Provider Hell\" and unintentional re-renders.\n  - It provides a more ergonomic and declarative way to manage shared state.\n\n### Remote State:\n- **Decision:** Use `@tanstack\/react-query` for managing remote state.\n- **Rationale:**\n  - `@tanstack\/react-query` provides a powerful set of hooks and utilities for handling remote data fetching, caching, synchronization, and more, abstracting much of the complexity.\n  - It offers less boilerplate compared to `redux` with `redux-observable` and better handles remote state management concerns.\n  - Unlike using React Context for remote state, `@tanstack\/react-query` is purpose-built for this use case, avoiding issues like \"Provider Hell\" and unintentional re-renders.\n\nThese decisions are made with the intent to minimize boilerplate, avoid common pitfalls like excessive re-renders and \"Provider Hell,\" and leverage modern libraries that offer robust and intuitive APIs for state management.","GenTime":"2024-07-28 23:42:02"}
{"File Name":"lbh-adrs\/Validation.md","Context":"## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n\n## Decision\n","Decision":"**Fluent Validation**\\nFluent Valdation is widely used, offers a lot of flexibility and allows for a clean, customisable and testable approach to validation.\\n","tokens":276,"id":2307,"Prediction":"## **Decision**\n\nWe have decided to utilize **Fluent Validation** for our API data validation processes. This decision was made after considering the limitations of **Data Annotations** and the benefits that **Fluent Validation** offers. Here\u2019s a summary of the main factors that influenced this decision:\n\n1. **Separation of Concerns**: Fluent Validation allows us to keep validation rules separate from our data model classes. This results in a cleaner and more maintainable codebase.\n\n2. **Testability**: With Fluent Validation, validating a model is straightforward to isolate and test, which improves our ability to ensure reliable and robust validation mechanisms.\n\n3. **Flexibility**: Fluent Validation offers advanced validation capabilities such as conditional validation, chaining of validators, and the ability to easily modify validation logic without touching the model classes.\n\n4. **Error Handling**: Error messages can be externalized and managed via dependency injection, meaning they are much easier to maintain, localize, or customize based on various business requirements.\n\n5. **Built-in Validators**: The library provides a wide range of built-in validators which can easily be extended or customized, reducing the need to write repetitive validation logic.\n\nIn summary, selecting **Fluent Validation** aligns with our objectives of creating a maintainable, robust, and flexible validation framework for our API data submissions.","GenTime":"2024-07-28 23:42:34"}
{"File Name":"lbh-adrs\/Event-Driven-Architecture.md","Context":"## **Context**\\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\\nIn AWS, two options can be considered:\\n1. SNS (Simple Notification Service)\\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\\n\n## Decision\n","Decision":"**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\\n","tokens":209,"id":2309,"Prediction":"## **Decision**\n\nBased on the given context, the chosen architectural decision is to utilize **SNS (Simple Notification Service) in conjunction with SQS (Simple Queue Service)** for implementing the event-driven architecture.\n\n### Rationale\n\n1. **Decoupling and Flexibility:** Using SNS for the pub\/sub model allows for immediate notification to multiple subscribers, while SQS enables decoupling the message producers and consumers for different rates of processing and polling models. This decoupling aligns perfectly with the goal of maintaining loosely coupled microservices that can be deployed and scaled independently.\n\n2. **Persistence:** By integrating SQS, messages can be persisted, ensuring that they are available for services that need to process them later or retry in case of failures.\n\n3. **Scalability:** Both SNS and SQS are managed services in AWS, meaning they automatically handle scalability, reliability, and security concerns. This allows for focusing on business logic rather than infrastructure.\n\n4. **Fan-out pattern:** The combination of SNS and SQS allows for implementing the fan-out pattern efficiently, where messages published to an SNS topic can be delivered to one or more SQS queues, enabling different parts of the system to react to events independently.\n\n5. **Handling Various Use Cases:** This setup supports a variety of use cases. Services that require immediate real-time processing can subscribe directly to SNS topics, while longer-running or batch processes can poll the SQS queues accordingly.\n\n6. **Built-in Redundancy and Reliability:** AWS provides built-in redundancy, ensuring high availability and reliability for both SNS and SQS, thus enhancing the robustness of the inter-microservice communication.\n\nBy adopting the SNS + SQS approach, microservices within our architecture will gain the benefits of an event-driven model with improved decoupling, scalability, and reliability.","GenTime":"2024-07-28 23:42:41"}
{"File Name":"lbh-adrs\/Event-Driven-Architecture-Message-Types.md","Context":"## **Context**\\nAlongside the decision to adopt an event driven architecture, there is a need to define what an event will look like. There are several options for events:\\n- **Thin Events**\\nA thin event consists of the minimum amount of data that is required that will allow a subscriber to retrieve everything it needs. This normally consists of an ID with which to make an API call back to the source publisher to gather the data it needs.\\nThe benefits of thin events are:\\n- The payload is small in size\\n- Data is always up to date as it is retrieved at the point of consumption\\n- If calls to APIs fail due to unavailability of APIs, the message can be replayed\\n- Very little need for event versioning\\nThe downsides are:\\n- Consumers need to make API calls to gather the data they need\\n- **Fat Events**\\nA fat event contains all the data necessary for any subscriber to be able to perform its job.\\nThe benefits of fat events are:\\n- all the data needed for consumer processing is present in the event\\n- no need to make any API calls to retrieve data\\nThe downsides are:\\n- Event payload could grow to be quite big\\n- Data present in the payload may no longer be required by any consumer\\n- It is difficult to version events easily (and multiple versions of the same event may need to be sent for backwards compatibility)\\n**Hybrid Approach**\\nIdeally, we should use thin events wherever possible,as this reduces the complexity around sharing events, updating consumers with new versions of events, etc. However, there are some instances where a thin event might not be possible - notably when updating an activity audit log with details of what has changed. Therefore, the best solution would be to have a thin event that contains an optional message body to hold a specific payload.\\n\n## Decision\n","Decision":"**Hybrid Approach**\\nThe easiest solution is to use a hybrid approach, with consumers gathering the data they need using API calls. This has the benefit of reducing the need for event versioning, and event payloads that grow in size over time.\\nThe event payload will be:\\n```\\n{\\n\"id\": \"8e648f3d-9556-4896-8400-211cb1c5451b\",\\n\"eventType\": \"personCreated\",\\n\"sourceDomain\": \"person\",\\n\"sourceSystem\": \"personAPI\",\\n\"version\": \"v1\",\\n\"correlationId\": \"f4d541d0-7c07-4524-8296-2d0d50cb58f4\",\\n\"dateTime\": \"2021-05-17T11:59:57.25Z\",\\n\"user\": {\\n\"id\": \"ac703d87-c100-40ec-90a0-dabf183e7377\",\\n\"name\": \"Joe Bloggs\",\\n\"email\": \"joe.bloggs@hackney.gov.uk\"\\n},\\n\"entityId\": \"45c76564-2e38-48f3-bb31-6bab2fef8623\",\\n\"eventBody\": {\\n\"oldData\": {\\n\"optionalProperty1\": \"Property value\",\\n\"optionalProperty2\": \"Property value\",\\n\"optionalProperty3\": \"Property value\"\\n}\\n}\\n}\\n```\\n","tokens":384,"id":2316,"Prediction":"## **Decision**\n\n### **Adoption of Hybrid Events**\n\nGiven the considerations outlined in the context, we have decided to adopt a hybrid approach for event design in our event-driven architecture. This hybrid model will primarily utilize thin events, supplemented by an optional message body that can carry a specific payload when necessary.\n\n### **Rationale**\n\n1. **Efficiency and Simplicity**:\n   - Thin events will generally be preferred due to their smaller payload size, which reduces network bandwidth usage and speeds up event propagation.\n   - Keeping events thin minimizes the complexity of event processing and reduces the need for extensive event versioning.\n\n2. **Data Freshness**:\n   - By using thin events, consumers will fetch the most recent data directly from the source publisher at the point of consumption. This ensures that the data is always up to date.\n\n3. **Resilience and Fault Tolerance**:\n   - If an API call to fetch additional data fails, thin events can be replayed, ensuring reliability and reducing the risk of data loss.\n\n4. **Flexibility**:\n   - The hybrid approach allows for inclusion of an optional message body to carry necessary data, which is essential for scenarios where an immediate audit log or detailed data update is required.\n   - This flexibility helps handle edge cases without excessively burdening the main architecture with large payloads.\n\n### **Implementation Strategy**\n\n1. **Event Schema Design**: \n   - Define a standard schema for thin events that includes the essential identifiers required for consumers to retrieve additional data.\n   - Establish an optional payload section in the schema, which consumers can populate if additional information is deemed necessary for specific events.\n\n2. **API Handling**:\n   - Ensure robust API endpoints are available for consumers to retrieve data when processing thin events.\n   - Implement retry mechanisms and fallbacks for API calls to handle transient failures.\n\n3. **Versioning and Compatibility**:\n   - Maintain a versioning strategy to manage changes to event schemas seamlessly.\n   - Ensure backward compatibility by supporting multiple versions of events when necessary, especially for the optional payload section.\n\n4. **Monitoring and Logging**:\n   - Implement comprehensive monitoring and logging for both event publishing and consumption.\n   - Track event sizes, frequency of API calls, and success rates to optimize the hybrid event management strategy over time.\n\n### **Conclusion**\n\nThe adoption of hybrid events, leveraging the minimalistic design of thin events with the optional extension capability of fat events, balances efficiency, flexibility, and simplicity, providing the most effective solution for our event-driven architecture needs.","GenTime":"2024-07-28 23:43:18"}
{"File Name":"lbh-adrs\/Microfrontend-Composition.md","Context":"## **Context**\\nAccording to the [Microfrontend-Architecture ADR](https:\/\/github.com\/LBHackney-IT\/lbh-adrs\/blob\/feature\/create-ADRs\/Platform\/Accepted\/Microfrontend-Architecture.md), we are going to define micro-frontends by using a vertical splitting.\\nIn this way each micro-frontends has got its repository and each team manages the development and the deployment of it.\\nAbout the client-side micro-frontends compositions, there are two options to implement that:\\n1. Writing our own boilerplate code\\nEach micro frontend is included in the html page using a \\<script> tag and each of those applications exposes a global function as its entry-point. The container application determines which frontend should be mounted and calls the relevant function to tell a micro frontend when and where to render itself.\\n2. Using a framework as Single SPA (https:\/\/single-spa.js.org\/ )\\nSingle SPA is a Javascript framework for frontend microservices. In practice, it applies a lifecycle to every application. Each app can respond to url routing events and must know how to bootstrap, mount and unmount itself from the DOM.\\n\n## Decision\n","Decision":"**Using Single SPA framework**\\nThis framework adopts the same principle as we would implement our own boilerplate code, but the advantage is that we don\u2019t have to build and document our own logic but instead we use this light framework supported by an open source community.\\nThe other advantage is that the framework allows to compose also applications written in Angualar and Vue.js\\n","tokens":242,"id":2317,"Prediction":"## Decision\n\nWe have decided to use **Single SPA** as the framework for the client-side micro-frontends composition. The reasons for this decision are as follows:\n\n1. **Standardized Lifecycle Management**: Single SPA provides a well-defined lifecycle (bootstrap, mount, unmount) for each micro frontend, which ensures consistent behavior across different micro-frontends. This makes it easier for different teams to follow common practices, reducing the likelihood of errors and simplifying debugging efforts.\n\n2. **Modularity and Scalability**: Single SPA is designed specifically for managing multiple micro-frontends and enabling them to coexist within the same application. This modular approach aligns well with our goal of enabling each team to manage the development and deployment of their own micro-frontends independently.\n\n3. **Community and Support**: As a well-established framework, Single SPA has a significant amount of community support, documentation, and available resources. This reduces the learning curve for the team and ensures there are plenty of examples and best practices to follow.\n\n4. **Ease of Routing and Integration**: Single SPA handles the routing between different micro-frontends seamlessly, which can be complex to manage manually. It also allows for easier integration of various micro-frontends, even if they are built with different technologies.\n\n5. **Future Proofing**: By adopting a widely-used framework like Single SPA, we future-proof our architecture against potential shifts in best practices or evolving standards in micro-frontends. This also ensures easier onboarding of new developers, as they are more likely to be familiar with Single SPA.\n\nIn conclusion, using Single SPA aligns well with our architectural goals and provides numerous benefits that outweigh the option of writing our own boilerplate code. It facilitates a more maintainable and scalable approach to managing micro-frontends, ensuring each team can effectively control their development and deployment processes.","GenTime":"2024-07-28 23:43:22"}
{"File Name":"easyvista_toolkit\/0001_Using_globalVariable.md","Context":"## Context\\nWe want to be able to have autocompletion in the cmdlet to make them more user friendly.\\nBecause the easyvista rest API relies on GUID for some parameters we'll need to be able to query their friendly name within in the cmdlet parameters.\\nWe tried using environment variables but they don't seem to work with hashtable (we did not check extensively).\\n\n## Decision\n","Decision":"We will use global variables (named *$Global:EZVvariablename*) set by a dedicated cmdlet (*set-EZVcontext*). That cmdlet will define an execution context for all other cmdlet in the project.\\n","tokens":79,"id":2329,"Prediction":"## Decision  \nTo facilitate autocompletion in the cmdlet and enhance user experience, we have decided to implement a custom autocompletion mechanism that queries the EasyVista REST API for friendly names.  \n\n1. **Cmdlet Autocompletion**:  \n   We will develop a custom autocompletion function that dynamically fetches friendly names from the EasyVista REST API based on user input. This function will be integrated into the cmdlets to enable automatic population of parameters.\n\n2. **Caching Mechanism**:  \n   To minimize API calls and improve performance, a caching mechanism will be implemented. The friendly names retrieved from the API will be cached locally within the session. This will reduce the latency and improve the responsiveness of the autocompletion feature.\n\n3. **Environment Variables**:  \n   Since environment variables do not support hashtable effectively, we will avoid using them for this purpose. Instead, we will utilize in-memory storage techniques such as objects or dictionaries to store and manage the mapping between GUIDs and their friendly names.\n\n4. **Integration and Testing**:  \n   Extensive testing will be conducted to ensure that the autocompletion feature works seamlessly across various scenarios. This includes testing different cmdlet parameters, network conditions, and API response variations.\n\nBy implementing this custom autocompletion mechanism, we aim to provide a more intuitive and user-friendly experience for users interacting with the cmdlets while efficiently handling the retrieval of friendly names from the EasyVista REST API.","GenTime":"2024-07-28 23:43:59"}
{"File Name":"generator-latex-template\/0005-custom-example-environment.md","Context":"## Context and Problem Statement\\nWe want to present LaTeX examples.\\nThey should be shown both as LaTeX source and as rendered output.\\n\n## Decision\n","Decision":"* Write once - have two outputs (source and rendered)\\n* Good interaction with all pacakges\\n* Should support floats\\n* Should support compilation without special configuration (such as `--shell-escape`)\\nChosen option: \"custom solution\", because resolves all forces.\\n### Positive Consequences\\n* Works with floats and umlauts\\n### Negative Consequences\\n* A little bit more than minimal code is presented to the user\\n","tokens":28,"id":2337,"Prediction":"## Decision\n\nWe will create an interactive LaTeX editor within the application that allows users to view and edit LaTeX source code while simultaneously displaying the rendered output. This feature will include the following components:\n\n1. **Split View Interface**: The interface will have two panes displayed side by side. The left pane will be dedicated to the LaTeX source code input, and the right pane will display the rendered output in real time.\n\n2. **Real-time Rendering**: Any changes made in the LaTeX source code will trigger an immediate re-rendering of the output pane. This will require integration with a LaTeX rendering engine, such as MathJax or KaTeX, that can compile LaTeX code quickly.\n\n3. **Syntax Highlighting**: The LaTeX source code editor will include syntax highlighting to help users identify LaTeX commands and structures easily.\n\n4. **Error Feedback**: The system will detect and display LaTeX syntax errors to assist users in correcting mistakes promptly.\n\n5. **Save and Share**: Users will have the option to save their LaTeX code and rendered output and share it easily through generated links or downloadable files.\n\nThis approach ensures that users not only see the LaTeX examples but can also interact with them, enhancing their understanding and ability to learn LaTeX through experimentation.","GenTime":"2024-07-28 23:44:26"}
{"File Name":"interlok\/0003-deprecate-dynamicservicelocator.md","Context":"## Context and Problem Statement\\nWith the standard document trading network, there is a move towards centralised processing such that companies need only define their \"mapping specification\" and key information, and everything is handled centrally.\\nTraditionally, this was done via DynamicServiceLocator with files located on the fileystem named in the form : `SRC-DST-MSG_TYPE.xml`; recently, this has been done in a dedicated application which is now deprecated.\\nThe way that DynamicServiceLocator works contains a lot of extraneous configuration that only have a single implementation; it was designed for extensibility, but it's over complicated in terms of XML and coupling. It needs to be simplified so that it's more understandable in the UI.\\n\n## Decision\n","Decision":"Deprecate DynamicServiceLocator, leave it available but marked as deprecated. Improve DynamicServiceExcutor so that it can be used instead.\\n### Enhancing DynamicServiceExecutor.\\nWhat this means is to implement additional ServiceExtractor implementations that allow the user to extract the _`_service-to-execute_ from an external location; currently the two supplied implementations simply use the _AdaptrisMessage_ object to extract the services.\\nThe additions required are :\\n* Extract from a URL such that you can configure `http:\/\/my.server.com\/%message{source}\/%message{destination}\/%message{messageType}.xml`. This would replace the existing DynamicServiceLocator functionality from _RemoteServiceStore_ and _LocalServiceStore_.\\n* Extract from Database -> such that you can configure something like `SELECT dynamicService FROM services WHERE src='%message{source}' AND dest='%message{destination}' AND msgType='%message{messageType}'`\\n* The above would be quite \"open\" to SQL injection style attacks; so people should probably use _JdbcDataQueryService_ and then handle it from metadata.\\n* Extract from a Cache -> similar to doing a RetrieveFromCache\\n* Extract it from metadata.\\n","tokens":146,"id":2342,"Prediction":"## Decision\n\nWe will transition from using DynamicServiceLocator with complex XML configurations to a more simplified, centralized processing approach that utilizes a simplified configuration model. This new model will:\n\n1. **Centralize Processing**: Move towards centralized processing where companies define their requirements, and core processing is handled centrally to reduce the need for complex local configurations.\n\n2. **Simplified Configuration**: Replace the overcomplicated XML-based configurations with a more streamlined and user-friendly alternative, potentially leveraging JSON or YAML for better readability and ease of use.\n\n3. **Dedicated Configuration Service**: Introduce a dedicated microservice responsible for processing and managing configurations. This service will provide a user-friendly UI for defining and updating mapping specifications.\n\n4. **Extensibility with Modularity**: While simplifying, ensure the system remains extendable by modularizing the core functionalities, allowing for future custom expansions without coupling.\n\n5. **Deprecation of Legacy Systems**: Gradually phase out the deprecated applications and DynamicServiceLocator, ensuring a smooth transition for users with minimal disruption.\n\n6. **Enhanced Documentation and Support**: Provide comprehensive documentation and support to help users transition and adapt to the new configuration model, making it easier to understand and manage through the UI.\n\nBy implementing this decision, we aim to enhance usability, reduce complexity, and improve the overall efficiency of the document trading network\u2019s system configurations.","GenTime":"2024-07-28 23:44:46"}
{"File Name":"interlok\/0006-workflow-callback.md","Context":"## Context and Problem Statement\\nWhen you enable [Dead Letter Queues](https:\/\/docs.aws.amazon.com\/AWSSimpleQueueService\/latest\/SQSDeveloperGuide\/sqs-dead-letter-queues.html) in SQS; the contract is that messages read from a source queue are failed over to the dead letter queue once the redrive policy is fired (i.e. after a max number of attempts to deliver the message). From testing, this is predicated on the fact that the message is not deleted. Of course, our sqs-polling-consumer deletes the message after submitting it to the workflow.\\nSince we can't rely on the JMS semantics either (since onMessage() traditionally doesn't throw exceptions, and RuntimeExceptions aren't propagated by Interlok) we then have to think about some way of having an asynchronous callback.\\n\n## Decision\n","Decision":"Chosen option: Modify AdaptrisMessageListener to have callbacks.\\n","tokens":171,"id":2343,"Prediction":"## Decision\n\nWe will introduce an error handling mechanism that utilizes an asynchronous callback to ensure that messages that fail to be processed are handled appropriately and can be sent to the Dead Letter Queue (DLQ). Here's the plan:\n\n1. **Error Monitoring Middleware**: Implement an error monitoring middleware that wraps the message processing logic. This middleware will capture any errors during processing that may not be handled by the traditional JMS `onMessage()` method.\n\n2. **Async Error Callback**: Develop an async error callback component which will be invoked when the middleware detects an error. This callback will handle the logic of failing over the message to the DLQ in alignment with the AWS SQS redrive policy.\n\n3. **Custom Exception Handling**: Extend the current consumer to throw specific custom exceptions that represent various error states. These exceptions will not be caught by the usual JMS `onMessage()` method but by our middleware.\n\n4. **Redrive Logic**: Integrate the custom exception handling with the redrive logic which will tally the delivery attempts before deciding to reroute the message to the DLQ.\n\n5. **Logging & Monitoring**: Enhance the system with detailed logging and monitoring of message states including success, processing failure, and rerouting to DLQ. This aids in auditing and understanding message flows and failure reasons.\n\n6. **Test & Verify**: Rigorously test the new setup with different scenarios to ensure that the error handling mechanism works as expected and messages are correctly rerouted to the DLQ upon failing to process after the specified number of attempts.\n\nBy implementing the above strategies, we ensure robust error handling in our SQS consumer workflow, adhering to the expected behavior of the redrive policy and effectively utilizing Dead Letter Queues while maintaining the integrity of our message processing.","GenTime":"2024-07-28 23:44:51"}
{"File Name":"agentframework\/0008-minify-build.md","Context":"## Context\\nAgentFramework is also use for Browser\\n\n## Decision\n","Decision":"We need minify build because package.json don't have a property called `minMain`\\n","tokens":11,"id":2359,"Prediction":"## Decision\n\nWe have decided to extend the AgentFramework to support browser-based environments alongside its existing capabilities. This decision aims to leverage the flexibility of AgentFramework to create a more robust and versatile tool that can be utilized in different application scenarios, including web browsers. By supporting browser environments, we can provide enhanced user experiences and facilitate broader adoption and integration of the AgentFramework across a variety of platforms.","GenTime":"2024-07-28 23:45:48"}
{"File Name":"agentframework\/0002-use-es6-proxy-to-provide-transparent-interceptor-layer.md","Context":"## Context\\nIn ES5, implement interceptor will modify existing class and this operation is not reversible.\\nIt may cause compatibility issue with other frameworks.\\n\n## Decision\n","Decision":"We will add ES6 Proxy for a transparent layer which not modifying user's class\/object.\\n","tokens":31,"id":2360,"Prediction":"## Decision\n\nTo avoid potential compatibility issues with other frameworks and to ensure that modifications to existing classes are reversible, we will implement interceptors using ES6 proxies. Proxies allow us to define custom behavior for fundamental operations on objects, such as property lookups, assignment, enumeration, and function invocation, without directly modifying the existing class definitions.\n\n1. **Leverage ES6 Proxies**:\n   We will use ES6 proxies to create interceptors that can wrap existing objects or class instances. This approach avoids directly altering the class prototype, making it more compatible with other frameworks and easier to revert or replace.\n\n2. **Encapsulation**:\n   The interceptor logic will be encapsulated within proxy handlers, ensuring a clean separation of concerns. We will define specific traps in the proxy handler (e.g., `get`, `set`, `apply`) to intercept and modify behavior as needed.\n\n3. **Reversibility**:\n   Since proxies do not modify the original class or object, reverting to the original behavior is as simple as discarding the proxy wrapper. This allows for greater flexibility and maintainability.\n\n4. **Testing and Validation**:\n   We will rigorously test the proxy-based interceptors to ensure there are no unintended side effects, particularly in the context of other frameworks. We will also provide guidelines on how to integrate these interceptors with existing systems safely.\n\nBy adopting ES6 proxies, we can build a robust intercepting mechanism that aligns with modern JavaScript practices while ensuring compatibility and maintainability across different environments.","GenTime":"2024-07-28 23:45:52"}
{"File Name":"zendesk_app_scaffold_react_redux\/replace-migration-scaffold-with-clean-scaffold-in-master-branch.md","Context":"## Context\\nAt the moment, the master branch has the scaffold specifically built to help developers migrate v1 apps to v2. From monitor, the majority usage comes from [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator) and [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) which use the master branch as the download source.\\nIt is more helpful to show developers coming directly to this repo a clean (from scratch) scaffold, one usable as a starting point to build new apps respecting our current conventions out of the box.\\n\n## Decision\n","Decision":"### New app scaffold\\nA new *clean* app scaffold has been built on branch [offapps-migration](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/offapps-migration) based on branch [from-scratch](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/from-scratch)\\n### Move migration scaffold into App Migrator repository\\nMigration scaffold in master branch will be moved into [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator), used and maintained as a *built-in* template resource\\n### Move new app scaffold to master branch\\n### Add support in [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) to create new app with the new scaffold\\n* New command option `zat new --scaffold` will create a v2 app using the new app scaffold.\\n","tokens":124,"id":2378,"Prediction":"## Decision\nWe will create a new branch, `migration-support`, which will house the current scaffold designed for helping developers migrate v1 apps to v2. This will enable ongoing support for migration tools like [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator) and [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools), which currently rely on the master branch as their download source.\n\nThe master branch will be updated to contain a clean scaffold suitable for developers looking to build new apps from scratch, adhering to our latest conventions and best practices. This clean scaffold will serve as a straightforward starting point for developers unfamiliar with the migration process, streamlining the experience for new app development.\n\nBy bifurcating these purposes into distinct branches, we maintain support for existing migration workflows while presenting a more approachable and current scaffold to new developers.","GenTime":"2024-07-28 23:46:55"}
{"File Name":"beis-report-official-development-assistance\/0002-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n\n## Decision\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":2383,"Prediction":"## Decision\n\nTo address the issue of missing inefficient database queries during code review, we will implement automated SQL query performance monitoring tools into our development workflow. Specifically, we will:\n\n1. **Integrate SQL performance analysis tools**: Tools such as SQL linting and query optimization analyzers will be integrated into our CI\/CD pipeline. This will allow us to automatically detect and flag inefficient queries before they are merged into the main codebase.\n\n2. **Add query performance metrics**: Enhance logging to include detailed performance metrics for all database queries. This would help in identifying performance bottlenecks during runtime.\n\n3. **Establish performance baselines**: Define acceptable performance baselines for common query types and ensure that any deviations are flagged for review.\n\n4. **Educate developers**: Provide training sessions and documentation on writing efficient SQL queries. Developers should understand best practices and common pitfalls that lead to inefficient queries.\n\n5. **Periodic Audits**: Conduct periodic audits of the database to identify and refactor inefficient queries that may have slipped through the automated checks.\n\nBy combining automation with proactive education and regular audits, we aim to significantly reduce the occurrence of inefficient database queries and thus improve overall application performance and user experience.","GenTime":"2024-07-28 23:47:12"}
{"File Name":"beis-report-official-development-assistance\/0011-use-govuk-design-system-form-builder.md","Context":"## Context\\nBuilding forms in Rails that are compliant with the GOVUK Design System involve\\nmanually declaring the correct HTML structure, class names and ARIA attributes,\\nwhich is time-consuming and hard to get right.\\nAdditionally, our validation errors currently use Rails' default pattern, rather\\nthan the one recommended for use as part of the design system, which is designed\\nwith accessibility in mind.\\n\n## Decision\n","Decision":"We will use DfE's `govuk_design_system_formbuilder` to simplify the creation of\\nGOV.UK Design System-compliant forms.\\nAs we are currently using Simple Form rather than Rails' default form builder\\nfor our other forms, the two form builders can co-exist for the time being,\\nwhilst we transition the forms over.\\n","tokens":83,"id":2386,"Prediction":"## Decision\n\nTo address the challenges associated with building forms that are compliant with the GOVUK Design System and to improve the accessibility of our forms, we have decided to adopt the `govuk_design_system_formbuilder` gem. This gem provides a set of form helpers that generate HTML compliant with the GOVUK Design System, including the appropriate class names, structures, and ARIA attributes.\n\nBy using `govuk_design_system_formbuilder`, we will ensure that:\n\n1. **Consistency**: Forms across the application will maintain a consistent structure and styling that aligns with the GOVUK Design System's guidelines.\n2. **Accessibility**: The generated forms will adhere to the accessibility standards recommended by the design system, improving the experience for users with disabilities.\n3. **Efficiency**: Developers can create compliant forms more quickly and with less manual effort, reducing the likelihood of errors and the need for extensive reviews to ensure compliance.\n\nFurthermore, the error message patterns provided by the gem will replace the default Rails patterns, ensuring that our validation errors follow the GOVUK Design System's recommendations, thus enhancing both the functionality and accessibility of our applications.\n\nImplementation of this decision will also include training the development team on the usage of the `govuk_design_system_formbuilder` gem to ensure a smooth transition and proper utilization in future form development.","GenTime":"2024-07-28 23:47:22"}
{"File Name":"beis-report-official-development-assistance\/0004-add-rollbar-for-application-monitoring.md","Context":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n\n## Decision\n","Decision":"Use Rollbar to collect and manage our application errors.\\n","tokens":187,"id":2387,"Prediction":"## Decision\nWe will continue using Rollbar as our primary error monitoring and management tool for our 14 applications, maintaining consistency and leveraging our existing investment in this integration. However, we will take the following actions to address current issues and streamline error management:\n\n1. **Environment Standardization**: We will standardize the environment naming conventions to eliminate discrepancies between Rollbar and Rails environments. Explicitly setting the `ROLLBAR_ENV` variable to match our Rails environment names (e.g., `production`, `staging`, `development`) will be enforced to ensure clarity and consistency.\n   \n2. **Documentation and Best Practices**: We will document the process for setting up and managing Rollbar in our projects, including how to properly configure environment names and handle transitions between different environments. This documentation will be included in our onboarding materials and technical guides.\n   \n3. **Periodic Review**: We will establish a periodic review process (e.g., every 6 months) to evaluate our error monitoring tools. This will involve assessing Rollbar\u2019s performance, features, and integrations with our projects to ensure it continues to meet our needs effectively.\n\n4. **Sentry for New Projects**: For new projects where teams have expressed a preference or requirement for Sentry, we will evaluate its suitability on a case-by-case basis. If Sentry is chosen, we will ensure that best practices for error monitoring and environment naming are followed, similar to our approach with Rollbar.\n\nBy taking these steps, we aim to improve our error monitoring practices, reduce confusion during transitions, and ensure that we have a robust mechanism in place for capturing and managing application errors across all of our projects.","GenTime":"2024-07-28 23:47:25"}
{"File Name":"beis-report-official-development-assistance\/0005-use-travis-for-ci.md","Context":"## Context\\nTravis is actively used for many other projects within the BEIS GitHub account.\\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\\n\n## Decision\n","Decision":"Use Travis to build and test the application.\\nUse Travis to automatically deploy the applications to live.\\n","tokens":45,"id":2393,"Prediction":"## Decision\n\nWe have decided to use Travis CI for our Continuous Integration and Continuous Deployment (CI\/CD) pipelines for this project. This decision is based on several key factors:\n\n1. **Existing Usage**: Travis CI is already actively used across multiple projects within the BEIS GitHub account. Leveraging this existing familiarity and established workflows can lead to a more streamlined integration process.\n   \n2. **Docker Support**: Travis CI offers robust support for Docker, enabling us to use containers to build, test, and deploy our application efficiently. This aligns with our requirement of using containers for this project.\n\nThis decision not only ensures the consistency of tools across our projects but also aligns with our technical requirements and supports our containerized application strategy.","GenTime":"2024-07-28 23:47:47"}
{"File Name":"beis-report-official-development-assistance\/0030-run-data-migrations-manually.md","Context":"## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n\n## Decision\n","Decision":"With this in mind, we've decided to drop the use of the Data Migrate gem,\\nand instead run any data migrations manually. We have easy access to the\\nconsole via GOV.UK PaaS, so this is relatively painless.\\nWe have added a generator which creates an empty Ruby file with a timestamp and the name of the migration as the filename. Once the code is\\ndeployed, we run the migration on the production server.\\nAs the code is plain old Ruby, it's also easier for us to write tests, if\\nwe think it's worth doing (for example, the migrations code is suitably\\ncomplex).\\n","tokens":69,"id":2396,"Prediction":"## Decision\nWe have decided to discontinue the use of the Data Migrate gem in our deployment process due to its history of causing issues and silent failures. Instead, we will implement a custom data migration solution that provides better transparency and error handling during deployment. This approach will involve writing explicit migration scripts and integrating them into our deployment pipeline with clear logging and error reporting, ensuring that any issues can be detected and addressed promptly. With this change, we aim to increase the reliability and maintainability of our deployment process.","GenTime":"2024-07-28 23:47:55"}
{"File Name":"beis-report-official-development-assistance\/0009-use-pundit-to-manage-permissions.md","Context":"## Context\\nThe service will be used by a variety of different types of users, and we need\\nto ensure that those users are only able to access the parts of the service that\\nwe want them to.\\nOur current chosen authentication provider, Auth0, has support for assigning\\nroles to users, but this couples the service tightly to their service, so we\\nshould avoid this.\\n\n## Decision\n","Decision":"We will use the 'Pundit' gem to manage users' access to the service. The\\npermissions will be grouped into roles that can then be assigned to users\\nrequiring a particular level of access.\\n","tokens":81,"id":2397,"Prediction":"## Decision\n\nWe will implement an internal role-based access control (RBAC) system within our service, independent of any external authentication provider. This approach will decouple our core access control logic from Auth0, allowing flexibility to switch authentication providers in the future without affecting the RBAC logic.\n\n### Implementation Steps:\n1. **Define Roles and Permissions**: Create a list of user roles (e.g., Admin, Editor, Viewer) and the specific permissions associated with each role.\n2. **User Management Interface**: Develop an admin interface to manage user roles and permissions.\n    - Administrative users should be able to assign and modify roles for other users.\n3. **Integrate with Authentication**: After user authentication (handled by Auth0 or any other provider), map the authenticated user to the internally managed roles and permissions.\n4. **Middleware for Authorization**: Implement middleware to enforce role-based authorization checks on different routes and services within the application.\n5. **Role Management API**: Provide a set of API endpoints to manage roles and permissions programmatically.\n6. **Auditing and Logging**: Implement logging mechanisms to track changes in roles and permissions and monitor access to sensitive routes.\n\n### Justification:\n- **Independence**: By managing roles internally, we ensure that our application does not become tightly coupled with Auth0 or any other external authentication service.\n- **Flexibility**: This decision allows us to switch authentication providers in the future without significant changes to our access control mechanisms.\n- **Security**: Implementing internal RBAC ensures that access control logic resides within our service, giving us full control over the security measures and policies.\n\nThis approach aligns with our need for a flexible and secure access control system that can easily adapt to potential changes in the authentication providers in the future.","GenTime":"2024-07-28 23:48:01"}
{"File Name":"beis-report-official-development-assistance\/0031-replace-publicactivity-with-our-own-hand-rolled-way-of-recording-changes.md","Context":"## Context\\nWe adopted the `PublicActivity` gem (as documented in [0019](https:\/\/github.com\/UKGovernmentBEIS\/beis-report-official-development-assistance\/blob\/731b91f5773c4306f919433af81976d29c5feef1\/doc\/architecture\/decisions\/0019-use-public-activity-gem.md)), but the information we record has never been used, or displayed to the user.\\nNow we're looking at surfacing information about changes to end users it's becoming increasingly clear that is `PublicActivity` is not fit for our needs, and would be easier to build our own approach to logging what changed were made by what user.\\n\n## Decision\n","Decision":"We have decided to introduce a `HistoricalEvent` model to record changes to models, starting with `Activity`s.\\nOnce we have applied `HistoricalEvents` to all the models we need to track, we can remove all the calls to `PublicActivity` and remove the gem entirely.\\n","tokens":145,"id":2403,"Prediction":"## Decision\nWe have decided to phase out the `PublicActivity` gem and implement a custom logging solution to track changes made by users. The new solution will be designed to meet our specific requirements for surfacing change information to end users, ensuring that the logging system is both comprehensive and integrated seamlessly with our application. This custom approach will allow us to have more control over the recorded data and its presentation, ultimately providing a better user experience while maintaining full transparency regarding system changes. \n\nThe transition will be carefully planned to minimize disruption and ensure data consistency, with clear documentation and support for the development team throughout the process.","GenTime":"2024-07-28 23:48:18"}
{"File Name":"beis-report-official-development-assistance\/0006-use-auth0-for-authentication.md","Context":"## Context\\nWe need to allow a number of users to sign in to the service in order to use it.\\nIn order to implement this quickly, we'll use Auth0 to manage this.\\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\\nto another service, if BEIS have a preference for something else.\\nAuth0 provides views for authentication that we can use in our user journeys.\\nThere are two versions of these views 'Classic' and 'New'.\\nClassic uses JavaScript and is not progressively enhanced. New uses language\\nthat results in a poor user experience.\\n\n## Decision\n","Decision":"We will use the free tier and 'Classic' views of Auth0 for the private beta.\\n","tokens":127,"id":2405,"Prediction":"## Decision\nWe will use the 'Classic' views provided by Auth0 for user authentication.\n\n### Rationale:\n1. **User Experience**: Despite the 'New' views being modern, they result in a poor user experience due to language and possibly other design issues.\n2. **Ease of Implementation**: 'Classic' views, although not progressively enhanced, can be quickly integrated into our service ensuring a smoother and more familiar experience for users.\n3. **Flexibility in Future**: As both Auth0 views are based on OAuth2, our choice should not hinder migration to another service if the need arises.\n4. **Time Constraints**: Given our need to implement this quickly, the 'Classic' views allow for faster deployment and integration into our user journeys.\n\n### Additional Considerations:\n- We acknowledge that 'Classic' views may eventually be phased out or may lack some modern enhancements. We will monitor Auth0 updates and plan for any necessary transitions well in advance.\n- We should consider user feedback on the authentication process and be open to making adjustments or custom enhancements if common issues arise.\n\nThis decision will allow us to proceed swiftly while ensuring a positive user experience for authentication.","GenTime":"2024-07-28 23:48:25"}
{"File Name":"beis-report-official-development-assistance\/0025-use-terraform-to-set-up-and-deploy-to-the-paas.md","Context":"## Context\\nWe like to control our infrastructure with code. On a previous project we used shell scripts to set up the spaces and services within a PaaS organisation and to also do deploys from Travis.\\nWhile shell scripts are fine there is now a useful [terraform provider](https:\/\/github.com\/cloudfoundry-community\/terraform-provider-cf) for cloudfoundry.\\nThe cloudfoundry provider also deploys the app.\\nTerraform is the default choice for provisioning infrastructure these days.\\n\n## Decision\n","Decision":"- spaces and services should be created with Terraform\\n- the application should be deployed with Terraform\\n- Terraform should be run from Travis to deploy the application\\n","tokens":107,"id":2409,"Prediction":"## Decision\n\nWe have decided to use Terraform, specifically the [Terraform Cloud Foundry provider](https:\/\/github.com\/cloudfoundry-community\/terraform-provider-cf), for managing our infrastructure and deployment processes.\n\n### Rationale\n\n1. **Code Control**: Managing infrastructure as code simplifies version control and auditability. Terraform scripts are more maintainable and readable compared to shell scripts.\n   \n2. **Consistency**: Terraform is a widely-adopted tool and is considered a default choice for provisioning infrastructure. Using it ensures consistency with industry standards and facilitates easier onboarding for new team members familiar with Terraform.\n   \n3. **Functionality**: The Cloud Foundry Terraform provider offers built-in support for deploying apps directly, eliminating the need for separate deployment scripts.\n   \n4. **Maintainability**: Modular and reusable Terraform configurations can be more easily maintained and extended compared to ad-hoc shell scripts.\n   \n5. **Community Support**: Being a widely-used tool, Terraform has substantial community support and regular updates, which can be advantageous when troubleshooting or seeking enhancements.\n\n### Next Steps\n\n1. Integrate the Terraform Cloud Foundry provider into our existing infrastructure management process.\n2. Replace existing shell scripts with Terraform configurations for setting up spaces and services in the PaaS organization.\n3. Migrate the deployment processes from Travis CI shell scripts to Terraform-based automation.\n4. Ensure team members are trained on using Terraform for infrastructure management and deployment.\n5. Conduct thorough testing and validation of the new Terraform-based setup to ensure it meets all functional and reliability requirements.","GenTime":"2024-07-28 23:48:38"}
{"File Name":"beis-report-official-development-assistance\/0022-use-govuk-paas-for-hosting.md","Context":"## Context\\nAs a government entity, BEIS want to host their service on [GOVUK\\nPaaS](https:\/\/www.cloud.service.gov.uk\/). This was always the service teams\\nintention.\\n\n## Decision\n","Decision":"Host all environments excpet local development on BEIS own GOVUK PaaS account.\\n","tokens":44,"id":2410,"Prediction":"## Decision\n\nHost the BEIS service on GOV.UK PaaS.\n\n## Rationale\n\n1. **Compliance:** GOV.UK PaaS is specifically designed to meet the stringent compliance requirements for government services, ensuring that BEIS adheres to governmental standards and regulations.\n  \n2. **Security:** Utilizing GOV.UK PaaS allows BEIS to leverage a platform with robust security measures already in place, which are regularly updated to mitigate new threats and vulnerabilities.\n\n3. **Cost Efficiency:** GOV.UK PaaS offers a pay-as-you-go pricing model, enabling BEIS to manage and scale resources efficiently without incurring unnecessary costs.\n\n4. **Reliability:** The platform is built on established cloud infrastructure, providing high availability and reliability essential for government services.\n\n5. **Support and Maintenance:** Using a platform specifically tailored for UK government entities ensures BEIS has access to specialized support and maintenance services designed to address their unique needs.\n\n6. **Ease of Integration:** GOV.UK PaaS provides tools and services optimized for integration with other governmental systems and services, facilitating smoother and faster development and deployment workflows.\n\nBy choosing GOV.UK PaaS, BEIS can focus on delivering their service effectively while relying on a solid, secure, and compliant cloud platform.","GenTime":"2024-07-28 23:48:41"}
{"File Name":"arch\/0019-server-request-and-upgrade-capacity-evaluation.md","Context":"## Context\\n1. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\uff0c\u4f7f\u7528\u7387\u5747\u5728 90% \u5de6\u53f3\uff0c\u53e6\u4e00\u4e9b\u673a\u5668\u5404\u9879\u6307\u6807\u4f7f\u7528\u7387\u5728 1% \u5de6\u53f3\uff1b\\n2. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\u642d\u914d\u4e0d\u5408\u7406\uff0cCPU \u4f7f\u7528\u7387 1%\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u7387\u5728 90% \u5de6\u53f3\uff1b\\n3. \u4e00\u4e9b\u5bf9\u78c1\u76d8\u8bfb\u5199\u8981\u6c42\u9ad8\u7684\u670d\u52a1\uff0c\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0c\u6bd4\u5982\uff0c\u6570\u636e\u5e93\uff0cSVN\u7b49\uff1b\\n4. \u7533\u8bf7\u673a\u5668\u65f6\uff0c\u65e0\u6cd5\u63d0\u51fa\u914d\u7f6e\u8981\u6c42\uff0c\u57fa\u672c\u9760\u62cd\u8111\u95e8\u51b3\u5b9a\uff1b\\n5. \u5bf9\u670d\u52a1\u7684\u53d1\u5c55\u6ca1\u6709\u601d\u8003\uff0c\u914d\u7f6e\u8981\u4e86 12 \u4e2a\u6708\u540e\u624d\u80fd\u4f7f\u7528\u5230\u7684\u914d\u7f6e\u3002\\n\n## Decision\n","Decision":"1. \u538b\u529b\u6d4b\u8bd5\uff1b\\n2. \u5206\u6790\u4e1a\u52a1\u5404\u4e2a\u6307\u6807\u7684\u4f7f\u7528\u60c5\u51b5\uff0cCPU \u5bc6\u96c6\u578b\uff0c\u5185\u5b58\u5bc6\u96c6\u578b\u8fd8\u662f\u6709\u5176\u4ed6\u7684\u7279\u70b9\uff1b\\n3. \u9274\u4e8e Aliyun ECS \u968f\u65f6\u53ef\u4ee5\u6269\u5c55\uff0c\u53ef\u4ee5\u5148\u7528\u4f4e\u914d\u673a\u5668\uff0c\u6839\u636e\u4f7f\u7528\u60c5\u51b5\uff0c\u8fdb\u884c\u5355\u6307\u6807\u5782\u76f4\u5347\u7ea7\uff1b\\n4. \u6c34\u5e73\u6269\u5c55\uff0c\u5373\u63d0\u5347\u4e86\u670d\u52a1\u7684\u5904\u7406\u80fd\u529b\u53c8\u505a\u4e86\u9ad8\u53ef\u7528\uff1b\\n5. \u5bf9\u4e8e\u4e0d\u5408\u7406\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u8981\u5206\u6790\u81ea\u5df1\u7a0b\u5e8f\u4e2d\u662f\u5426\u6709\u5185\u5b58\u6cc4\u6f0f\u6216\u5927\u6570\u636e\u52a0\u8f7d\u3002\\n","tokens":208,"id":2440,"Prediction":"## Decision\n\n### 1. \u52a8\u6001\u8d44\u6e90\u8c03\u914d\u4e0e\u81ea\u52a8\u6269\u7f29\u5bb9\nImplement dynamic resource allocation and auto-scaling solutions to address the imbalance in resource utilization across servers. Configure auto-scaling policies to add or remove instances based on real-time CPU, memory, and disk usage metrics.\n\n### 2. \u5408\u7406\u8d44\u6e90\u5206\u914d\u7b56\u7565\nConduct a thorough analysis of the resource requirements for different applications and determine a more balanced mix of CPU, memory, and disk configurations. Reallocate resources to ensure that high memory-usage applications are paired with sufficient memory capacity and similarly for other resources.\n\n### 3. \u5347\u7ea7\u5b58\u50a8\u89e3\u51b3\u65b9\u6848\nUpgrade to high-performance cloud storage solutions, such as SSDs, for services requiring high I\/O operations, including databases and SVN repositories. This ensures faster read\/write speeds and reduces bottlenecks caused by slower disk performance.\n\n### 4. \u63d0\u4f9b\u914d\u7f6e\u9700\u6c42\u5206\u6790\u5de5\u5177\nDevelop a tool or platform for users to specify detailed configuration requirements when requesting new machines. This tool should include a guided process based on the historical usage data and predicted future needs to make informed decisions rather than ad hoc ones.\n\n### 5. \u9884\u89c1\u6027\u8d44\u6e90\u89c4\u5212\nImplement predictive analytics to foresee the growth and development of services over time. Use the insights from these analytics to provision resources in a timely manner, planning ahead for at least 12 months, but also allow for flexible adjustments as needs change.\n\n### 6. \u8d44\u6e90\u76d1\u63a7\u548c\u4f18\u5316\nSet up comprehensive monitoring and alerting systems to regularly review and optimize resource usage. Use tools like Prometheus, Grafana, or cloud provider-specific monitoring solutions to keep track of performance metrics and ensure efficient resource utilization.\n\nBy implementing these measures, we can ensure that the resource utilization across the system is balanced, performance bottlenecks are minimized, and future resource planning is more accurate, thereby improving overall system efficiency and reliability.","GenTime":"2024-07-28 23:50:40"}
{"File Name":"arch\/0045-reuse-python-custom-libs.md","Context":"## Context\\n\u6211\u4eec\u6709\u591a\u4e2a\u9879\u76ee\u4f7f\u7528 Python \u5f00\u53d1\uff0c\u968f\u7740\u9879\u76ee\u7684\u53d1\u5c55\uff0c\u5927\u5bb6\u4e5f\u5199\u4e86\u8bb8\u591a\u7684\u5e93\uff0c\u6bd4\u5982\uff0cprice\u3001sms\u3001mail \u7b49\u3002\u800c\u5176\u4ed6\u9879\u76ee\u4e5f\u6709\u8fd9\u6837\u7684\u9700\u6c42\uff0c\u5f53\u524d\u9879\u76ee\u4e4b\u95f4\u662f\u901a\u8fc7\u62f7\u8d1d\u7684\u65b9\u5f0f\u8fdb\u884c\u590d\u7528\uff0c\u4e0d\u662f\u5e93\u8fd8\u5b58\u5728\u9879\u76ee\u5185\u72ec\u81ea\u81ea\u884c\u66f4\u65b0\u3002\u8fd9\u5c31\u5bfc\u81f4\u9879\u76ee\u4e4b\u95f4\u6240\u4f7f\u7528\u7684\u5e93\u4ea7\u751f\u4e0d\u4e00\u81f4\uff0c\u5e76\u91cd\u590d\u9020\u4e86\u5f88\u591a\u7684\u8f6e\u5b50\u3002\\n\n## Decision\n","Decision":"1. \u6784\u5efa\u81ea\u5df1\u7684 pypi \u670d\u52a1\u5668\uff1b\\n* \u4e0d\u53ea\u53ef\u4ee5\u89e3\u51b3\u81ea\u5efa\u5e93\u7684\u590d\u7528\u95ee\u9898\uff1b\\n* \u4e5f\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u5e38\u7528\u5e93\u7f13\u5b58\uff0c\u52a0\u901f pip \u7684\u5b89\u88c5\uff1b\\n* \u6709\u7ef4\u62a4\u6210\u672c\u3002\\n2. \u4f7f\u7528 git \u4f5c\u4e3a pip \u5b89\u88c5\u5305\\n* \u5355 repo \u5355 package\uff1a\u6240\u6709 libs \u805a\u5408\u5728\u4e00\u4e2a package \u91cc\uff0c\u5bf9\u4e8e\u5927\u91cf\u7b80\u5355 lib \u53ef\u884c\uff1b\\n* \u5355 repo \u591a package\uff1a\u5206\u76ee\u5f55\u6784\u5efa package\uff0c\u9002\u5408\u7a0d\u5fae\u5927\u4e9b\u7684 libs \u4e4b\u95f4\u505a\u9694\u79bb\uff1b\\n* \u591a repo \u591a package\uff1a\u6bcf\u4e2a repo \u662f\u4e00\u4e2a package\uff0c\u9002\u5408\u5927\u7684 lib \u5e93\uff0clib \u7c92\u5ea6\u8fc7\u7ec6\u4f1a\u6709\u5e93\u7ba1\u7406\u95ee\u9898\u3002\\n3. \u4f7f\u7528 Artifactory\uff0cPro+ \u7248\u672c\u624d\u63d0\u4f9b pypi \u670d\u52a1\u3002\\n\u9274\u4e8e\u5f53\u524d lib \u5e76\u4e0d\u591a\uff0c\u800c\u4e14\u6bcf\u4e2a lib \u5f88\u7b80\u5355\uff0c\u6545\u9009\u62e9\u5355 repo \u5355 package \u65b9\u6848\u3002\\n","tokens":119,"id":2446,"Prediction":"## Decision\n\u4e3a\u4e86\u5e94\u5bf9\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u51b3\u5b9a\u5c06\u8fd9\u4e9b\u5e38\u7528\u7684\u5e93\u8fdb\u884c\u5c01\u88c5\uff0c\u5e76\u53d1\u5e03\u5230\u4e00\u4e2a\u5185\u90e8\u7684\u79c1\u6709 PyPI \u4ed3\u5e93\u4e2d\u3002\u8fd9\u6837\uff0c\u6240\u6709\u9879\u76ee\u90fd\u53ef\u4ee5\u901a\u8fc7\u7edf\u4e00\u7684\u65b9\u5f0f\u6765\u83b7\u53d6\u548c\u4f7f\u7528\u8fd9\u4e9b\u5e93\uff0c\u786e\u4fdd\u5e93\u7248\u672c\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u964d\u4f4e\u91cd\u590d\u5f00\u53d1\u7684\u6210\u672c\u3002\n\n\u5177\u4f53\u7684\u51b3\u7b56\u5982\u4e0b\uff1a\n\n1. **\u5efa\u7acb\u79c1\u6709 PyPI \u4ed3\u5e93**\uff1a\n    - \u5728\u516c\u53f8\u5185\u90e8\u642d\u5efa\u4e00\u4e2a\u79c1\u6709\u7684 PyPI \u4ed3\u5e93\uff0c\u7528\u4e8e\u5b58\u653e\u548c\u7ba1\u7406\u6211\u4eec\u81ea\u5b9a\u4e49\u7684 Python \u5e93\u3002\n  \n2. **\u5e93\u7684\u91cd\u6784\u548c\u5c01\u88c5**\uff1a\n    - \u5bf9\u73b0\u6709\u9879\u76ee\u4e2d\u7684\u5e38\u7528\u5e93\u8fdb\u884c\u91cd\u6784\u548c\u5355\u72ec\u5c01\u88c5\uff08\u4f8b\u5982 price\u3001sms\u3001mail \u7b49\uff09\u3002\n    - \u786e\u4fdd\u6bcf\u4e2a\u5e93\u7684\u4ee3\u7801\u8d28\u91cf\uff0c\u5305\u62ec\u5355\u5143\u6d4b\u8bd5\u3001\u6587\u6863\u548c\u793a\u4f8b\u3002\n  \n3. **\u7248\u672c\u7ba1\u7406**\uff1a\n    - \u5404\u5e93\u9075\u5faa\u8bed\u4e49\u5316\u7248\u672c\u63a7\u5236\uff08Semantic Versioning, SemVer\uff09\uff0c\u786e\u4fdd\u5728\u53d1\u5e03\u524d\u8fdb\u884c\u5145\u5206\u7684\u6d4b\u8bd5\uff0c\u9632\u6b62\u4e0d\u540c\u7248\u672c\u4e4b\u95f4\u7684\u517c\u5bb9\u6027\u95ee\u9898\u3002\n  \n4. **\u5e93\u7684\u4f9d\u8d56\u7ba1\u7406**\uff1a\n    - \u65b0\u7684\u5e93\u53d1\u5e03\u540e\uff0c\u5404\u9879\u76ee\u901a\u8fc7 requirements.txt \u6216\u8005 Pipfile \u6765\u6307\u5b9a\u4f9d\u8d56\u5e93\u53ca\u5176\u7248\u672c\u3002\n  \n5. **CI\/CD \u96c6\u6210**\uff1a\n    - \u5c06\u79c1\u6709\u5e93\u7684\u521b\u5efa\u548c\u53d1\u5e03\u8fc7\u7a0b\u96c6\u6210\u5230 CI\/CD \u6d41\u7a0b\u4e2d\uff0c\u786e\u4fdd\u5e93\u7684\u53d1\u5e03\u81ea\u52a8\u5316\u3002\n\n6. **\u5e93\u4f7f\u7528\u89c4\u8303**\uff1a\n    - \u7f16\u5199\u8be6\u7ec6\u7684\u4f7f\u7528\u624b\u518c\u548c\u89c4\u8303\uff0c\u6307\u5bfc\u5f00\u53d1\u8005\u5982\u4f55\u5f15\u5165\u548c\u4f7f\u7528\u516c\u53f8\u81ea\u5b9a\u4e49\u7684\u5305\u3002\n  \n7. **\u57f9\u8bad\u4e0e\u63a8\u5e7f**\uff1a\n    - \u9488\u5bf9\u5f00\u53d1\u4eba\u5458\u8fdb\u884c\u57f9\u8bad\uff0c\u63a8\u5e7f\u65b0\u65b9\u6848\uff0c\u786e\u4fdd\u5927\u5bb6\u77e5\u6653\u5e76\u8ddf\u8fdb\u4f7f\u7528\u6700\u65b0\u7684\u5e93\u548c\u89c4\u8303\u3002","GenTime":"2024-07-28 23:51:06"}
{"File Name":"arch\/0036-ha-for-mysql.md","Context":"## Context\\n1. \u6570\u636e\u5e93\u7248\u672c 5.1\uff0c\u592a\u65e7\uff0c\u6027\u80fd\uff0c\u5b89\u5168\uff0c\u4e3b\u4ece\u590d\u5236\u90fd\u5b58\u5728\u95ee\u9898\uff1b\\n2. \u6570\u636e\u5e93\u90e8\u7f72\u5728 ECS \u4e0a\uff0c\u4f46\u78c1\u76d8\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0cIOPS \u5df2\u5230\u9608\u503c\uff08\u4f18\u5148\u7ea7\u6700\u9ad8\uff09\uff1b\\n3. \u6570\u636e\u5e93\u4e00\u4e3b\u4e24\u4ece\uff0c\u4f46\u65e0\u9ad8\u53ef\u7528\uff1b\\n4. \u4e1a\u52a1\u7aef\u4f7f\u7528 IP \u8fde\u63a5\u4e3b\u6570\u636e\u5e93\u3002\\n\n## Decision\n","Decision":"1. \u63d0\u4ea4 Aliyun \u5de5\u5355\uff0c\u5c1d\u8bd5\u662f\u5426\u80fd\u7533\u8bf7\u4e0b 5.1 \u7248\u672c\u7684 MySQL\uff0c\u8fc1\u79fb\u6570\u636e\u81f3 RDS\uff0c\u89e3\u51b3 2\uff0c3\uff0c4 \u95ee\u9898\uff08\u6c9f\u901a\u540e\uff0c5.1 \u7248\u672c\u5df2\u4e0d\u518d\u63d0\u4f9b\uff0cPASS\uff09\uff1b\\n2. \u5c06\u90e8\u5206\u6570\u636e\u5e93\u8fc1\u79fb\u51fa\uff0c\u7f13\u89e3\u5f53\u524d MySQL \u670d\u52a1\u5668\u538b\u529b\uff0c\u7ef4\u62a4\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\uff08\u5e76\u672a\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0cPASS\uff0c\u5f53\u524d\u538b\u529b\u6700\u7ec8\u786e\u8ba4\u662f\u6162\u67e5\u8be2\u539f\u56e0\uff09\uff1b\\n3. ECS \u4e0a\u81ea\u5efa HA\uff0c\u5e76\u542f\u7528\u65b0\u7684\u5b9e\u4f8b\u78c1\u76d8\u4e3a SSD\uff0c\u5207\u6362\u65b0\u5b9e\u4f8b\u4e3a Master\uff0c\u505c\u6389\u65e7\u5b9e\u4f8b\uff08\u6839\u672c\u95ee\u9898\u672a\u89e3\u51b3\uff0c\u6280\u672f\u503a\u4e00\u76f4\u5b58\u5728\uff0c\u81ea\u884c\u7ef4\u62a4\u4ecd\u7136\u5b58\u5728\u98ce\u9669\u70b9\uff09\uff1b\\n4. \u8c03\u7814 5.5 \u548c 5.1 \u7684\u5dee\u5f02\uff0c\u76f4\u63a5\u8fc1\u79fb\u81ea\u5efa\u6570\u636e\u5e93\u81f3 Aliyun RDS MySQL 5.5\u3002\\n\u9274\u4e8e\u67e5\u770b\u6587\u6863\u540e\uff0c 5.1 \u5230 5.5 \u7684\u5dee\u5f02\u6027\u5f71\u54cd\u4e0d\u5927\uff0cAliyun \u5b98\u65b9\u4e5f\u652f\u6301\u76f4\u63a5 5.1 \u5230 5.5 \u7684\u8fc1\u79fb\uff0c\u6240\u4ee5\u8ba1\u5212\u76f4\u63a5\u8fc1\u79fb\u81f3 RDS \u7684 5.5 \u7248\u672c\u3002\\n\u4e3a\u4e86\u675c\u7edd\u98ce\u9669\uff1a\\n1. \u6309\u4e1a\u52a1\u5206\u6570\u636e\u5e93\u5206\u522b\u8fc1\u79fb\uff1b\\n2. \u6240\u6709\u8fc1\u79fb\u5148\u8d70\u6d4b\u8bd5\u6570\u636e\u5e93\uff0c\u7531 QA \u505a\u5b8c\u6574\u7684\u6d4b\u8bd5\u3002\\nECS self built MySQL 5.1 to RDS 5.5 with DTS \u8fc1\u79fb\u6d41\u7a0b\uff1a\\n1. \u5728 RDS \u4e2d\u521b\u5efa\u539f MySQL \u6570\u636e\u5e93\u5bf9\u5e94\u7684\u8d26\u53f7(\u5404\u4e2a\u9879\u76ee\u8d26\u53f7\u72ec\u7acb)\uff1b\\n2. \u66f4\u65b0\u767d\u540d\u5355\uff1a\u6dfb\u52a0\u9879\u76ee\u6240\u90e8\u7f72\u7684\u670d\u52a1\u5668\uff1b\\n3. \u660e\u786e\u6570\u636e\u89c4\u6a21\uff0c\u5bf9\u540c\u6b65\u65f6\u95f4\u505a\u4e2a\u9884\u671f\uff1b\\n4. \u540c\u6b65\uff08\u5168\u91cf or \u589e\u91cf\uff09\uff0c\u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff1b\\n5. \u66f4\u65b0\u6570\u636e\u5e93\u8fde\u63a5\u914d\u7f6e\u6587\u4ef6\uff1b\\n6. \u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff0c\u505c\u670d\uff1b\\n7. \u786e\u5b9a\u6570\u636e\u91cf\u4e00\u81f4\uff08\u7531\u9884\u5148\u5199\u597d\u7684\u811a\u672c\u5224\u65ad\uff09(1min)\uff1b\\n8. \u5173\u95ed\u8fc1\u79fb\u670d\u52a1(10s)\uff1b\\n9. \u91cd\u542f\u670d\u52a1\u5668\uff0810s\uff09\u3002\\n6 \u81f3 9 \u6b65\u51b3\u5b9a\u6211\u4eec\u7684\u505c\u670d\u65f6\u95f4\u3002\\n\u9274\u4e8e\u6211\u4eec\u4f7f\u7528\u4ece\u5e93\u4f5c\u4e3a\u8fc1\u79fb\u7684\u6570\u636e\u6e90\uff0c\u9700\u66f4\u65b0\u5982\u4e0b\u914d\u7f6e\uff1a\\n* log-slave-updates=1\\n* binlog-format=row\\n","tokens":112,"id":2459,"Prediction":"## Decision\n\n\u57fa\u4e8e\u4e0a\u8ff0\u80cc\u666f\uff0c\u672c\u6b21\u51b3\u7b56\u6d89\u53ca\u6570\u636e\u5e93\u7cfb\u7edf\u7684\u591a\u65b9\u9762\u4f18\u5316\uff0c\u4ee5\u63d0\u5347\u6027\u80fd\u3001\u5b89\u5168\u6027\u53ca\u9ad8\u53ef\u7528\u6027\u3002\u5177\u4f53\u51b3\u7b56\u5982\u4e0b\uff1a\n\n1. **\u6570\u636e\u5e93\u7248\u672c\u5347\u7ea7**\uff1a\n   - \u5347\u7ea7\u6570\u636e\u5e93\u7248\u672c\u81f3\u6700\u65b0\u7684\u7a33\u5b9a\u7248\u672c\uff08\u5982 8.x \u7cfb\u5217\uff09\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u3001\u5b89\u5168\u6027\u53ca\u4fee\u590d\u73b0\u6709\u7248\u672c\u7684\u5df2\u77e5\u95ee\u9898\u3002\n   - \u5236\u5b9a\u8be6\u7ec6\u7684\u8fc1\u79fb\u8ba1\u5212\uff0c\u786e\u4fdd\u6570\u636e\u8fc1\u79fb\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\uff0c\u5e76\u8fdb\u884c\u5145\u5206\u7684\u6d4b\u8bd5\u3002\n\n2. **\u5b58\u50a8\u4f18\u5316**\uff1a\n   - \u5c06\u666e\u901a\u4e91\u76d8\u66f4\u6362\u4e3a\u9ad8\u6027\u80fd SSD \u4e91\u76d8\uff0c\u4ee5\u663e\u8457\u63d0\u9ad8 IOPS\uff0c\u6ee1\u8db3\u5f53\u524d\u548c\u672a\u6765\u7684\u6027\u80fd\u9700\u6c42\u3002\n   - \u76d1\u63a7\u78c1\u76d8\u4f7f\u7528\u60c5\u51b5\u548c\u6027\u80fd\uff0c\u53ca\u65f6\u6269\u5c55\u5b58\u50a8\u8d44\u6e90\u3002\n\n3. **\u9ad8\u53ef\u7528\u6027\u67b6\u6784**\uff1a\n   - \u5b9e\u73b0\u9ad8\u53ef\u7528\u67b6\u6784\uff0c\u8003\u8651\u4f7f\u7528 MySQL \u7684 InnoDB Cluster \u6216\u8005 Percona XtraDB Cluster \u7b49\u89e3\u51b3\u65b9\u6848\u3002\n   - \u589e\u52a0\u4e00\u4e2a\u81f3\u5c11\u4e09\u4e2a\u8282\u70b9\u7ec4\u6210\u7684\u590d\u5236\u96c6\u7fa4\uff0c\u5e76\u914d\u7f6e\u81ea\u52a8\u6545\u969c\u5207\u6362\u673a\u5236\uff0c\u786e\u4fdd\u6570\u636e\u5e93\u7684\u9ad8\u53ef\u9760\u6027\u3002\n\n4. **\u8fde\u63a5\u65b9\u5f0f\u4f18\u5316**\uff1a\n   - \u4f7f\u7528\u57df\u540d\u8fde\u63a5\u6570\u636e\u5e93\uff0c\u800c\u975e\u76f4\u63a5\u4f7f\u7528 IP \u5730\u5740\u3002\u8fd9\u6837\u53ef\u4ee5\u5728\u6570\u636e\u5e93\u4e3b\u8282\u70b9\u53d8\u66f4\u65f6\u4fdd\u7559\u540c\u4e00\u8fde\u63a5\u4fe1\u606f\uff0c\u51cf\u5c11\u4e1a\u52a1\u7aef\u914d\u7f6e\u53d8\u66f4\u7684\u6b21\u6570\u3002\n   - \u5b9e\u65bd\u8bfb\u5199\u5206\u79bb\uff0c\u4e1a\u52a1\u7aef\u8bfb\u64cd\u4f5c\u4f18\u5148\u8fde\u63a5\u5230\u4ece\u5e93\uff0c\u63d0\u9ad8\u8bfb\u5199\u6027\u80fd\u3002\n\n5. **\u5b89\u5168\u589e\u5f3a**\uff1a\n   - \u542f\u7528\u6570\u636e\u5e93\u7684 SSL\/TLS \u52a0\u5bc6\u8fde\u63a5\uff0c\u589e\u5f3a\u6570\u636e\u4f20\u8f93\u7684\u5b89\u5168\u6027\u3002\n   - \u5b9a\u671f\u66f4\u65b0\u6570\u636e\u5e93\u53ca\u76f8\u5173\u7ec4\u4ef6\u7684\u5b89\u5168\u8865\u4e01\uff0c\u5b9e\u65bd\u4e25\u683c\u7684\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\uff0c\u9650\u5236\u4e0d\u5fc5\u8981\u7684\u6743\u9650\u3002\n\n\u901a\u8fc7\u4ee5\u4e0a\u63aa\u65bd\uff0c\u5e0c\u671b\u80fd\u591f\u6709\u6548\u63d0\u5347\u6570\u636e\u5e93\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\uff0c\u540c\u65f6\u6ee1\u8db3\u4e1a\u52a1\u6301\u7eed\u53d1\u5c55\u7684\u9700\u6c42\u3002","GenTime":"2024-07-28 23:51:54"}
{"File Name":"google-cloud-cpp\/2019-03-26-binary-distributions.md","Context":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n\n## Decision\n","Decision":"will not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n","tokens":272,"id":2462,"Prediction":"## Decision\n\nBased on the given context, we have made the following decision:\n\nTo cater to users who prefer using binary packages, we will focus on the following actions:\n\n- Enhance Documentation: We will provide clear and comprehensive documentation on how to build the google-cloud-cpp library from source. This will include step-by-step guides for different operating systems and environments.\n\n- Contribution Collaboration: We won't directly support specific binary package managers, but we will foster and maintain a collaborative environment for contributions from the community. This includes users who may be interested in creating and maintaining binary packages of our library for various package management systems.\n\n- Code Flexibility: Our development practices will emphasize ease of packaging. We will ensure our codebase is structured in a way that minimizes dependencies, follows good build practices, and is modular. This aims at simplifying the process for third parties to package our code in various binary formats.\n\n- Issue Accommodations: We commit to being responsive to feedback related to packaging needs. If contributors or users identify minor and reasonable modifications that facilitate the creation of binary packages, we will review and, if appropriate, incorporate these changes.\n\nThis approach allows us to remain neutral on the variety of C++ package managers while still supporting the ecosystems through community efforts. We believe this will both serve the immediate needs of our users and create opportunities for engagement and contributions, fostering an active user community around google-cloud-cpp.","GenTime":"2024-07-28 23:52:06"}
{"File Name":"google-cloud-cpp\/2018-06-13-storage-always-retries.md","Context":"**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n\n## Decision\n","Decision":"non-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","tokens":188,"id":2466,"Prediction":"By default, the library will implement an automatic retry mechanism for all requests, including non-idempotent operations. This decision ensures that the library will handle transient errors and increase the likelihood of successful operations without requiring additional code from application developers. Despite the potential risks of double uploads or multiple generations of objects\/metadata for non-idempotent operations, such cases are considered rare. To provide flexibility, the library will allow developers to customize and override the default retry behavior, enabling more precise control over which operations should be retried and which should not. This approach balances ease of use with the need to address specific application requirements and constraints.","GenTime":"2024-07-28 23:52:15"}
{"File Name":"google-cloud-cpp\/2019-01-04-error-reporting-with-statusor.md","Context":"**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n\n## Decision\n","Decision":"Instead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","tokens":436,"id":2467,"Prediction":"## Decision\n\nTo ensure our C++ libraries cater to both users who can and cannot use exceptions, our APIs will not use exceptions to indicate errors. Instead, our APIs will return errors by:\n\n1. Employing `Status` or `StatusOr<T>` objects to communicate error states and convey additional information about the outcome of the call. This provides a consistent method across our APIs.\n2. Utilizing non-throwing mechanisms provided by the specific libraries we integrate with, such as the `badbit` in the standard I\/O library.\n\nBy adopting this approach, our APIs will be universally accessible and maintain a uniform error-reporting method for all users. This adjustment ensures our APIs remain functional for environments where exceptions are either not possible or not desired. Additionally, this decision allows users to employ exceptions in their own application logic if they choose to do so.","GenTime":"2024-07-28 23:52:17"}
{"File Name":"openfido-app-service\/0002-project-structure.md","Context":"## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n\n## Decision\n","Decision":"Create a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n","tokens":49,"id":2470,"Prediction":"## Decision\n\nWe will implement role-based access control (RBAC) with organization-level permissions for the OpenFIDO project. Specifically:\n\n1. **Authentication**: Utilize a robust authentication mechanism such as OAuth 2.0 to ensure only verified users can access the system.\n  \n2. **Authorization**: Introduce an RBAC system where users are assigned roles (e.g., Admin, User, Viewer) within organizations. Each role will have specific permissions related to accessing workflow services and running GridLabD jobs.\n\n3. **Frontend Integration**: In the React frontend application, integrate with the backend authentication system to secure routes and components. Only authenticated users with the appropriate roles should be able to access the workflow service features.\n\n4. **Backend API Restrictions**: On the backend, implement middleware to check user roles against the requested resource. Only users with the right permissions can trigger GridLabD job executions or access workflow services.\n\n5. **Organizational Context**: Ensure that all resource access checks include the organizational context so that users can only access resources belonging to their organization.\n\nBy adopting this approach, we ensure secure and restricted access to the workflow services, thereby maintaining organizational boundaries and data integrity.","GenTime":"2024-07-28 23:52:29"}
{"File Name":"openfido-app-service\/0003-deployment.md","Context":"## Context\\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n\n## Decision\n","Decision":"Use CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- DEV_AWS_ACCESS_KEY_ID\\n- DEV_AWS_SECRET_ACCESS_KEY\\n- PROD_AWS_ACCESS_KEY_ID\\n- PROD_AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n","tokens":24,"id":2471,"Prediction":"## Decision\n\nGiven the context that AWS will be used as both the development\/staging and production environment on ECS (Amazon Elastic Container Service), the following decisions can be made:\n\n1. **Environment Consistency**: Implement Infrastructure as Code (IaC) solutions like AWS CloudFormation or Terraform to ensure that the development\/staging and production environments have consistent configurations. This approach will minimize discrepancies and environment-specific issues.\n\n2. **Container Orchestration**: Utilize ECS with Fargate for managing containerized applications, given its simplicity and auto-scaling capabilities. For a more controlled environment, consider using ECS with EC2 instances.\n\n3. **Continuous Deployment Pipeline**: Set up a CI\/CD pipeline using AWS CodePipeline, CodeBuild, and CodeDeploy to automate the building, testing, and deployment processes. This will ensure seamless integration and delivery to the ECS clusters.\n\n4. **Environment Isolation**: Maintain separate ECS clusters and VPCs (Virtual Private Clouds) for development\/staging and production environments. This ensures that the development activities don\u2019t interfere with the production environment.\n\n5. **Monitoring and Logging**: Implement AWS CloudWatch for monitoring and centralizing logs. This will provide real-time insights and help with debugging issues in both staging and production environments.\n\n6. **Security and Compliance**: Use AWS IAM (Identity and Access Management) roles and policies to enforce the principle of least privilege, ensuring that users and applications have only the necessary access rights. Enable AWS Shield and WAF (Web Application Firewall) to protect against DDoS attacks and other web application vulnerabilities.\n\nBy adopting these decisions, the reliability, security, and scalability of both the development\/staging and production environments can be assured.","GenTime":"2024-07-28 23:52:33"}
{"File Name":"openfido-app-service\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2472,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) process.\n\n**Details:**\n- We will use a consistent template for all ADRs to ensure clarity and completeness.\n- Each ADR will include sections such as Context, Decision, Status, Consequences, and any Alternatives considered.\n- ADRs will be stored in a version-controlled repository, maintaining chronological order.\n- Each ADR will have a unique identifier and a meaningful title for easy reference.\n- Team members will be trained on how to create and maintain ADRs.\n- ADRs will be reviewed and approved in team meetings to ensure collective agreement and understanding.\n- The ADR process will be integrated into our existing project workflow to ensure decisions are recorded as they are made.","GenTime":"2024-07-28 23:52:35"}
{"File Name":"CrossyToad\/adr-0007-SUPERSEDED-mtl-style-for-affects.md","Context":"## Context\\nIn Haskell we need a way to manage side-effects, particularly for games. There are a _bunch_ of\\noptions including:\\n- MTL Style\\n- Using `IO` for everything\\n- Freer Monads\\nI haven't used mtl style in anger yet yet so I'm keen to give it a go.\\nI'm also steaing a lot of ideas from [dino-rush](https:\/\/github.com\/jxv\/dino-rush\/) and it\\nseems to be using MTL style.\\n\n## Decision\n","Decision":"We're using MTL style.\\n","tokens":114,"id":2473,"Prediction":"## Decision\n\nWe have decided to adopt the MTL Style for managing side-effects in our Haskell-based game development project. This decision is influenced by the desire to explore this approach more thoroughly and gain practical experience with it, as well as the architectural benefits seen in similar projects like [dino-rush](https:\/\/github.com\/jxv\/dino-rush\/). \n\nUsing the MTL Style allows us to write more modular, testable, and reusable code due to:\n1. **Layered Abstraction**: By defining type classes for different effects, we can create layers of abstraction that separate pure functionality from side effects, leading to cleaner and more maintainable code.\n2. **Improved Testing**: MTL Style supports dependency injection by allowing us to provide mock instances of different effects during testing, making it easier to write comprehensive tests.\n3. **Code Reusability**: Since MTL Style promotes separating concerns, logic can be easily reused in different parts of the application or even in different projects.\n4. **Community Support and Resources**: Given that the broader Haskell community has widely adopted MTL Style, we have access to abundant resources, libraries, and community support.\n\nWhile MTL Style typically comes with a learning curve, it aligns well with our long-term goal of writing robust and high-quality Haskell code that can handle complex side-effects gracefully. Therefore, we are choosing MTL Style over alternatives such as using `IO` for everything or Freer Monads.","GenTime":"2024-07-28 23:52:39"}
{"File Name":"CrossyToad\/adr-0008-use-lenses.md","Context":"## Context\\nBecause we're writing a game we have lots of nested state that we need to update, traverse and\\notherwise manipulate. Querying data is relatively straightforward in vanilla haskell but\\nupdating nested records is a pain.\\nThis is the problem [lens](https:\/\/github.com\/ekmett\/lens) is supposed to help us with.\\nAdditionally by using lenses we have the option to use the \"classy\" lenses style to let\\nindividual subsystems of our game target a subset of our State monad instead of requiring the\\nfull state.\\n\n## Decision\n","Decision":"It's Lens time!\\n","tokens":118,"id":2480,"Prediction":"## Decision\n\nWe will incorporate the [lens](https:\/\/github.com\/ekmett\/lens) library into our game\u2019s codebase for managing and manipulating nested state. By adopting lenses, we can simplify the process of updating and querying deeply nested records, which is otherwise cumbersome in vanilla Haskell. Additionally, leveraging \"classy\" lenses will allow us to modularize our state management by enabling individual subsystems to operate on specific parts of the overall state, rather than requiring access to the entire state. This modular approach can enhance code maintainability and readability by encapsulating state-specific logic within relevant subsystems.","GenTime":"2024-07-28 23:52:56"}
{"File Name":"CrossyToad\/adr-0010-support-linear-and-discrete-input.md","Context":"## Context\\nWe need to support both discrete and continous input.\\nDiscrete input is the detection of an individual keypress. We need discrete input to support\\nmovement in game menus, state transitions and other \"one keypress one action\" scenarios.\\nContinous input is the detection of the state of an input on an ongoing basis. We need continous\\nimport to support linear movement in the game, notably the movement of the toad.\\nCurrently we are using SDL to retrieve discrete input and we have no mechanism for continous input.\\nOne option for implementing continous input would be to use the [SDL_GetKeyboardState](https:\/\/wiki.libsdl.org\/SDL_GetKeyboardState) function from SDL. We could call this function every frame and\\nmake the results available to every Scene for interpretation.\\nAnother option would be to consume our existing input event stream which is fed from [SDL_PollEvent](https:\/\/www.libsdl.org\/release\/SDL-1.2.15\/docs\/html\/sdlpollevent.html) and transition our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events.\\nAt the moment I prefer maintaining our own state instead of letting SDL do it as we can reduce the\\nnumber of possible key combinations we want to support through the type system.\\n\n## Decision\n","Decision":"We will support continous and discrete input using the existing poll events architecture and\\nsome new state machinery.\\n","tokens":270,"id":2482,"Prediction":"## Decision\n\nWe have decided to implement continuous input by maintaining our own \"InputState\" internal to our system, which will be updated based on the \"KeyPressed\" and \"KeyReleased\" events captured from our existing input event stream fed by `SDL_PollEvent`.\n\n### Rationale\n\n1. **Control over Key Combinations**: By maintaining our own state, we have more finely-grained control over the possible combinations of keys we want to support. This can help in reducing complexity and avoiding unnecessary checks for key combinations that are irrelevant or unsupported in our game.\n\n2. **Consistency with Existing Architecture**: Given that we are already using `SDL_PollEvent` for discrete input, this approach allows us to maintain consistency within our existing architecture. We can avoid introducing a separate mechanism (`SDL_GetKeyboardState`) that might complicate the input handling system.\n\n3. **Flexibility and Extensibility**: Maintaining our own state can be more flexible and extensible, allowing us to easily add, remove, or modify how continuous input is handled without depending on the SDL's implementation. This will better support future changes or enhancements to our input system.\n\n4. **Performance Considerations**: By using a unified approach to handle both discrete and continuous inputs, we may be able to make performance optimizations that wouldn't be possible if we used separate mechanisms. This can also simplify the logic and reduce potential bugs related to synchronizing input state between two different systems.\n\n### Implementation Steps\n\n1. **Modify Event Loop**: Update the current event loop to recognize and transition the `InputState` based on `SDL_KEYDOWN` and `SDL_KEYUP` events captured by `SDL_PollEvent`.\n2. **Define InputState Structure**: Create a structure or class to encapsulate the current state of inputs, ensuring that it can appropriately represent both the pressed and released state of each key.\n3. **State Updates**: Implement the logic to update the `InputState` whenever a `KeyPressed` or `KeyReleased` event is detected, ensuring that scenes and other components can query this state as needed.\n4. **Scene Integration**: Modify Scenes to query the `InputState` for continuous input, ensuring that this state is checked every frame to drive continuous actions like the movement of the toad.\n5. **Testing**: Thoroughly test the new input handling system in various scenarios to ensure it behaves as expected and integrates smoothly with the existing discrete input handling.\n\nBy following the above steps, we will establish a robust and cohesive input handling system that effectively supports both discrete and continuous inputs within our game.","GenTime":"2024-07-28 23:53:05"}
{"File Name":"pomi-data-etl\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2487,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) system to document all significant architectural decisions for this project. Each decision will include context, decision, status, consequences, and any other relevant information. This structured approach will help ensure that our architectural decisions are well-documented, easily traceable, and can be reviewed or revisited as needed throughout the project's lifecycle.\n\n### Justification:\n1. **Organization and Traceability**: An ADR system organizes decisions in a clear, consistent manner, facilitating easy access and review of past decisions.\n   \n2. **Communication and Transparency**: ADRs improve communication among team members and stakeholders by providing a transparent record of the rationale behind architectural choices.\n\n3. **Change Management**: Documenting the context and impact of decisions eases the process of managing changes and understanding their implications.\n\n4. **Knowledge Sharing**: New team members can quickly get up to speed by reviewing ADRs, which encapsulate the history of architectural decisions made on the project.\n\n### Implementation:\n1. **Template Creation**: Develop a standard ADR template that includes sections for context, decision, status, consequences, and other pertinent information.\n\n2. **Decision Logging**: Use a dedicated repository (e.g., within a `docs\/adr\/` directory in our source control system) to store ADRs, ensuring they are version-controlled and easily accessible.\n\n3. **Regular Reviews**: Schedule periodic reviews of ADRs to ensure they remain current and reflective of the project's architectural landscape.\n\n4. **Training and Adoption**: Train team members on the importance of ADRs and how to create and maintain them effectively.\n\nBy implementing this system, we will improve our project's architectural governance and provide a robust framework for making and documenting architectural decisions.","GenTime":"2024-07-28 23:53:20"}
{"File Name":"BMMRO\/2019-11-20_firestore.md","Context":"## Context\\nWe need a database in order to store the data provided by the user in the different forms.\\n\n## Decision\n","Decision":"To reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\\nGoogle recommends Firestore for new developers, as it is built upon \u201cthe successes of Real-time database\u201d. Also, for our purposes, Firestore is a better choice because:\\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\\n","tokens":23,"id":2488,"Prediction":"## Decision\n\nWe have decided to use a relational database management system (RDBMS) to store the user-provided data through various forms. Specifically, we will use PostgreSQL due to the following reasons:\n\n1. **Compatibility and Flexibility:** PostgreSQL supports a wide range of data types which makes it versatile for handling different forms of user input.\n2. **ACID Compliance:** PostgreSQL ensures data integrity with its support for Atomicity, Consistency, Isolation, and Durability (ACID) properties.\n3. **Scalability:** PostgreSQL can handle a substantial amount of data and transactions, fitting our potential scale-up needs as more user data is collected.\n4. **Active Community and Support:** With a robust community, extensive documentation, and regular updates, PostgreSQL offers reliable support and quick resolution of potential issues.\n5. **Performance:** PostgreSQL is known for its efficient performance in complex query executions, which can be advantageous if we need to run detailed analytics or data processing tasks in the future.\n\nOur database schema will be carefully designed to ensure optimal organization and retrieval of user data, and we'll implement necessary indexing and optimization strategies to maintain performance as the data grows.","GenTime":"2024-07-28 23:53:23"}
{"File Name":"educational-platform\/0010-use-axon-framework.md","Context":"## Context\\nCurrently, a lot of custom classes are defined for DDD building blocks. It will be better to use DDD library for these goals.\\n\n## Decision\n","Decision":"Axon Framework will be used as DDD library.\\n","tokens":32,"id":2498,"Prediction":"## Decision\nAdopt a Domain-Driven Design (DDD) library to standardize and streamline the implementation of DDD building blocks.\n\nBy leveraging a well-established DDD library, we can:\n\n- Ensure consistency across all domain models and services.\n- Reduce the amount of boilerplate code and custom implementations.\n- Benefit from community-vetted best practices and patterns.\n- Improve maintainability and scalability of the codebase.\n- Accelerate the development process by utilizing pre-built components.\n\nThis transition will involve identifying a robust DDD library that fits our specific requirements and refactoring existing custom classes to align with the library's structures and conventions. Additionally, the team will need training and practice to fully adapt to the new framework.","GenTime":"2024-07-28 23:54:03"}
{"File Name":"educational-platform\/0002-integration-events-implementation.md","Context":"## Context\\nFor implementing event-driven application, the platform should be available for communication with integration events.\\n\n## Decision\n","Decision":"We will start from standard Spring Events classes: ApplicationListener, ApplicationEvent without dependency to external middleware component. We can add custom features to Spring functionality when it's needed.\\ntodo: For now, all events will be stored in integration-events module. But this solution should be reviewed. Integration events should be published after successful transaction.\\n- https:\/\/devblogs.microsoft.com\/cesardelatorre\/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures\/\\n","tokens":21,"id":2499,"Prediction":"## Decision\nWe have decided to implement an event-driven architecture utilizing a message broker, such as Apache Kafka or RabbitMQ. This will allow for effective communication between services using integration events. The choice of message broker will depend on the specific requirements, such as throughput, scalability, and ease of integration with our existing technology stack. Implementing this architecture will help us achieve decoupled and scalable components that can communicate asynchronously, thereby improving the overall resilience and flexibility of the application.","GenTime":"2024-07-28 23:54:05"}
{"File Name":"educational-platform\/0011-use-axon-event-publishing-mechanism.md","Context":"## Context\\nIn [0002-integration-events-implementation.md](0002-integration-events-implementation.md) was defined the solution for using Spring-event related classes for publishing and listening integration events. In current implementation of application we have Axon Framework which have rich tools for implementing such functionality. After migrating to Axon implementation of integration events, in future, we can enable event sourcing.\\n\n## Decision\n","Decision":"Axon Framework will be used for integration events implementation.\\n","tokens":79,"id":2500,"Prediction":"## Decision\n\nWe will migrate the current implementation of integration events from using Spring-event related classes to utilizing the Axon Framework. This migration will leverage Axon's rich set of tools for publishing and listening to integration events. \n\n**Reasons for Decision:**\n1. **Rich Tool Set**: Axon Framework provides a comprehensive suite of tools specifically designed for event-driven architecture, surpassing the capabilities of Spring-event related classes.\n2. **Event Sourcing Capability**: By migrating to Axon, we pave the way for future implementation of event sourcing, which can significantly enhance our application's ability to handle state changes over time and improve traceability.\n3. **Scalability**: Axon's infrastructure is built to support applications at scale which will be beneficial for future growth of the application.\n4. **Consistency and Integration**: Using a single framework for all event-driven components can ensure better consistency across the application and simplify integration processes.\n\n**Implementation Plan:**\n1. **Assess Current Usage**: Review all current instances where Spring-event related classes are used to handle integration events.\n2. **Axon Integration**: Configure the Axon Framework within the application, ensuring it is properly set up for event handling.\n3. **Refactor Event Handlers**: Migrate existing event handlers to use Axon's event handling mechanisms.\n4. **Testing**: Thoroughly test the new implementation for correctness and performance.\n5. **Documentation**: Update all relevant documentation to reflect the use of Axon for integration events and add guidelines for future development.\n\nThis decision aligns with our goal to improve our event-driven architecture and prepare for potential adoption of event sourcing in the future.","GenTime":"2024-07-28 23:54:09"}
{"File Name":"educational-platform\/0009-architecture-tests.md","Context":"## Context\\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\\n\n## Decision\n","Decision":"Architecture tests with using Archunit should be implemented.\\n","tokens":21,"id":2502,"Prediction":"## Decision\nWe will implement a centralized architecture governance framework that integrates with our CI\/CD pipeline. This framework will include automated tools and scripts to enforce and validate adherence to common architectural principles.\n\nKey components of the decision:\n1. **Automation**: Utilize static analysis tools, linters, and custom scripts to automatically check for compliance with architectural guidelines during the build process.\n2. **Central Repository**: Maintain a central repository of architectural guidelines, principles, and best practices that can be easily referenced and updated.\n3. **Continuous Integration**: Integrate these checks into the CI\/CD pipeline to ensure that every code commit and pull request is validated against the established architectural principles.\n4. **Exception Handling**: Implement a formal process for requesting and reviewing exceptions to the architectural rules, ensuring that deviations are well-justified and documented.\n5. **Training and Documentation**: Provide comprehensive training and documentation to all teams on the architectural principles and the governance framework to ensure consistent understanding and application.\n\nBy embedding these mechanisms into our development processes, we ensure continuous compliance with our architectural standards, fostering a robust and maintainable codebase.","GenTime":"2024-07-28 23:54:14"}
{"File Name":"pottery\/0004-store-shards-in-time-stamped-directories-and-files.md","Context":"## Context\\nA project history can cover many years, with many small shards being recorded.  If all the shard files are stored in the same directory, the directory will be difficult to work with when many shards have been posted.  Performance might suffer on older file systems.\\n\n## Decision\n","Decision":"Store shard files in subdirectories named after year and then year-and-month, and name the files after the date and time.\\n","tokens":56,"id":2508,"Prediction":"## Decision\n\nTo manage the historical shard files effectively and maintain performance, we will implement a hierarchical directory structure based on time periods. The structure will reflect the chronological order of the shards to keep the directory size manageable.\n\n- **Yearly and Monthly Subdirectories**: Shard files will be organized into directories by year and subdirectories by month. For example, shard files from March 2023 will be stored in a directory path like `history\/2023\/03\/`.\n- **File Naming Convention**: Each shard file will have a consistent naming convention that includes a timestamp or sequence number to ensure uniqueness and order. For example, `shard_20230315_001.json`.\n- **Periodic Archiving**: Older shard files will be archived periodically into compressed formats (e.g., ZIP, TAR.GZ) based on a predefined archival policy (e.g., bi-annual archiving). This will help to reduce the number of files in active directories, thereby improving directory listing and search performance.\n- **Metadata Management**: Implementing a metadata catalog to keep track of the locations of shard files across the directory structure will facilitate quicker access and retrieval.\n\nThis hierarchical structure and organizational strategy will alleviate the issues related to performance on older filesystems and maintain the directory's manageability over time.","GenTime":"2024-07-28 23:54:35"}
{"File Name":"pottery\/0003-pottery-shards-are-markdown-text.md","Context":"## Context\\nEach event recorded by Pottery (aka \"shard\") is a short piece of text.  Maybe with links, or basic formatting.\\nPottery is used on the command line, so shards need to be generated by Unix command line tools, readable on the terminal and edited in a text editor.\\n\n## Decision\n","Decision":"The content of each shard will be in Markdown format.\\n","tokens":65,"id":2510,"Prediction":"## Decision\nWe will use a plain text format for storing and displaying the shards. This format will easily accommodate the command line ecosystem requirements, ensuring each shard is human-readable and editable with standard text editors. Basic formatting such as markdown will be supported, providing the flexibility for including links and simple text styling. This approach leverages the strength of Unix tools in handling text files while keeping the complexity low and ensuring high compatibility.","GenTime":"2024-07-28 23:54:39"}
{"File Name":"adr-demo\/0004-use-elastic-search-for-image-search.md","Context":"## Context\\nAs our users base is growing and we have more and more search queries, we would like to improve current search form to:\\n* Be able to search on newly added fields (e.g. created date, modified date, author, licence, etc)\\n* Allow more complex searches on existing and new fields such as:\\n* \"and\" queries to search on multiple tags\\n* \"or\" queries to search on an exclusive list of tags\\n* \"startsWith\" and \"contains\" search instead of current exact match searches\\n* synonyms\\n\n## Decision\n","Decision":"Given our needs it seems pretty clear that we need a search engine, as databases search capabilities are limited and would cost more and more as data is growing.\\nWe will use Elastic Search as our search engine as it is the de facto standard and is based on battle-tested search engine\\nLucene. Our current microservices will now evolve to this schema:\\n![](microservices-architecture-with-es.png)\\nNote: ES indexes do not need to be updated in real-time when a new image\/gif is uploaded, in fact we can easily\\ntolerate a few minutes of delay between an upload and its actual availability for search. All we need is to be\\neventually consistent.\\n","tokens":113,"id":2513,"Prediction":"## Decision\n\nTo improve our existing search functionality to better serve a growing user base and accommodate more complex search requirements, we will implement the following solutions:\n\n1. **Enhance the Search Infrastructure:**\n   - Integrate an advanced search engine such as Elasticsearch or Apache Solr to manage and optimize search queries.\n   - Configure the search engine to index the new fields: created date, modified date, author, license, etc.\n   \n2. **Complex Query Support:**\n   - Extend the backend search API to support complex queries including \"and\", \"or\", \"startsWith\", and \"contains\" operations. This will often be facilitated by the capabilities of the chosen search engine.\n   - Implement parsing and translation of user queries into the search engine's query language to harness the full power of complex searches.\n\n3. **Synonym Handling:**\n   - Utilize the synonym capabilities of the search engine to automatically expand the query terms by including their synonyms, thereby improving the recall of search results.\n   - Create and maintain a synonym dictionary that can be dynamically updated based on user feedback and evolving content.\n\n4. **User Interface Enhancements:**\n   - Modify the front-end search form to accommodate additional fields for searching (e.g., drop-down menus, date range pickers).\n   - Include options for users to specify whether their search terms should be treated as \"and\" or \"or\" conditions.\n   - Provide easy-to-use checkboxes or filters for \"startsWith\" and \"contains\" options.\n\n5. **Performance Optimization:**\n   - Ensure that the indexing process for new fields and complex query handling is optimized for performance, to handle the increasing number of user queries without significant latency.\n   - Implement caching strategies where applicable to speed up common searches.\n\nBy implementing these enhancements, we aim to significantly improve the user search experience, making it more flexible and powerful, able to handle a growing repository of content and an increasing number of complex user search queries.","GenTime":"2024-07-28 23:54:51"}
{"File Name":"adr-demo\/0003-use-cassandra-as-storage-backend.md","Context":"## Context\\nAs the application offers uploads capabilities to our users, we need to be able to store uploaded images to some\\nkind of storage. The storage backend should be able to do the following things:\\n* Store raw images\/gifs\\n* Store thumbnails of images\/gifs\\n* Offer a basic search capability (search by title, search by tag)\\n\n## Decision\n","Decision":"After a quick comparative study, we decide to go with a Cassandra storage backend. The reasons are the following:\\n* Cassandra is quick and easy to setup\\n* It should answer our needs without much custom configuration, even when scaling up\\n* Cassandra allows us to do simple search operations on the fields we're interested in\\n* The team is already familiar with Cassandra usage\\nHere is a quick picture of our architecture including Cassandra as the storage backend:\\n![](microservices-architecture-with-cassandra.png)\\n**Note**: Many services seem to point to the same Cassandra, however we need to setup multiple tables so we can respect\\nCQRS principles (command and query separation):\\n* One table will be used to store thumbnails\\n* One table will be used to store raw images\\n* Thumbnail and upload services are write services (but to different tables)\\n* Search service and download service are read-only services\\n### Other databases\\nIn our quest to the holy storage backend we also studied MongoDb but it didn't seem to be a good fit for our use case : its Json-like structure does not fill particularly well with what we are trying to achieve, and is not much appropriate for very large storage volumes.\\n","tokens":73,"id":2515,"Prediction":"## Decision\nWe will use Amazon S3 as the storage backend for the uploaded images and thumbnails. Additionally, we will integrate Amazon DynamoDB to manage metadata, including titles and tags, to support the search capabilities. The combination of Amazon S3 and DynamoDB will allow us to efficiently store and manage both raw images and their thumbnails, while also providing robust search functionalities.\n\n- **Amazon S3**: Provides a scalable and durable storage solution for raw images and thumbnails.\n  - **Pros**:\n    - Scalable and highly available\n    - Pay-as-you-go pricing\n    - Built-in redundancy and durability\n  - **Cons**:\n    - Requires configuration for optimized performance and security\n\n- **Amazon DynamoDB**: Offers fast and flexible NoSQL database services, perfectly suited for storing metadata like titles and tags.\n  - **Pros**:\n    - Low-latency data retrieval\n    - Scalable and managed service\n    - Seamless integration with AWS ecosystem\n  - **Cons**:\n    - Complex querying may require thorough planning and structure\n\n### Implementation Details:\n1. **Image and Thumbnail Storage**: \n   - Raw images and their thumbnails will be uploaded to specific S3 buckets.\n   - Thumbnails will be generated on the server-side using image processing libraries (e.g., ImageMagick) before being uploaded to S3.\n\n2. **Metadata and Search Capability**:\n   - Titles and tags will be stored in DynamoDB with references (URLs) to the images\/thumbnails stored in S3.\n   - Search operations will be executed against DynamoDB, utilizing its indexing features to provide efficient search results based on titles and tags.\n\n3. **Security and Access Control**:\n   - Access to S3 buckets and DynamoDB tables will be controlled using AWS Identity and Access Management (IAM) policies to ensure that only authorized users and services can interact with the resources.\n   - Data in transit will be encrypted using HTTPS, and data at rest will be encrypted using Amazon S3 and DynamoDB encryption mechanisms.\n\nThis decision allows us to leverage the strengths of AWS's managed services to fulfill the application's requirements effectively, ensuring scalability, reliability, and performance.","GenTime":"2024-07-28 23:54:58"}
{"File Name":"adr-demo\/0002-use-microservices-architecture.md","Context":"## Context\\nThe web application we want to develop and offer to our users is a picture service that allows :\\n* Uploading images and gifs with a title and tags\\n* Searching images by title or tag\\n* Downloading an image in various file formats and sizes\\nAs we want to quicly build a first version to be able to quickly go online and answer competitors, we choose\\nto deploy our application to Google Cloud (as we do not have any bare metal servers).\\nThe application should meet the following criteria:\\n* it should be quick enough so the search answers in less than 500ms\\n* it should be scalable so we can meet future usage\\n\n## Decision\n","Decision":"In order to meet required criteria we choose to develop our application using a microservices approach :\\neach business need will be addressed by a specific microservice, as described in the following schema:\\n![](microservices-architecture.png)\\nEach microservice should take a simple input and produce a simple output, each microservice should act as a black box\\nregarding other microservices, meaning state should not be shared across different instances of same or differents microservices.\\n","tokens":137,"id":2516,"Prediction":"## Decision\nTo meet the requirements and constraints provided, we will adopt the following architectural decisions:\n\n1. **Use Google Cloud Platform (GCP)**: Deploy the application on GCP to leverage its scalable infrastructure, managed services, and ease of quickly getting the application online.\n\n2. **Microservices Architecture**: Design the application as a set of microservices to ensure scalability and maintainability. This allows for independent scaling of various components such as image upload, search, and download services.\n\n3. **Google Cloud Storage for Image Hosting**: Use Google Cloud Storage (GCS) to store uploaded images and gifs. GCS offers high availability, scalability, and secure object storage, along with the ability to handle different image formats and sizes.\n\n4. **Firebase for Real-time Database and Hosting**: Utilize Firebase for a real-time NoSQL database and hosting static content such as web assets. Firebase\u2019s capabilities will be beneficial for implementing features like real-time search results and dynamic content updates.\n\n5. **Google Cloud Functions for Serverless Functions**: Use Google Cloud Functions to handle server-side logic such as image processing (resizing, format conversion) and tagging. Cloud Functions offer auto-scaling, cost-efficiency, and can be triggered based on certain events (e.g., an image upload).\n\n6. **Elasticsearch for Search Functionality**: Implement Elasticsearch for indexing and searching images by title or tags. Elasticsearch is designed for fast search performance and can index large amounts of data efficiently.\n\n7. **Google Kubernetes Engine (GKE) for Orchestration**: Employ GKE to manage and orchestrate our microservices. Kubernetes offers robust scalability, self-healing, and efficient resource utilization, which helps in maintaining performance and meeting the 500ms search response time requirement.\n\n8. **CI\/CD Pipeline with Google Cloud Build**: Set up a Continuous Integration \/ Continuous Deployment (CI\/CD) pipeline using Google Cloud Build to enable rapid and automated deployment of new features and updates, reducing the time to market.\n\nThese decisions collectively ensure that the application is scalable, can handle the expected performance requirements, and can be quickly deployed and updated as needed.","GenTime":"2024-07-28 23:55:03"}
{"File Name":"adr-demo\/0001-record-architecture-decisions.md","Context":"## Context\\nMost projects suffer from a common problem : they usually don't keep a clear history of all the architectural decisions if the project.\\nIt might not appear as an issue at first but as the project evolves it becomes less and less clear why each change was made,\\nleading to somewhat wrong decisions : should we change existing code and take the risk of breaking the application because\\nwe might have missed an important decision, or should we keep it (fearing we might break something) and take the risk of\\nparalyzing the project with an accumulation of potentially wrongly-kept decisions and changes ?\\nTo avoid this dilemna it appears we have to do something to keep a record of all architectural decisions.\\n\n## Decision\n","Decision":"We will start using Lightweight Architecture Decision Records (further refered as ADR) as explained\\n[here](https:\/\/blog.stack-labs.com\/code\/adr-to-remember-past-archiectural-decisions\/#format-d-un-adr)\\nor [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nHere are a few hints of how we will use ADRs :\\n* We will keep all ADRs in a Git respository so they can be versioned\\n* We will materialize Each ADR as a separate file within the repository\\n* We will prefix each ADR by an ordered number (0001 to 9999), and keep ADRs numbers sequential\\n* We will keep each ADR as short as possible, trying to limit ourselves to 1-2 pages max\\n* We will use Markdown as the template engine of our ADRs\\n* We will always keep ALL written ADRs but we will mark old ADRs as superseded if they are\\nThe markdown format we will use when writing an ADR is the following :\\n```markdown\\n# Title: These documents have names that are short noun phrases. For example, \"ADR-0001 - Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\nThis section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":146,"id":2517,"Prediction":"## Decision\n\nTo address the problem of lacking a clear history of architectural decisions, we have decided to implement and maintain an Architectural Decision Record (ADR) system for our project. This system will document every significant architectural decision made, including the context, the decision itself, and the consequences of the decision. \n\nEach ADR will be version-controlled and stored in the project's repository, ensuring it remains accessible and up-to-date. By adopting this practice, we aim to provide a transparent history of our architectural evolution, which will aid in understanding past decisions, evaluating risks associated with changes, and making informed decisions moving forward.","GenTime":"2024-07-28 23:55:05"}
{"File Name":"volley-management\/0006-use-azure-table-storage-for-persistence.md","Context":"## Context and Problem Statement\\nWe need a persistent state for the system. Previously I've used Azure SQL but it is quite pricey but has it's own set of features including relational model. But given that system is not very complex I can model persistence mechanism for NoSQL model.\\n## Decision Drivers\\n* Cost\\n* Backup capabilities\\n\n## Decision\n","Decision":"* Cost\\n* Backup capabilities\\nChosen option: \"Azure Table Storage\", because cost.\\n### Positive Consequences <!-- optional -->\\n* Persistence bill should be down from ~$5\/month to less than $1\/month\\n### Negative Consequences <!-- optional -->\\n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing. We will have to come up with a strategy to get them good experience.\\n","tokens":68,"id":2520,"Prediction":"## Decision\nAfter evaluating both Azure SQL and a NoSQL alternative, we have decided to adopt a NoSQL database for our persistence mechanism. Specifically, we will use Amazon DynamoDB due to its lower cost compared to Azure SQL, its sufficient backup capabilities, and the flexibility it offers in modeling data for our not very complex system. This choice aligns well with our primary decision drivers\u2014cost efficiency and adequate backup support.","GenTime":"2024-07-28 23:55:15"}
{"File Name":"volley-management\/0002-use-sendgrid-as-mailing-system.md","Context":"## Context and Problem Statement\\nWe need a way to send emails to users for several workflows.\\n\n## Decision\n","Decision":"Chosen option: SendGrid, because it is SaaS and has generous free tier. Integration is easy.\\n### Positive Consequences <!-- optional -->\\n* Fast time to develop\\n### Negative Consequences\\n* We need to think about setting up development environment\\n","tokens":20,"id":2523,"Prediction":"## Decision\n\nWe will use a third-party email service provider (ESP) to handle the sending of emails for our workflows. Specifically, we will integrate with a well-established and reliable ESP such as SendGrid, Mailgun, or Amazon SES. This decision is based on the following considerations:\n\n1. **Reliability**: These providers have robust infrastructure to ensure high deliverability and minimal downtime.\n2. **Scalability**: They can handle large volumes of emails as our user base grows.\n3. **Features**: Many ESPs offer features such as tracking, analytics, templates, and more advanced email functionalities that would be beneficial for our workflows.\n4. **Security**: These providers offer secure handling of email data, compliant with regulations like GDPR and HIPAA (where applicable).\n5. **Cost-Effectiveness**: Outsourcing email functionality can be more cost-effective than maintaining our own mail servers and infrastructure, especially when considering the operational overhead.\n6. **Ease of Integration**: Most ESPs provide well-documented APIs and SDKs for various programming languages, allowing for seamless integration into our existing codebase.\n\nBy choosing an ESP, we aim to ensure that our email communication is reliable, efficient, and scalable, while also freeing our development and operations teams to focus on core product functionalities.","GenTime":"2024-07-28 23:55:27"}
{"File Name":"volley-management\/0007-follow-optimistic-concurrency.md","Context":"## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n\n## Decision\n","Decision":"* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\nChosen option: \"Optimistic concurrency\", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).\\n### Positive Consequences <!-- optional -->\\n* Concurrency check will fail very rarely so it will have almost no overhead\\n* It is aligned with Azure Table storage Optimistic concurrency\\n### Negative Consequences <!-- optional -->\\n* Whole application will have a responsibility to manage versions properly\\n* If we change a datastore we might need to have a larger change\\n","tokens":86,"id":2526,"Prediction":"## Decision\nWe have decided to implement Optimistic Concurrency Control (OCC) for handling concurrent changes to data in Volley Management.\n\n### Rationale:\n1. **Performance:** Optimistic Concurrency Control is well-suited for high-throughput scenarios where conflicts are relatively rare. This approach will allow the system to proceed with operations without locking resources, thus maintaining high performance.\n2. **Maintainability:** OCC typically involves fewer code changes compared to other concurrency control mechanisms like Pessimistic Concurrency Control (PCC), where complex locking mechanisms might need to be implemented. By using version numbers or timestamps, we can minimize the amount of code needed to handle concurrency checks.\n\n### Implementation Detail:\n- **Versioning:** Each record in the database will have a version number. Whenever an update is to be made, the system will check the version number of the record. If the version number in the database matches the one in the application, the update proceeds. If not, a conflict resolution strategy (e.g., retry or inform the user) is triggered.\n- **Conflict Handling:** In case of version mismatch, a retry mechanism will be employed to attempt the operation again. If the conflict persists, the user will be informed to manually resolve the conflict.\n\nThis approach will cater to both our performance and maintainability goals, while ensuring the integrity and consistency of the data during concurrent operations.","GenTime":"2024-07-28 23:55:41"}
{"File Name":"gp-data-etl\/0002-store-local-copy-of-syndication-data.md","Context":"## Context\\nThe [NHS's Syndicated Content](http:\/\/www.nhs.uk\/aboutNHSChoices\/professionals\/syndication\/Pages\/Webservices.aspx)\\nholds information about GP surgeries.\\nSeveral of Connecting to Services applications need to make use of GP data, including the\\n[GP Finder](https:\/\/github.com\/nhsuk\/gp-finder) and [Profiles](https:\/\/github.com\/nhsuk\/profiles) projects.\\nThe Syndication API is not intended for direct access in production environments.\\nThe Syndication API returns data in XML, and information is spread across several subpages,\\ni.e. overview, services, facilities.\\n\n## Decision\n","Decision":"The syndication XML API should be scraped nightly to create a local copy of the data.\\nWhat were multiple pages for a practice on the Syndication API will be merged into a single record per practice and\\nconverted into the Connecting to Services teams preferred format, JSON.\\n","tokens":137,"id":2530,"Prediction":"## Decision\n\nTo mitigate the limitations of the NHS Syndicated Content API for direct access in production environments, we will develop an intermediate service that will consume and aggregate GP surgery data from the NHS Syndication API. This service will:\n\n1. **Data Aggregation:** Collect and consolidate data from the necessary subpages (overview, services, and facilities).\n2. **Data Transformation:** Convert the XML responses into a more easily consumable format, such as JSON.\n3. **Caching Mechanism:** Implement caching to reduce the number of requests made directly to the NHS Syndication API, improving performance and reducing load on the API.\n\nThis intermediate service will serve as a single point of access for all applications requiring GP data, such as the GP Finder and Profiles projects, ensuring a more robust, efficient, and scalable solution.","GenTime":"2024-07-28 23:55:55"}
{"File Name":"terraform-aws-dynamodb\/0004-terraform-requirements-in-module.md","Context":"## Context\\nTerraform enables you to constrain the version of Terraform able to run a\\nmodules, as well as the minimium allowed versions of providers. Many of LIC\\nexisting modules do not leverage these configurations.\\n\n## Decision\n","Decision":"The Terraform version able to run this module will be set in `main.tf`.\\nThe miniumum allowed versions of providers will be set in `main.tf`\\n","tokens":46,"id":2536,"Prediction":"## Decision\n\nWe have decided to implement version constraints for Terraform and provider versions across all existing modules at LIC. This involves specifying the required versions of Terraform and the minimum allowed versions of providers in each module\u2019s configuration files.\n\n- For Terraform itself, we will add a version constraint to ensure that modules only run with versions that we have tested and verified.\n- For providers, we will specify the minimum version required to ensure compatibility and leverage the latest features and security updates.\n\nWe will review and update each module to add these version constraints, as part of our ongoing maintenance and improvement efforts. This decision aims to enhance the stability, security, and predictability of our infrastructure codebase.","GenTime":"2024-07-28 23:56:10"}
{"File Name":"terraform-aws-dynamodb\/0005-feature-toggles.md","Context":"## Context\\nToggles are a useful aspect of Terraform modules that provide the ability to\\nturn on\/off functionality.\\nCapital One have written about\\n[Building Feature Toggles into\\nTerraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)\\n. This build on top of the use of the count and ternary to provide a more robust\\nfeature toggle.\\nAs this module is envisioned to be part of a larger Data Storage Module there is\\na concept where no DynamoDB table may need to be provisioned. In addition, when\\nusing DynamoDB Autoscaling can be a useful feature. Providing the capabilities\\nto turn this on and off based on requirements is also a useful requirement.\\n\n## Decision\n","Decision":"Feature toggles will be leveraged to determine:\\n* If any resources should be created\\n* If a DynamoDB resource should be created\\n* If the Autoscaler should be enabled\\n","tokens":160,"id":2538,"Prediction":"## Decision\nIncorporate feature toggles within the Terraform module to control the provisioning and configuration of DynamoDB tables and Autoscaling functionality.\n\n### Justification:\n1. **Modular Flexibility:** Feature toggles will allow different consumers of the module to enable or disable specific features, thereby offering flexibility to accommodate varying requirements.\n2. **Resource Efficiency:** By enabling toggling, unnecessary resources can be avoided, leading to potential cost savings and improved performance.\n3. **Scalability:** With the ability to toggle Autoscaling on and off, the module can be adaptable to different scaling needs and use cases, optimizing resource allocation accordingly.\n\n### Implementation Strategy:\n1. **Provisioning Configuration:**\n   - Use a `count` parameter combined with a ternary operator to control the creation of the DynamoDB table based on a variable (e.g., `dynamodb_table_enabled`).\n   - Implement a similar toggle for Autoscaling (e.g., `autoscaling_enabled`).\n\n2. **Configuration Example:**\n   ```hcl\n   variable \"dynamodb_table_enabled\" {\n     description = \"Enable\/Disable DynamoDB table provisioning\"\n     type        = bool\n     default     = true\n   }\n\n   resource \"aws_dynamodb_table\" \"example\" {\n     count      = var.dynamodb_table_enabled ? 1 : 0\n     name       = \"example_table\"\n     hash_key   = \"ID\"\n     ...\n   }\n\n   variable \"autoscaling_enabled\" {\n     description = \"Enable\/Disable DynamoDB Autoscaling\"\n     type        = bool\n     default     = false\n   }\n\n   resource \"aws_appautoscaling_target\" \"dynamodb_target\" {\n     count      = var.autoscaling_enabled ? 1 : 0\n     ...\n   }\n   ```\n\n3. **Documentation:**\n   - Update the module documentation to include the new variables and provide guidelines for enabling or disabling these features.\n   - Include example scenarios showcasing the toggle functionality for better user understanding.\n\nBy incorporating these toggles, the Terraform module will be more adaptable and align seamlessly with the broader objectives of the Data Storage Module, ensuring both flexibility and efficiency in resource management.","GenTime":"2024-07-28 23:56:19"}
{"File Name":"terraform-aws-dynamodb\/0010-terraform-backend-in-module.md","Context":"## Context\\nWhen LIC teams begun using this module it became apparent that the current\\nimplementation pattern does not meet their needs. Without a backend in the\\nmodule teams would need to add a Terraform `backend` configuration into there\\nlocal implementation for it to be picked up.\\n\n## Decision\n","Decision":"Restored the `backend` into the module for the time being.\\nWe still feel this should be removed at some time and teams become familiar with\\nhow to use Terragrunt\/Terraform configuring there own `backend`.\\n","tokens":57,"id":2539,"Prediction":"## Decision\n\nTo address the issue identified within the LIC teams, the decision is to implement a centralized backend configuration for the module. Instead of requiring each team to manually configure a Terraform `backend` in their local implementations, we will introduce a default backend configuration as part of the module itself. This change aims to standardize the setup process, reduce configuration overhead for teams, and ensure consistency across different environments.\n\n### Implementation Steps:\n\n1. **Define Centralized Backend Configuration**: Incorporate a default backend configuration (e.g., S3 for AWS or Azure Storage for Azure) within the module. This ensures all teams are using a uniform backend storage.\n   \n2. **Parameterize Backend Configuration**: Allow teams to override the default backend settings if they have specific needs. This can be achieved by introducing optional variables that can be passed to customize the backend configuration.\n\n3. **Documentation Update**: Provide clear documentation on how to leverage the default backend and how to override it if necessary. Include examples and migration steps for teams transitioning to the new setup.\n\n4. **Backward Compatibility**: Ensure the new backend configuration does not disrupt existing implementations. Introduce transitional guidance and support for teams currently using their own backend configurations.\n\nBy adopting this approach, we aim to streamline the workflow for the LIC teams, enabling them to focus on core development rather than repetitive configuration management tasks.","GenTime":"2024-07-28 23:56:22"}
{"File Name":"terraform-aws-dynamodb\/0007-map-list-variables.md","Context":"## Context\\nDynamoDB provides the ability to supply additional attributes, a local\\nsecondary index and a global secondary index. These additional attributes\\nconsumed by the DynamoDB AWS Provider as maps\/lists.\\nIn addtion to the consumption as maps\/lists there are additional requirements\\nthat the range\/hash keys be added to the additional attributes if declared. They\\nare not added if undeclared.\\n\n## Decision\n","Decision":"The module will use a `null_resource` to manage the secondary indexes. Creating\\nthem from the existences of appropriate maps\/lists.\\nProperties related to these resources will consume a list of maps as input.\\nThese will them be mapped to the appropriate resource within in the module.\\nThe range\/hash key will be added automatically to the additional attributes by\\nthe module, reducing the load on the consumer with implementation detail.\\n","tokens":80,"id":2543,"Prediction":"## Decision\n\nWe have decided to implement additional attributes as maps\/lists within DynamoDB using the AWS Provider, enabling robust functionality that aligns with our requirements. Specifically:\n\n1. **Usage of Maps\/Lists for Additional Attributes**: We will utilize maps\/lists to represent additional attributes in DynamoDB tables. This provides a flexible schema and allows us to easily manage and extend attribute sets as needed.\n\n2. **Local and Global Secondary Indexes**: Both local secondary indexes (LSIs) and global secondary indexes (GSIs) will support these additional attributes. This ensures that secondary indexes maintain consistent and versatile query capabilities across different attribute sets.\n\n3. **Inclusion of Range\/Hash Keys**: \n    - When additional attributes include range\/hash keys, we will ensure they are incorporated into the declared attributes. This will maintain the integrity and usability of the data, allowing for accurate indexing and querying.\n    - If range\/hash keys are not declared within the additional attributes, they will not be included. This approach ensures that our attribute set remains concise and avoids unnecessary data pollution.\n\n4. **Implementation Strategy**: \n    - A thorough review of existing schemas will be undertaken to identify where additional attributes and indexing strategies can be applied.\n    - Testing will be conducted to verify that maps\/lists and range\/hash key conditions are correctly implemented, ensuring no disruption or performance degradation.\n    - Documentation will be created to assist developers in understanding how to use additional attributes effectively within our DynamoDB implementations.\n\nBy adopting this approach, we can enhance our data model's flexibility and scalability, ensuring our application supports a wide range of querying and indexing requirements without compromising on data integrity.","GenTime":"2024-07-28 23:56:36"}
{"File Name":"launcher\/2018-06-15_request_enrollment_details.md","Context":"## Context\\nWhen enrolling osquery into a remote server, it's desirable to also have a set of initial fields about the osquery node. Usually, servers like Fleet queue ad-hoc queries to get this information, but it's not always reliable. For example, a osquery host can succeed during the Enroll method, and then immediately crash, leaving the server operator with little context about which osquery host to troubleshoot. Another effect of populating host details via distributed query is that they can add some latency to the enrollment process depending on the query interval.\\n\n## Decision\n","Decision":"Add a EnrollmentDetails structure to the RequestEnrollment method which sends the results of a population query as part of the enrollment. Below is a list of chose attributes that launcher will always send:\\n```\\nmessage EnrollmentDetails {\\nstring os_version = 1;\\nstring os_build = 2;\\nstring os_platform = 3;\\nstring hostname = 4;\\nstring hardware_vendor = 5;\\nstring hardware_model = 6;\\nstring hardware_serial = 7;\\nstring osquery_version = 8;\\nstring launcher_version = 9;\\n}\\n```\\n","tokens":116,"id":2546,"Prediction":"## Decision\n\nTo ensure reliable and immediate capture of initial fields about the osquery node during the enrollment process, the decision is to modify the osquery's Enroll method to include these initial fields as part of the enrollment payload. This means that instead of relying on subsequent ad-hoc queries, which introduce potential unreliability and latency, the necessary node information will be sent and logged during the enrollment itself.\n\n### Rationale\n1. **Reliability**: Including initial fields in the enrollment payload guarantees that essential node information is captured right at the moment of enrollment, even if the osquery host crashes immediately afterward.\n2. **Reduced Latency**: This approach eliminates the waiting period associated with distributed queries, resulting in quicker availability of node information.\n3. **Improved Troubleshooting**: Immediate logging of node details enhances the ability to troubleshoot, as operators will have contextual information available from the outset.\n4. **Simplified Workflow**: By eliminating the need for additional ad-hoc queries post-enrollment, the enrollment process becomes more straightforward and efficient.\n\n### Implementation\n- Modify the Enroll method in the osquery codebase to capture and include key initial fields about the host (e.g., hostname, IP address, operating system version, etc.) in the enrollment payload.\n- Ensure backward compatibility and allow for optional fields so that the enrollment process does not break for clients that do not need this additional information.\n- Update the server-side logic to handle and store these additional fields upon receiving the enrollment payload.\n\n### Potential Drawbacks\n- This approach may slightly increase the size of the enrollment payload, but this is generally outweighed by the benefits of having immediate and reliable node information.\n- Modifying the Enroll method will require coordination and updates across both the osquery client and the server-side components to ensure smooth integration.\n\nBy implementing this change, the system will have a more robust and efficient mechanism for capturing and utilizing initial osquery node details upon enrollment.","GenTime":"2024-07-28 23:56:49"}
{"File Name":"launcher\/2018-03-29_add_crc_and_uuid_to_log_lines.md","Context":"## Context\\n**TL;DR**\\nA lack of unique info per-log line makes it hard to uniq osquery data downstream, especially in cases when distinct log lines with identical data can be generated. To improve visibility, we should add a UUID to each emitted line. Additionally, adding a hash of the data lets you uniq log line data without inspecting all the fields of these log lines.\\n**Detail**\\nWhile a unique ID is generated for every batched log event emitted from launcher, individual log lines within that log cannot be determined to be unique. This is both true for distinct logs which are identical in their contents, and in the instance that the downstream data pipeline cannot guarantee \"exactly once\" delivery of messages.\\nBy including a UUID for each log line, distinct but semantically identical log lines of data can be determined not to be duplicates.\\nAdditionally, by including a CRC of the data inside each log line, downstream consumers of the log can identify log lines with identical data without inspecting the entirety of log line's data.\\nNote that the crc must be calculated before adding the uuid, or else it won't be useful\\n\n## Decision\n","Decision":"Launcher should unpack logs from osquery and decorate each line with both a CRC of the data, and a UUID.\\n","tokens":236,"id":2549,"Prediction":"## Decision\nWe will enhance the log emission process by including a UUID and a CRC in each log line. This change will ensure that each log line is uniquely identifiable and allow downstream processes to efficiently determine data uniqueness without examining all fields of a log line.\n\n### Implementation Specifics:\n1. **UUID Generation**:\n   - Each log line emitted from the launcher will include a unique UUID.\n   - The UUID will be generated at the time of log line creation.\n\n2. **CRC Calculation**:\n   - A CRC will be calculated for the data contained within each log line (excluding the UUID to avoid altering the intended data CRC).\n   - This CRC will be appended to the log line.\n\n### Benefits:\n- **Enhanced Uniqueness**: The inclusion of UUIDs will eliminate ambiguities regarding log line uniqueness and prevent confusion arising from semantically identical but distinct log lines.\n- **Efficient Data Inspection**: The CRC will allow downstream consumers to identify identical data efficiently without comparing all log line fields, thus optimizing processing and reducing computational overhead.\n\n### Considerations:\n- The UUID should be generated using a standard, reliable library to ensure uniqueness.\n- The CRC algorithm should be chosen based on performance and collision resistance characteristics.\n- Potential impact on log storage and transmission sizes should be evaluated, given the added UUID and CRC fields.\n\nThis improvement will provide better data integrity and streamlined processing within our data pipeline, ensuring more accurate and reliable osquery data handling.","GenTime":"2024-07-28 23:57:02"}
{"File Name":"titania-os\/0002-docker-for-dapps.md","Context":"## Context\\nTitania is supposed to run multiple dApps. There are multiple techologies and concepts on how to define and isolate a dApp.\\n\n## Decision\n","Decision":"We will use LXC technology and represent a dApp as a container. We will use Docker to build, run and manage said containers.\\n","tokens":31,"id":2573,"Prediction":"## Decision\nAfter evaluating various technologies and methods for defining and isolating decentralized applications (dApps) within Titania, we have decided to adopt the containerization approach using Kubernetes as the orchestration platform.\n\n### Rationale:\n1. **Scalability**: Kubernetes allows for easy scaling of dApps based on demand, ensuring that resources are allocated efficiently.\n2. **Isolation**: By containerizing each dApp, we can ensure that they run in isolated environments, preventing dependency conflicts and enhancing security.\n3. **Flexibility**: Kubernetes supports a wide range of container runtimes and provides extensive support for both stateless and stateful applications, accommodating a variety of dApp requirements.\n4. **Orchestration**: Kubernetes' powerful orchestration capabilities will manage the deployment, scaling, and operations of containerized applications, simplifying the operational overhead.\n5. **Community and Support**: Kubernetes has a large and active community, providing extensive documentation, community support, and ongoing improvements.\n\nUsing Kubernetes to manage the deployment and lifecycle of dApps will provide the necessary isolation and resource management, while also allowing Titania to scale as the number of dApps grows.","GenTime":"2024-07-28 23:58:34"}
{"File Name":"titania-os\/0005-networkmanager-for-networking.md","Context":"## Context\\nSystemD provides native tools for managing the network connections. NetworkManager is an alternative service that provides more features and can be controlled and queried with a DBus interface. Wicd is another alternative, but is rarely used in embedded applications. There are several other options, but they are not feature full compared to NetworkManager.\\n\n## Decision\n","Decision":"We will disable systemd network capability and use NetworkManager for managing the networks (duh!).\\n","tokens":69,"id":2574,"Prediction":"## Decision\n\nWe have decided to utilize NetworkManager for managing network connections within our system. The key reasons for this decision include:\n\n1. **Feature-Rich**: NetworkManager provides a wide range of features that are essential for our application requirements, surpassing the capabilities of SystemD's native tools and other alternatives like Wicd.\n   \n2. **DBus Interface**: NetworkManager's ability to be controlled and queried through a DBus interface offers greater flexibility and ease of integration with other components in our system.\n\n3. **Community and Support**: NetworkManager has a large user base and is actively maintained, ensuring continued support and availability of updates and documentation.\n\n4. **Suitability for Embedded Applications**: While Wicd is rarely used in embedded applications, NetworkManager is more commonly adopted and better suited for our needs in this context.\n\nBy choosing NetworkManager, we ensure a robust, flexible, and feature-complete solution for managing network connections in our application.","GenTime":"2024-07-28 23:58:37"}
{"File Name":"slingshot\/0003-use-one-geoserver-instance-with-multiple-workspaces.md","Context":"## Context\\nThere are a few problems that need to be solved around handling authentication with GeoServer:\\n1. We have some layers which can only be accessed by authenticated users and some that are publicly accessible.\\n2. The user never interacts directly with GeoServer. Requests to GeoServer for data come from the user's browser either through `img` tags or through AJAX requests from Leaflet. This means the user needs to be preauthenticated with GeoServer in some way.\\n3. GeoServer's authentication is incredibly difficult to work with.\\nIn the past, we solved these problems by leveraging the SSO capabilities of Shibboleth and simply running two GeoServer instances--one that was publicly accessible and one that was behind Shibboleth. Touchstone authentication happened for the user through the Rails application. Given the decision to move away from Shibboleth this is no longer an option. Running two GeoServer instances also adds additional burdens to deployment, management and the data publication process.\\n\n## Decision\n","Decision":"Run a single GeoServer instance. GeoServer supports what it calls workspaces, which is just a way of partitioning layers within the system. The authentication for each workspace can be configured separately. Configure one workspace to be readable by anyone and one workspace to require Basic Auth using a predefined username and password.\\nAccess to this GeoServer instance will need to be proxied. The proxy (our Geoblacklight instance) will handle SAML authentication and augment the proxy request to GeoServer with the Basic Auth login if the user has authenticated with Geoblacklight.\\n","tokens":200,"id":2578,"Prediction":"## Decision\n\nTo address the issues of authenticating user access to specific data layers in GeoServer and streamline the deployment and management processes, the following actions will be taken:\n\n1. **Implement OAuth2 Authentication Mechanism:**\n   We will integrate OAuth2 for authentication, allowing users to preauthenticate transparently through a central identity provider. This allows for single sign-on (SSO) capabilities without the complexities of Shibboleth.\n\n2. **Configure GeoServer for Token-Based Authentication:**\n   Modify the GeoServer setup to accept and validate OAuth2 tokens in its requests. This involves:\n   - Setting up an OAuth2 Provider (e.g., Keycloak, Auth0) that issues tokens upon successful authentication.\n   - Customizing GeoServer to validate these OAuth2 tokens and enforce access control based on the token's claims.\n\n3. **Single GeoServer Instance with Access Control Rules:**\n   Operate a single GeoServer instance, simplifying deployment and data management. Configure GeoServer's security subsystem to control access to layers based on user roles and permissions derived from the OAuth2 tokens.\n\n4. **Proxy Server for Request Handling:**\n   Introduce a lightweight proxy server (e.g., Nginx, Node.js) that handles the authentication flow before routing requests to GeoServer. This proxy server will:\n   - Validate OAuth2 tokens in incoming requests.\n   - Redirect unauthenticated users to the OAuth2 authentication endpoint.\n   - Forward validated requests to GeoServer, appending the necessary headers or tokens that GeoServer requires for authorization.\n\n5. **Client-Side Adjustments:**\n   Update the client-side code (Leaflet AJAX requests, `img` tags) to include the OAuth2 token in headers or as query parameters when making requests to the proxy server. This ensures the user is preauthenticated before any request reaches GeoServer.\n\nBy implementing OAuth2 and a proxy server, we streamline the handling of authenticated access to GeoServer data layers while avoiding the complexity of running multiple GeoServer instances and moving away from Shibboleth. This approach also provides flexibility for future changes in the authentication provider.","GenTime":"2024-07-28 23:58:55"}
{"File Name":"thing\/0002-authentication-via-touchstone-saml.md","Context":"## Context\\nA mechanism for providing end-user authentication of MIT users is required for\\ncore aspects of this application.\\nMIT officially only supports Shibboleth \/ Touchstone which traditionally is enabled via an Apache httpd module.\\nThe [MIT OpenID Pilot](https:\/\/mitlibraries.github.io\/oauth.html) was determined\\nunacceptable for this project.\\nIn order to use Shibboleth we intended to containerize the application. However,\\nwhile that process ended in a successful proof-of-concept, it used SAML and not\\nmod_shib. Once we went down the path of not using the officially MIT supported\\nmod_shib solution, we felt using SAML in-app should be explored.\\n\n## Decision\n","Decision":"We will use an in-app Touchstone SP using ruby-saml as documented in our\\n[Developer Documentation](https:\/\/mitlibraries.github.io\/touchstone_saml.html).\\n","tokens":144,"id":2582,"Prediction":"## Decision\n\nWe have decided to implement Shibboleth via SAML in-app for end-user authentication of MIT users. This decision allows us to align with MIT's officially supported authentication method while providing the flexibility to integrate the authentication mechanism directly within our application's codebase. \n\nBy containerizing the application and using SAML in-app, we create a more consistent development and deployment environment, allowing us to maintain control over the authentication process and streamline user access management. This approach also mitigates the downsides encountered with the MIT OpenID Pilot and the limitations posed by the traditional Apache httpd mod_shib solution.","GenTime":"2024-07-28 23:59:07"}
{"File Name":"thing\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2583,"Prediction":"## Decision\n\nWe will implement an Architectural Decision Record (ADR) process to systematically document and manage the architectural decisions made during the project. This process ensures that all decisions are captured, the context and consequences are well understood, and the reasons for making decisions are clear and accessible to all stakeholders.\n\n### Key Steps\n1. **Template Selection:** We will use a standardized ADR template that captures essential information such as context, decision, rationale, and consequences.\n2. **Decision Logging:** All major architectural decisions will be documented using the chosen template. Each ADR will be version-controlled, allowing for historical tracking and reference.\n3. **Review Process:** Each ADR will be reviewed by key stakeholders to ensure a comprehensive understanding and agreement before finalizing and implementing the decision.\n4. **Storage and Access:** ADs will be stored in a centralized repository accessible to all team members. This could be a shared directory, a wiki, or a dedicated ADR management tool.\n5. **Regular Updates:** Periodic reviews will be conducted to update the ADRs to reflect any changes or new insights that emerge over time.\n\nBy following this process, we aim to maintain transparency, encourage collaboration, and ensure that our architectural decisions are deliberate and well-considered.","GenTime":"2024-07-28 23:59:10"}
{"File Name":"Corvus.Extensions.Newtonsoft.Json\/0001-json-framework-independence.md","Context":"## Context\\nWe frequently need to deal with information that will be serialized as JSON, either because it needs to be used in a web API, or because it is serialized in some JSON-based storage system. For many years, Json.NET has been the de facto API for working with JSON in .NET applications. However, now that .NET Core has `System.Text.Json` built in, there are good reasons to want to move to that. This means that libraries that depend on Json.NET become problematic.\\nSo we need ways for our libraries to work with JSON data without forcing the decision of whether to use Json.NET or `System.Text.Json`.\\n\n## Decision\n","Decision":"We have moved types previously in `Corvus.Extensions.Newtonsoft.Json` into `Corvus.Json.Abstractions`, having modified them to remove any direct dependency on Json.NET.\\nCurrently, only the property bag type has been moved. (And it is now an `IPropertyBag` interface.) A great deal of what's in `Corvus.Extensions.Newtonsoft.Json` is already in the right place, because it deals directly with Json.NET-specific concerns (e.g., custom type converters).\\n","tokens":132,"id":2585,"Prediction":"## Decision\n\nTo ensure flexibility and avoid forcing a dependency on either `Json.NET` or `System.Text.Json`, we will implement an abstraction layer for JSON serialization and deserialization within our libraries. This abstraction will define a common interface that both `Json.NET` and `System.Text.Json` can implement. \n\n### Specific Actions:\n1. **Create a JSON Serialization Abstraction:**\n   - Define an interface `IJsonSerializer` with methods for serializing and deserializing objects to and from JSON strings.\n\n2. **Implement the Interface for `System.Text.Json`:**\n   - Create a class `SystemTextJsonSerializer` that implements `IJsonSerializer` using `System.Text.Json`.\n\n3. **Implement the Interface for `Json.NET`:**\n   - Create a class `JsonNetSerializer` that implements `IJsonSerializer` using `Json.NET`.\n\n4. **Factory or Dependency Injection:**\n   - Utilize a factory pattern or dependency injection to allow consumers of the library to choose their preferred JSON serialization implementation.\n\n5. **Configuration Option:**\n   - Provide configuration options within the library to easily switch between `System.Text.Json` and `Json.NET` based on consumer preference.\n\n### Example Interface:\n```csharp\npublic interface IJsonSerializer\n{\n    string Serialize<T>(T obj);\n    T Deserialize<T>(string json);\n}\n```\n\n### Example Implementation for `System.Text.Json`:\n```csharp\npublic class SystemTextJsonSerializer : IJsonSerializer\n{\n    public string Serialize<T>(T obj)\n    {\n        return JsonSerializer.Serialize(obj);\n    }\n\n    public T Deserialize<T>(string json)\n    {\n        return JsonSerializer.Deserialize<T>(json);\n    }\n}\n```\n\n### Example Implementation for `Json.NET`:\n```csharp\npublic class JsonNetSerializer : IJsonSerializer\n{\n    public string Serialize<T>(T obj)\n    {\n        return JsonConvert.SerializeObject(obj);\n    }\n\n    public T Deserialize<T>(string json)\n    {\n        return JsonConvert.DeserializeObject<T>(json);\n    }\n}\n```\n\n### Rationale:\n- **Flexibility:** Allows users to choose their JSON library based on their project requirements.\n- **Future-proofing:** Eases future transitions, should Microsoft further improve `System.Text.Json` or if new libraries become industry standards.\n- **Minimal Disruption:** By decoupling the choice of JSON library from the core functionality, we avoid disruptions and breaking changes in the main application logic.\n","GenTime":"2024-07-28 23:59:19"}
{"File Name":"jfluentvalidation\/0001-primitive-array-constraints.md","Context":"## Context and Problem Statement\\nMy first pass at building out array constraint was to use a generic parameter `A` with `java.lang.reflect.Array` to obtain\\nthe length of the property representing `A`.\\nI was curious what the cost of using `java.lang.reflect.Array` compared to grabbing the `length` property from a known type was.\\nAnything with a name like `reflect*` gives me nightmares about terrible performance.\\nI decided to write a JMH benchmark to determine the performance impact `java.lang.reflect.Array` to assist in determining\\nwhich implementation to use.\\n## Decision Drivers\\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\n\n## Decision\n","Decision":"1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\nI decided to choose option 1 as I prioritized performance above the overhead to maintain additional classes and having\\nduplicate logic.\\nWhile it might be premature optimization and such a small impact (1 - 2 ns) the benchmark results below still convinced me.\\nI'm sure someone can convince me that the overhead is insignificant or that I simply messed up the bencharmark at which point\\nit should be easier refactor to option 2.\\nI've included a rough [implementation of option 2](#option-2-implementation) just in case.\\n```java\\npackage jfluentvalidation.constraints.array;\\nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;\\nimport jfluentvalidation.rules.PropertyRule;\\nimport jfluentvalidation.validators.RuleContext;\\nimport jfluentvalidation.validators.ValidationContext;\\nimport org.openjdk.jmh.annotations.*;\\nimport org.openjdk.jmh.runner.Runner;\\nimport org.openjdk.jmh.runner.options.OptionsBuilder;\\nimport java.util.concurrent.TimeUnit;\\n@BenchmarkMode(Mode.AverageTime)\\n@OutputTimeUnit(TimeUnit.NANOSECONDS)\\n@State(Scope.Benchmark)\\npublic class LengthBenchmark {\\npublic static class Foo {\\nprivate boolean[] bar;\\npublic Foo(boolean[] bar) {\\nthis.bar = bar;\\n}\\n}\\nRuleContext<Foo, boolean[]> ruleContext;\\nBooleanArrayExactLengthConstraintAlternative booleanArrayExactLengthConstraintAlternative;\\nArrayExactLengthConstraint arrayExactLengthConstraint;\\nBooleanArrayExactLengthConstraint booleanArrayExactLengthConstraint;\\n@Setup\\npublic void prepare() {\\nFoo f = new Foo(new boolean[5]);\\nPropertyRule propertyRule = new PropertyRule(foo -> f.bar, \"bar\");\\nruleContext = new RuleContext<>(new ValidationContext(f), propertyRule);\\nbooleanArrayExactLengthConstraintAlternative = new BooleanArrayExactLengthConstraintAlternative(5);\\narrayExactLengthConstraint = new ArrayExactLengthConstraint(5);\\n}\\n@Benchmark\\npublic void booleanArrayExactLengthConstraintAlternative() {\\nbooleanArrayExactLengthConstraintAlternative.isValid(ruleContext);\\n}\\n@Benchmark\\npublic void arrayExactLengthConstraint() {\\narrayExactLengthConstraint.isValid(ruleContext);\\n}\\npublic static void main(String[] args) throws Exception {\\nnew Runner(new OptionsBuilder()\\n.include(LengthBenchmark.class.getSimpleName())\\n.forks(1)\\n.warmupIterations(2)\\n.measurementIterations(5)\\n.build())\\n.run();\\n}\\n}\\n```\\nRun 1\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.504 | \u00b1 0.143 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.099 | \u00b1 0.022 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 2\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.436 | \u00b1 0.049 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.041 | \u00b1 0.013 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 3\\n| Benchmark                                                     | Mode  | Cnt | Score     | Error       | Units  |\\n|---------------------------------------------------------------|-------|-----|-----------|-------------|--------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | thrpt | 25  | 0.424     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | thrpt | 25  | 0.626     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt  | 25  | 2.362     | \u00b1 0.022     | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt  | 25  | 1.599     | \u00b1 0.007     | ns\/op  |\\n| LengthBenchmark.arrayExactLengthConstraint                    | ss    | 5   | 48261.200 | \u00b1 4493.186  | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | ss    | 5   | 18690.000 | \u00b1 23530.434 | ns\/op  |\\nA ~20% performance impact on throughput and even larger on average time is a bit too much for me to ignore.\\n### Positive Consequences\\n- Gain roughly a 20% performance instead in average time and throughput.\\n### Negative Consequences\\n- We need 36 classes (8 primitive types + 1 object type and 4 constraint classes for each type) compared to 4 constraint classes\\n(ExactLength, BetweenLength, MinimumLength, MaximumLength) to implement array length constraints for primitive arrays.\\n- Duplicate logic in constraint classes across types.\\n","tokens":164,"id":2591,"Prediction":"## Decision\nWe will use `java.lang.reflect.Array` to manage array constraints despite initial concerns about performance. The JMH benchmark has provided empirical evidence supporting that the performance overhead of using `java.lang.reflect.Array` is within acceptable limits for our use case. This method provides a more universal solution that avoids the need for extensive type checks and the introduction of additional classes, adhering to our second decision driver. \n\nBy leveraging `java.lang.reflect.Array`, we can maintain a single, clean implementation for handling array constraints without the risk of code duplication across different types. Additionally, this approach will make the codebase easier to maintain and extend in the future. Therefore, the performance trade-off is justified by the reduction in complexity and improved maintainability.","GenTime":"2024-07-28 23:59:38"}
{"File Name":"cljdoc\/0008-use-circleci-as-analysis-sandbox.md","Context":"## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https:\/\/github.com\/martinklepsch\/cljdoc\/blob\/72da65055ab94942f33fb63b29b732e81b559508\/doc\/isolation.md)\\n\n## Decision\n","Decision":"For a first version of cljdoc we will use CircleCI to run analysis for us. The result of\\nthis analysis will be made available as a build artifact which can then be laoded in\\na trusted environment to import data into Grimoire and build HTML (or other) documentation\\nfrontends.\\n","tokens":160,"id":2601,"Prediction":"## Decision\nTo address the potential security risks and computational cost of running untrusted Clojure code, the decision is to use a managed service that provides inherent isolation and monitoring capabilities. We will use AWS Fargate, a serverless compute engine for containers on AWS, to execute these jobs inside isolated Docker containers. AWS Fargate handles the underlying infrastructure management, which mitigates the risk of compute-stealing issues. By leveraging Fargate, we also benefit from AWS's security features, such as IAM roles and network isolation, providing a robust environment to safely execute potentially untrusted code. This decision avoids the complexity of managing our own Docker execution infrastructure and the security risks of using continuous integration services or AWS Lambda for untrusted code execution.","GenTime":"2024-07-29 00:00:22"}
{"File Name":"cljdoc\/0014-add-support-for-examples.md","Context":"## Context\\nExamples are an often suggested feature for cljdoc that could enable library authors and the community to further guide library users.\\n\n## Decision\n","Decision":"Add support for examples to cljdoc. Allow libary users to provide examples through their Git repository but also run a community-maintained examples repository where examples can be maintained outside of a project's own Git repository.\\n","tokens":28,"id":2616,"Prediction":"## Decision\nWe will implement a feature that allows library authors and the community to contribute examples to cljdoc. These examples will provide practical, real-world use cases and code snippets to help guide library users in understanding and effectively using the libraries documented on cljdoc.\n\nAfter evaluating various approaches, the following decisions have been made:\n\n1. **Example Contributions**: Authors and the community can submit examples directly through a dedicated interface on the cljdoc platform. This includes annotations, code snippets, and possibly multimedia content.\n2. **Review and Approval Process**: All submitted examples will undergo a review process to ensure quality and relevance. A dedicated team or trusted community reviewers will be responsible for this.\n3. **Categorization and Tagging**: Examples will be categorized by topics, difficulty levels, and tags to enhance discoverability. Users can search and filter examples based on these categories.\n4. **Integration with Documentation**: Examples will be directly integrated into the relevant sections of the library documentation to provide context-sensitive help.\n5. **Version Control**: Examples will be versioned alongside the documented library versions to ensure compatibility and relevance as the library evolves.\n6. **Community Voting**: Implement a community voting system to allow users to upvote or downvote examples, helping to surface the most useful examples.\n7. **Contributor Recognition**: Contributors will be acknowledged within the example section to promote community involvement and recognition.\n\nThis feature will enhance the usability and educational value of cljdoc, making it a more comprehensive and user-friendly platform for both new and experienced users.","GenTime":"2024-07-29 00:01:19"}
{"File Name":"containers\/2-.md","Context":"## Context\\n======\\nFedora has minimal base image too, and it will typically have more current versions of any software we need compared to the Ubuntu images.\\n\n## Decision\n","Decision":"======\\nChange all Dockerfiles to use the latest `fedora-minimal` image as the base and update any necessary commands to work with Fedora system tools like `dnf`.\\n","tokens":34,"id":2619,"Prediction":"## Decision\n======\n\nWe have decided to use the Fedora minimal base image for our software's Docker containers. This decision is based on the fact that Fedora typically provides more current versions of the software we need compared to the Ubuntu images. This will help us ensure that our software runs on the most up-to-date dependencies, potentially improving performance, security, and compatibility.","GenTime":"2024-07-29 00:01:25"}
{"File Name":"ftd-scratch3-offline\/0009-use-jackson-to-read-json-files.md","Context":"## Context\\nScratch save files are zip files that contain the actual code in a `project.json` file.\\nTo read this file we need a deserialization library.\\nJackson, GSON and org.json are common libraries.\\nJackson seems to offer the most features and the authors already have used it.\\nGSON offers many features Jackson also features.\\norg.json seems to be only a json parser and the use has to create the resulting object themselves, while the other libraries also feature object mapping.\\n\n## Decision\n","Decision":"We will use [Jackson](https:\/\/github.com\/FasterXML\/jackson-databind\/).\\n","tokens":104,"id":2625,"Prediction":"## Decision\n\nWe will use Jackson as the deserialization library to read the `project.json` file from the Scratch save files. \n\n### Justification:\n\n1. **Feature-Rich**: Jackson offers a comprehensive set of features, including JSON parsing, object mapping, and data binding, which are necessary for our use case.\n  \n2. **Familiarity**: The authors already have experience with Jackson, reducing the learning curve and development time required to implement and manage the deserialization process.\n\n3. **Performance**: Jackson is known for its performance in terms of read and write speeds, which can be advantageous for efficiently processing Scratch save files.\n\n4. **Community Support**: Jackson has a vast and active user community along with well-maintained documentation, providing ample resources for troubleshooting and enhancements.\n\nBy choosing Jackson, we ensure robust and efficient handling of the JSON data within our application aligned with the team's capabilities and the project's requirements.","GenTime":"2024-07-29 00:01:49"}
{"File Name":"ftd-scratch3-offline\/0004-use-multiple-projects.md","Context":"## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n\n## Decision\n","Decision":"We will try to modularize the software and will use multiple projects to achieve this goal.\\n","tokens":67,"id":2626,"Prediction":"## Decision\n\nWe have decided to structure the software into multiple smaller Gradle subprojects instead of one big monolithic Gradle project.\n\n### Rationale\n1. **Reusability**: By breaking the project into smaller, well-defined Gradle subprojects, we facilitate the reuse of code across different projects. Each subproject can be treated as a standalone module with clearly defined interfaces and dependencies.\n\n2. **Code Quality**: Smaller subprojects require us to define clear API boundaries, encouraging better modularization and separation of concerns. This leads to cleaner, more maintainable code over time.\n\n3. **Scalability**: With well-defined modules, it's easier to scale and manage the project as it grows. Each module can be independently developed, tested, and maintained.\n\n4. **Flexibility**: Individual subprojects can be independently versioned and released, allowing for more agile and flexible development cycles.\n\n5. **Team Collaboration**: Multiple subprojects can be developed in parallel by different teams, thus improving overall development efficiency and collaboration.\n\n### Implementation\n- Define a root Gradle project with multiple subprojects representing different modules of the software.\n- Ensure each subproject has its own `build.gradle` file and explicitly define dependencies between them.\n- Create clear API contracts between modules to promote clean integration.\n- Implement CI\/CD pipelines to manage builds and deployments across different subprojects.\n\nBy adopting this modular approach, we aim to achieve a more flexible, maintainable, and high-quality software product.","GenTime":"2024-07-29 00:01:53"}
{"File Name":"ftd-scratch3-offline\/0011-implement-complex-scratch-functions-in-a-helper-function.md","Context":"## Context\\nSome scratch\/ftduino functions take complex arguments that need to be verified.\\nSome scratch\/ftduino functions are complex to implement.\\nThe code for these functions could be directly generated in the java files for the specific function.\\nThe code for these functions could also be written inside a helper function, such that only a single line - that calls the helper function - is generated for such a function.\\nWriting these functions in a helper function also makes changes and code reuse easier.\\n\n## Decision\n","Decision":"Complex scratch\/ftduino functions will be implemented in helper functions and those will be called by the generated code.\\n","tokens":99,"id":2629,"Prediction":"## Decision\nWe will implement the complex scratch\/ftduino functions using helper functions. This approach will involve generating code that calls these helper functions rather than establishing the complex logic directly within the Java files for each specific function. \n\n### Rationale:\n1. **Ease of Implementation:** By offloading the complex logic to helper functions, we simplify the development process for each specific function, making it easier to manage and implement.\n\n2. **Code Reuse:** Helper functions facilitate reuse of common code, reducing redundancy and promoting maintainability.\n\n3. **Future Changes:** Any changes to the core logic only need to be updated within the helper functions, making it simpler to propagate changes across all functions that use them.\n\n4. **Verification of Arguments:** Centralized helper functions enable consistent and centralized argument verification, reducing the risk of errors and improving the robustness of the code.\n\nBy taking this approach, we achieve a more modular, maintainable, and understandable codebase, aligning with best practices in software development.\n","GenTime":"2024-07-29 00:02:02"}
{"File Name":"ftd-scratch3-offline\/0015-use-floats-for-storing-numbers-not-doubles-as-scratch-does.md","Context":"## Context\\nScratch uses Javascript's numbers type to store all numbers it uses. The Javascript number type is effectively a 64 bit double.\\nThe ftduino only supports 32-bit floats.\\n64-bit double emulation might be possible but slow and likely exhaust the available memory and program space.\\n\n## Decision\n","Decision":"We will use 32-bit floats to store any numbers.\\n","tokens":60,"id":2631,"Prediction":"## Decision\n\nGiven the constraint that the ftduino only supports 32-bit floats, and considering the performance and memory limitations of emulating 64-bit doubles, we will align with the 32-bit float representation for handling numbers within the ftduino platform. This decision ensures compatibility with the ftduino hardware without compromising on performance or memory usage.\n\n### Justification:\n\n1. **Performance:** Emulating 64-bit doubles on a platform that inherently supports only 32-bit floats could introduce significant performance overhead, slowing down the execution of the program.\n\n2. **Memory Constraints:** The ftduino has limited memory and program space. Emulating 64-bit doubles would likely exhaust these resources, leading to potential instability and infeasibility for larger applications.\n\n3. **Simplicity:** Directly using 32-bit floats on the ftduino simplifies the implementation, reducing the complexity and potential bugs associated with 64-bit double emulation.\n\n4. **Consistency:** Aligning with the hardware capabilities of the ftduino ensures predictable behavior and easier debugging.\n\n### Implementation Steps:\n\n1. **Data Handling:** Modify all numerical operations and data handling to utilize 32-bit floats.\n   \n2. **Compatibility Layer:** If necessary, develop a compatibility layer that converts numbers from 64-bit double to 32-bit float when interfacing with the ftduino.\n\n3. **Testing:** Conduct thorough testing to ensure that the numerical precision and performance meet the required standards for application scenarios.\n\n4. **Documentation:** Update documentation to reflect the use of 32-bit floats, including any limitations or considerations for developers.\n\nBy making this decision, we prioritize the pragmatic and efficient use of the ftduino's hardware capabilities, ensuring stable and performant numerical computations within the constraints of the platform.","GenTime":"2024-07-29 00:02:09"}
{"File Name":"ftd-scratch3-offline\/0013-communicate-with-the-scratch-app-via-a-local-web-server.md","Context":"## Context\\nThe scratch app has to invoke the arduino-cli program to compile the converted programs.\\nThe scratch app has to invoke the scratch-to-c program to convert the scratch blocks to a runnable arduino program.\\nIn both cases the scratch app would have to invoke a native program.\\nThis is not possible in current web browsers.\\nThis would be possible using e.g. Electron, but the authors don't know Electron and Electron would add around 100 MB size overhead and also mean that without Electron the app can't be used.\\nUsing a local web-server all that is needed is any browser, the scratch ftduino app and the local web-server.\\n\n## Decision\n","Decision":"We will use a local web-server that will be used by the scratch app to invoke certain native programs.\\nThe local web-server may also be used to host the scratch app itself for example when there is no Internet connectivity available.\\n","tokens":135,"id":2634,"Prediction":"## Decision\n\nWe have decided to implement a local web-server solution to handle the execution of native programs needed by the scratch app. This approach allows us to:\n\n1. Avoid the significant overhead of incorporating Electron, which adds around 100 MB in size and steepen the learning curve for the authors.\n2. Keep the deployment flexible, as users can utilize any web browser alongside the scratch ftduino app and the local web-server.\n3. Simplify the architecture by maintaining lightweight browser-client functionality while leveraging the power of local server processes to perform compilation and conversion tasks.\n\nThe local web-server will be responsible for handling HTTP requests from the scratch app, invoking `arduino-cli` to compile the converted programs, and running `scratch-to-c` to convert scratch blocks into runnable Arduino programs. This separation of concerns ensures that the app remains browser-agnostic and does not require additional dependencies for the end-users beyond a standard web-browser and the local server software.","GenTime":"2024-07-29 00:02:17"}
{"File Name":"ftd-scratch3-offline\/0017-don-t-support-scratch-images-sprites-and-image-sprite-related-blocks.md","Context":"## Context\\nScratch supports images\/sprites.\\nThe ftduino has no way to show images\/sprites.\\n\n## Decision\n","Decision":"Scratch images\/sprites and image\/sprite related blocks are not supported.\\n","tokens":23,"id":2637,"Prediction":"## Decision\nWe will implement a workaround for the lack of image\/sprite support on the ftduino by utilizing LED matrices or external displays as an intermediary. By connecting the ftduino to an external display module, we'll be able to show images and sprites that are handled by Scratch. The external display module can be controlled via the ftduino using appropriate communication protocols (e.g., I2C, SPI). This approach provides a method for visual representation while leveraging the ftduino's capabilities and extends its functionality to support visual feedback.","GenTime":"2024-07-29 00:02:24"}
{"File Name":"dotcom-rendering\/009-static-types.md","Context":"## Context\\nAdding static types to a JavaScript project can be incredibly useful to reduce the likelihood of introducing runtime type errors.\\nWe have considered two methods of adding static types to dotcom-rendering.\\n### Flow\\nFlow makes it easy to retrofit a useful level of implicit type checking to a legacy project.\\nHowever, it produces error messages that are difficult to understand.\\nFlow requires developers to check in types for 3rd party libraries. These can be automatically generated based on the `package.json`, but developers must remember to keep these up to date.\\nIts configuration is often too coarse-grained, meaning code gets littered with opt-outs and `any` types. It doesn't feel like a natural type system, as it is only annotations on top of a dynamic language.\\nFlow's types are not exact by default.\\nThe ecosystem is still immature.\\n### TypeScript\\nTypeScript produces more understandable error messages.\\nIt allows developers to install versioned types for third party libraries.\\nTypeScript is a language in its own right, and feels more natural. It can improve JavaScript language features to make them more type-friendly. Its configuration is more fine-grained.\\nTypes are exact by default.\\nThe ecosystem is more mature.\\n\n## Decision\n","Decision":"Use TypeScript for static typing.\\n","tokens":252,"id":2652,"Prediction":"## Decision\n\nWe have decided to adopt TypeScript for adding static types to our JavaScript project.\n\n### Reasoning:\n1. **Understandable Error Messages:** TypeScript provides clearer and more understandable error messages, which will help developers debug issues more effectively.\n2. **Third-party Library Types:** TypeScript allows us to install and maintain versioned types for third-party libraries without the need for manual updates, thus reducing maintenance overhead.\n3. **Natural Integration:** As TypeScript is a language in its own right, it integrates more naturally with JavaScript projects and can enhance JavaScript's features to be more type-friendly.\n4. **Fine-grained Configuration:** TypeScript offers more granular configuration options, reducing the need for littering code with opt-outs and `any` types.\n5. **Exact Types by Default:** TypeScript's approach of exact types by default provides a more accurate type-checking mechanism, reducing the likelihood of introducing type errors.\n6. **Mature Ecosystem:** TypeScript has a more mature ecosystem, with extensive community support and tooling, making it a more reliable choice for long-term maintenance and scalability.\n\nBy choosing TypeScript, we aim to improve code quality, reduce runtime errors, and create a more robust and maintainable codebase.","GenTime":"2024-07-29 00:03:24"}
{"File Name":"dotcom-rendering\/004-emotion.md","Context":"## Context\\nUsing a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.\\nStyletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.\\nStyled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).\\n\n## Decision\n","Decision":"We will use Emotion as our CSS-in-JS library.\\n","tokens":138,"id":2655,"Prediction":"## Decision\nWe will adopt Emotion as our CSS-in-JS library for styling components in our application.\n\n### Rationale:\n1. **Feature Set**: Emotion supports a more comprehensive set of CSS features, including nesting and sibling selectors, which are not supported by Styletron.\n2. **Component Composition**: Both Styled Components and Emotion offer a similar API for composing styled components, which will make it easier for developers to adopt and use effectively.\n3. **Additional Flexibility**: Emotion provides the added flexibility of applying styles directly using the `css` prop on React components, allowing for more dynamic and reusable styling solutions.\n4. **Performance**: Emotion has a smaller Gzipped size (5.66KB) compared to Styled Components (16.27KB), leading to potentially better performance and faster load times for our web application.\n5. **Community and Ecosystem**: Emotion has demonstrated strong community support and a mature ecosystem, ensuring ongoing maintenance, community-driven improvements, and a pool of reusable resources and examples.\n\nBy choosing Emotion, we aim to balance performance, flexibility, and ease of use, thereby enhancing both the developer experience and the performance of our web application.","GenTime":"2024-07-29 00:03:36"}
{"File Name":"dotcom-rendering\/018-react-context-api.md","Context":"# React Context API\\n## Context\\nWe don't use any state management in dotcom at the moment and this means props have to be\\nexplicitly passed around; this can lead to 'prop drilling'.\\n[This PR](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801) was a spike to demonstrate using\\nthe react context api to extract the `edition` property to prevent this.\\n\n## Decision\n","Decision":"-   Our component tree is shallow so we shouldn't implement any 'magic' that\\nsteps away from having explicit props showing where and what data a component is using.\\n-   This decision should be revisited as the codebase grows.\\n","tokens":88,"id":2667,"Prediction":"## Decision\n\nWe have decided to adopt the React Context API for state management in our dotcom project to avoid the issues associated with 'prop drilling'. This initial implementation will focus on managing the `edition` property, which has been demonstrated successfully in the PR [#801](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801).\n\nUsing the React Context API will allow us to share the `edition` state across various components without the need to pass it down through multiple levels of the component tree explicitly. This decision aims to improve code maintainability and reduce the complexity associated with managing state through props.\n\nMoving forward, we will gradually extend the usage of the React Context API to other frequently-used state properties, ensuring a more streamlined and modular approach to state management throughout the application.","GenTime":"2024-07-29 00:04:13"}
{"File Name":"archcolider\/008 At least once delivery for ready to pay order.md","Context":"## Context\\n\"Ready to pay\" orders should be handled with special care. We'd like to guarantee \"at least once delivery\" for each order. Order payment processing is a business-critical scenario because selling meals is the whole point. In this case, there should be a guarantee, that the order store and payment processor can pick the order for execution.\\nAt the same time, it's very important to avoid double payments, because of concurrency issues. When an order with a \"Ready to pay\" state arrives, in theory, it can be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.\\n\n## Decision\n","Decision":"Delivery of \"ready to pay\" orders performed by a MessageQueue software with a message acknowledgment option. Additionally, we expect that the order comes with a unique id from client devices at the time of processing. During order processing, the existence of the order with the same id can be checked and the version number should be used for staleness validation. In this case, the event with the same version will be discarded by the processing service.\\n","tokens":139,"id":2672,"Prediction":"## Decision\n\n### Ensure Exactly Once Payment Processing with idempotency and message handling\n\nTo address the requirement of at least once delivery for each order while avoiding the issue of double payments, we will implement an idempotent payment processing system along with reliable message handling mechanisms. Here are the key decisions and strategies to be adopted:\n\n1. **Idempotency Key:** Generate and use an idempotency key for each \"Ready to pay\" order. This key will ensure that repeated payment processing requests for the same order do not result in multiple charges. A unique idempotency key can be generated based on the order ID and stored alongside the order in the database.\n\n2. **Distributed Message Queue:** Utilize a distributed message queue (such as Kafka or RabbitMQ) to manage the delivery of \"Ready to pay\" orders to the payment processor. This ensures that orders are delivered at least once, while the message queue's retry mechanism helps to handle transient failures.\n\n3. **Transaction Management:** Implement a transaction management system to hold both the order processing and payment processing steps. If either step fails, the entire transaction can be rolled back to maintain consistency.\n\n4. **Database Locking\/Optimistic Concurrency Control:** Use database locking or optimistic concurrency control methods to prevent concurrent processing of the same order. This can be achieved by setting a flag or version number associated with the order's state.\n\n5. **Retry Mechanism:** Implement a retry mechanism that retries the payment processing in case of failures, using exponential backoff to avoid overwhelming the payment processor or the message queue.\n\n6. **Payment Processing Confirmation:** Ensure that the payment processor confirms the payment status (successful or failed) before updating the order status in the database.\n\nBy combining these strategies, we can ensure that each \"Ready to pay\" order is processed exactly once, fulfilling the business-critical requirement of guaranteed delivery while avoiding the problem of double payments. This approach provides a robust and scalable solution that aligns with the business needs of the order payment processing system.","GenTime":"2024-07-29 00:04:40"}
{"File Name":"archcolider\/011 Every meal delivery has pick up pin code.md","Context":"## Context\\nNetwork connection might be lost, when a meal is delivered to a fridge and a user comes to grab it. Then the fridge can't check data of the user online by card swapping, or in-app distance opening.\\nBut the fridge has a pin pad keyboard and still has quite sophisticated software and internal memory to process orders.\\nUpdate 2020-11-24:\\nWe expect that every meal has its own unique id provided by the kitchen because some meals might be customized from the general catalog. Let's say lactose-free lasagna should be addressed to a specific user.\\nThen, at the purchase or production process, we can update the user's device with the meal's unique id and generate an access code based on meal ID.\\n\n## Decision\n","Decision":"Meals dispatched from a Ghost Kitchen, will have a special 6-8 digit code.\\n","tokens":154,"id":2678,"Prediction":"## Decision\n\nImplement an Offline Access Code Verification System\n\nGiven the potential for network disruptions and the existing capability of the fridge hardware (i.e., the pin pad keyboard and internal memory), we will implement an offline access code verification system.\n\n### System Description\n1. **Unique Meal ID & User Device Sync:** Each meal will have a unique identifier provided by the kitchen. When a user purchases or customizes a meal, this meal ID is synced with the user's device.\n  \n2. **Access Code Generation:** An access code will be generated based on the meal's unique ID and the user information at purchase or production time.\n\n3. **Fridge Software Update:** The fridge's software will be updated to recognize and process these access codes locally. The internal memory will be leveraged to store and process order-related information.\n\n4. **Verification Process:** When the user arrives to collect their meal, they will enter the access code via the pin pad keyboard. The fridge will verify the code against its stored data and grant access accordingly.\n\n5. **Fallback Mechanism:** In the event of a network failure, the access code verification ensures consistent and disrupted service for users to collect their meals. Once the network is restored, the system can synchronize the data as needed.\n\n### Benefits\n- **Reliability:** Ensures that users can always access their meals, even when the network is down.\n- **Security:** Each meal is tied to a specific user, and access is controlled via the unique access code.\n- **Scalability:** The system can easily handle multiple users and customized meals without requiring constant network connectivity.\n\n### Next Steps\n1. Update the fridge software to support local access code verification.\n2. Implement the access code generation system on the user\u2019s device.\n3. Test the system to ensure smooth offline functionality and user validation.\n\nThis decision leverages the current hardware capabilities and ensures a seamless and robust user experience, irrespective of network connectivity.","GenTime":"2024-07-29 00:05:05"}
{"File Name":"archcolider\/010 Feedback System separation.md","Context":"## Context\\nA user would like to read or write a review for a meal that he has ordered. Businesses would like to obtain feedback on its service in order to improve it.\\nTwo different feedback methods:\\n- surveys (occasional questionnaires about general aspects of the app\/service)\\n- feedback (is an opinion about an order or app\/service)\\n\n## Decision\n","Decision":"We'll create a simple in app feedback system that will allow users to provide feedback about orders and service.\\nWe'll incorporate surveys into feedback in order to take advantage of 3rd parties services which specialize in feedback acquisitions such survey monkey, google surveys, etc..\\n","tokens":74,"id":2681,"Prediction":"## Decision\n\nWe will implement two distinct feedback mechanisms: surveys and order-specific feedback.\n\n1. **Surveys**: We will periodically send out questionnaires to users to gather feedback on general aspects of the app and overall service quality. These surveys will be designed to cover a variety of topics within the app and service, allowing businesses to gain insights into broader areas for improvement.\n\n2. **Order-Specific Feedback**: We will enable users to provide feedback on specific orders directly. After an order is completed, users will have the opportunity to rate and review their experience with that particular meal. This feedback will be focused on immediate, actionable insights related to individual transactions and their specific details.\n\n**Rationale**:\n- *User Engagement*: Separating surveys and order-specific feedback will ensure that users can provide detailed opinions without feeling overwhelmed. Surveys will be less frequent and more general, while order-specific feedback will be easier for users to provide immediate comments on their latest experience.\n- *Actionable Insights for Businesses*: The dual approach allows businesses to receive both high-level feedback to improve overall service and detailed information about specific orders, helping them identify and resolve specific issues.\n- *Feedback Convenience*: Clearly delineating the two types of feedback makes it easier for users to understand where their input is directed, increasing the quality and relevance of the information collected.\n\nThis decision balances the need for comprehensive service evaluation with user convenience and actionable insights.","GenTime":"2024-07-29 00:05:19"}
{"File Name":"functionaut\/0002-use-google-closure-compiler.md","Context":"## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n\n## Decision\n","Decision":"We acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n","tokens":38,"id":2687,"Prediction":"## Decision\nWe will integrate Babel and ESLint into our development workflow.\n\n- **Babel:** This will enable the transformation of modern JavaScript syntax and features into a syntax compatible with older JavaScript environments. By using Babel, we ensure that our application can run in all supported browsers while allowing developers to use the latest JavaScript standards.\n\n- **ESLint:** This will be used to automatically check code against a predefined set of rules to catch common development mistakes and enforce coding standards. Integrating ESLint will help maintain code quality and consistency across the development team.\n\n- **Webpack:** Additionally, we will use Webpack as our module bundler to streamline the process of transforming and optimising our JavaScript code for download and execution. With appropriate plugins and loaders configured, Webpack will ensure that our code is minified and optimised for performance.\n\nBy combining these tools, we will create a robust development environment that promotes modern JavaScript usage while maintaining compatibility with older environments and ensuring code quality and performance.","GenTime":"2024-07-29 00:05:33"}
{"File Name":"functionaut\/0003-test-distributed-files-only.md","Context":"## Context\\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\\n\n## Decision\n","Decision":"Testing will be made against the production bundle to catch compilation errors before they reach our users.\\n","tokens":67,"id":2689,"Prediction":"## Decision\n\nTo ensure that our code distributed to the public works as intended and to avoid false positives in testing, we will adopt the following strategies:\n\n1. **Test Against Production Builds**: We will establish a pipeline in our CI\/CD process to run our complete suite of tests against the production bundle, not just the development sources. This ensures any transformation made by the advanced compilation mode of the Google Closure Compiler is accurately tested.\n\n2. **Use Specialized Tools**: Leverage tools specifically designed for testing compiled JavaScript code, such as the Google Closure Compiler's own testing framework. These tools are more suited to identify and resolve issues that arise from the advanced compilation process.\n\n3. **Incremental Compilation Checks**: Implement incremental testing where changes are compiled and subjected to comprehensive testing progressively. This approach will help catch errors early in the compilation process.\n\n4. **Debugging and Source Mapping**: Utilize source maps generated by the Closure Compiler to enable debugging of the minified code, so any issues can be traced back to the original source.\n\n5. **Manual Verification Process**: In addition to automated tests, we will incorporate a manual verification step in our deployment process. This includes a checklist of critical functionalities to verify the application works as intended in a production-like environment.\n\n6. **Version Control and Rollbacks**: Maintain stringent version control and rollback mechanisms to quickly revert to a previous stable state if issues are detected post-deployment.\n\n7. **Continuous Improvement**: Regularly review and update the testing strategies and tools to adapt to any changes in the build process or the introduction of new features.\n\nBy implementing these strategies, we can minimize the risk of discrepancies between development and production environments, ensuring a more reliable and consistent user experience.","GenTime":"2024-07-29 00:05:42"}
{"File Name":"operational-data-hub\/0017-ddd-defines-ubiquitous-language.md","Context":"## Context\\nDomain-driven design (DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the business domain. For example, if a software processes loan applications, it might have classes such as LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.\\nDDD connects the implementation to an evolving model.\\nDomain-driven design is predicated on the following goals:\\n- placing the project's primary focus on the core domain and domain logic;\\n- basing complex designs on a model of the domain;\\n- initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.\\nConcepts of the model include:\\n### Context\\nThe setting in which a word or statement appears that determines its meaning;\\n### Domain\\nA sphere of knowledge (ontology), influence, or activity. The subject area to which the user applies a program is the domain of the software;\\n### Model\\nA system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain;\\n### Ubiquitous Language\\nA language structured around the domain model and used by all team members to connect all the activities of the team with the software.\\n\n## Decision\n","Decision":"We In the context of the ODH we will use the pricniples of Domain Driven Design and use the language which is related to the business where it is operated.\\n","tokens":261,"id":2696,"Prediction":"## Decision\n\n**Adopt Domain-Driven Design (DDD) for Software Development:**\n\nWe have decided to adopt Domain-Driven Design (DDD) for our software development process to ensure the structure and language of the software code align closely with the business domain. This decision encompasses the following actions and outcomes:\n\n1. **Focus on Core Domain and Domain Logic:**\n   - Identify and concentrate on the core domain where the business holds the most value.\n   - Ensure that the majority of the efforts and resources are allocated to refining the core domain models and logic.\n\n2. **Base Design on Domain Model:**\n   - Develop an evolving model that reflects the domain, to be used as a foundation for complex design decisions.\n   - Continuously enhance the software model to capture new insights and adapt to changing business needs.\n\n3. **Foster Collaboration Between Experts:**\n   - Initiate and maintain a collaborative environment involving both technical team members and domain experts.\n   - Conduct regular workshops and meetings to iteratively refine the conceptual model.\n\n4. **Define and Use Ubiquitous Language:**\n   - Establish a common, ubiquitous language structured around the domain model.\n   - Ensure all team members, including developers, domain experts, and stakeholders, use this language for all activities related to the project.\n\nBy implementing Domain-Driven Design, we aim to create a more cohesive and understandable software system that accurately represents and addresses the complexities of the business domain. This approach is expected to yield higher quality software with better alignment to business goals and more effective capacity to evolve alongside the domain.","GenTime":"2024-07-29 00:06:10"}
{"File Name":"operational-data-hub\/0059-runtime-dependency-only-on-gcp.md","Context":"## Context\\nAvailability of systems can be improved by reducing the number of dependencies. Each additional dependency comes with the risk of that service breaking, causing issues to our system. On the other hand, 3rd party services can be leveraged to quickly build and apply managed services at low cost, compared to creating and running these services by ourselves. Therefore, the right balance between dependency on and benefitting from external services is important.\\nDuring run the changes in 3rd party dependencies do not change. Therefore, caching or copying the functionality of these dependencies is possible in many cases. In build and development environments, the dependencies on 3rd party components and services change more often. Therefore, it is oftentimes not opportune to invest in becoming independent on these components and services.\\n\n## Decision\n","Decision":"During runtime, we will only depend on resources services from the Google Cloud Platform.\\n","tokens":158,"id":2701,"Prediction":"## Decision\nWe will adopt a hybrid approach to manage 3rd party dependencies that differentiates between runtime and build\/development environments:\n\n1. **Runtime Environment**:\n    - **Caching and Local Copies**: For 3rd party services and dependencies that are critical to the runtime environment of our systems, we will implement systematic caching and create local copies of service functionalities where feasible. This approach will ensure that our runtime availability is not compromised due to external service disruptions.\n    - **Monitoring and Alerts**: Establish robust monitoring for all 3rd party services to ensure early detection of potential issues. Immediate fallback mechanisms will be implemented to switch to cached\/local copies as needed.\n\n2. **Build and Development Environment**:\n    - **Frequent Syncing and Validation**: For 3rd party dependencies in the build and development environments, we will implement automated processes that frequently sync and validate these dependencies. This will ensure that we are consistently up-to-date with the latest versions and changes.\n    - **Embedded Testing**: As part of our CI\/CD pipeline, include comprehensive testing of these dependencies to validate their integration and performance. This will mitigate the risks associated with dependency changes and maintain development consistency.\n\n3. **Dependency Assessment**:\n    - **Critical vs. Non-Critical Dependencies**: Classify dependencies into critical and non-critical segments. Aim to reduce critical dependencies on external services by either replicating functionality locally or ensuring robust failover mechanisms.\n    - **Cost-Benefit Analysis**: Regularly conduct cost-benefit analyses for leveraging 3rd party services versus building them in-house, focusing on both direct financial costs and indirect costs like potential downtime and complexity.\n\nBy adopting this differentiated approach, we aim to balance the benefits of using 3rd party services with the risks associated with increased dependency on external systems, thereby enhancing the overall availability and resilience of our systems.","GenTime":"2024-07-29 00:06:32"}
{"File Name":"operational-data-hub\/0003-repo-directory-structure.md","Context":"## Context\\nWe feel the need to standardize the directory structure for github repos.\\n\n## Decision\n","Decision":"### 1. General rules\\n* Directory names must always:\\n* Always use underscores (\u201c\\_\u201d) in names for folders and files to split between words.\\n* use lowercase.\\n### 2. Cloud functions\\n* The root directory for cloud functions is 'functions\/'.\\n* A '.gcloudignore' file is provided to limit the uploaded files.\\n### 3. API\\n* the root directory for the API is 'app\/'.\\n* For OpenApi APIs see the separate ADR for working with OpenApi generated code.\\n### 4. Frontend\\n* the root directory for the APP is 'app\/'.\\n* The directory structure below the app directory is based on the recommendations of the framework.\\n* For the angular applications the recommendations can be found at [Angular Workspace and project file structure](https:\/\/angular.io\/guide\/file-structure).\\n### 5. Tests\\n* Files needed for testing are stored in the 'test' directory.\\n### 6. SAST\\n* Files needed for the SAST scan are stored in the 'sast-config' directory.\\n","tokens":18,"id":2703,"Prediction":"## Decision\nAfter considering various options and the potential benefits of standardizing our directory structures, we have decided to formalize a specific directory structure for all Github repositories within our organization. This structure will include standard directories such as `src` for source code, `tests` for unit and integration tests, `docs` for documentation, `configs` for configuration files, and `scripts` for automation and utility scripts. This will ensure consistency across projects, making it easier for team members to navigate and contribute to repositories.\n\nThe standardized directory structure will look as follows:\n\n```\n\/root\n  \/src\n  \/tests\n  \/docs\n  \/configs\n  \/scripts\n  \/build (optional)\n  README.md\n  LICENSE\n  .gitignore\n```\n\nThis decision aims to improve collaboration, streamline project setup, and enhance the maintainability of our codebase by providing a clear and predictable layout for all repositories.","GenTime":"2024-07-29 00:06:37"}
{"File Name":"operational-data-hub\/0013-dataflow-diagrams.md","Context":"## Context\\nWe feel the need to define that every config folder should contain at least one dataflow diagram.\\n\n## Decision\n","Decision":"We decided that every config folder should contain at least one dataflow diagram.\\n### Dataflow Diagram\\nEvery config with a cloudbuild should contain a dataflow diagram. This diagram contains the dataflow of the GCP project the config is for. It is also possible to have multiple diagrams if there are multiple dataflows. [Here](https:\/\/github.com\/vwt-digital-config\/snb-link2-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) and [here](https:\/\/github.com\/vwt-digital-config\/snb-tmob-problm-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) examples of dataflow diagrams can be found.\\nThe diagrams are made via the python package [diagrams](https:\/\/github.com\/mingrammer\/diagrams).\\n","tokens":23,"id":2706,"Prediction":"## Decision\nWe will mandate that every `config` folder must include at least one dataflow diagram. This decision aims to ensure that all configurations have a clear visual representation of data movement and processing within the system. The dataflow diagram should be in a standardized format (e.g., .png, .jpg, .svg) and must be kept updated to reflect any changes in the configuration. This requirement will facilitate better understanding, collaboration, and maintenance of the system architecture among team members. \n\n### Implementation Steps:\n1. **Standardize Diagram Format:** Define acceptable formats for dataflow diagrams (e.g., .png, .jpg, .svg, .drawio).\n2. **Documentation:** Update the system documentation to include guidelines on creating and maintaining dataflow diagrams.\n3. **Folder Structure:** Ensure every `config` folder contains a `diagrams` subfolder for storing the dataflow diagram.\n4. **Review Process:** Incorporate checks in the code review process to ensure the presence and accuracy of dataflow diagrams.\n5. **Tool Integration:** Integrate tools (e.g., CI\/CD pipelines) to verify the existence of dataflow diagrams in every `config` folder and alert if missing.\n\n### Rationale:\n- **Clarity:** Enhances the clarity and understanding of the configuration by providing a visual representation.\n- **Collaboration:** Facilitates better collaboration among team members by having a shared and easily understandable reference.\n- **Maintainability:** Improves system maintainability by ensuring up-to-date and accurate documentation of data flows.\n- **Documentation:** Provides a systematic approach to documenting data movement and processing within the system configurations.","GenTime":"2024-07-29 00:06:50"}
{"File Name":"operational-data-hub\/0019-single-schema-per-topic.md","Context":"## Context\\nA schema is a vocabulary that allows you to annotate and validate documents. Every topic has a schema that can validate the messages the topic receives.\\n\n## Decision\n","Decision":"Since every topic only receives messages in JSON format (see [21. Messages are in JSON format](0021-messages-are-in-json-format.md) ), we define a JSON Schema for every topic that can validate the messages received by said topic.\\n","tokens":32,"id":2710,"Prediction":"## Decision\nWe will implement a schema registry to manage and enforce schemas for all topics in our system. The schema registry will ensure that every message conforms to the predefined schema associated with its respective topic, thereby improving data consistency and quality. This implementation will involve the following steps:\n\n1. **Select Schema Definition Format**: Adopt a standard format for defining schemas, such as JSON Schema, Avro, or Protobuf, based on the interoperability and flexibility requirements of our system.\n   \n2. **Integrate Schema Registry**: Deploy a schema registry service to store and manage schema definitions. This service can be a dedicated component or a cloud-based solution depending on our infrastructure.\n\n3. **Schema Evolution and Versioning**: Implement mechanisms to handle schema versioning and evolution, ensuring backward and forward compatibility wherever necessary. This will include a version control system for schemas within the registry.\n\n4. **Validation Middleware**: Integrate a middleware component for schema validation that intercepts messages before they are published to or consumed from a topic. This middleware will use the schema registry to fetch and apply validation rules to the messages.\n\n5. **Error Handling and Logging**: Design and implement error handling procedures for scenarios where a message fails schema validation. Clear logging and alert systems will be established to notify relevant stakeholders about validation errors.\n\n6. **Developer Guidelines and Training**: Provide guidelines and training to developers on how to define, register, and utilize schemas effectively. Document best practices for schema design and management.\n\n7. **Performance Considerations**: Conduct performance testing to ensure that schema validation processes do not introduce significant latency or bottleneck issues in our messaging pipeline.\n\nBy taking these measures, we aim to enhance the robustness, scalability, and maintainability of our messaging system while ensuring that all data conforms to the expected structure and constraints.","GenTime":"2024-07-29 00:07:07"}
{"File Name":"operational-data-hub\/0002-use-serverless-infra-components.md","Context":"## Context\\nServerless computing offers a number of advantages over traditional cloud-based or server-centric infrastructure. For many developers, serverless architectures offer greater scalability, more flexibility, and quicker time to release, all at a reduced cost. With serverless architectures, developers do not need to worry about purchasing, provisioning, and managing backend servers.\\n\n## Decision\n","Decision":"We will use serverless infrastructure components where possible.\\n","tokens":68,"id":2723,"Prediction":"## Decision\nBased on the advantages highlighted in the context, we have decided to adopt a serverless computing approach for our new project. This strategic shift aims to leverage the inherent benefits of serverless architectures, such as enhanced scalability, cost efficiency, operational flexibility, and reduced infrastructure management overheads. We believe this approach will accelerate our development cycles and expedite time to market, allowing our developers to focus more on innovation and less on server maintenance.\n\nTo implement this decision, we will utilize serverless offerings from a leading cloud provider (e.g., AWS Lambda, Google Cloud Functions, or Azure Functions), carefully evaluate our application's requirements, and design our architecture to optimally utilize these serverless services.","GenTime":"2024-07-29 00:07:45"}
{"File Name":"operational-data-hub\/0012-repository-organisation-rules.md","Context":"## Context\\nWe feel the need to define rules to select the correct GitHub organisation for source repositories.\\n\n## Decision\n","Decision":"We identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.\\n### VWT Digital\\nThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,\\nit **has to pass** one of the following applications:\\n- An ODH-specific solution;\\n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration;\\n- Any solution business logic.\\nWhen creating a name for the repository, it **has to start** with either `odh-<repo_name>` or `dat-<repo_name>`:\\n- `odh-`: When the repository contains source code specifically made for the Operational Data Hub;\\n- `dat-`: When the repository contains source code created for generic usage within the Digital Ambition Team domain.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Solutions\\nThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:\\n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration.\\nWhen creating a name for the repository, it has to start with the abbreviation of the solutions it is a part of.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Configuration\\nThis is the main organisation for all configuration code. When creating a new repository, it **has to pass** the following application:\\n- Google Cloud Platform (GCP) project-specific configuration code that is for private usage only.\\nWhen creating a name for the repository, it has to contain the GCP project they are connected to minus the customer,\\nenvironment and location. Furthermore, it has to end with `-config`.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n","tokens":21,"id":2725,"Prediction":"## Decision\n\nWe will establish standardized rules to determine the appropriate GitHub organization for each source repository. This will ensure consistency, improve governance, and facilitate easier management and collaboration. \n\n**Decisions are as follows:**\n\n1. **Core Product Codebase**: Source repositories directly related to the core product will reside within the \"CoreProduct\" GitHub organization.\n\n2. **Customer Projects**: Repositories specific to customer projects will reside within a dedicated \"CustomerProjects\" GitHub organization, with naming conventions that identify the customer.\n\n3. **Open Source Contributions**: For any work intended to be shared with the open-source community, repositories will be created under the \"OpenSourceContributions\" GitHub organization.\n\n4. **Internal Tools and Utilities**: Repositories that support the internal processes and operations will be placed in the \"InternalTools\" GitHub organization.\n\n5. **Experimentation and Prototyping**: For repositories associated with experimental features and prototypes, the \"Labs\" GitHub organization will be used.\n\n6. **Third-party Integrations**: Repositories that are specifically for third-party integrations will be housed in the \"ThirdPartyIntegrations\" GitHub organization.\n\n**Implementation steps:**\n\n- Implement a pre-commit hook or automated tooling to validate that new repositories are created in the appropriate organization based on established rules.\n- Document these rules in the company\u2019s engineering guidelines and ensure all relevant teams are trained and aware.\n- Regularly audit existing repositories to ensure compliance with the new rules and migrate any that are misplaced.\n- Consider setting permissions and governance structures according to the nature of each GitHub organization.\n\nBy adhering to these rules, we aim to streamline repository management, improve access control, and enhance our overall development workflow.","GenTime":"2024-07-29 00:07:54"}
{"File Name":"operational-data-hub\/0039-least-privilege-access.md","Context":"## Context\\nThe principle of least privilege means that every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose.\\nThe principle of least privilege works by allowing only enough access to perform the required job. In an IT environment, adhering to the principle of least privilege reduces the risk of attackers gaining access to critical systems or sensitive data by compromising a low-level user account, device, or application. Implementing the principle helps contain compromises to their area of origin, stopping them from spreading to the system at large.\\n\n## Decision\n","Decision":"We will apply the principle of least privilege.\\n","tokens":128,"id":2737,"Prediction":"## Decision\n\nTo implement the principle of least privilege in our software system, we have decided to take the following actions:\n\n1. **Role-Based Access Control (RBAC)**: We will define and implement a role-based access control system where users are assigned roles that grant only the permissions needed to perform their specific job functions. Roles will be granular and tailored to the specific needs of each department or user group.\n\n2. **Permission Audits**: Regular audits will be conducted to ensure that permissions granted to users and systems are appropriate for their current roles and responsibilities. Any unnecessary or outdated permissions will be revoked promptly.\n\n3. **Privilege Escalation Processes**: Establish and enforce strict processes for temporary privilege escalation. For tasks requiring higher-level access, users must submit a formal request that includes a justification and is subject to approval by designated administrators.\n\n4. **Least-Privilege Defaults**: All new users, systems, and applications will be provisioned with the minimum set of privileges necessary for initial access. Any additional permissions required must go through the formal request process.\n\n5. **Segregation of Duties**: Implement segregation of duties by distributing critical tasks among multiple users or processes to reduce the risk of fraud or error. No single user will have full control over all aspects of sensitive or critical operations.\n\n6. **Automation and Monitoring**: Utilize automation tools to enforce least privilege policies consistently across the system and implement monitoring solutions to track the use of privileges. Any breach or misuse of privileges will trigger an alert for immediate investigation.\n\n7. **Employee Training**: Conduct regular training sessions for employees to educate them on the importance of least privilege and how to adhere to the established policies and processes.\n\n8. **Continuous Improvement**: Regularly review and update least privilege policies and practices to adapt to changing business requirements and emerging security threats.\n\nBy following these steps, we aim to enhance the security of our IT environment, reduce the risk of unauthorized access to critical systems and sensitive data, and ensure that any potential compromises are contained to their area of origin.","GenTime":"2024-07-29 00:08:44"}
{"File Name":"operational-data-hub\/0009-uri-identifies-data.md","Context":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme (e.g. http:\/\/).\\nTo be able to unambiguously identify any element of data, a URI will be defined for every element of data on the ODH platform. Building this URI from meaningful attributes instead of technical randomly generated ids makes it recognizable and easier to use. However, selecting the right attributes and context information to assure uniqueness is challenging.\\nThe URI scheme to use is [tag](https:\/\/tools.ietf.org\/html\/rfc4151). Tag URIs are quite human readable, but, unlike http URIs, not resolvable. This makes them suitable for identifying real-life things (entities) that have a representation on the ODH. A URI can be defined from the hierarchy of the organisation, forming a path to the resource. For example, if mycompany has a catalog with articles and one of the articles has article# 313, an article URI could be defined as `tag:vwt.digital,2020:mycompany\/catalog\/articles\/313`\\n\n## Decision\n","Decision":"We define a URI according to the [tag scheme](https:\/\/tools.ietf.org\/html\/rfc4151) for each entity that has a representation on the ODH platform.\\n","tokens":252,"id":2740,"Prediction":"## Decision\n\nTo ensure that each element of data on the ODH platform is unambiguously identifiable and easily recognizable, we will adopt the 'tag' URI scheme as defined by [RFC 4151](https:\/\/tools.ietf.org\/html\/rfc4151). This decision is made with the following considerations:\n\n1. **Human Readability:** Tag URIs provide a readable format that allows quick identification of the resource based on its attributes, improving usability and reducing cognitive load for users who need to recognize or utilize these URIs.\n\n2. **Non-Resolvable Requirement:** Since tag URIs are inherently non-resolvable and are designed specifically for identifying entities that do not necessarily need to be dereferenced, they are a fitting choice for our use case in the ODH platform where representation and identification are the primary needs.\n\n3. **Context and Uniqueness:** Constructing URIs from meaningful attributes such as organizational hierarchy and relevant identifiers (e.g., company names, catalog items) ensures that each URI is both unique and contextually rich. This construction can follow a hierarchical path format, making it intuitive and maintainable.\n\n### Example Implementation:\n\nIf our platform needs to identify an article within a catalog of a company named \"mycompany,\" having an article number 313, the URI will be constructed as:\n\n```\ntag:vwt.digital,2020:mycompany\/catalog\/articles\/313\n```\n\nIn this example:\n- `vwt.digital` indicates the responsible organization\u2014VWT Digital\u2014creating or maintaining the URIs.\n- `2020` indicates the year in which the namespace is defined, ensuring versions or updates can be handled effectively.\n- `mycompany\/catalog\/articles\/313` gives a clear path to the resource, leveraging organizational context and a meaningful identifier instead of a random, technical ID.\n\n### Implementation Steps:\n1. **Define URI Structure:** Establish a standard format for the URI elements (e.g., organization, year, resource type, resource identifier).\n2. **Namespace Management:** Ensure that the base namespace (`tag:vwt.digital,2020`) is consistent across all URIs to maintain uniformity.\n3. **Attribute Selection:** Determine which attributes will be used to construct the hierarchical path for various data elements, ensuring they provide uniqueness.\n4. **Validation Mechanism:** Implement a validation mechanism to ensure that URIs are unique and conform to the established structure.\n\nThis decision aligns with our goal of creating a user-friendly, maintainable, and unique identification system for data elements on the ODH platform while leveraging the strengths of the tag URI scheme.","GenTime":"2024-07-29 00:08:56"}
{"File Name":"operational-data-hub\/0003-create-cloud-native-solutions.md","Context":"## Context\\nCloud-native architecture fully takes advantage of the [serverless computing](0002-use-serverless-infra-components.md) to maximise your focus on writing code, creating business value and keeping customers happy.\\nAll the developer has to worry about is orchestrating all the infrastructure they need ([sofware defined](0004-create-software-defined-everything.md)) and the application code itself.\\n\n## Decision\n","Decision":"We will build cloud-native solutions.\\n","tokens":83,"id":2752,"Prediction":"## Decision\n\nWe will adopt a cloud-native architecture approach, leveraging serverless computing for our infrastructure components. This decision is driven by the desire to maximize our focus on writing application code, creating business value, and ensuring customer satisfaction.\n\n### Justification:\n\n1. **Streamlined Focus:** By utilizing serverless computing, developers can concentrate on the core application logic and business requirements instead of managing and provisioning infrastructure.\n  \n2. **Efficiency:** The serverless model allows for on-demand scaling, resource optimization, and efficient management, reducing overheads and operational burdens.\n  \n3. **Cost-Effective:** Serverless computing can be a cost-effective solution, as it typically follows a pay-as-you-go pricing model.\n  \n4. **Agility:** The adoption of cloud-native architecture and serverless infrastructure promotes rapid development, testing, and deployment cycles.\n  \n5. **Resilience and Adaptability:** Serverless architectures inherently provide resilience and can easily adapt to varying workloads and demands.\n\nBy adopting this approach, we will leverage the full potential of our cloud infrastructure, ensuring that our development efforts are focused on delivering value to our customers and maintaining a competitive edge in the market.","GenTime":"2024-07-29 00:09:43"}
{"File Name":"operational-data-hub\/0023-iso-8601-to-specify-date-and-time-with-timezone.md","Context":"## Context\\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https:\/\/tools.ietf.org\/html\/rfc3339) describes the usage of the ISO-8601 standard.\\n\n## Decision\n","Decision":"We will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.\\nAll date objects must have a time-zone included.\\n","tokens":66,"id":2755,"Prediction":"## Decision\nWe decide to adopt the ISO 8601 standard for formatting date-time strings in our JSON responses. By adhering to the ISO 8601 standard as described in RFC 3339, we ensure that our date-time representations are consistent, unambiguous, and easily interpretable by other systems and libraries. This decision promotes interoperability, reducing the risk of errors related to date-time parsing and handling. Specifically, date-time strings will follow the format \"YYYY-MM-DDTHH:MM:SSZ\" (e.g., \"2023-10-05T14:48:00Z\") to represent Coordinated Universal Time (UTC).","GenTime":"2024-07-29 00:09:50"}
{"File Name":"operational-data-hub\/0027-a-gcp-project-belongs-to-a-single-domain.md","Context":"## Context\\nThe projects structure of the platform can be used to protect components. By the seperation into projects a modular, loosely coupled design is created. A project belongs to a single [domain](0025-domains-correspond-to-business-departments.md), a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\\n\n## Decision\n","Decision":"The set of functions implemented in one GCP project belongs to a single domain.\\n","tokens":75,"id":2756,"Prediction":"## Decision\n\nWe will adopt a project-based structure for our platform to ensure modularity and loose coupling of components. Each project will be dedicated to a single domain, with the possibility of a domain encompassing multiple projects. This structure will promote a coherent and cohesive implementation of functions within each domain, enhancing maintainability and scalability by isolating responsibilities and simplifying interdependencies.","GenTime":"2024-07-29 00:09:51"}
{"File Name":"operational-data-hub\/0029-components-are-named-according-to-naming-conventions.md","Context":"## Context\\nOrganize your cloud assets to support operational management and accounting requirements. Well-defined naming and metadata tagging conventions help to quickly locate and manage resources. These conventions also help associate cloud usage costs with business teams via chargeback and showback accounting mechanisms.\\n\n## Decision\n","Decision":"The naming cloud object naming convention is based on the Ironstone Naming conventions and can be found on our interal Wiki\\n","tokens":52,"id":2757,"Prediction":"## Decision\n\nTo support effective operational management and accounting requirements, we will implement a structured and consistent naming and metadata tagging convention for our cloud assets. This will ensure ease of locating, managing, and associating the resources with relevant business teams for efficient cost allocation processes.\n\n### Naming Convention Guidelines:\n1. **Resource Groups:** \n    - Format: `rg-{environment}-{team}-{application}`\n    - Example: `rg-prod-data-analytics`\n\n2. **Virtual Machines:**\n    - Format: `vm-{environment}-{team}-{function}`\n    - Example: `vm-stg-web-server`\n\n3. **Storage Accounts:**\n    - Format: `stg-{environment}-{team}-{type}`\n    - Example: `stg-dev-devops-blob`\n\n### Metadata Tagging Guidelines:\n1. **Department:**\n    - Tag: `Department`\n    - Example: `Department: Finance`\n\n2. **Environment:**\n    - Tag: `Environment`\n    - Example: `Environment: Production`\n\n3. **Application:**\n    - Tag: `Application`\n    - Example: `Application: CustomerPortal`\n\n4. **Cost Center:**\n    - Tag: `CostCenter`\n    - Example: `CostCenter: CC1234`\n\n5. **Owner:**\n    - Tag: `Owner`\n    - Example: `Owner: JohnDoe`\n\n### Implementation Plan:\n1. Define and document the naming and tagging conventions in the organization's cloud governance policy.\n2. Train relevant teams on the importance and application of these conventions.\n3. Implement automated scripts and tools to enforce the conventions and correct deviations.\n4. Regularly review and update the naming and tagging conventions to align with evolving business needs and cloud architecture changes.\n\nBy adopting these conventions, we aim to facilitate effective operational management, cost tracking, and accountability, thereby improving overall cloud governance and financial oversight.","GenTime":"2024-07-29 00:09:55"}
{"File Name":"operational-data-hub\/0005-build-open-source-solutions.md","Context":"## Context\\nOpen source software is software with source code that anyone can inspect, modify, and enhance.\\nBy design, open source software licenses promote collaboration and sharing because they permit other people to make modifications to source code and incorporate those changes into their own projects. They encourage computer programmers to access, view, and modify open source software whenever they like, as long as they let others do the same when they share their work.\\nOpen source software development furthers several principles, described below.\\nTransparency. Whether we're developing software or solving a business problem, we all have access to the information and materials necessary for doing our best work. And when these materials are accessible, we can build upon each other's ideas and discoveries. We can make more effective decisions and understand how decisions affect us.\\nCollaboration. When we're free to participate, we can enhance each other's work in unanticipated ways. When we can modify what others have shared, we unlock new possibilities. By initiating new projects together, we can solve problems that no one can solve alone. And when we implement open standards, we enable others to contribute in the future.\\nRelease early and often. Rapid prototypes can lead to rapid discoveries. An iterative approach leads to better solutions faster. When you're free to experiment, you can look at problems in new ways and seek answers in new places. You can learn by doing.\\nInclusive meritocracy. Good ideas can come from anywhere, and the best ideas should win. Only by including diverse perspectives in our conversations can we be certain we've identified the best ideas, and decision-makers continually seek those perspectives. We may not operate by consensus, but successful work determines which projects gather support and effort from the community.\\nCommunity. Communities form when different people unite around a common purpose. Shared values guide decision making, and community goals supersede individual interests and agendas.\\n\n## Decision\n","Decision":"We make our code available as open source.\\n","tokens":382,"id":2759,"Prediction":"## Decision\n\nWe will adopt an open source software development model for our project.\n\nThis decision is based on the principles and benefits enumerated in the provided context:\n\n1. **Transparency**: By making our source code publicly available, we ensure that all stakeholders can access essential information, leading to better decision-making and the opportunity to build upon each other's work and discoveries.\n\n2. **Collaboration**: An open source model will allow us to leverage the collective expertise of a global community. Contributors can enhance our work in ways we may not anticipate, and open standards will invite future contributions.\n\n3. **Release Early and Often**: Releasing our software early and regularly will help us gather user feedback and iterate quickly. This iterative approach will help us identify and fix issues faster, ensuring a more reliable and refined final product.\n\n4. **Inclusive Meritocracy**: By adopting an open source approach, we ensure that good ideas from diverse perspectives are considered and adopted based on merit. This inclusivity will enable us to tap into the best solutions and create a more robust product.\n\n5. **Community**: Building an open source project facilitates the creation of a strong community with shared values and goals. This community can provide ongoing support, development, and advocacy for the project, driving its success forward.\n\nImplementing an open source strategy aligns with our ethos of promoting transparency, collaboration, and inclusivity while fostering a thriving community around our project.","GenTime":"2024-07-29 00:10:03"}
{"File Name":"operational-data-hub\/0057-naming-convention-defines-uri-tags.md","Context":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme\\n\n## Decision\n","Decision":"We will use [the 'tag' URI Scheme](https:\/\/tools.ietf.org\/html\/rfc4151).\\n","tokens":55,"id":2763,"Prediction":"## Decision\nWe have decided to use Uniform Resource Identifiers (URIs) for resource identification in our software system. This decision is based on several key factors:\n1. **Standardization**: URIs offer a standardized way to identify resources, ensuring compatibility and interoperability within and outside of our system.\n2. **Extensibility**: The hierarchical naming scheme inherent in URIs supports future extensions and modifications without disrupting existing systems.\n3. **Clarity and Uniformity**: Using URIs guarantees consistent and unambiguous identification of resources, which helps maintain clarity across different components and modules of our software.\n4. **Integration**: URIs are commonly used in web technologies and are supported by numerous tools and frameworks, facilitating easier integration with other systems and services.\n\nBy adopting URIs, we ensure that our software's resource identification is aligned with established practices, which will contribute to the robustness and scalability of the system.","GenTime":"2024-07-29 00:10:13"}
{"File Name":"CICD-pipeline\/002-usage-and-import-of-shared-library.md","Context":"## Context\\nWe need to decide how the shared library should be loaded for usage.\\n- A library version can be a tag, a branch or a commit SHA.\\n- A library can be loaded explicitly or implicitly\\n- A library default version can be configured globally or not\\n### Loading implicitly\\n````groovy\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- a default version of the library needs to be defined globally\\n- there is no need to import the library, as it will be loaded automatically\\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\\n### Loading explicitly\\n````groovy\\n@Library('my-shared-library@1.0.0') _\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- an import statement is always needed\\n- a default version could be defined globally, then the version can be left out when importing\\n### Using a global predefined version\\n- can be overriden in the pipeline\\n- does not force users to select a version\\n### No globel predefined version\\n- import will not work without version\\n- consumers are aware of the version that they use\\n- can be dynamic (auto updating) when using release channels with branches\\n\n## Decision\n","Decision":"We will use explicit loading without a predefined version and release channels for releasing based on branches.\\n- reduces the magic of the used version\\n- can still be auto updating\\n- consumers know that a library is used and what the version is\\n","tokens":314,"id":2766,"Prediction":"## Decision\n\n### Decided Approach: Hybrid Explicit Loading with Global Default Version\n\nWe have decided to implement a hybrid approach of explicit library loading, while also allowing a global default version configuration. This offers the clarity and control of explicit loading, with the flexibility and ease-of-use provided by a global default version.\n\n**Rationale:**\n\n1. **Explicit Loading by Default:**\n    - Ensures clarity and intentionality by requiring users to import the library explicitly.\n    - Encourages awareness of the specific library version being used, essential for debugging and consistency in the pipeline.\n\n2. **Global Default Version for Simplicity:**\n    - Simplifies user experience by providing a fallback version if none is specified.\n    - Users can opt-out of specifying a version, leveraging the global default for simplicity.\n    - Allows for consistent library behavior across multiple pipelines by default.\n\n3. **Version Overriding:**\n    - Users retain the flexibility to override the global default on a per-pipeline basis.\n    - Facilitates both stability (with a global stable version) and experimentation (with individual pipeline overrides).\n\n**Implementation Plan:**\n\n1. **Define a Global Default Version:**\n    - Configure a central place for the default library version (e.g., a configuration file or environment variable).\n    - Ensure documentation and communication about the globally defined version for reference.\n\n2. **Explicit Loading in Pipelines:**\n    - Enforce explicit library loading in pipeline scripts.\n    - Example:\n      ```groovy\n      @Library('my-shared-library') _\n      pipeline {\n        agent any\n        stages {\n          stage {\n            steps {\n              someMagicallyAvailableLibraryFunction()\n            }\n          }\n        }\n      }\n      ```\n    - Allow the version to be specified if different from the default:\n      ```groovy\n      @Library('my-shared-library@1.0.0') _\n      pipeline {\n        agent any\n        stages {\n          stage {\n            steps {\n              someMagicallyAvailableLibraryFunction()\n            }\n          }\n        }\n      }\n      ```\n\n3. **Fallback to Global Default:**\n    - If a version is not specified within the `@Library` annotation, fallback to the globally defined default version.\n\n4. **Documentation and Training:**\n    - Provide comprehensive documentation on the new process, including how to override the global default version.\n    - Conduct training sessions or provide support to familiarize stakeholders with the hybrid loading mechanism.\n\n**Benefits:**\n\n- Maintains clarity and control over library versions used in the pipeline.\n- Reduces accidental usage of unintended library versions, aiding in maintainability.\n- Simplifies and accelerates workflow setup by leveraging a predefined default version, reducing the requirement for extensive configuration for standard cases.\n\nThe hybrid model strikes a balance between ease-of-use and control, catering to both novice users who prefer simplicity and advanced users who require precise version management.","GenTime":"2024-07-29 00:10:28"}
{"File Name":"CICD-pipeline\/001-open-source-shared-jenkins-library.md","Context":"## Context\\n1) We are [open by default](https:\/\/github.com\/baloise\/baloise.github.io\/blob\/master\/design\/OpenByDefault.adoc)\\n1) The myCloud API spec [is not confidential](https:\/\/github.com\/baloise\/CICD-pipeline\/issues\/15)\\n1) Openness is a driver for clean, secure design and good documentation\\n1) There are currently no obvious drivers for confidentiality\\n\n## Decision\n","Decision":"We release the Shared Jenkins Library under Apache 2.0.\\n","tokens":90,"id":2768,"Prediction":"## Decision\n\nWe will make the myCloud API specification publicly accessible.\n\n### Rationale\n1) Aligning with our principle of being \"open by default.\"\n2) The API specification itself does not contain confidential information.\n3) Promoting openness encourages better design, increased security, and improved documentation quality through community feedback.\n4) There is no current need for confidentiality, making the decision low-risk.\n\nThis approach will likely foster greater collaboration, attract valuable external contributions, and ultimately lead to a more robust and well-documented API.","GenTime":"2024-07-29 00:10:34"}
{"File Name":"exercise3-group-6\/0006-create-observation-repository-search.md","Context":"## Context\\nResearchers may require methods to search for a particular Whale Observation or set of Whale Observations during their\\nstudies. For the program to accomplish this task two search methods may require implementation.\\n\n## Decision\n","Decision":"Create `getByDate` and `getById` methods in ObservationRepository.\\n`getByDate` uses the Observation iterator to compare Observation objects `sightingTime` (Date) to the date passed to the\\nmethod and creates an array list of matching objects. This function uses an iterator rather than `Collections.binarySearch()`\\nto reduce the runtime given that there may exist multiple objects of a similar date.\\n`getById` uses `Collections.binarySearch()` to search and possibly return the object with the `ObservationId` matching\\nthe long passed to the method.\\nCollection.sort()` used in `getById` implements the abstract strategy of the `Comparator` interface using the concrete\\nstrategy `compareByDate` and returns an integer referring to the order of the objects.\\n","tokens":41,"id":2772,"Prediction":"## Decision\n\nWe will implement two search methods for whale observations to cater to the needs of researchers:\n\n1. **Basic Search**: This method will allow researchers to search for whale observations using key attributes such as date, location, species, and individual whale identifiers. This will provide a straightforward and quick way to find specific observations based on commonly known details.\n  \n2. **Advanced Search**: This method will offer more complex search options, including filtering by additional parameters such as behavior patterns, group size, environmental conditions, and observational notes. Advanced search will enable researchers to perform more granular queries and drill down into rich data sets for in-depth studies.\n\nBy providing both basic and advanced search functionalities, the application can cater to a wide range of research needs from simple queries to detailed investigations.","GenTime":"2024-07-29 00:10:55"}
{"File Name":"exercise3-group-6\/0003-create-two-observation-comparators.md","Context":"## Context\\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\\n\n## Decision\n","Decision":"Implement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\\ncomparator class for field `sightingTime` (Date) in Observation.\\n","tokens":39,"id":2775,"Prediction":"## Decision\n\nIn order to efficiently handle the variety of qualities in whale observations and facilitate research, we will implement a dual comparison strategy within our software system. Each of the two proposed sorting methods will be paired with its own distinct comparison function. This will ensure that observations can be accurately and effectively sorted according to the specific criteria relevant to each sorting method.\n\n1. **Comparison Method A**: This method will be tailored for sorting observations based on physical characteristics of the whales, such as size, coloration, and patterning. It will enable researchers to categorize observations according to visual and measurable traits, aiding studies focused on physical variations and morphological comparisons.\n\n2. **Comparison Method B**: This method will be designed to sort observations based on behavioral and environmental factors, such as location, time of sighting, and observed behaviors. This will facilitate research that aims to understand patterns in whale behavior and their interaction with the environment.\n\nBy adopting this dual comparison strategy, we can ensure that our system provides robust support for research activities, allowing for precise and meaningful categorization of whale observations based on different research needs. This approach also maintains flexibility and extensibility, should new sorting criteria be identified in the future.","GenTime":"2024-07-29 00:11:06"}
{"File Name":"delayer-aws\/0003-dymamodb-update-strategy.md","Context":"## Context and Problem Statement\\nIn order to maintain the serverless nature of the project, a DynamoDB table was\\nchosen as main persistence mechanism, which means that a schedule is primarily\\nstored in DynamoDB and then distributed to other components, which in turn\\ngoes back to DynamoDB to update state. So, DynamoDB holds both state and\\nhistorical data.\\nThe problem here is that both ~~warmer~~ `task-1minute-enqueuer` and ~~poller~~ `task-1minute-sqs2sns` will concur by Dynamo resources and probably will be throttled (it's easy to reproduce this behavior only by setting Dynamo's read and write capacity to 1 and trying to send some hundreds of schedules while some other are ~~moving from *WARM* state~~ being enqueued in delayer queue).\\n## Decision Drivers\\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\n\n## Decision\n","Decision":"*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\nTake the decision for the use of the DynamoDB introduced a new concept for the entire architecture: the layered concern.\\nThe `delayer-aws` solution aims to provide a way to schedule future operations reliably. It's not part of this system store or ingest or even present information about these schedules. In this sense, the use of DynamoDB is needed only because there's a need of store schedules that could not be posted in delayer queue, and there's only 2 options for those records: or they are in the delayer queue, or they're not. That's why the \"state\" field is needed, but it will not hold the *entire* lifecycle of a schedule.\\nWith this in mind, we realize that all 3 options will be considered, but in different contexts:\\n-   Present data of scheduler is not `delayer-aws`'s concern, but it will be needed. So all the data events should be published by `delayer-aws` to be consumed by another \"view\" platform - this is a kind of *event driven approach*.\\n-   In this sense, if another system will ingest all of this published data, state and historical data will be stored in different persistent stores;\\n### How it should work\\nWhen a schedule is inserted, if their timeframe was greater then the timeframe of the delayer queue, the schedule is stored in DynamoDB with a *currentStatus* marked as `NEW` and a TTL of 1 hour after schedule time.\\nWhen `task-1minute-enqueuer` runs and finds this scheduler, it will be updated to `PROCESSED`. After that, no more updates are made in DynamoDB.\\nFor each event that occurs with this schedule, a message should be posted in the correspondent topic.\\n### Don't use state as a field in database\\nThe state of a schedule is noted by currentStatus\/currentState attribute in\\nDynamoDB. The proposal of this optional is to treat state as part of the\\ngeneral flow of application:\\n1.  When scheduled, it's COLD\\n1.  When *warmer* puts it on SQS, it's WARM\\n1.  When *poller* publishes it on SNS is DOING\\n1.  When *actions* execute\\n*   Good, simplified update policy\\n*   Good, reduced DynamoDB costs\\n*   Bad, no information about past events\\n","tokens":211,"id":2791,"Prediction":"## Decision\n\nTo mitigate the throttling issue with concurrent access to DynamoDB from `task-1minute-enqueuer` and `task-1minute-sqs2sns`, the following architectural decisions will be implemented:\n\n1. **Adopt DynamoDB Auto-Scaling:** Enable Auto-Scaling for both read and write capacity units on the DynamoDB table. This will automatically adjust the provisioned capacity based on the traffic patterns, thus preventing throttling without manual intervention.\n\n2. **Use DynamoDB Streams:** Implement DynamoDB Streams to capture changes in the table. This will help to decouple the writing and updating processes from the immediate read\/write operations, reducing the immediate load and allowing other services to react to changes asynchronously.\n\n3. **Introduce Exponential Backoff:** Implement exponential backoff in the SDK configuration for both `task-1minute-enqueuer` and `task-1minute-sqs2sns`. This will reduce the retry frequency and distribute the load more evenly over time.\n\n4. **Deploy In-Memory Caching:** Utilize an in-memory caching layer (e.g., AWS ElastiCache with Redis) to store frequently accessed schedules. This will reduce the number of read requests to the DynamoDB table, offloading the direct request burden.\n\n5. **Batch Operations:** Wherever possible, batch read and write operations to minimize the number of transactions and optimize throughput.\n\n6. **Optimize Indexing:** Regularly review indexing on the DynamoDB table to ensure optimal performance for both `task-1minute-enqueuer` and `task-1minute-sqs2sns`, and add necessary Global Secondary Indexes (GSIs) or Local Secondary Indexes (LSIs) to improve query performance.\n\nThese decisions align with the key drivers by keeping the solution simple and maintaining the serverless nature of the architecture while addressing concurrency issues effectively.","GenTime":"2024-07-29 00:11:54"}
{"File Name":"govuk-kubernetes-discovery\/0005-consistent-structure-for-terraform-files.md","Context":"## Context\\nWe should have a consistent structure for working with our Terraform files. We\\nshould be able to look into a file and understand it's function is and what it\\ncreates, what variables it needs to run and what it outputs after it's finished.\\nWe should not have to spend time picking apart the code to understand what is\\ncreated by Terraform.\\nWe should not have to work with multiple files to understand a single component.\\n\n## Decision\n","Decision":"Create style guidelines based upon the following:\\n- Header with title and description\\n- Variables listed\\n- Outputs listed\\n- Full descriptions of what variables and outputs do in the code\\n- `terraform fmt` across each file\\nCreate a style guideline document explaining this structure.\\n","tokens":93,"id":2793,"Prediction":"## Decision\n\nStandardize the structure of Terraform files to improve readability, maintainability, and ease of understanding. The following conventions will be adopted for organizing our Terraform code:\n\n1. **Directory Structure**:\n    - Use a separate directory for each major component or module.\n    - Example:\n      ```\n      \/project-root\n        \/networking\n          main.tf\n          variables.tf\n          outputs.tf\n        \/compute\n          main.tf\n          variables.tf\n          outputs.tf\n        \/storage\n          main.tf\n          variables.tf\n          outputs.tf\n      ```\n\n2. **File Naming Conventions**:\n    - Split configurations into distinct files for `main.tf`, `variables.tf`, and `outputs.tf`.\n    - `main.tf` will primarily contain the resource declarations.\n    - `variables.tf` will contain all the input variables organized with clear descriptions and default values.\n    - `outputs.tf` will define the outputs with clear descriptions, making it easy to know what is being exposed by the module.\n\n3. **Documentation within Files**:\n    - At the top of each `.tf` file, provide a brief comment describing the file's purpose.\n    - Use comments liberally inside the configurations to explain complex logic or important details.\n\n4. **Consistent Module Usage**:\n    - When creating reusable components, use the module structure to encapsulate component configurations.\n    - Ensure that modules follow the same directory and file naming conventions to maintain consistency across the project.\n\n5. **Variables and Outputs**:\n    - Define all the input variables inside `variables.tf` with proper type constraints, descriptions, and default values whenever applicable.\n    - Define all the outputs in `outputs.tf` with clear annotations about their purpose and usage.\n\n6. **State Management**:\n    - Use consistent state management practices, such as remote state storage, to manage Terraform state files efficiently and securely.\n    - Document the backend configuration for state storage clearly in the respective module's `main.tf`.\n\nBy adopting these conventions, we aim to minimize the time spent understanding and maintaining Terraform code, reduce cognitive load, and ensure a consistent approach to infrastructure as code across the entire project.","GenTime":"2024-07-29 00:12:03"}
{"File Name":"akvo-product-design\/ADR-004.md","Context":"## Context\\nAs we introduce new and more complex response types, we need to come up with a consistent, homogeneous solution for serializing all the values.\\nThe historical solution for this issue has been to separate multiple values with pipes, and in some scenarios, creating a JSON-encoded response (i.e. geoshapes).\\nWe currently need to add a new `code` property to some responses, namely to cascade and option types, but we also want to avoid introducing new properties as the response content grows.\\n\n## Decision\n","Decision":"As response values are getting more and more complex, there's an increasing need for finding a common solution for all values, and adding new properties will not scale well.\\nThe decision is to extend the JSON-based schema, and encode complex answers in this packed string.\\nWe will also review the meaning of each *response type*, which will determine the encoding of each value (JSON? string? array? etc).\\nMultiple responses (barcodes, option questions, cascades, etc) will be encoded using a **JSON array**.\\n","tokens":103,"id":2802,"Prediction":"## Decision\nWe will adopt a standardized JSON format for serializing all response types. This approach ensures consistency, extensibility, and readability across different response types, while also addressing the need to add new properties seamlessly.\n\n### Rationale:\n1. **Consistency**: Using JSON across all response types ensures that our serialization method is uniform, eliminating the need for different parsing logic for different response types.\n2. **Extensibility**: JSON inherently supports complex nested structures and additional properties, offering us the flexibility to introduce new fields (e.g., `code` property) without disrupting existing implementations.\n3. **Readability**: JSON is widely used and human-readable, making it easier for developers to understand and debug the serialized data.\n4. **Maintainability**: A single serialization format reduces the cognitive load on developers and simplifies maintenance, as there is only one method to understand and update.\n\n### Implementation Plan:\n- **Step 1**: Update the serialization logic to convert all response types into a standardized JSON format.\n- **Step 2**: Refactor existing code that handles pipe-separated values and other encoding formats to align with the new JSON serialization method.\n- **Step 3**: Implement backward compatibility measures to support legacy data formats during the transition period.\n- **Step 4**: Update documentation and provide guidelines for using the new JSON format.\n- **Step 5**: Perform thorough testing to ensure the correctness and performance of the new serialization method.\n\nThe adoption of JSON as our serialization format will future-proof our response handling mechanism and ensure seamless integration of upcoming features and properties, such as the `code` property for cascade and option types.","GenTime":"2024-07-29 00:12:30"}
{"File Name":"bananatabs\/0002-react-testing-library.md","Context":"## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state\/model.\\n\n## Decision\n","Decision":"We will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\\n","tokens":111,"id":2816,"Prediction":"## Decision\nWe will adopt `react-testing-library` for our integration tests to facilitate testing interactions with the rendered DOM without relying on implementation details. This will allow us to focus on user-facing elements and their behavior, such as toggling visibility of tabs and window groups, ensuring our tests remain robust and maintainable. \n\nHowever, we will strike a balance by primarily using the library for triggering and verifying UI interactions. For verifying the correctness of the rendered view, we will rely on model\/state verification to ensure it accurately represents the expected output. This two-pronged approach will improve test reliability while avoiding unnecessary and brittle tests of the actual DOM structure.\n\nBy doing so, we achieve:\n1. Reduced reliance on implementation details in tests.\n2. Robust verification of user interactions.\n3. Confidence that the model\/state accurately represents the view during the rendering process.","GenTime":"2024-07-29 00:13:22"}
{"File Name":"openchs-adr\/0009-dos-and-donts-for-building-health-module-api-responses.md","Context":"## Context\\nSome rules regarding usage of health-module apis.\\n\n## Decision\n","Decision":"Input for health-modules can be anything from openchs-models\\nOutput of health-modules is loose right now. There is no reason at present to fix this.\\nIf a method returns an array of decisions, it has to return the same array everytime. For example, if it has [{\"highRisk\": ['overweight']}], even if there are no high risks detected, decisions have to return ['highRisk': []], and not an empty array.\\n","tokens":13,"id":2823,"Prediction":"## Decision\n\nTo ensure secure and reliable usage of the health-module APIs, the following rules have been established:\n\n1. **Authentication**: All requests to the health-module APIs must be authenticated using OAuth 2.0 tokens to ensure that only authorized users can access them.\n\n2. **Authorization**: Implement fine-grained access control using ACL (Access Control Lists) to restrict access to specific APIs based on user roles and permissions.\n\n3. **Rate Limiting**: To prevent abuse and ensure fair usage, rate limiting will be applied to all health-module API endpoints. Each client will be allowed a maximum of 1000 requests per hour.\n\n4. **Data Encryption**: All sensitive data transmitted through the health-module APIs must be encrypted using TLS\/SSL to protect against data breaches and ensure privacy.\n\n5. **Logging and Monitoring**: Implement comprehensive logging and monitoring for all API requests and responses to track usage patterns, identify anomalies, and facilitate troubleshooting.\n\n6. **Versioning**: APIs should be versioned to ensure backward compatibility. Any breaking changes must be introduced in new API versions.\n\n7. **Input Validation**: Perform thorough input validation on all APIs to avoid security vulnerabilities such as SQL injection and cross-site scripting (XSS).\n\n8. **Rate Limiting Exemptions**: Critical healthcare operations that require higher request limits should be evaluated on a case-by-case basis and explicitly whitelisted for higher rate limits, subject to approval from the security team.\n\n9. **Error Handling**: Implement standardized error handling to ensure that meaningful error messages are returned to clients without exposing sensitive internal details.\n\n10. **Documentation**: Comprehensive and up-to-date API documentation must be maintained and made available to all consumers of the health-module APIs to promote proper usage and integration.\n\nBy adhering to these rules, we aim to maintain a secure, reliable, and scalable health-module API ecosystem that effectively supports various user needs while safeguarding critical healthcare data.","GenTime":"2024-07-29 00:13:49"}
{"File Name":"openchs-adr\/0005-deploy-openchs-server-on-the-cloud.md","Context":"## Context\\n1. Cost - Most users of OpenCHS are NGOs having 5-10 health workers. A running postgres instance and a tomcat server per installation is hard to support\/maintain.\\n2. Availability of internet - While users might not have internet connectivity in the field, they are usually attached to a head office where there is internet connectivity. They usually come to this common location regularly. Since sync is the only reason for internet connectivity, it can be limited during these visits.\\n3. Future direction - 3g\/4g is getting more common across the country, and cloud-based solutions are also getting more pervasive.\\n\n## Decision\n","Decision":"Openchs server will have capabilities to be deployed on the cloud\\n","tokens":132,"id":2824,"Prediction":"## Decision\n\nGiven the provided context, the following decision has been made:\n\n1. **Centralized Cloud-Based Solution with Offline Sync Capability:**\n   - **Rationale**: To address the cost and maintenance concerns, a centralized cloud-based solution will be implemented. This eliminates the need for individual installations of PostgreSQL and Tomcat servers for each NGO. Instead, all data will be stored and processed on a single cloud server, reducing overall costs and simplifying maintenance.\n   - **Offline Capability**: A mobile or desktop application will be developed for field health workers, which will allow them to work offline. The data collected will be stored locally and will be synchronized with the centralized server when internet connectivity is available, typically during visits to the head office.\n   - **Future-Ready**: As 3G\/4G connectivity becomes more prevalent, this centralized and cloud-based architecture will be scalable and adaptable to leverage better internet access, reducing the frequency of manual syncs and the reliance on local storage.\n\n2. **Periodic Data Sync Mechanism:**\n   - **Rationale**: To cope with irregular internet availability, a background sync mechanism will be embedded into the application which will automatically synchronize data whenever a stable internet connection is detected. This will minimize the need for manual intervention and optimize the data consistency between local and central databases.\n   - **Sync Scheduling**: Sync operations will be scheduled during known periods of connectivity (e.g., during office visits), ensuring data integrity while avoiding operational disruption for field workers.\n\n3. **Cost-Efficient Cloud Infrastructure:**\n   - **Rationale**: To further manage costs, a cost-efficient cloud infrastructure will be selected (e.g., AWS, Google Cloud, Azure with consideration of plans suitable for small-scale NGOs). Leveraging managed database services and scalable compute resources will ensure that the server environment can grow with demand without incurring unnecessary upfront expenses.\n\nBy adopting this centralized and cloud-based architecture with offline sync capabilities, we address the immediate cost and maintenance challenges while positioning the solution to adapt to future improvements in internet connectivity.","GenTime":"2024-07-29 00:13:53"}
{"File Name":"openchs-adr\/0006-use-row-level-security-in-postgres-to-implement-multitenancy.md","Context":"## Context\\nMultitenancy is required because we are now deploying openchs-server on the cloud. This can be achieved by\\n1. Multiple databases\\n2. Multiple schemas\\n3. Same database with a discriminator column\\n### Constraints\\n1. An organisation should not be able to view or update another organisation's data\\n2. There is common metadata for forms, concepts that organisations can override\\n### Other concerns\/requirements\\n1. The architecture should prevent mistakes in code to leak data from one organisation to another\\n2. Maintaining multiple copies of metadata (if using multiple dbs\/schemas) is hard\\n3. Data per organisation is low\\n\n## Decision\n","Decision":"- Use discriminators to achieve multitenancy.\\n- Enforce multitenancy using Postgres RLS policies\\n- Create a hierarchical organisation structure where organisations can read metadata from current or parent organisations, and read\/write data for their own organisation\\n- Use OpenCHS as the grandparent organisation. It will hold the default program metadata\\n","tokens":131,"id":2828,"Prediction":"## Decision\n\nBased on the provided context and constraints, the decision is to implement multitenancy using the same database with a discriminator column.\n\n### Rationale\n1. **Security and Isolation**:\n   - Using a discriminator column ensures that each organisation's data is logically separated, preventing organisations from viewing or updating each other's data.\n   - Access control logic can be implemented at the application level to ensure that queries are automatically filtered by the discriminator column, preventing data leakage.\n\n2. **Metadata Management**:\n   - Since there is common metadata (e.g., forms and concepts) that organisations can override, maintaining a single set of metadata is simpler than managing multiple copies across databases or schemas.\n   - Overrides can be managed by adding versioning or organisation-specific customisations within the same tables.\n\n3. **Ease of Development**:\n   - The likelihood of code mistakes leading to data leaks is minimized since the architecture inherently filters data based on the discriminator column, simplifying the development and testing process.\n   - Centralized database management facilitates easier maintenance and application of updates or migrations.\n\n4. **Performance Considerations**:\n   - Given the low data volume per organisation, performance degradation due to larger tables (caused by multitenancy) is unlikely. Indexing on the discriminator column can further optimize query performance.\n\n### Implementation Notes\n- Ensure that all database queries enforce filtering by the organisation's discriminator column.\n- Implement appropriate RBAC (Role-Based Access Control) to ensure that users from one organisation cannot access data of another.\n- Regularly audit and test access permissions and query filters to prevent accidental data leaks.\n- Use database indexing on the discriminator column to maintain query performance and efficiency.\n\nThis approach balances security, maintainability, and simplicity while adhering to the given constraints and requirements.","GenTime":"2024-07-29 00:14:05"}
{"File Name":"openchs-adr\/0012-create-a-generic-relationship-framework-to-link-between-mother-and-child.md","Context":"## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n\n## Decision\n","Decision":"Create an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\\n","tokens":103,"id":2829,"Prediction":"## Decision\n\nImplement a generic relationship modeling framework to support both the creation of new children during the delivery process and the establishment of family structures with defined relationships.\n\n### Rationale:\n1. **Flexibility and Scalability**: A generic relationship model will allow the system to adapt to various cases without the need for multiple specific solutions, thus enhancing scalability.\n2. **Ease of Data Management**: By having a unified structure, it becomes easier to manage and navigate relationships between individuals, facilitating smoother transitions between views and data entries during deliveries and PNC (Postnatal Care) visits.\n3. **Consistency**: Ensures consistent handling of relationships in different contexts (e.g., between mothers and children, family members).\n4. **Reusability**: This approach promotes code reuse and reduces redundancy by utilizing the same relationship framework for different scenarios.\n5. **Future-proofing**: A generic model will be more adaptable to future requirements or changes, whether it's adding new types of relationships or modifying existing ones.\n\n### Implementation Strategy:\n1. **Define Relationship Types**: Establish various types of relationships such as \"mother-child,\" \"father-child,\" and \"head of household-family member.\"\n2. **Model Relationships as Entities**: Create a relationship entity in the database that can link individual entries based on predefined relationship types.\n3. **Dynamic Linking**: Enable dynamic linking and switching between related individuals within the user interface to facilitate ease of access and updates.\n4. **User Interface Integration**: Ensure the user interface can seamlessly switch between related entity views during data entry and display, with minimal disruption.\n\n### Example Scenarios:\n1. **Delivery Process**: When recording a birth, the system will create a new child entry and link it to the mother within the record automatically.\n2. **Family Structure**: A family view where individuals are linked to the head of the household, displaying relationships clearly (e.g., mother, father, children).\n\nBy adopting a generic relationship modeling approach, we aim to provide a robust, flexible, and scalable solution to support both programmatic requirements efficiently.","GenTime":"2024-07-29 00:14:10"}
{"File Name":"kafkarator\/0003-kafkarator-api-is-focused-around-dedicated-crds.md","Context":"## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n\n## Decision\n","Decision":"- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n","tokens":151,"id":2834,"Prediction":"## Decision\nWe will create two distinct Custom Resource Definitions (CRDs) to represent the two separate concerns for configuring Kafkarator. Having separate CRDs will provide clarity and modularity, ensuring that each aspect of Kafkarator's configuration is handled independently and with specific focus. This decision allows developers to supply the necessary resources explicitly when performing a NAIS deploy. Implementing this approach will streamline the development process and make the API more intuitive and manageable in the long run.","GenTime":"2024-07-29 00:14:19"}
{"File Name":"kafkarator\/0004-only-for-aiven.md","Context":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n\n## Decision\n","Decision":"Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","tokens":70,"id":2837,"Prediction":"## Decision\n\nThe decision is to transition from the on-premise Kafka setup to a cloud-based Kafka service provided by Aiven. To facilitate this shift, the integration of Kafkarator will be implemented. This integration will streamline the process by automating configuration and setup tasks, thereby reducing manual work associated with maintaining Kafka clusters. This move to a managed Kafka service will leverage Aiven's hosted solution, promising enhanced scalability, reliability, and reduced operational overhead compared to the current on-premise setup.","GenTime":"2024-07-29 00:14:28"}
{"File Name":"marsha\/0002-videos-languages.md","Context":"## Context\\nWe want to think Marsha as accessible from the beginning. At least from\\nthe point of view of the videos, which are the main content available.\\nWe can think about a video as a main content, with many auxiliary\\ncontents.\\n### Auxiliary contents\\n#### Audio\\nWe have a main video, with an audio track included. The author could\\npropose many other audio tracks, as audio files, and in the player the\\nviewer can change the one to use.\\n#### Timed texts\\nIn addition to audio tracks, many timed text tracks can be available.\\n#### Sign language\\nSome people with disabilities could want a video with the sign language\\ntranscript. For this it can be a video incorporated in the original one,\\nor an other video displayed on the site.\\nAs sign languages are not the same for every spoken language, there can\\nbe several sign languages videos for a single video.\\n\n## Decision\n","Decision":"We decided to take all these elements into account right from the\\nbeginning.\\nSo we have a main Django model named `Video`, from an author, with the\\nlink to the main video file, including the default audio track.\\nFor the other audio tracks, we have an `AudioTrack` Django model, with a\\n`ForeignKey` to the `Video` instance, named `video`, and a `language`\\nfield (with only one audio track for each video+language)\\nIt's the same for timed text tracks, we have a `TimedTextTrack` Django model,\\nwith the same `video` and `language` fields, but with an additional `mode`\\nfield to indicate that this timed text track is either a simple subtitle,\\na \"[closed\\ncaptioning](https:\/\/en.wikipedia.org\/wiki\/Closed_captioning)\" ie subtitles\\nfor deaf or hard of hearing viewers, or a transcript. So there can be up to\\n3 timed text tracks for each video+language: one for each mode.\\nAnd finally, for sign-languages videos, it's the same as for audio\\ntracks: a Django model named `SignTrack` with the same `video` and\\n`language` field.\\n","tokens":191,"id":2843,"Prediction":"## Decision\n\nMarsha will adopt a multi-faceted approach to ensure accessibility from the onset, especially focusing on the main content: videos. The following decisions outline the strategy to make videos more accessible:\n\n1. **Multi-Audio Track Support**:\n    - Each video can have multiple audio tracks to cater to different languages and accessibility needs.\n    - The video player will include functionality allowing users to switch between different audio tracks seamlessly.\n\n2. **Timed Texts Integration**:\n    - Each video will support multiple timed text tracks such as subtitles and captions in various languages.\n    - These timed text tracks will be encoded in standard formats like SRT or WebVTT, and users will be able to toggle through these tracks via the video player interface.\n\n3. **Sign Language Videos**:\n    - We will support the addition of sign language overlays or separate sign language video tracks that can be displayed alongside the main video.\n    - Given the differences in sign languages across regions, multiple sign language options will be available, each associated with the corresponding spoken language of the primary audio track.\n\n4. **User Interface Considerations**:\n    - The video player will have accessible controls, ensuring that users with disabilities can easily navigate through options like switching audio tracks, enabling timed texts, and selecting sign language videos.\n    - All interactive elements will be compliant with W3C Web Content Accessibility Guidelines (WCAG) to ensure they are operable via keyboard navigation and screen readers.\n\n5. **Metadata and Content Organization**:\n    - Metadata will be included for each auxiliary content type to make browsing and selection intuitive.\n    - For example, audio tracks, timed texts, and sign languages will be clearly labeled with the language and type of content (e.g., \"Spanish Audio\", \"English Subtitles\", \"ASL Sign Language\").\n\nBy implementing these strategies, Marsha will ensure that video content is accessible to a broad audience, including those with auditory or visual impairments, from the very beginning.","GenTime":"2024-07-29 00:14:39"}
{"File Name":"protagonist\/0001-composite-handler-design.md","Context":"## Context and Problem Statement\\n### Context\\nA design for the implementation and delivery of a component of the DLCS\\nwhich can process \"composite\" artifacts. Such artifacts typically take\\nthe form of a single object containing multiple embedded resources, each\\nof which can be rasterized and treated as an individual resource within\\nthe DLCS.\\n\n## Decision\n","Decision":"Delivery of a standalone component which consumes a DLCS ingest payload\\ntailored to \"composite\" artifacts, and triggers an ingestion workflow\\nwhich rasterizes each of the constituent parts and invokes the existing\\nDLCS ingestion workflow.\\n![0001-flowchart.png](images\/0001-flowchart.png)\\n","tokens":69,"id":2859,"Prediction":"## Decision\n\nTo effectively manage and process composite artifacts within the DLCS, we have decided to implement a component that performs the following key functions:\n\n1. **Decomposition of Composite Artifacts:** The component will dissect the composite artifact into its individual embedded resources. This ensures that each resource can be handled and processed separately within the DLCS framework.\n\n2. **Metadata Extraction and Management:** For each embedded resource within the composite artifact, the component will extract relevant metadata. This metadata will include, but not be limited to, resource identifiers, resource types, sizes, and any pertinent descriptive information.\n\n3. **Resource Rasterization:** Each embedded resource will be rasterized independently. This allows the resources to be treated as standalone objects within the DLCS, facilitating better storage, retrieval, and manipulation capabilities.\n\n4. **Storage and Indexing:** Post-rasterization, the individual resources will be stored separately while maintaining a reference to the original composite artifact. This will be achieved through a structured indexing mechanism, improving searchability and manageability.\n\n5. **Integrative Processing Pipeline:** The new component will be integrated into the existing DLCS processing pipeline. This ensures that the decomposition, extraction, rasterization, and indexing are part of a seamless workflow within the system.\n\n6. **Scalability and Performance Optimization:** The design of the component will focus on scalability to handle a high volume of composite artifacts efficiently. Performance optimizations will be applied to minimize processing time and resource usage.\n\n7. **Error Handling and Logging:** Comprehensive error handling mechanisms will be put in place to manage any issues that arise during the decomposition and processing stages. Detailed logging will be implemented to aid in monitoring, debugging, and auditing processes.\n\nBy adopting this approach, we aim to enhance the DLCS's ability to handle composite artifacts, ensuring each embedded resource is processed precisely and efficiently, thereby improving the overall functionality and performance of the system.","GenTime":"2024-07-29 00:15:53"}
{"File Name":"james-project\/0032-distributed-mail-queue-cleanup.md","Context":"## Context\\nRead [Distributed Mail Queue](0031-distributed-mail-queue.md) for full context.\\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\\nideal both from a privacy and space storage costs point of view.\\nNote that **enqueuedMailsV3** and **deletedMailsV2** rely on timeWindowCompactionStrategy.\\n\n## Decision\n","Decision":"Add a new `contentStart` table referencing the point in time from which a given mailQueue holds data, for each mail queue.\\nThe values contained between `contentStart` and `browseStart` can safely be deleted.\\nWe can perform this cleanup upon `browseStartUpdate`: once finished we can browse then delete content of **enqueuedMailsV3**\\nand **deletedMailsV2** contained between `contentStart` and the new `browseStart` then we can safely set `contentStart`\\nto the new `browseStart`.\\nContent before `browseStart` can safely be considered deletable, and is applicatively no longer exposed. We don't need an\\nadditional grace period mechanism for `contentStart`.\\nFailing cleanup will lead to the content being eventually updated upon next `browseStart` update.\\nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueue\\nmanagement APIs.\\n","tokens":97,"id":2880,"Prediction":"## Decision\n\nWe have decided to implement an automated cleanup mechanism for **enqueuedMailsV3** and **deletedMailsV2** to address privacy concerns and reduce storage costs due to unreferenced blobs.\n\n### Steps for Implementation:\n\n1. **Time-Based Expiry:**\n   - Implement a time-based expiry policy where emails and their associated metadata in **enqueuedMailsV3** and **deletedMailsV2** will be automatically purged after a specified duration. This duration will be configurable to allow flexibility based on operational needs.\n\n2. **Garbage Collection:**\n   - Integrate a garbage collection process that scans for and removes orphaned blobs from the storage, ensuring that there are no lingering artifacts after the corresponding mail records have been purged.\n\n3. **Regular Maintenance Jobs:**\n   - Schedule regular maintenance jobs that will run during off-peak hours to delete expired entries and trigger the garbage collection process, minimizing the impact on system performance.\n\n4. **Monitoring and Logging:**\n   - Incorporate comprehensive monitoring and logging to track the cleanup process, ensuring any issues can be quickly identified and addressed. This will also provide visibility into the space reclaimed and ensure compliance with privacy regulations.\n\n5. **Validation and Testing:**\n   - Conduct thorough testing to validate that the cleanup mechanism accurately identifies and removes the appropriate records and blobs without impacting active data. This includes unit tests, integration tests, and system tests in a staging environment before deployment.\n\n### Rationale:\n\n- **Privacy Compliance:** By automatically purging old email records and their associated blobs, we ensure compliance with data retention policies and privacy regulations.\n- **Cost Efficiency:** Removing unreferenced blobs reduces ongoing storage costs, making the solution more cost-effective.\n- **System Performance:** Regular maintenance ensures that the storage system is not bogged down by excessive unused data, maintaining optimal performance.\n- **Flexibility and Control:** Configurable expiry policies provide flexibility to adapt to different operational requirements and regulatory environments.\n\nImplementing this cleanup mechanism will enhance the reliability, performance, and compliance of our distributed mail queue system.","GenTime":"2024-07-29 00:17:09"}
{"File Name":"james-project\/0009-disable-elasticsearch-dynamic-mapping.md","Context":"## Context\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\nThis causes a serie of functional issues:\\n- Maximum field count can easily be exceeded\\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\n\n## Decision\n","Decision":"Rely on nested objects to represent mail headers within a mapping\\n","tokens":96,"id":2884,"Prediction":"## Decision\n\nWe have decided to disable dynamic mapping for mail headers in our JSON map. Instead, we will implement a controlled schema-based approach with predefined mappings for the headers we expect to encounter.\n\n### Rationale\n\n1. **Prevent Maximum Field Count Exceedance**: By using a predefined schema, we can strictly control the number of fields that are indexed, preventing the maximum field count from being exceeded.\n\n2. **Avoid Field Type Errors**: Predefining the field types will eliminate the risk of incorrect type guesses, which can cause subsequent headers to be omitted or misrepresented in the index.\n\n3. **Minimize Performance Impact**: With a static mapping, we can avoid the need to pause document indexing for mapping changes. This ensures smoother and more consistent performance as there will be no need to handle concurrent changes dynamically.\n\n4. **Consistency and Predictability**: A schema-based approach ensures that the structure of our indexed data is consistent and predictable, making it easier to manage and query.\n\n### Implementation Plan\n\n1. **Schema Design**: Define the necessary fields and their types based on the most commonly encountered headers. Develop a comprehensive schema that covers these headers and anticipates possible future additions.\n\n2. **Mapping Update**: Update the index mapping with the new JSON schema, ensuring that any necessary re-indexing is planned to accommodate the new structure without significant downtime.\n\n3. **Monitoring and Adjustments**: Implement logging and monitoring to track any occurrences of unmapped headers and adjust the schema as necessary. This ensures continued flexibility while maintaining controlled growth.\n\n4. **Documentation and Training**: Document the new mapping strategy and train relevant team members on how to handle unmapped headers and update the schema when required.\n\nBy moving away from dynamic mapping to a more controlled schema-based model, we aim to improve the robustness, consistency, and performance of our mail header indexing process.","GenTime":"2024-07-29 00:17:26"}
{"File Name":"james-project\/0047-jmap-push-over-websockets.md","Context":"## Context\\nJMAP Push notifications allow clients to efficiently update (almost) instantly to stay in sync with data changes on the server.\\nIn order to support this, we need to have the server emit newest state changes to the client over a PUSH channel.\\n\n## Decision\n","Decision":"We will implement [RFC-8887 - A JSON Meta Application Protocol (JMAP) Subprotocol for WebSocket](https:\/\/tools.ietf.org\/html\/rfc8887)\\nas a transport mechanism for PUSH notifications.\\nWe will [generalize EventBus](0046-generalize-event-bus.md) in order to achieve an out-of-the box Publish-Subscribe\\nsystem for JMAP related events, that does not fit in the mailbox API.\\nWe will implement listeners registered on the JMAP event bus for WebSocket clients in order to carry over state changes\\nto the client.\\n","tokens":53,"id":2886,"Prediction":"## Decision\n\nWe will implement a JMAP Push notification system to update clients about the latest state changes on the server. This system will use a push channel for nearly instant data synchronization.\n\n### Key Points:\n\n1. **Push Mechanism**: We will utilize WebSockets as the primary channel for push notifications due to their real-time communication capabilities and wide support in client and server technologies.\n   \n2. **Scalability**: To ensure scalability and manage high loads, we will integrate the push notification system with a message broker like RabbitMQ or Kafka. This will decouple the event generation from event distribution, ensuring that notifications are delivered efficiently even under high traffic.\n\n3. **Authentication and Authorization**: Each push connection will be authenticated using existing JMAP auth tokens. Authorization checks will be added to ensure that notifications are sent only to clients with the appropriate permissions.\n\n4. **Data Payload**: The payload for push notifications will include a minimal set of data including resource type, ID, and the type of change (e.g., created, updated, deleted). Detailed data will be fetched by clients using standard JMAP API calls, thereby optimizing the payload size.\n\n5. **Client API Changes**: Clients will be provided with a new API to subscribe and unsubscribe from push notifications. This will include the ability to specify the types of resources they are interested in (e.g., emails, calendars).\n\n6. **Reliability**: To manage potential issues with missed notifications, clients will periodically reconcile their state with the server using the existing JMAP state API. This will ensure that they can handle scenarios where transient errors cause missed push notifications.\n\n7. **Rate Limiting and Throttling**: Implement rate limiting and throttling mechanisms to prevent abuse and ensure that server resources are used efficiently.\n\n### Implementation Steps:\n\n1. **Prototype Development**:\n   - Create a prototype of the WebSocket-based push channel.\n   - Integrate with a message broker for scalable event handling.\n\n2. **Integration**:\n   - Implement server-side event generation for the supported resource types.\n   - Develop client interfaces and libraries to handle push updates.\n\n3. **Testing**:\n   - Unit and integration testing of the push system.\n   - Load testing and performance benchmarking to ensure system scalability.\n\n4. **Deployment**:\n   - Deploy the push notification system in a phased manner, starting with a small set of clients.\n   - Monitor system performance and gather feedback for iterations.\n\n5. **Documentation & Training**:\n   - Update API documentation to include the new push notification capabilities.\n   - Provide training sessions and materials for client developers.\n\nBy adopting this approach, we ensure that our JMAP Push notifications system is robust, scalable, and provides nearly real-time synchronization capabilities to our clients.","GenTime":"2024-07-29 00:17:34"}
{"File Name":"james-project\/0017-file-mail-queue-deprecation.md","Context":"## Context\\nJames offers several implementation for MailQueue, a component allowing asynchronous mail processing upon smtp mail\\nreception. These includes:\\n- Default embedded ActiveMQ mail queue implementation, leveraging the JMS APIs and using the filesystem.\\n- RabbitMQMailQueue allowing several James instances to share their MailQueue content.\\n- And FileMailQueue directly leveraging the file system.\\nWe introduced a junit5 test contract regarding management features, concurrency issues, and FileMailQueue do not meet this\\ncontract. This results in some tests being disabled and in an unstable test suite.\\nFileMailQueue tries to implement a message queue within James code, which does not really makes sense as some other projects\\nalready provides one.\\n\n## Decision\n","Decision":"Deprecate FileMailQueue components.\\nDisable FileMailQueue tests.\\nTarget a removal as part of 3.6.0.\\n","tokens":146,"id":2888,"Prediction":"## Decision\n\nBased on the outlined context, it is decided to deprecate and eventually remove the FileMailQueue implementation from the James project. The use of FileMailQueue is creating unstable test suites and it attempts to re-implement message queue functionality that is already well-supported by other dedicated messaging systems.\n\nInstead, the project will focus on the existing robust solutions, such as:\n- The Default embedded ActiveMQ mail queue implementation.\n- The RabbitMQMailQueue implementation for scenarios requiring mail queue content sharing across multiple James instances.\n\nThese solutions are based on mature, established systems that are designed to handle message queuing efficiently and reliably, and they conform to the junit5 test contract for management features and concurrency issues.\n\nThis decision will simplify the codebase, enhance reliability and maintainability, and leverage existing technologies optimized for mail queuing tasks.","GenTime":"2024-07-29 00:17:40"}
{"File Name":"james-project\/0031-distributed-mail-queue.md","Context":"## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n\n## Decision\n","Decision":"Distributed James should ship a distributed MailQueue composing the following softwares with the following\\nresponsibilities:\\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\\nas well in term of Input\/Output operation per seconds.\\nHere are details of the tables composing Cassandra MailQueue View data-model:\\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\\nthe emails that have ever been in the mailQueue. Its content is never deleted.\\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\\nto filter out deleted\/purged items.\\n- **browseStart** store the latest known point in time from which all previous emails had been deleted\/dequeued. It\\nenables to skip most deleted items upon browsing\/deleting queue content. Its update is probability based and\\nasynchronously piggy backed on dequeue.\\nHere are the main mail operation sequences:\\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\\nis fired on *rabbitMQ*.\\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\\nuntil the first non deleted \/ dequeued email is found. This point becomes the new browse start. BrowseStart can never\\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\\ncurrent browse start.\\n- Upon **delete\/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\\nthe condition are marked as deleted in *enqueuedMailsV3*.\\n- Upon **getSize**, we perform a browse and count the returned elements.\\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\\nand of the mailQueue throughput:\\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\\ndecreasing the bucket count might result in some buckets to be lost.\\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue\/deletes. We recommend choosing\\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\\niterating through slices with all their content deleted. This value can be changed freely.\\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\\n","tokens":321,"id":2892,"Prediction":"## Decision\n\nTo address the requirements of a distributed MailQueue system that supports advanced administrative operations and efficient mail processing, the following decision has been made:\n\n**Adopt a Hybrid MailQueue Solution Combining RabbitMQ, Cassandra, and ObjectStorage**\n\n### Rationale:\n\n1. **RabbitMQ for Messaging:**\n   - Continue using RabbitMQ for effective message queuing and basic dequeue operations. RabbitMQ is adept at handling a large volume of messages and ensuring efficient processing.\n\n2. **Cassandra for Metadata:**\n   - Utilize Cassandra for storing metadata, but avoid relying solely on it due to the limitations and inefficiencies related to tombstones in high-queue scenarios. \n   - Implement a caching layer or utilize time-series data strategies to minimize the impact of tombstones.\n\n3. **ObjectStorage for Mail Content:**\n   - Store the actual email content in ObjectStorage (e.g., Swift or S3). This decouples the email content from the metadata and queue operations, leveraging the scalability and durability of ObjectStorage.\n\n### Administrative Operations:\n\nTo support advanced administrative operations such as traffic review, email discarding, resetting wait delays, purging the queue, etc., a composite architecture will be employed:\n\n1. **Metadata Manager Service:**\n   - Develop a dedicated service to handle metadata-related operations. This service will interface with Cassandra to manage metadata efficiently and will be responsible for operations like `browse`, `getSize`, and `arbitrary mail removal`.\n   \n2. **Unified Admin Interface:**\n   - Provide a centralized administrative interface that abstracts the complexity of the underlying distributed components. Administrators can interact with this single point to perform various MailQueue administrative tasks without having to deal with individual James servers.\n\n### Implementation Strategy:\n\n- **Separation of Concerns:**\n  - Clearly separate concerns by using RabbitMQ for queuing, Cassandra for metadata, and ObjectStorage for email content. This separation enhances modularity and scalability.\n\n- **Data Consistency:**\n  - Implement strategies to ensure consistency between metadata in Cassandra and content in ObjectStorage. This can include transactional workflows or eventual consistency models, depending on the operational requirements.\n\n- **Scalability and Load Management:**\n  - Introduce mechanisms to handle load peaks and ensure the system can scale out horizontally across distributed infrastructures. RabbitMQ's distributed capabilities will be leveraged alongside Cassandra\u2019s partitioning and ObjectStorage\u2019s scalability.\n\nBy adopting this hybrid approach, the MailQueue system will benefit from the strengths of each component while mitigating their individual weaknesses. This solution will ensure efficient mail processing, robust administrative capabilities, and seamless operation in a distributed setup.","GenTime":"2024-07-29 00:17:55"}
{"File Name":"james-project\/0025-cassandra-blob-store-cache.md","Context":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n\n## Decision\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2896,"Prediction":"## Decision\n\nAdopt a Hybrid Storage Strategy\n\nGiven the varying sizes and access patterns of the data blobs in the system, a hybrid storage architecture will be implemented to optimize both performance and efficiency. Here are the key components of the decision:\n\n1. **Mailbox Message Headers**\n   - **Storage Solution**: Cassandra\n   - **Rationale**: Cassandra offers low-latency access (approximately 4ms) which is suitable for the small and frequently accessed mailbox message headers.\n\n2. **Mailbox Message Bodies**\n   - **Storage Solution**: Object Storage (e.g., Amazon S3, Google Cloud Storage)\n   - **Rationale**: Object Storage is optimized for storing large objects and the relatively lower access frequency of message bodies aligns well with the characteristics of object storage.\n\n3. **DeletedMessageVault Message Headers**\n   - **Storage Solution**: Lightweight Database or Distributed Cache (e.g., Redis, Cassandra)\n   - **Rationale**: Although these headers are small, they are infrequently accessed, necessitating a cost-effective storage solution. A lightweight database or distributed cache can balance cost with performance.\n\n4. **Abstract Storage Layer**\n   - **Implementation**: Introduce an abstract storage layer that can route data to the appropriate storage backend based on type, size, and access frequency.\n   - **Rationale**: This abstraction ensures that the system remains flexible and can be extended to support new types of data or changes in access patterns without significant refactoring.\n\n5. **Monitoring and Analysis**\n   - **Strategy**: Implement monitoring and logging to continuously analyze access patterns and performance.\n   - **Rationale**: Ongoing analysis will help in dynamically adjusting storage strategies and optimizing resource allocation.\n\nBy adopting this hybrid approach, the system can leverage the strengths of both high-performance databases like Cassandra and cost-effective, scalable object storage solutions, thereby improving overall system performance and user experience.","GenTime":"2024-07-29 00:18:12"}
{"File Name":"james-project\/0041-replace-jcloud-with-s3.md","Context":"## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n\n## Decision\n","Decision":"* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n","tokens":209,"id":2897,"Prediction":"## Decision\nTo simplify the implementation and align with the Reactive style of the codebase, we will transition from using the Apache jcloud library for Object Storage to adopting the AWS SDK for S3 interactions. \n\nKey Points:\n1. **Remove Existing jclouds Implementation**:\n    - The current jclouds-based implementation will be deprecated and removed.\n    - This will help in reducing the complexity and the build time of the project.\n\n2. **Adopt AWS SDK for S3 Interactions**:\n    - The AWS SDK is a mature, efficient, and widely-used library which will be integrated for S3 interactions.\n    - As Swift offers good S3 compatibility, this approach will support both S3 and Swift object storage without needing separate implementations.\n\n3. **Reactive Programming Style**:\n    - Ensure the new implementation adheres to the Reactive programming style currently adopted in the codebase.\n    - This will improve the maintainability and performance of the blob-store interface within the existing ecosystem.\n\n4. **Evaluate Performance**:\n    - Monitor and evaluate the performance and cost implications of this transition.\n    - Make adjustments as necessary to optimize for both storage cost concerns and the desired performance.\n\n5. **Documentation and Testing**:\n    - Update the documentation to reflect the new implementation approach.\n    - Ensure thorough testing to validate the functionality and compatibility of the new blob-store implementation.\n\nBy streamlining the blob storage implementation with the AWS SDK and focusing on the S3 API, we can achieve a more maintainable, performant, and cost-effective solution that aligns with our current technology stack and development practices.","GenTime":"2024-07-29 00:18:15"}
{"File Name":"banking-cqrs-es-go\/0002-use-adr-tools-to-manage-the-adrs.md","Context":"## Context\\nA tool to make creating these ADRs helps reduce friction around creating documentation.\\n`adr-tools` is a simple cli tool that has been used in the past on previous projects to great effect (https:\/\/github.com\/npryce\/adr-tools)\\n\n## Decision\n","Decision":"Install `adr-tools` locally and use it to help quickly generate consistent ADRs\\n","tokens":56,"id":2914,"Prediction":"## Decision\n\nWe will integrate `adr-tools` into our current project workflow to facilitate the process of creating and managing Architectural Decision Records (ADRs). This decision is based on the tool's proven utility and effectiveness in past projects, as well as its ability to reduce the friction commonly associated with documentation tasks. Using `adr-tools` will streamline the creation of ADRs, ensuring consistency and ease of use for all team members.","GenTime":"2024-07-29 00:19:24"}
{"File Name":"SAP-Cloud\/node-package-lock.md","Context":"## Context\\nThe node package manager (npm) creates a so called [\"lock file\"](https:\/\/docs.npmjs.com\/files\/package-locks) when running `npm install`.\\nThe recommended action is to commit this file to version control, as stated by the command itself:\\n```\\n$ npm install\\nnpm notice created a lockfile as package-lock.json. You should commit this file.\\n```\\nSome npm commands, which are part of the pipeline, such as `ci` and `audit` require that a package lock file exists.\\nThus, if a project lacks the package lock file, there are tree options:\\n* Refuse to build the project, which enforces the recommended practice of committing the lock file\\n* Silently create a package lock file as part of the pipeline\\n* Avoid npm commands which require a package lock\\n* Would disable the npm audit stage\\n\n## Decision\n","Decision":"If a project has a `package.json` file, but no package lock, we create a package lock file as part of the pipeline and warn the user about this.\\nThe created package lock file is not archived as part of the pipeline build artifacts.\\n","tokens":183,"id":2924,"Prediction":"After considering the context and the available options, the decision is:\n\n### Decision\nRefuse to build the project if the package lock file (`package-lock.json`) is not committed to version control.\n\n### Justification\n1. **Alignment with Best Practices**: Adhering to the recommended practice of committing the lock file ensures that the exact dependency tree used by the development and CI environments is preserved. This minimizes \"it works on my machine\" issues and maintains consistency across environments.\n   \n2. **Security and Reliability**: The `npm ci` and `npm audit` commands enhance the security and reliability of the project. By ensuring that the package lock file exists, we can leverage these tools to catch potential vulnerabilities and ensure that the exact dependency versions are used.\n\n3. **Maintaining Pipeline Integrity**: Automatically creating a package lock file within the pipeline could introduce discrepancies between the development environment and the pipeline. Avoiding npm commands that require a package lock would downgrade the security and integrity checks, leading to potential issues.\n\n### Implementation Steps\n1. **Pre-Build Check**: Introduce a pre-build step in the CI pipeline that checks for the existence of the `package-lock.json` file.\n2. **Fail the Build**: If the `package-lock.json` file is missing, the build should fail with a clear error message instructing the developer to commit the lock file.\n3. **Documentation Update**: Update project documentation to clearly state the requirement of committing the `package-lock.json` file and the rationale behind it.\n4. **Developer Communication**: Inform all team members about this new requirement to ensure everyone's awareness and compliance. \n\nBy refusing to build projects without the package lock file, we ensure consistency, reliability, and security throughout the development lifecycle.","GenTime":"2024-07-29 00:20:02"}
{"File Name":"deeplearning4j\/0007 - Nd4j classifiers.md","Context":"## Context\\nNd4j relies upon the c++ library [libnd4j](..\/libnd4j) for native math execution.\\nIt uses [javacpp](https:\/\/github.com\/bytedeco\/javacpp) to link against\\nlibnd4j. Libnd4j is capable of being compiled a myriad of different ways allowing different trade offs to be made\\nin terms of performance and dependencies. This presents complexity in exposing this flexibility to the end user.\\n\n## Decision\n","Decision":"In order to allow users to pick which configuration they would like to use, while avoiding adding a lot of different artifact\\nids to the project, the following javacpp platform extensions are used:\\ncompiled type (avx etc or blank if normal) - software linked against (cudnn, onednn, armcompute) - version\\nAn example for the one dnn platform extension could be:\\ndnnl-2.2\\navx256-dnnl-2.2\\nThis presents 2 examples where a special compilation is enabled and one where it's not\\nboth linking against dnnl\/onednn 2.2.\\n","tokens":99,"id":2932,"Prediction":"## Decision\n\nTo manage the complexity and flexibility of compiling libnd4j in various ways while providing ease of use for the end user, we have decided to adopt a modular approach by creating pre-built binary packages for the most common configurations of libnd4j. This will be done using javacpp to ensure seamless integration with Nd4j.\n\n### Key Points:\n1. **Pre-built Binaries**: We will offer a set of pre-built binaries tailored for different performance and dependency trade-offs, such as CPU-only, CUDA-enabled, and different optimization levels.\n2. **Configuration Profiles**: Create standardized configuration profiles that cater to the majority of use cases. These profiles will be predefined and documented, helping users select the right binary package without needing deep technical knowledge of the underlying library.\n3. **Package Management**: Utilize Java's package management ecosystem (e.g., Maven, Gradle) to distribute and manage these pre-built binaries. Users can simply specify the desired configuration in their project\u2019s dependencies.\n4. **Fallback to Custom Builds**: Provide clear documentation and tooling for advanced users who need to compile libnd4j with custom settings. This will allow power users to optimize libnd4j for their specific needs without obstructing the path for standard users.\n5. **Continuous Integration**: Implement a continuous integration pipeline to build, test, and distribute these binaries regularly to ensure they remain up to date and compatible with the latest versions of Nd4j and libnd4j.\n\nBy adopting this approach, we aim to balance flexibility and ease of use, allowing both novice and advanced users to effectively leverage the power of libnd4j through Nd4j.","GenTime":"2024-07-29 00:20:31"}
{"File Name":"deeplearning4j\/0008 - Nd4j eager  shape computation .md","Context":"## Context\\nNd4j's model import framework often has the need to\\ncompute  shapes as variables are created.\\nThis is  in order to resolve how to properly\\ncreate a graph based on a graph descriptor from another framework\\nsuch as tensorflow or pytorch.\\nThis is often called eager mode. This proposal focuses on just eager shape computation\\nintended for use in model import. The assumption is that we could\\nbuild on this later for fully eager computation.\\n\n## Decision\n","Decision":"In order to aid building model import easier,\\nthis proposal is focused on implementing just dynamic shape computation\\nfor use in the model import context.\\nThis will be composed of a few parts:\\n1. Each outputVariables() call in SDVariable triggers\\nan Nd4j.getExecutioner().exec(..) call on the relevant operation\\nto extract out op shapes. It then sets the appropriate shapes\\nbased on the result for each SDVariable field.\\n2. This will intentionally include dummy calls for control flow ops\\nsuch as if, enter, and while. Shapes from these don't matter\\nbeyond knowing the number of outputs.\\n3. Each SameDiff instance will have an eager mode boolean\\nthat will determine whether this functionality is invoked.\\nThis eager mode variable will be required for some model import use cases.\\nUsually the model import framework will turn eager on as needed\\nwithout the user needing to be involved.\\n4. Each SameDiff instance will have a separate ArrayHolder\\nthat will be used for looking up ndarrays relevant\\nto the eager computation. This will not use proper sessions\\nbut instead store that will be used once for computing shapes.\\n","tokens":97,"id":2933,"Prediction":"## Decision\n\nWe will implement an eager shape computation mechanism within Nd4j\u2019s model import framework specifically designed for model import functionality. This feature will compute shapes of variables in real-time as they are created, based on the graph descriptor from external frameworks like TensorFlow or PyTorch. \n\nThe design will focus initially on shape computation for model import, with the potential to be extended to fully support eager computation in the future. This incremental approach allows us to validate and optimize the shape computation process in a controlled manner before expanding its scope.\n\nThis decision is expected to improve the accuracy and efficiency of importing models from other frameworks by ensuring that the graph structure can be correctly and dynamically inferred during the import process.","GenTime":"2024-07-29 00:20:32"}
{"File Name":"deeplearning4j\/0003-dealing_with_inconsistencies_in_java_naming.md","Context":"## Context\\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\\n`BernoulliDistribution`.\\nTwo possible solutions where suggested:\\n1. Add an additional property that provides us with the correct class name\\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\\n\n## Decision\n","Decision":"For now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n","tokens":92,"id":2934,"Prediction":"## Decision\nWe will proceed with the second solution: renaming classes in ND4J to ensure consistency while providing backwards compatibility via deprecated subclasses.\n\n### Rationale:\n1. **Consistency:** Renaming the classes ensures that there is a consistent and intuitive mapping between factory methods and class names.\n2. **Backward Compatibility:** Deprecated subclasses will allow existing codebases to continue functioning without disruption, providing developers ample time to transition to the new naming conventions.\n3. **Maintainability:** This approach will simplify the codebase in the long term, as future developers will not need to deal with inconsistencies or additional properties to map methods to classes.\n4. **Documentation:** This change will make it easier to document and understand the relationships between factory methods and their corresponding classes.\n\nImplementation will involve:\n- Renaming the existing classes to match the factory method names (e.g., `BernoulliDistribution` to `Bernoulli`).\n- Creating deprecated subclasses that alias the old names to the new classes (e.g., `class BernoulliDistribution extends Bernoulli`).\n\nA comprehensive list of all affected classes and methods will be created, and appropriate deprecation warnings will be added to guide developers during the transition period.","GenTime":"2024-07-29 00:20:35"}
{"File Name":"deeplearning4j\/0001-SameDiff_File_Format.md","Context":"## Context\\nSameDiff models need to be serializable - i.e., something we can save to disk or send over the network.\\nAdditionally, we need to be able to save and load model files in C++, and have those be readable in other languages (mainly Java).\\nCurrently, we have a FlatBuffers-based format for SameDiff graph serialization, but it has a number of problems, as discussed in this issue: https:\/\/github.com\/eclipse\/deeplearning4j\/issues\/8312\\n\n## Decision\n","Decision":"We will transition from a pure FlatBuffers to a Zip + FlatBuffers model format.\\nFlatBuffers will be used for the graph structure only. Parameters will be stored separately to the graph structure, also within the zip.\\nWe will introduce the ability to support multiple versions of a graph in the model files.\\nThis will enable the model file to support storing\\n* Multiple data types (for example, a FP32 version and a quantized INT8 version)\\n* Multiple different checkpoints (parameters after 1000 iterations, after 5000, and so on)\\n* Multiple versions of a given model (English vs. Chinese, or cased\/uncased, etc)\\nBy default when loading a graph (unless it is otherwise specified) we will load the most recent model tag.\\nTags must be valid file\/folder identifiers, and are not case sensitive.\\nThe structure of the zip file will be as follows:\\n```\\ntags.txt                         \/\/List of graph tags, one per line, in UTF8 format, no duplicates. Oldest first, newest last\\n<tag_name>\/graph.fb              \/\/The graph structure, in FlatBuffers format\\n<tag_name>\/params.txt            \/\/The mapping between variable names and parameter file names\\n<tag_name>\/params\/*.fb           \/\/The set of NDArrays that are the parameters, in FlatBuffers format\\n<tag_name>\/trainingConfig.fb     \/\/The training configuration - updater, learning rate, etc\\n<tag_name>\/updater.txt           \/\/The mapping between variable names and the updater state file names\\n<tag_name>\/updater\/*.fb          \/\/The set of NDArrays that are the updater state\\n```\\nNote that params.txt will allow for parameter sharing via references to other parameters:\\n```\\nmy_normal_param 0\\nshared_param <other_tag_name>\/7\\n```\\nThis means the parameters values for parameter \"my_normal_param\" are present at `<tag_name>\/params\/0.fb` within the zip file, and the parameter values for \"shared_param\" are available at `<other_tag_name>\/params\/7.fb`\\nNote also that the motivation for using the params.txt file (instead of the raw parameter name as the file name) is that some parameters will have invalid or ambiguous file names - \"my\/param\/name\", \"&MyParam*\" etc\\nIn terms of updater state, they will be stored in a similar format. For example, for the Adam updater with the M and V state arrays (each of same shape as the parameter):\\n```\\nmy_param 0 1\\nother_param 2 3\\n```\\nThat means my_param(M) is `<tag_name>\/updater\/0.fb` and my_param(V) is at `<tag_name>\/updater\/1.fb`\\nThis format also allows for updater state sharing, if we need it.\\n**Graph Structure**\\nThe graph structure will be encoded in FlatBuffers format using a schema with 2 parts:\\n1. A list of variables - each with name, datatype, and (for placeholders, constants and parameters) a shape\\n2. A list of operations - each with a name, op name\/type, input variable names, output variable names, and arguments\\nNote that both legacy and custom ops will be encoded in the same way. For legacy ops, we simply need the operation type, and the operation number.\\nOperation argument encoding will be done using named arguments: essentially, a `Map<String,T>` structure, where T is one of `{long, double, boolean, datatype}`.\\nThis allows for improved backward compatibility (no ambiguity as ops are modified after a graph file was written) and improved interpretability compared to using simple arrays of iargs, bargs, targs and dargs.\\nOne consequence\/downside of this is that we need to define mapping between our named arguments and iargs\/bargs\/targs\/dargs. In Java we have essentially done this manually, though clearly don't want to replicate this work in C++ (or any future languages).\\nTo avoid the need to do a significant amount of work (such as moving the name\/arg mapping to code generation) the following is proposed:\\nThe `Map<String,T>` is split up in the FlatBuffers schema into 4 pairs of fields.\\n* `String[] iArgNames`, `long[] iArgs`\\n* `String[] tArgNames`, `double[] dArgs`\\n* `String[] bArgNames`, `boolean[] bArgs`\\n* `String[] dArgNames`, `DataType[] dArgs`\\nClearly the name and value arrays (for each pair) would each be the same length, and name\/value correspondence is by array index.\\nThis is essentially equivalent to the `Map<String,T>` representation, but has the benefit of not needing us to define the mapping for named args to array-style args any time soon in C++, but also allowing us to add it in the future (mainly before we can write graphs from C++, or have better\/proper backward compatibility after op changes)\\n**Extensibility to Other Types**\\nSuppose in the future we want to store other data for a variable, not just an array?\\nExamples include lists and maps (for example, for NLP applications).\\nWhile we will not implement this right now, there are a number of options for adding this without breaking backward compatibility.\\nFirst: we can enhance the params.txt file format, perhaps using something like the following:\\n```\\nmap_param 0 MAP\\n```\\nSecond: We can add a similar text file for other types. For example, a params_maps.txt, same format as params.txt, with content at `<tag_name>\/params_maps\/*.fb`\\n","tokens":104,"id":2944,"Prediction":"## Decision\n\nWe will move forward with the implementation of Protocol Buffers (protobuf) for the serialization of SameDiff models. \n\n**Rationale:**\n\n1. **Cross-Language Support:** Protocol Buffers provide great support for multiple languages, including both C++ and Java, which are critical to our use case.\n2. **Industry Standard:** Protocol Buffers are widely adopted and trusted in the industry, ensuring long-term maintenance and community support.\n3. **Optimization:** Protocol Buffers offer faster serialization\/deserialization compared to FlatBuffers, which will improve performance.\n4. **Human-Readable Format:** Although not as human-readable as JSON or XML, Protocol Buffers provide better readability for debugging purposes than FlatBuffers.\n5. **Compatibility:** The transition from FlatBuffers to Protocol Buffers can be managed such that legacy models serialized in FlatBuffers can still be deserialized using a backward-compatible or conversion approach.\n6. **Support for Advanced Features:** Protocol Buffers support features like schema evolution, which will allow us to evolve our model serialization format in the future without breaking compatibility.\n\nBy adopting Protocol Buffers, we aim to solve the current serialization issues identified and provide a robust, performant, and future-proof methodology for SameDiff model serialization.","GenTime":"2024-07-29 00:21:03"}
{"File Name":"deeplearning4j\/0009 - Import node pre processing.md","Context":"## Context\\nNd4j's model import framework supports different protobuf based frameworks\\nfor importing and executing models. This was introduced in [0003-Import_IR.md](0003-Import_IR.md)\\nOne problem with importing models is compatibility between different versions of frameworks.\\nOften,migrations are needed to handle compatibility between versions. A node pre processor is proposed\\nthat: when combined with the model import framework allows for\\nannotation based automatic upgrades of graphs.\\n\n## Decision\n","Decision":"In order to handle preprocessing a node to handle things like upgrades.\\nAn end user can specify a pre processor via a combination of 2 interfaces:\\n1. An annotation for specifying a class that implements a relevant rule\\nfor processing. This will automatically be discoverable via annotation scanning\\nsimilar to other frameworks. This annotation looks as follows:\\n```kotlin\\nannotation class NodePreProcessor(val nodeTypes: Array<String>, val frameworkName: String)\\n```\\nThe information include the nodeTypes which are the operation types to scan for when doing upgrades on a graph.\\nThe framework name: relevant if multiple import modules are on the classpath. Filters rules\\nby their intended framework for import.\\n2. The necessary pre processing hook that will handle processing the node\\nand may modify the graph. Graph modification maybe necessary if we need to add new nodes to compensate\\nfor modification of a node such as an attribute moving to being an input.\\n```kotlin\\ninterface NodePreProcessorHook<NODE_TYPE : GeneratedMessageV3,\\nTENSOR_TYPE : GeneratedMessageV3,\\nATTRIBUTE_TYPE : GeneratedMessageV3,\\nATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_TYPE>\\nwhere  DATA_TYPE: ProtocolMessageEnum {\\nfun modifyNode(\\nnode: IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>,\\ngraph: IRGraph<GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, ProtocolMessageEnum>\\n): IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>\\n}\\n```\\n","tokens":94,"id":2945,"Prediction":"## Decision\n\nWe will implement a node pre-processor within the Nd4j model import framework that supports annotation-based automatic upgrades of graphs. \n\n### Key Components:\n\n1. **Node Pre-Processor**: This component will intercept and process nodes during the import process, ensuring compatibility between different versions of the model's framework.\n   \n2. **Annotations for Upgrade**: Introduce a set of annotations that can be used to specify the required transformations or migrations necessary to upgrade nodes from older versions to newer versions.\n   \n3. **Integration with Model Import Framework**: Seamlessly integrate the node pre-processor with the existing model import framework, ensuring it runs automatically during the import process without requiring manual intervention.\n\n### Benefits:\n\n1. **Automatic Compatibility Handling**: Provides a streamlined way to handle version migrations automatically, reducing the manual efforts and potential errors involved in upgrading models.\n   \n2. **Modular and Maintainable**: Using annotations allows for a clean, modular approach to specifying upgrades, which will be easier to maintain as new versions and update paths are added.\n   \n3. **Enhanced Flexibility**: Allows for flexible and customizable upgrades, accommodating various model versions and specific framework quirks.\n\n### Action Plan:\n\n1. Design the node pre-processor and define the necessary interfaces and data structures.\n2. Develop the set of annotations required for specifying upgrades.\n3. Implement the pre-processor logic that applies the annotated transformations.\n4. Test the integrated system extensively with models from different versions to ensure robustness and reliability.\n5. Document the usage of the annotation-based upgrade process and provide examples to facilitate adoption by developers.\n\nBy merging these components into the existing model import framework, we improve the framework's robustness and flexibility, ensuring smoother transitions between model versions.","GenTime":"2024-07-29 00:21:07"}
{"File Name":"pcmt\/adr-007.md","Context":"## Context\\nPCMT wants to accomplish a number of goals through the use of versions:\\n1. Convey to stakeholders the initial suitability of the project:\\n* Pre-v1: In development for global stakeholders.\\n* Version 1: Production ready for global stakeholders.\\n* Version 2: Production ready for national stakeholders.\\n1. Leverage semantic versioning to convey to dependents the nature of the\\nrelease.  e.g. is it a patch-release, major new functionality, etc.\\n1. Mark a specific state of the code-base, tied to a released asset.\\n1. Indicate which version of Akeneo that PCMT is derived from.\\n\n## Decision\n","Decision":"1. We will version PCMT by [Semantic Versioning 2.0][semver].  However since\\nDocker tags do not support the use of the `+` symbol, we'll use `-` in\\nits place.\\n1. We will have a build number (from the commit SHA), which is from a CI\\npipeline, that includes a distinct set of commits.\\n1. We will expire and remove old Build numbers, so they need to be pruned\\nregularly.\\n1. We will not publish assets if a pipeline fails, and conversely if a pipeline\\nis re-run, it consists of the same commits (state of the code), and therefore\\nit should have the same build number.\\n1. We will promote build numbers to a showcase or UAT server, manually.\\n1. We will promote build numbers to a release, manually.\\n1. We will keep the semantic version of the system separate from the build\\nnumber, in a file in the SCM.  When an asset is labeled from\\nthe CI pipeline, it'll use the full form by placing the build number in\\nthe build meta-data of the Semantic Version.  e.g. `1.0.0+48af4a30`\\n1. When promoting to a release we will co-label the published assets such that\\none asset will have the build number, and the released asset will not include\\nthat number. e.g. version 1.0.0 might be `image:v1.0.0+SHA` and\\n`image:v1.0.0`, which are equivalent assets.\\n","tokens":141,"id":2951,"Prediction":"## Decision\nWe will implement a comprehensive versioning strategy for PCMT that accommodates all the outlined goals. The strategy will include the following components:\n\n1. **Version Labels for Stakeholder Communicat**:\n    - **Pre-v1.0.0**: Development phase, indicating global stakeholders that the project is still in the construction stage. Versions will follow a `0.x.y` format.\n    - **v1.0.0**: Production-ready release for global stakeholders.\n    - **v2.0.0**: Production-ready release for national stakeholders.\n\n2. **Semantic Versioning**:\n    - Adopt [Semantic Versioning 2.0.0](https:\/\/semver.org\/) to ensure that version numbers convey the nature of the release.\n    - Version numbers will follow the `MAJOR.MINOR.PATCH` format:\n        - **MAJOR** version increments when there are incompatible API changes.\n        - **MINOR** version increments when functionality is added in a backward-compatible manner.\n        - **PATCH** version increments when backward-compatible bug fixes are introduced.\n\n3. **Code-Base State Marker**:\n    - Each release version will mark a specific state of the code-base.\n    - Tagging in the version control system (e.g., git tags) will be used to mark these states, ensuring traceability and reproducibility of each released version.\n\n4. **Indication of Akeneo Base Version**:\n    - PCMT versions will include metadata that specifies which version of Akeneo it is derived from.\n    - Metadata can be included in the version documentation, release notes, and possibly as an additional version tag (e.g., `v1.0.0-akeneo3.2` for a release based on Akeneo 3.2).\n\nBy adopting this structured versioning approach, PCMT will effectively communicate the state and stability of the project to all stakeholders, while also providing clear guidance on the nature of each release to its dependents.","GenTime":"2024-07-29 00:21:24"}
{"File Name":"status-react\/0007-masking-sensitive-data.md","Context":"## Context\\nWe have some data that we don't want to appear in the logs (user passwords are\\na good example). Currently, they are passed around as strings, that could be\\nprinted out by mistake in a log entry (see https:\/\/github.com\/status-im\/status-react\/issues\/4053)\\n\n## Decision\n","Decision":"To minimize the risk of leaking passwords through logs, we should not pass\\npasswords as strings in our codebase. We introduced a new type `MaskedData` in\\n`status-im.utils.security`.\\nWe use `(security\/mask-data <data to hide>` to wrap sensitive data into this\\ntype and then use `(security\/unmask <masked-data>)` to get the plaintext back.\\nIt is important to keep that sensitive data masked as much as possible, until\\nyou need the plaintext to pass to the extenral APIs.\\nExample:\\n```clojure\\n(println (security\/mask-data \"my-plaintext-password\")) ;; Outputs \"******\"\\n(println (security\/unmask (security\/mask-data \"my-plaintext-password\"))) ;; Outputs \"my-plaintext-password\"\\n```\\n","tokens":63,"id":2958,"Prediction":"## Decision\nSensitive data, such as user passwords, should not be passed around as plain strings to mitigate the risk of them appearing in log entries. Instead, we will adopt a strategy for redacting or substituting sensitive data in logs.\n\n1. **Use Secure Data Types:** Implement custom secure data types or wrappers for sensitive information that prevent direct logging. These types will override typical logging methods to ensure sensitive data is either masked or omitted from logs.\n\n2. **Automatic Redaction:** Integrate a library or middleware that automatically scans for and redacts predefined sensitive data patterns in log entries before they are written. This may involve incorporating Regex patterns or similar techniques to detect and mask information.\n\n3. **Logger Configuration:** Configure the logging framework to apply filters or hooks that identify and redact sensitive information. For instance, modifying log formats and including specific sanitization steps during log serialization can help ensure sensitive data is not exposed inadvertently.\n\n4. **Developer Guidelines:** Establish and enforce coding guidelines for developers to avoid logging sensitive information directly. This may include code reviews and audits focused on verifying adherence to secure logging practices.\n\n5. **Monitoring & Auditing:** Implement ongoing monitoring and auditing mechanisms to periodically review log entries for potential leaks of sensitive data to catch and address lapses swiftly.\n\nBy enforcing these practices, we aim to eliminate the risk of sensitive user data being compromised through logging activities.","GenTime":"2024-07-29 00:21:45"}
{"File Name":"status-react\/0002-extension-uri.md","Context":"## Context\\nExtensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.\\n\n## Decision\n","Decision":"URI follows ethereum URI schema as specified in [EIP 831](https:\/\/github.com\/ethereum\/EIPs\/blob\/master\/EIPS\/eip-831.md)\\n```\\nuri                     = \"ethereum:\" path\\npath                    = \"status:extension\" \":\" storage \"@\" id\\nstorage                 = STRING (e.g. IPFS)\\nid                      = STRING (e.g. IPFS hash)\\n```\\nAn example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`\\n","tokens":26,"id":2963,"Prediction":"## Decision\nWe will adopt an EDN (Extensible Data Notation) file format for defining extensions and utilize a decentralized storage solution for accessing these files. Additionally, we will implement a specific URI schema to uniquely identify and retrieve these EDN files from the decentralized storage.\n\n### Rationale:\n1. **Interoperability**: The EDN file format is widely supported and adds simplicity by adhering to a standard that can be easily parsed and manipulated by various programming languages.\n2. **Decentralization**: Using decentralized storage (e.g., IPFS) ensures high availability, resilience, and tamper-resistance for extension files, eliminating reliance on a single point of failure.\n3. **Scalability**: The decentralized storage system can scale seamlessly with the growing number of extension files without requiring significant changes or additional overhead.\n4. **Security**: Decentralized storage coupled with a unique URI schema can enhance security through immutability and verifiable access paths.\n5. **Ease of Retrieval**: A well-defined URI schema allows for straightforward and consistent access to extension files, simplifying the logic required for retrieving and loading extensions in the system.\n\n### Implementation:\n- **Define a URI schema** that could look something like `edn:\/\/<hash>` where `<hash>` would be the unique identifier in the decentralized storage.\n- **Develop utilities** for reading and writing EDN files to\/from the decentralized storage, ensuring these utilities handle required checks and validations.\n- **Integrate the URI-based retrieval mechanism** within the system's extension management modules, ensuring extensions can be dynamically loaded and updated as needed.\n\n### Risks:\n- **Performance**: Potential latency issues when accessing files from decentralized storage.\n- **Complexity**: Added complexity in managing the decentralized infrastructure and ensuring data consistency.\n\n### Mitigation:\n- **Caching**: Implement local caching mechanisms to reduce latency and improve performance.\n- **Monitoring**: Establish monitoring and automated consistency checks to detect and resolve any issues arising from decentralized storage.","GenTime":"2024-07-29 00:22:02"}
{"File Name":"status-react\/0011-tweak-pr-process.md","Context":"## Context\\nThere was a generally dissatisfaction with our PR flow process from multiple stakeholders, including devs, QA and design. These largely centered around size, speed of integration and quality of PRs.\\nFor more details, please see pain points in the meeting notes held end of February here: https:\/\/notes.status.im\/C5pj8g7gQOu9Wo8PtDZsMw?edit# as well as the preceeding Discuss thread: https:\/\/discuss.status.im\/t\/better-pull-requests-process\/1044\\nAlso see conversations in Core Dev Call #12 and #13: https:\/\/github.com\/status-im\/pm\/\\n\n## Decision\n","Decision":"These specific policy changes were agreed upon (a la 'rough consensus' model). These are heuristics, and should be more of a guide than strict rules. Exception to these should be minimal though.\\n**1. Reject PRs that don't have tests**\\nAs a rule of thumb, coverage should increase.\\n**2. Don't use or promote nightly as something for end users.**\\n**3. Reject PRs that don't write what tests the author have done and what platforms you used to test, including screenshots, videos or logs**\\nThis means QA and dev have to spend less time catching obvious problems.\\n**4. Run tests when a PR is at \"Review\" stage on https:\/\/github.com\/status-im\/status-react\/projects\/7 (or add new 'e2e stage' between review and `TO TEST`)**\\n**5. Dev asks for 2-3 reviewers, including a designer if the change affects UI, and these reviewers review within a day.**\\nThis ensures we don't have *diffusion of responsibility* and that PRs are reviewed quickly.\\n","tokens":135,"id":2964,"Prediction":"## Decision\n\nAfter carefully reviewing the feedback and concerns raised by multiple stakeholders, including developers, QA, and design teams, we have decided to adopt the following measures to improve our Pull Request (PR) flow process:\n\n1. **Enforce Smaller PRs**: PRs must be scoped to deliver smaller, incremental changes to make code reviews and integrations more manageable. We will provide guidelines on how to effectively break down features and tasks into smaller PRs.\n\n2. **Implement PR Templates**: Introduce standardized PR templates that outline required information such as related issue numbers, detailed descriptions of changes, testing instructions, and dependency documentation. This aims to ensure comprehensive context for reviewers and streamline the review process.\n\n3. **Introduce a \"WIP\" (Work In Progress) Label**: PRs that are not yet ready for review will be labeled as WIP. This allows developers to share early drafts without overwhelming reviewers and stakeholders with unfinished work.\n\n4. **Design Review Checkpoints**: For changes that significantly affect the user interface or user experience, introduce mandatory design review checkpoints before final review. This ensures design alignment and reduces back-and-forth iterations.\n\n5. **Establish Code Review Time Limits**: Introduce a maximum time limit (e.g., 48 hours) for initial PR reviews to ensure timely feedback and reduce PR queue times. Assign backup reviewers to cover absences and ensure continuous flow.\n\n6. **Automated Testing**: Enhance automated testing frameworks to cover more scenarios, reducing QA workloads and enhancing PR quality. Every PR should ideally pass these tests before moving to the review stage.\n\n7. **Regular Retrospectives**: Conduct regular retrospectives involving all stakeholders to gather continuous feedback on the PR process and make iterative improvements.\n\nThese changes will be implemented in stages, with initial updates to the PR guidelines and templates being prioritized. Training sessions will also be conducted to ensure all team members are familiar with the new process.","GenTime":"2024-07-29 00:22:07"}
{"File Name":"manuela\/0003-gitops-approach.md","Context":"##  Context\\nSee [2. Overall context and container platform](0002-overall-context-and-container-platform.md)\\n\n## Decision\n","Decision":"We use git to address the requirement of a data store:\\n* it is a replicated data store\\n* the hierarchy of deployment targets can be represented as hierarchy of directores in the git repository\\n* can host kubernetes manifests describing application deployments\\n* approval workflows, auditability, etc... is built in\\n* can create replicas with limited content, e.g. a regional DC only replicates what applies to the region with all its factories and line data servers, a factory DC then only replicates what applies to the factory dc and the line data servers, etc.\\n","tokens":25,"id":2968,"Prediction":"## Decision\n\nAfter reviewing the overall context and container platform as outlined in document [0002-overall-context-and-container-platform.md](0002-overall-context-and-container-platform.md), we have made the decision to adopt Kubernetes as our container orchestration platform.\n\n### Rationale:\n\n1. **Scalability**: Kubernetes provides robust mechanisms for scaling applications both up and down. This will allow us to meet variable demand efficiently.\n   \n2. **Portability**: Kubernetes is cloud-agnostic, which ensures that our applications can be deployed across multiple cloud service providers or even on-premises without significant changes to the deployment processes or infrastructure.\n\n3. **Community and Support**: Kubernetes has a large and active community, which translates to continuous improvements, regular updates, and extensive documentation. This community support will be valuable in troubleshooting and optimizing our deployments.\n\n4. **Rich Ecosystem**: Kubernetes boasts a rich ecosystem of tools and integrations, such as Helm for package management, Prometheus for monitoring, and Istio for service mesh capabilities, which will help in building a more efficient and manageable application environment.\n\n5. **Mature Feature Set**: Features like automated rollouts and rollbacks, self-healing capabilities, and secret management are all built into Kubernetes and align well with our operational requirements and best practices for software deployment.\n\nThis decision aligns with our strategic goals and provides a solid foundation for the deployment and management of our containerized applications. We will move forward with planning the migration of our existing services and systems to Kubernetes, ensuring minimal disruption and aligning with our long-term goals for infrastructure enhancement.","GenTime":"2024-07-29 00:22:17"}
{"File Name":"architectural-decision-log\/0005-convention-based-logging-classes.md","Context":"## Context and Problem Statement\\nHamlet Deploy as a product is made up from several \"components\". They are not written in the same coding\/scripting language and so their outputs - particularly when it comes to log behaviour and error messages - are quite different from each other. When an error or debug message is received, how can the end-user and the Hamlet Deploy developer both gain greater insight from the message that is received?\\n## Decision Drivers <!-- optional -->\\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\n\n## Decision\n","Decision":"* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\nChosen option:  _\"Use Logging Codes Following a Numerical Convention\"_ with _\"Per-Component Code Range\"_\\nIt was deemed easier to implement and there seemed little benefit and greater complexity to implementing an inheritance-based approach.\\nA code range allows each component to implement the definitions in the way that most makes sense to it.\\n","tokens":122,"id":2975,"Prediction":"## Decision\nTo address the inconsistency and variability of log behaviours and error messages from different components of Hamlet Deploy, we will implement a unified logging and error-handling framework. This framework will be integrable with Bash, Python, and Freemarker (Java) components. \n\n### Key Aspects of the Decision:\n1. **Standard Logging Format:** \n   - Introduce a standard logging format that includes essential information such as timestamp, component name, severity level, and the actual message. This ensures that logs from various components are uniformly structured.\n   \n2. **Logging Library\/Module:** \n   - Develop or integrate a logging library\/module for each of the languages (Bash, Python, and Java) to adhere to the standard logging format. This can either be a custom script for simplicity or an extension of existing libraries.\n\n3. **Centralized Log Aggregation:**\n   - Implement a centralized log aggregation system where logs from all components are collected. This can be achieved using tools like ELK (Elasticsearch, Logstash, Kibana) stack or similar technologies that support multi-language log ingestion.\n\n4. **Error Handling Standardization:**\n   - Define a standardized way of handling and displaying error messages. This will include error codes, descriptions, and possible steps for remediation. Each component will map its errors to this standard.\n\n5. **Configuration Management:**\n   - Use a configuration file or environment variables to manage log levels (DEBUG, INFO, WARN, ERROR) and determine where the logs are sent (e.g., console, file, centralized log system). This configuration will be consistently applied across all components.\n\n### Implementation Steps:\n\n1. **Define a Logging Interface:**\n   - Create an interface description detailing the format and fields required for logs.\n\n2. **Develop Logging Modules:**\n   - **Bash:** Develop a logging function\/script that adheres to the standard log format and can be sourced in any Bash script.\n   - **Python:** Implement a Python module (e.g., using the `logging` library) that encapsulates the standard logging format.\n   - **Freemarker (Java):** Utilize or extend existing logging frameworks (such as SLF4J, Logback) to adhere to the standard format.\n\n3. **Centralized Log Aggregation Setup:**\n   - Set up and configure a centralized log aggregation tool that can ingest logs from Bash, Python, and Java components.\n\n4. **Error Handling Guidelines:**\n   - Develop a document specifying the standardized error-handling approaches and ensure all components adhere to it.\n\n5. **Documentation and Training:**\n   - Create comprehensive documentation on how to use the new logging and error-handling system. Provide training or references for developers to integrate these into their existing components.\n\nBy adopting this unified framework, both the end-users and developers will benefit from consistent, informative, and actionable log and error messages, thereby enhancing the troubleshooting and debugging process across multiple components of Hamlet Deploy.","GenTime":"2024-07-29 00:22:44"}
{"File Name":"grout\/adr-2-rename-package.md","Context":"## Context\\nThe name `ashlar` [is already taken on PyPi](https:\/\/pypi.org\/project\/ashlar\/).\\nSince PyPi requires unique names for packages, this means that if we want to\\ndistribute our package on PyPi, we'll have to either:\\n1. Convince the owners of `ashlar` to give it to us\\n2. Name the PyPi package something similar to `ashlar` but slightly different,\\nlike `ashlar-core`\\n3. Come up with a new name for the project\\nOption 1 seems unlikely, given the maturity of the ashlar package on PyPi and\\nhow recent the last release was (April 2018, less than four months ago). Number\\n2 is perfectly functional but frustrating from a branding and distribution perspective,\\nsince it has the potential to introduce some confusion and\/or competition with\\nthe existing `ashlar` package.\\nInstead, I believe that the best course of action is to choose option 3 and rename the project.\\nThis will require us to come up with a new name for Ashlar, a [notoriously\\ndifficult decision](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html).\\nSome options that I considered, all based on the idea of \"flexible\\nconstruction materials\":\\n- [Joist](https:\/\/en.wikipedia.org\/wiki\/Joist)\\n- [Lintel](https:\/\/en.wikipedia.org\/wiki\/Lintel)\\n- [Silicone](https:\/\/en.wikipedia.org\/wiki\/Silicone)\\n- [Grout](https:\/\/en.wikipedia.org\/wiki\/Grout)\\n\n## Decision\n","Decision":"I propose that we rename the project to **Grout**. Among the options above,\\n\"Grout\" is the name that sounds the best to me, and it's the one that I believe\\noffers the closest allegory for the project.\\nGrout is a construction material widely known for its physical flexibility and its\\npractical versatility: a fluid concrete used to create waterproof seals in\\nmasonry structures.\\nSome advantages of the name \"Grout\" include:\\n- \"Grout\" respects the origins of the project by referencing a masonry material,\\nbut unlike \"Ashlar\", the name \"Grout\" emphasizes the core features of the project\\n-- its flexibility and versatility as a base material\\nthat can scaffold and tie together much more complex projects.\\n- \"Grout\" is one syllable (one fewer than \"Ashlar\") and the easiest word to\\npronounce among the options I considered.\\n- Perhaps most importantly, `grout` is [available on\\nPyPi](https:\/\/pypi.org\/project\/grout).\\n","tokens":339,"id":2980,"Prediction":"## Decision\n\nWe have decided to rename the project to `Joist`.\n\n### Rationale\n\nAfter carefully considering the available options given the conflict with the existing `ashlar` package on PyPi, the decision to rename the project to `Joist` is based on the following factors:\n\n1. **Uniqueness:** The name `Joist` is not currently taken on PyPi, ensuring that we can distribute our package without any conflict.\n2. **Branding:** `Joist` fits well within the thematic scope of \"flexible construction materials,\" aligning with the original concept of `ashlar`.\n3. **Memorability:** The name `Joist` is straightforward and easy to remember, which is beneficial from a branding and user adoption standpoint.\n4. **Avoiding Confusion:** By selecting a completely new name, we avoid any potential confusion or competition with the existing `ashlar` package on PyPi, ensuring clear differentiation.\n\n### Implementation\n\n1. Inform all team members and stakeholders about the project name change.\n2. Update all references to `ashlar` in the codebase, documentation, website, and marketing materials to `Joist`.\n3. Register the new `Joist` package on PyPi to ensure availability and prevent any other conflicts.\n4. Communicate the change through all appropriate channels, including GitHub, project documentation, and release notes to ensure that users are aware of the new project name.\n\nBy adopting this decision, we aim to maintain a clear and consistent brand while ensuring a smooth distribution process on PyPi.","GenTime":"2024-07-29 00:23:02"}
{"File Name":"sre-challenge\/0004-using-aws-ecs-as-platform.md","Context":"## Context\\nI want to use docker at AWS but I don't want to provision a docker\\ninfrastructure myself.  So I'm going to use AWS ECS managed service and create\\na cluster.\\n\n## Decision\n","Decision":"Done.\\n","tokens":42,"id":2997,"Prediction":"## Decision\n\nWe will utilize Amazon Elastic Container Service (ECS) for managing our Docker containers in AWS. This decision is driven by our need to avoid the overhead and complexity of manually provisioning and maintaining a Docker infrastructure while leveraging a fully managed service. AWS ECS will handle the orchestration and cluster management, allowing us to focus on application development and deployment efficiencies.\n\nThe rationale behind this decision includes:\n* **Simplicity and Efficiency**: AWS ECS abstracts the complexity of managing Docker orchestrations.\n* **Scalability**: ECS can easily scale with the infrastructure needs of our application.\n* **Integration with AWS Services**: Seamless integration with other AWS services such as IAM, CloudWatch, and S3 will enhance our deployment and monitoring capabilities.\n* **Cost-Effectiveness**: By using a managed service, we can reduce operational costs and maintenance overhead associated with self-managed Docker setups.\n\nNext steps include setting up the ECS cluster, defining task definitions, and deploying our Dockerized applications to the ECS-managed infrastructure.","GenTime":"2024-07-29 00:23:47"}
{"File Name":"sre-challenge\/0008-use-taurus-for-benchmark-tests.md","Context":"## Context\\nTo perform benchmarking tests, I want to set some resources capable to generate\\na high volume of requests against the cluster.\\nTo do this, I could use several tools, like JMeter, Garret, or Taurus.\\nBut I like Blazemeter's Taurus over all because it is very simple to setup and\\nexecute tests.\\nWith Taurus you can create JMeter tests, or use other tools like Selenium, in\\norder to execute and collect results.\\nThen It generates a report that can be easily imported in a JMeter tool or\\nopen it in a data analysis tool like jupyter notebook to draw new diagrams.\\n\n## Decision\n","Decision":"Create an instance with an easy test (Just request the keys and the metrics).\\n","tokens":137,"id":2998,"Prediction":"## Decision\n\nWe have decided to utilize Blazemeter's Taurus as the primary tool for performing benchmarking tests against our cluster.\n\n### Rationale:\n\n1. **Ease of Setup and Execution**: Taurus provides a straightforward setup and execution process, allowing for quicker iterations and less time spent on configuration compared to alternatives.\n   \n2. **Versatility**: With Taurus, we can create JMeter tests or leverage other tools like Selenium, giving us a broad range of testing capabilities.\n\n3. **Comprehensive Reporting**: Taurus generates detailed reports that are compatible with JMeter. These reports can also be easily imported into data analysis tools such as Jupyter Notebook, enabling in-depth analysis and custom diagram creation.\n\n4. **Future-Proofing**: Using Taurus aligns with our future goals of integrating test results into our broader data analysis workflows, potentially streamlining performance monitoring and improvement cycles.\n\nBy adopting Taurus, we aim to enhance our benchmarking capabilities, allowing for efficient performance testing and insightful analysis.","GenTime":"2024-07-29 00:23:49"}
{"File Name":"sre-challenge\/0007-use-a-dashboard-for-etcd.md","Context":"## Context\\nTo visualize the metrics of the etcd cluster, I would like to implement a\\ndashboard using the USE and RED methods.\\n- USE to measure the performance of the system hosting the etcd cluster\\n- RED to measure the performance of the gRPC side of the etcd cluster\\nBut I haven't found anything like this and I have no time to waste.  So\\nI have found a dashboard on the grafana site that has some metrics.\\n\n## Decision\n","Decision":"As I'm out of time, I'm just going to implement it at the configuration\\nmanagement system, to automatically provision the dashboard and the\\ndatasource.\\n","tokens":97,"id":3004,"Prediction":"## Decision\n\nIn order to quickly and efficiently visualize the metrics of the etcd cluster, we will implement an existing Grafana dashboard that provides relevant metrics. This decision is made considering the following:\n\n1. **Time Constraints**: Creating a custom dashboard using the USE and RED methods would be ideal, but given the current time limitations, it is not feasible to build it from scratch.\n   \n2. **Availability of Pre-built Solutions**: A pre-built dashboard for etcd metrics is available on the Grafana website, which can provide immediate insights and monitoring capabilities.\n\n3. **Coverage of Metrics**: While the existing dashboard may not fully cover both the USE and RED methods, it can still offer a valuable starting point. It may contain essential metrics that can later be extended or customized to include more specific USE and RED metrics as time permits.\n\nTherefore, we will proceed with the following actions:\n- Deploy the existing Grafana dashboard for immediate visualization of etcd cluster metrics.\n- Review and analyze the metrics provided by this dashboard to ensure it meets the critical monitoring needs.\n- Plan a phased approach to incorporate and customize the dashboard for additional USE and RED metrics as resources become available.\n\nThis decision allows us to balance immediate needs with long-term goals, ensuring that we have some level of monitoring in place while leaving room for future enhancements.","GenTime":"2024-07-29 00:24:07"}
{"File Name":"sre-challenge\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3006,"Prediction":"## Decision\nWe will adopt the Architectural Decision Record (ADR) format to document our architectural decisions throughout this project. This will include all significant design choices, the context in which they were made, the alternatives considered, and the rationale behind the chosen solution. The ADRs will be stored in a central repository accessible to all team members and updated regularly as the project evolves. By doing this, we aim to ensure transparency, maintain a historical record of decisions, and facilitate better communication and understanding among team members.\n\nADRs will contain:\n- A title reflecting the decision or its topic.\n- The date of the decision.\n- The context or background information leading to the decision.\n- The decision itself.\n- The rationale behind the decision.\n- Any alternatives considered and why they were not chosen.\n- The consequences, both positive and negative, of the decision.\n\nThis structured approach will help us manage architectural changes efficiently and provide a clear reference for future work and maintenance.","GenTime":"2024-07-29 00:24:12"}
{"File Name":"embvm-core\/0021-eliminate-name-variables-from-core-base-classes.md","Context":"## Context\\nUnder the initial design of the `embvm-core`, the following types stored a name in the base class:\\n- Driver\\n- Processor\\n- Hardware Platform\\n- Platform\\nIn some cases, like drivers, this data was duplicated (through the driver registry) or unnecessary (because you use the default name and provide the system name in the driver registry). We don't need to store two different instances of the same name!\\nIn other cases, like the processor and platforms, these names were never used.\\n\n## Decision\n","Decision":"These names will no longer be required by the base classes.\\n","tokens":107,"id":3023,"Prediction":"## Decision\nThe decision was made to remove the name storage from the base classes of Driver, Processor, Hardware Platform, and Platform in the `embvm-core` library. Instead, name handling responsibility will fall solely on the driver registry and any relevant system components that directly interact with names. This change reduces unnecessary redundancy and complexity within the codebase by consolidating name management to where it is actively needed and used. \n\n- **Driver**: The name will only be stored in the driver registry to avoid duplication.\n- **Processor & Platforms**: These components will no longer store names as the functionality is redundant and not utilized in the current design.\n\nThis decision enhances maintainability and performance by removing unused and redundant data storage.","GenTime":"2024-07-29 00:25:22"}
{"File Name":"embvm-core\/0014-refactor-driver-interfaces-to-use-namespaces.md","Context":"## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n\n## Decision\n","Decision":"Each driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\\n","tokens":225,"id":3027,"Prediction":"## Decision\nWe will refactor the driver interfaces and associated types to use C++ namespaces instead of struct-based pseudo-namespacing. This decision simplifies the inheritance hierarchy, makes the code more idiomatic, and leverages the existing language features effectively.\n\nThe updated approach will define the driver-specific types and enums within appropriate namespaces, making the code more readable and maintainable. The new format will look like this:\n\n### Updated `tof` Namespace\n```cpp\nnamespace embvm\n{\n    namespace tof\n    {\n        using distance_t = uint16_t;\n        using cb_t = stdext::inplace_function<void(distance_t)>;\n        constexpr distance_t INVALID_RANGE = UINT16_MAX;\n\n        enum class mode\n        {\n            defaultRange = 0,\n            shortRange,\n            medRange,\n            longRange,\n        };\n    }\n}\n```\n\n### Updated Interface Class\n```cpp\nclass TimeOfFlight : public embvm::DriverBase\n{\npublic:\n    virtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\n};\n```\n\nBy moving the `tof`-related members and enums into the `embvm::tof` namespace, we avoid unnecessary inheritance and better utilize C++ language features, resulting in clearer and more maintainable code.","GenTime":"2024-07-29 00:25:34"}
{"File Name":"reaction-component-library\/0007-publish-components.md","Context":"## Context\\nWe need to be able to pull in these components to other apps as one or more NPM packages. There are pros and cons to a single-package approach versus one package per component.\\n### One Package Per Component\\nPros:\\n- Minimum dependencies pulled in with each component used. You don't download or package anything you aren't using.\\n- When you need to pull in a fix to one component, you aren't unintentionally changing the behavior of a hundred other components.\\nCons:\\n- How do we track dependencies per component but also install them so that the whole Style Guide app can run as one?\\n- Each component package has to be installed by developers as it is needed\\n- Publishing them will be tricky, especially if there are any interdependencies. Lerna + semantic-release may help, but the typical Lerna repo structure may not be ideal with the context of the Style Guide app we have here.\\n### A Single Package Exporting All Components\\nPros:\\n- Install a single package and you get access to them all in your app\\n- Much simpler to publish vs multiple packages\\n- Easier for people working on the style guide repo to understand.\\nCons:\\n- Every time you bump the dependency version of the component package, you'll pull in potentially changed versions of every component in your app.\\n- Extra work will be required to ensure that bundled JavaScript does not include any components that an app does not use.\\n### What about the style guide\\nThere is a further complication here, which is \"how does the style guide release cycle relate to the release cycle of the components within it, and how does the style guide represent multiple versions of components as they change?\"\\n\n## Decision\n","Decision":"The complexity of setting up, maintaining, and understanding a one-package-per-component approach, even with the help of tools like Lerna, is a very strong con. So if we assume that a single package is our preference, let's look at how we might mitigate the cons of that apprach.\\nThe first con, pulling in unwanted component changes, is MAJOR. This causes a lot of headaches for a lot of people. There are a few things we can do to avoid it:\\n- Every component change that changes the appearance of a component in any way should be exported as a new component. The whole component folder can be copied into a `v2` folder. That way, any app that pulls in an updated package will not see any changes until they change `import { Button } from \"@reactioncommerce\/components\/v1` to `import { Button } from \"@reactioncommerce\/components\/v2`\\n- No React component in this library will import any other React component in this library. All components that a component needs to render will be passed in as props or children.\\n- Every component will have Jest snapshot testing, which will give a clue to developers and reviewers that a component's appearance may have changed. Part of the review process can be deciding whether the changes are visual and whether they require splitting off a new version of the component (i.e. are considered \"breaking\" changes). There is the possibility of automating this even more through automated screen shot comparison.\\nThis approach also answers the question of how the style guide will show all versions of the component. Since all versions will remain in the repo in versioned folder names, they'll just naturally appear.\\nThe second con of a single package can be avoided by the following suggestion:\\n- Apps using the component package should set up a \"tree shaking\" solution to avoid bundling any components they do not import.\\n","tokens":350,"id":3031,"Prediction":"## Decision\n\n### Single Package Exporting All Components\n\nFor this project, we will adopt the single-package approach that exports all components. This decision is based on the following considerations:\n\n#### Advantages:\n1. **Simplified Installation and Usage**: Developers can install a single package to access all components, reducing the complexity of managing multiple dependencies. This approach aligns with standard npm practices, making it easier for developers to start using the components quickly.\n\n2. **Simpler Publishing Workflow**: Managing and publishing a single package is significantly simpler than dealing with multiple packages. This reduces the risk of inconsistencies and errors during the release process.\n\n3. **Ease of Contribution**: Contributors working on the style guide repo will have a clearer understanding of the package structure. This enhances collaboration and reduces the learning curve for new contributors.\n\n4. **Consistent Versioning**: A single package ensures that all components are versioned together, providing a consistent experience. This is particularly important for maintaining the integrity of the style guide and ensuring that all components work seamlessly together.\n\n#### Mitigation of Cons:\n- **Managing Dependency Changes**: To address the issue of pulling in potentially changed versions of every component, we will implement robust testing and version control practices. By conducting comprehensive regression testing before each release, we can minimize unexpected changes in component behavior.\n  \n- **Bundling Optimization**: We will leverage tree shaking and other optimization techniques to ensure that the final bundled JavaScript only includes the components that an app uses. Tools like Webpack or Rollup can help in this optimization process.\n\n#### Style Guide Integration:\n- **Release Cycle Synchronization**: The release cycle of the style guide and its components will be synchronized. Whenever a significant change is made to a component, a new version of the style guide will be released. This ensures that the style guide always accurately represents the latest versions of the components.\n  \n- **Version Representation**: The style guide will maintain a version history for each component, allowing developers to understand changes over time. Documentation within the style guide will clearly indicate the version of each component currently in use.\n\nThis approach balances ease of use, maintainability, and component decoupling, ensuring that the development and maintenance of the component library remain efficient and effective.","GenTime":"2024-07-29 00:25:53"}
{"File Name":"reaction-component-library\/0003-choose-a-style-guide-generator-framework.md","Context":"## Context\\nWe want:\\n- Write all simple React components in one repository\\n- Document the React components with code comments\\n- Add additional markdown documentation for components when necessary\\n- Allow both designers and engineers to edit the docs\\n- Run the tool locally to make component development and testing easier\\n- Build into a hostable web app, which can be used by anyone to learn our style, pick an appropriate component, and edit the component on the page\\n- Be able to style\/theme anything about the style guide app as a whole to match our other docs\\n### Options\\n[React Storybook](https:\/\/storybook.js.org\/)\\n[React Styleguidist](https:\/\/react-styleguidist.js.org\/)\\n\n## Decision\n","Decision":"Use Styleguidist. They way it is built from markdown is more user-friendly for designers to edit vs. React Storybook. Also, it is more aimed at generating a living style guide, whereas Storybook is more of a developer's tool.\\n","tokens":147,"id":3035,"Prediction":"## Decision\n\nWe have decided to use **React Storybook** for documenting and demonstrating our React components. Below are the key reasons for this decision:\n\n1. **Component Isolation**: React Storybook provides an excellent environment for isolating, developing, and testing individual components. This makes it easier to ensure that each component functions correctly before integrating it into a larger application.\n\n2. **Rich Add-ons Ecosystem**: Storybook offers a wide array of add-ons that cater to different needs such as documentation (e.g., DocsPage), accessibility (e.g., a11y), theming (e.g., theming add-ons), and more. This flexibility allows us to extend the functionality of our component library as needed.\n\n3. **Designer and Developer Collaboration**: Storybook's ease of use and ability to render components with various states and properties makes it an ideal tool for collaboration between designers and engineers. Both can interact with and edit the documentation and examples directly within the Storybook interface.\n\n4. **Local Development Environment**: Storybook runs locally, making it straightforward for developers to develop, document, and test components in an isolated environment. This local environment ensures rapid feedback and iteration for all team members.\n\n5. **Web Hostable**: Storybook can be built into a static web application, making it accessible to anyone in the organization or even externally. This aligns with our requirement to have a hostable web app that can be used by designers, developers, and other stakeholders to learn about our style and components.\n\n6. **Custom Theming**: We can easily customize the look and feel of the Storybook UI through theming options, ensuring that it aligns with the overall branding of our other documentation.\n\nGiven these benefits, React Storybook aligns well with our requirements to not only develop and document our components effectively but also to maintain a cohesive experience between designers and engineers.","GenTime":"2024-07-29 00:26:09"}
{"File Name":"register-a-food-business-front-end\/0002-implement-open-source-govuk-react-components-library.md","Context":"## Context\\nThe website needs to comply with the GDS design language. In the future, the standard GDS design language might need to be themed to match food.gov colours, fonts, etc.\\n\n## Decision\n","Decision":"We will implement the open source govuk-react npm package hosted at [https:\/\/github.com\/penx\/govuk-react](https:\/\/github.com\/penx\/govuk-react) wherever it has a component that fits our needs.\\nWe will create custom components and layouts where necessary to follow the GDS design language.\\nWe will contribute back to the project to ensure it fulfils all of our needs.\\n","tokens":41,"id":3037,"Prediction":"## Decision\nWe will adopt the GDS (Government Digital Service) design language as the baseline for our website's UI\/UX to ensure compliance with government standards and best practices. \n\nTo allow for future customization, we will implement the design in a modular and themeable manner:\n\n1. **Modular Design Implementation:** The UI components will be built in a way that they can easily be themed or overridden to match specific brand requirements, such as food.gov colours and fonts.\n   \n2. **Theming Capability:**\n   - We will create a theming system that allows for easy application of different colour schemes, fonts, and other stylistic elements.\n   - This system will be built using CSS variables or a preprocessor like SASS\/LESS, allowing us to define a base GDS theme and a custom food.gov theme.\n\n3. **Separation of Concerns:**\n   - The core components will retain the default GDS design language, ensuring compliance and ease of updates as the GDS evolves.\n   - The custom theming layer will reside on top, allowing for straightforward adjustment of branding elements without altering the underlying structure.\n\n4. **Documentation and Guidelines:**\n   - Comprehensive documentation will be provided to guide developers on how to use and customize the themes.\n   - Guidelines will be established to ensure that any theming complies with accessibility standards and does not conflict with core GDS principles.\n\nBy proceeding with this decision, we ensure both compliance with necessary standards and the flexibility to adapt the design to our specific branding needs in the future.","GenTime":"2024-07-29 00:26:19"}
{"File Name":"register-a-food-business-front-end\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3041,"Prediction":"## Decision\nWe will adopt the use of Architectural Decision Records (ADRs) to document key architectural decisions during the lifecycle of the project. Each ADR will follow a consistent template to ensure clarity and completeness. This will help in maintaining a clear history of why certain technical choices were made and facilitate better communication among team members.\n\n### Specifics of the Decision:\n1. **Template**: We will use a standard ADR template that includes sections such as Context, Decision, Status, Consequences, and Alternatives Considered.\n2. **Tooling**: ADRs will be stored in a version-controlled environment, such as a dedicated directory within our Git repository, to ensure they are easily accessible and managed.\n3. **Frequency**: Architectural decisions will be documented as soon as they are made, preferably during or immediately after relevant discussions or meetings.\n4. **Approval**: Each ADR will be reviewed and approved by the project\u2019s architecture review board or designated technical leads before it is finalized.\n5. **Review Process**: Periodic reviews of existing ADRs will be conducted to ensure they remain relevant and reflect the current state of the project.\n\n### Rationale:\n- **Transparency**: Provides a transparent record of why technical decisions were made.\n- **Knowledge Sharing**: Facilitates onboarding of new team members and serves as a reference for solving similar future problems.\n- **Consistency**: Helps in maintaining consistency in decision-making processes.\n- **Documentation**: Acts as a formal documentation artifact that complements other project documents.\n\n### Next Steps:\n- Establish a designated folder or repository for ADRs.\n- Create and distribute the ADR template to team members.\n- Schedule initial training sessions to familiarize the team with the ADR process.","GenTime":"2024-07-29 00:26:39"}
{"File Name":"signals-frontend\/0009-stop-mocking-fetch-in-tests.md","Context":"## Context\\nFor mocking unittests we are using at this moment the `jest-fetch-mock` package. This works but there are new develoments on the market. As the creator of the [Testing Library](https:\/\/testing-library.com\/) suggests in this [article](https:\/\/kentcdodds.com\/blog\/stop-mocking-fetch), there is a new strategy available that would simplify testing of components that are making fetch calls.\\n# Technical description\\nThe `msw` server intercepts the fetch calls and returns the configured response. This is done outside the test file and only overrides of the standard responses from the default configuration need to be added to the respective test file. [Here is the documentation](https:\/\/github.com\/mswjs\/msw).\\n\n## Decision\n","Decision":"The mocking will be done as much as possible by using the the `msw` package.\\n","tokens":156,"id":3050,"Prediction":"## Decision\n\nAfter evaluating the current approach of using `jest-fetch-mock` for mocking unittests and considering the new strategy recommended by the creator of the Testing Library, we have decided to migrate to using [msw (Mock Service Worker)](https:\/\/github.com\/mswjs\/msw) for the following reasons:\n\n1. **Simplicity and Maintainability**:\n   - MSW provides a more streamlined approach by enabling interception of fetch calls at the network level, which simplifies the test setup and teardown process.\n   - The separation of the mock server configuration from individual test files enhances readability and maintainability.\n\n2. **Enhanced Realism**:\n   - MSW allows for more realistic testing by mimicking actual network conditions and handling, providing higher fidelity in test scenarios that involve fetch calls.\n\n3. **Flexibility**:\n   - MSW provides robust configuration options and flexibility to handle different mock scenarios, including diverse response types, delays, and error conditions, which aligns well with varying testing needs.\n\n4. **Community and Future Proofing**:\n   - With strong community support and active maintenance, MSW is a future-proof solution that will stay up-to-date with modern testing practices and tools.\n   - The recommended practice by influential figures in the testing community (such as Kent C. Dodds) gives confidence in MSW's adoption and longevity.\n\n### Migration Plan:\n1. **Initial Setup**:\n   - Integrate MSW into the project by following the [MSW setup documentation](https:\/\/mswjs.io\/docs\/getting-started\/install).\n   - Configure a default set of handlers that reflect the most common API responses used in our application.\n\n2. **Gradual Transition**:\n   - Gradually replace instances of `jest-fetch-mock` with MSW configurations, starting with non-critical and less complex tests to build confidence in the new approach.\n   - Document any differences in test behavior and adjust as necessary to ensure consistency and reliability.\n\n3. **Validation**:\n   - Conduct a thorough validation of the updated tests to ensure they provide accurate and reliable results.\n   - Collect feedback from the team to address any issues or concerns promptly.\n\n4. **Training and Documentation**:\n   - Provide training sessions or documentation to familiarize the development team with MSW and the new testing strategy.\n   - Update the project's testing guidelines to include best practices for using MSW.\n\nBy adopting MSW, we anticipate improved testing practices, more maintainable codebases, and enhanced confidence in the robustness of our tests.","GenTime":"2024-07-29 00:27:14"}
{"File Name":"boxwise-flask\/Python-ORM.md","Context":"## Context\\nUp to now, database changes have been managed through [Phinx](https:\/\/phinx.org\/) in PHP, with SQL queries written as strings in PHP. Since we are migrating off of the old PHP code base of Dropapp and into the new codebase of Python \/ React, we needed to decide how to handle reading and writing to the DB going forward (GraphQL does not solve this, as you still have to hook the GraphQL interface into the DB somehow).\\n\n## Decision\n","Decision":"1. Learning Curve\\n2. Community Support\\n3. Power and Reliability considering it will be used in a production environment\\nPeewee. Despite SQLAlchemy being the gold standard of ORMs, there are no long-term volunteers who have mastered that library aside from David C. The difficulty of ramping up everyone on SQLAlchemy and maintaining it outweighs the difficulty of having potential volunteers who are familiar with SQLAlchemy pick up one of the \"easier\" ORMs. Comparing the release notes of Peewee and PonyORM, David C. commented that Peewee's recent releases relate to support of edge cases and new technology stacks, whereas PonyORM releases still appear to involve developing functionality and fixing bugs around core uses cases. We therefore concluded that Peewee is the more production-ready solution.\\n","tokens":96,"id":3053,"Prediction":"## Decision\n\nWe have decided to migrate our database migration and management tool from Phinx (PHP) to Alembic (Python). Alembic is a lightweight database migration tool for use with SQLAlchemy, which is already a widely-adopted ORM (Object Relational Mapper) in the Python ecosystem.\n\n### Rationale\n1. **Language Consistency**: Since our new codebase is in Python, using Alembic allows us to keep our entire stack language-consistent, which simplifies the development environment and reduces the cognitive load for developers who only need to be proficient in one programming language.\n   \n2. **Integration with SQLAlchemy**: Alembic integrates seamlessly with SQLAlchemy, one of the most popular ORMs in the Python ecosystem. This allows us to leverage SQLAlchemy\u2019s powerful features for database interactions while managing schema changes with Alembic.\n   \n3. **Community Support and Documentation**: Alembic has a robust community and thorough documentation, making troubleshooting and the ramp-up process easier for new developers. This is a significant advantage over maintaining migration scripts in PHP when the main application code is in Python.\n\n4. **Flexibility and Control**: Alembic provides the flexibility to write raw SQL as well as use SQLAlchemy models within migration scripts. This allows us to use the best tool for the task at hand, whether it is complex schema migrations or simple data manipulations.\n\n### Steps for Migration\n1. **Set up Alembic**: Integrate Alembic with the current project structure and configure it to connect to the existing database.\n   \n2. **Convert Existing Migrations**: Systematically translate the current Phinx migrations into Alembic migration scripts. Care will be taken to ensure the order and integrity of the migrations remain intact.\n   \n3. **Run Tests**: Implement thorough tests to ensure that the migrations run correctly and that the new schema is compatible with the current dataset.\n\n4. **Documentation**: Document the new migration process using Alembic, including how to create new migrations, rollback changes, and other day-to-day operations for developers.\n\n5. **Training & Knowledge Transfer**: Provide training sessions and documentation to help the development team get up to speed with Alembic and the new migration process.\n\nThis decision aligns with our long-term goal of creating a scalable, maintainable codebase while leveraging modern, community-supported tools in the Python ecosystem.","GenTime":"2024-07-29 00:27:29"}
{"File Name":"otm-docs\/api_framework.md","Context":"## Context\\nUsing pure PHP is kinda an uphill battle. Frameworks deliver ready-made solutions. Choosing proper framework ensures smooth workflow.\\n\n## Decision\n","Decision":"Laravel 7\\n","tokens":27,"id":3062,"Prediction":"## Decision\n\nWe have decided to adopt the Laravel framework for our project.\n\n### Rationale:\n\n1. **Maturity and Community Support**: Laravel has a large and active community, offering a wealth of tutorials, forums, and third-party packages that facilitate solving common challenges.\n  \n2. **Comprehensive Documentation**: Laravel is well-documented, making it easier for developers to learn and implement new features.\n\n3. **Built-in Features**: Laravel provides a robust set of built-in features such as Eloquent ORM, Blade Templating Engine, and a powerful queue system that simplifies many common tasks.\n\n4. **Modular Architecture**: Laravel's modular approach encourages the creation of scalable and maintainable code. It can also easily integrate with other libraries and tools.\n\n5. **Security**: The framework includes features to handle authentication, authorization, and protection against common security vulnerabilities like SQL injection and CSRF attacks.\n\n6. **Performance and Optimization Tools**: Laravel comes with built-in caching mechanisms and supports a variety of caching backends which can significantly improve the performance of the application.\n\nBy choosing Laravel, we aim to leverage these advantages to streamline our development process, reduce the complexity of handling low-level functionality, and ultimately deliver a more robust and maintainable application.","GenTime":"2024-07-29 00:28:04"}
{"File Name":"terraform-aws-s3-static-website-module\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3068,"Prediction":"## Decision\n\nWe will implement an Architectural Decision Record (ADR) process to document all key architectural decisions made during the project. This process will follow the format championed by Michael Nygard, which emphasizes the context, decision, and consequences of each architectural choice. Each ADR will be created and stored in a dedicated `docs\/adr` directory within the project's repository.\n\n### Process and Implementation\n- **ADR Template:** We will use a standardized template for all ADRs, which includes sections for Title, Date, Context, Decision, Status, and Consequences.\n  \n- **Tooling:** Adopt an ADR management tool or a markdown editor to facilitate the creation and maintenance of ADRs. We can use existing tools like \"adr-tools\" for this purpose.\n  \n- **Review and Approval:** ADRs will be reviewed and approved during regular architectural review meetings to ensure alignment with project goals and stakeholder needs.\n\n- **Version Control:** All ADRs will be version-controlled using GitHub, ensuring that changes and updates are tracked over time.\n\n- **Visibility and Communication:** The ADRs will be made accessible to all team members and stakeholders, facilitating informed decision-making and enhancing transparency across the project lifecycle.\n\n- **Backlog and Prioritization:** A backlog of potential ADR topics will be maintained to track future architectural decisions that need to be addressed.\n\nBy implementing this ADR process, we aim to create a clear, comprehensive, and accessible record of architectural decisions, thereby improving project documentation, reducing knowledge silos, and supporting consistent decision-making practices.","GenTime":"2024-07-29 00:28:21"}
{"File Name":"moneycount-api\/003-Document_API_With_Swagger.md","Context":"## Context\\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\\n\n## Decision\n","Decision":"I decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n","tokens":67,"id":3069,"Prediction":"## Decision\n\nWe will use OpenAPI (formerly known as Swagger) to document our API. OpenAPI provides a standardized way to describe RESTful APIs, allowing for both machine-readable and human-readable documentation. By integrating OpenAPI specifications:\n\n1. **Standardization**: OpenAPI offers a common language for defining APIs, making it easier for other developers to understand and utilize our API offerings.\n2. **Interactive Documentation**: OpenAPI supports tools like Swagger UI or ReDoc, which generate interactive documentation. This lets users not only read about the API endpoints but also test them directly from the documentation interface.\n3. **Automation and Consistency**: By generating documentation directly from API code annotations and definitions, we reduce the risk of discrepancies between the API implementation and its documentation. Tools like Swagger Codegen and OpenAPI Generator can help automate the generation of client libraries and server stubs.\n4. **Maintenance**: With proper integration, updating the documentation becomes part of the development workflow, ensuring that the documentation is always in sync with the API codebase.\n\nOpenAPI will thus streamline the documentation process, making it more efficient, reducing manual effort, and providing a better experience for developers using and testing the API.","GenTime":"2024-07-29 00:28:23"}
{"File Name":"moneycount-api\/004-Use_Heroku_As_Deployment.md","Context":"## Context\\nIt's desirable to keep the project online for testing purposes, and it has to be an easy to use environment with no cost.\\n\n## Decision\n","Decision":"I decided to use Heroku, as it has an easy to use environment, CLI tool that makes it easy to deploy with just few commands and has a free plan. It also has a great integration with lots of platforms and recognizes Spring Boot out of the box.\\n","tokens":30,"id":3070,"Prediction":"## Decision\nWe will use GitHub Pages to host the project for testing purposes. GitHub Pages offers a free and user-friendly environment to deploy and test static websites and applications. This aligns with our goal of maintaining zero cost while ensuring the platform is easy to use for any involved stakeholders. Additionally, leveraging GitHub Pages allows for seamless integration with our existing version control system, facilitating continuous deployment and testing without additional overhead.","GenTime":"2024-07-29 00:28:25"}
{"File Name":"moneycount-api\/002-Choose_Persist_Currency_In_Json.md","Context":"## Context\\nI have to decide how to persist data related to supported currencies in the project. Choices are a relational database or text files such as txt, xml or json. Even NoSQL databases could be used.\\n\n## Decision\n","Decision":"I decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.\\n","tokens":44,"id":3071,"Prediction":"## Decision\n\n### Chosen Solution: Relational Database (e.g., MySQL, PostgreSQL, etc.)\n\n### Rationale:\n\n1. **Structured Data**: As currency data typically has a fixed schema (e.g., currency code, name, exchange rate), a relational database suits well with its structured nature.\n\n2. **Consistency**: Relational databases support ACID (Atomicity, Consistency, Isolation, Durability) properties, which ensures data consistency and reliability, crucial for financial data.\n\n3. **Ease of Querying**: SQL provides powerful querying capabilities, which will enable efficient data retrieval and complex queries that might be needed to process or report on currency data.\n\n4. **Data Relationships**: If the currency data has relationships with other entities (e.g., users, transactions), a relational database can effectively manage these relationships through foreign keys and joins.\n\n5. **Scalability and Performance**: Modern relational databases are highly optimized and can scale horizontally (via sharding) and vertically to handle large volumes of data and high transaction loads.\n\n6. **Maturity and Support**: Relational databases are mature technologies with extensive community and commercial support, providing a stable foundation for persistence.\n\n7. **Security**: Relational databases offer robust security features, which are paramount when handling financial data.\n\n### Considerations:\n- **Maintenance**: Relational databases require maintenance tasks such as schema migrations, indexing, and backup management.\n- **Flexibility**: While more rigid than NoSQL databases, the fixed schema can ensure data integrity and prevent inconsistencies.\n\n### Conclusion:\nGiven the structured nature of currency data, the need for consistency, ease of querying, and robust security, a relational database is the most appropriate choice for persisting currency-related data in this project.","GenTime":"2024-07-29 00:28:30"}
{"File Name":"mymove\/0061-use-opentelemetry-for-distributed-tracing.md","Context":"## Problem Statement\\n### Why Distributed Tracing?\\n> Observability is defined as the ability of the internal states of a system to\\nbe determined by its external outputs.\\nThere are several established techniques for gaining observability into complex\\nsoftware systems.\\n* Logging\\n* helpful for known knowns\\n* e.g. \"I want to know when a specific condition is reached\"\\n* Metrics\\n* helpful for assessing known unknowns\\n* e.g. \"How many requests per second was the system handling last Tuesday?\"\\n* Distributed Tracing\\n* helpful for learning about unknown unknowns\\n* e.g. \"What was the execution context for User X that caused their\\ninteraction to timeout last Tuesday?\"\\nSome of the benefits of distributed tracing, as outlined in\\n[this](https:\/\/petabridge.com\/blog\/why-use-distributed-tracing\/) article are:\\n* radically improves developer productivity and output\\n* works across multiple applications, programming languages, and transports\\n* improve time to market\\n* facilitates excellent cross-team communication and cooperation\\nHere are several example scenarios or questions that distributed tracing can\\nhelp answer.\\n* As a new engineer on the team, I want to understand how many separate systems\\nare involved when a certain user type logs in and the first page is rendered.\\n* As an operations engineer, I want to know how many SQL queries are executed\\nfor a given endpoint or interaction.\\n* As a product manager, I want to know if a new feature is being used by a\\ncertain cohort of users on a regular basis.\\n* As an engineer, I want to prove that an optimization I wrote is effective\\nin a production environment.\\n* As a load tester, after I have shown that a problem exists, I want to\\nunderstand how the system is interacting so I can debug and fix the issue.\\n### ADR Goals and Anti-goals\\n* Goal: Choose which set of libraries to use at callsites (across programming\\nlanguages) within the MilMove codebase, which will be used to generate\\ndistributed tracing data\\n* Anti-goal: Committing to a specific \"backend\", i.e. platform or service for\\ngathering, exploring, and displaying trace information\\n* Sub-goal: Leaving open as many options as possible for the backend\\n\n## Decision\n","Decision":"* Chosen Alternative: _Use OpenTelemetry_\\n* OpenTelemetry is an emerging industry standard\\n* vendors find benefit of being in the OpenTelemetry ecosystem because they\\nno longer have to create or support instrumentation libraries in an ever\\ngrowing array of languages, i.e. as soon as language library exists for\\nOpenTelemetry, the vendors automatically become available to support that\\ngiven language.\\n* OpenTelemetry is vendor agnostic\\n* tracing information can be sent to hosted services (e.g. Honeycomb.io, AWS\\nX-Ray, etc) or self-hosted Open Source implementations (e.g. Zipkin, Jaeger,\\netc)\\n* if left unconfigured, OpenTelemetry instrumentation calls default to\\nlightweight\/noop executions\\n* OpenTelemetry has well-maintained libraries for the languages used in the\\nlayers of the MilMove project\\n* i.e. Go (back-end); JavaScript (front-end); Python (load testing); etc\\n* Easily swappable back-ends\\n* e.g. could choose a local Docker version of OpenZipkin for an all-local\\ndevelopment environment\\n* e.g. can use Honeycomb.io in the experimental commercial-cloud hosted\\nenvironment\\n* e.g. can swap in AWS X-Ray for use in GovCloud hosted environments\\n* Cons\\n* as an abstraction layer, OpenTelemetry may prohibit usage of vendor-\\nspecific capabilities\\n* some OpenTelemetry libraries and tools may trail their vendor-supported\\ncounterparts\\n* instrumentation for tracing may be a vector for performance overhead\\n","tokens":475,"id":3081,"Prediction":"## Decision\n\n### Adopt OpenTelemetry for Distributed Tracing\n\n**Decision**: We will use OpenTelemetry as the set of libraries for generating distributed tracing data within the MilMove codebase across different programming languages.\n\n### Rationale\n\n1. **OpenTelemetry's Robustness**:\n    - **Cross-language Support**: OpenTelemetry libraries are available for a wide array of programming languages, which ensures consistency and compatibility across the various components of our system.\n    - **Community Support and Adoption**: It is a highly adopted tool with strong community and vendor support, making it a future-proof choice.\n\n2. **Goal Alignment**:\n    - **Generation of Tracing Data**: OpenTelemetry is specifically designed to facilitate the generation of distributed tracing data, directly addressing the ADR goal of choosing appropriate libraries for callsites.\n    - **Flexibility for Backends**: OpenTelemetry enables us to instrument our code today without locking into any specific backend. It supports multiple backends and can be easily reconfigured or extended to different backends in the future, which aligns well with the anti-goal of not committing to a specific backend at this stage.\n\n3. **Ease of Integration**:\n    - **Standardization**: By complying with the OpenTelemetry standards, we ensure that the tracing setup follows widely-accepted conventions, making it easier for new engineers to understand and contribute.\n    - **Interoperability**: OpenTelemetry integrates well with existing metrics and logging systems, ensuring comprehensive observability without having to overhaul current setups.\n\n4. **Improved Observability**:\n    - **Scope of Insights**: OpenTelemetry\u2019s tracing can help answer critical questions from various stakeholders (e.g., engineers, operations, and product managers), as outlined in the problem statement.\n    - **Efficiency and Performance**: The tool\u2019s lightweight nature ensures that our system\u2019s performance is minimally impacted while gaining deep insights into its operations.\n\n### Next Steps\n\n1. **Implementation Plan**:\n    - Begin with a pilot implementation in a controlled environment to ensure that OpenTelemetry's introduction does not adversely impact system performance.\n    - Gradually roll out OpenTelemetry to different parts of the MilMove codebase, starting with the most critical paths.\n    - Provide team-wide training on using OpenTelemetry for generating and interpreting tracing data.\n\n2. **Review and Feedback**:\n    - Collect feedback from various stakeholders, particularly developers and operations engineers, to continually refine the usage of distributed tracing within the system.\n    - Regularly review the effectiveness of the tracing setup and make necessary adjustments to ensure it meets evolving needs.\n\n3. **Monitoring and Metrics**:\n    - Implement monitoring to measure the performance impact and benefits derived from distributed tracing, ensuring that the system remains performant and the tracing data is actionable.\n\nBy adopting OpenTelemetry, we ensure a robust, flexible, and future-friendly approach to distributed tracing that aligns with our goals of gaining deeper observability into the MilMove codebase.","GenTime":"2024-07-29 00:29:03"}
{"File Name":"atlasdb\/0007-use-cql-for-column-paging-for-sweep.md","Context":"## Context\\nAs of version 0.12.0, our implementation of sweep for Cassandra relied upon the getPageWithRangeCreator method of\\nCassandraKeyValueService, which fetches values for all columns and timestamps from Cassandra. In cases where a cell with\\na large value was overwritten (with other large values) many times, this caused Cassandra to run out of memory, even\\nif only a single row was fetched. (internal issue 44272).\\nWe needed a way to run sweep without Cassandra running out of memory in such cases.\\n\n## Decision\n","Decision":"We are unaware of any way to fetch columns and timestamps without also temporarily loading values into memory (within\\nCassandra). Therefore, to avoid running out of memory, we needed to make it possible to fetch only a certain number of\\ncells (rather than rows) from Cassandra at once.\\nWe decided to introduce a more granular batching solution. In particular, we decided to page through the\\ncolumns for each row. Briefly, the algorithm implemented is as follows:\\n1. Fetch a number of rows equal to the row batch size (but only one column per row).\\n2. Use CQL queries with limits to collect the <column, timestamp> pairs.\\nFurther, since performing a number of additional CQL queries for every row will be less efficient, and unnecessary for\\nmost users, we decided to add an optional parameter, timestampsGetterBatchSize, in CassandraKVSConfig, and use the method\\ndescribed above only if this parameter is set.\\nWe chose CQL queries over thrift because CQL queries are simpler to write, and have been empirically shown to have\\nhigher throughput.\\n","tokens":112,"id":3090,"Prediction":"## Decision\n\nTo address the memory issue when running sweep operations in Cassandra involving cells with large values overwritten multiple times, we decided to switch from using the `getPageWithRangeCreator` method to a more efficient approach that limits the data fetched per query. Our revised solution involves fetching data in smaller, more manageable chunks with a controlled cap on memory usage.\n\nHere are the key elements of this decision:\n\n1. **Modify Data Fetching Method**:\n   - Replace the `getPageWithRangeCreator` method with a custom method that fetches cells in smaller increments to avoid loading an entire row into memory at once. This incremental fetching ensures that only the necessary portions of the data are kept in memory at any time.\n\n2. **Implement Pagination**:\n   - Utilize pagination to split the data retrieval process into smaller, fixed-size pages. This will prevent large blocks of data from being pulled into memory simultaneously, thus mitigating the risk of running out of memory.\n\n3. **Introduce Configurable Page Size**:\n   - Allow the page size to be configurable based on the specific needs of the deployment environment. This gives the flexibility to adjust the fetching size to balance between performance and memory usage.\n\n4. **Optimize Sweep Logic**:\n   - Refine the sweep logic to process these smaller data chunks efficiently. Ensure the sweeping process merges results from multiple pages seamlessly and commits them as necessary.\n\n5. **Monitor and Test**:\n   - Deploy the new implementation in a staging environment to continually monitor its impact on memory usage. Perform extensive testing under various scenarios to confirm that the new approach effectively prevents memory overflows without impacting Cassandra's performance adversely.\n\nBy adopting this adjusted approach, we aim to ensure the sweep operation can be successfully performed without causing memory issues, even in cases where rows contain multiple large values and numerous overwrites.\n\n","GenTime":"2024-07-29 00:29:29"}
{"File Name":"atlasdb\/0006-create-schema-lock-table-using-configuration.md","Context":"## Context\\nDue to [table creation issue](0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md) we need to be able to safely create _lock table as a one off operation for each keyspace that atlas uses. The discussed options include:\\n- Have Atlas clients refuse to start if the _locks table is missing and provide a CLI to create it.\\n- This does require manual interview for whoever is handling operations\\n- Is very, very hard to get wrong\\n- Should be easily automatable in most setups, removing the manual step\\n- Use the lock service for locking rather than the _locks table, then we don't have to create it.\\n- Completely automatic and removes code\\n- Leaves open the possibility of locks failing and operations never realising that the issue has been triggered\\n- Have each node create a unique table, then run paxos to decide which one is the winner\\n- This requires a bunch of extra, error prone code\\n- Create an additional entry in the configuration, lockLeader, to denote which host is responsible for creating the locks table.\\n- The host whose name is the same as lockLeader will create the lock table, others will wait until the lockLeader is up.\\n- Requires all hosts to have the same configuration for lock leader\\n\n## Decision\n","Decision":"We decided to use an extra item of configuration, because:\\n1. Internal tools enable us to be confident that different Atlas servers on the same cluster are configured consistently.\\n2. Running paxos to decide which table is the winner was more complex than anticipated.\\n","tokens":276,"id":3092,"Prediction":"## Decision\n\nWe have decided to implement a hybrid approach combining both the CLI-based and automatic creation processes to address the table creation issue in Cassandra. Here are the elements of the decision:\n\n1. **CLI for _locks Table Creation**:\n    - **CLI Tool Requirement**: Atlas clients will have a fail-fast mechanism that prevents them from starting if the _locks table is missing and provides a clear error message indicating that the _locks table must be created.\n    - **Automated Scripting**: Provide a CLI tool to manually create the _locks table. This CLI tool can also be embedded into automated scripts to remove the manual intervention steps where possible.\n    - **Benefit**: This approach is very safe and hard to get wrong, ensuring that environments are correctly configured before service operations can commence.\n\n2. **Automatic Creation with Lock Leader Configuration**:\n    - **lockLeader Configuration**: Introduce a configuration option `lockLeader` which designates a specific host responsible for creating the _locks table.\n    - **Host Responsibility**: The host specified by the `lockLeader` configuration will automatically attempt to create the _locks table at startup if it does not already exist.\n    - **Safety Check**: Other nodes will wait and check if the _locks table already exists, waiting for the `lockLeader` to complete its action, ensuring no race conditions.\n    - **Benefit**: This ensures a largely automated process, reducing the possibility of human error and allowing for the whole cluster to be aware of the designated lock leader without requiring disparate configurations.\n\n3. **Fall-back to Lock Service**:\n    - **Lock Service as Plan B**: In scenarios where the configuration for `lockLeader` is not feasible, we will default to using the lock service as mentioned. This provides a backup mechanism but will be clearly documented to highlight the risks associated with potential locks failing and operations not realizing that the issue has been triggered.\n\nBy combining the CLI-based manual approach with automated steps driven by a designated lock leader, we ensure high reliability and reduce operational overhead, all while maintaining a robust fail-safe mechanism.","GenTime":"2024-07-29 00:29:33"}
{"File Name":"atlasdb\/0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md","Context":"## Context\\nCassandra [has an issue](https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA-10699) which can cause data loss in the situation:\\n1. Node A and node B concurrently create table \"some.table\"\\n2. Both table creations succeed, creating tables with column family ids \"123\" and \"456\"\\n3. Cassandra picks \"123\" to be the correct table for \"some.table\"\\n4. Cassandra is restarted\\n5. After restart Cassandra gossips and decides that \"456\" is the correct table for \"some.table\"\\n6. All data that was written to \"123\" is now lost\\nTo fix this we must prevent tables from being created at the same time.\\n\n## Decision\n","Decision":"All schema mutations will globally synchronise via a check-and-set operation on a specific cell in a _lock table.\\n","tokens":151,"id":3094,"Prediction":"## Decision\n\nTo address the issue of concurrent table creation in Cassandra leading to data loss, we will implement a distributed mutual exclusion mechanism for table creation operations.\n\n### Solution:\n\n1. **Global Locking Mechanism:** Introduce a global lock mechanism to ensure that only one node can create a particular table at any given time. This lock can be implemented using a distributed lock service such as Apache ZooKeeper or using Cassandra's Lightweight Transactions (LWT).\n\n2. **Pre-Check Creation Requests:** Before a node attempts to create a table, it must check if the table creation is already in process or has been completed by another node. This will be done by querying a centralized lock or metadata table.\n\n3. **Lock Registration:**\n   - When a node wants to create a new table, it first registers its intention by acquiring the global lock.\n   - If the lock is acquired successfully, the node proceeds with table creation.\n   - If the lock is not available, the node will wait or retry after a brief interval.\n\n4. **Commit and Release Lock:** Once the table creation is successfully completed, the node releases the lock, updating the centralized metadata to reflect the new table schema and its associated column family id.\n\n5. **Conflict Resolution:** In the rare event of failure during table creation while the lock is held, there needs to be a mechanism to release the lock and clean up partially created resources. For example, a time-based expiry for locks can be implemented to avoid deadlocks.\n\n6. **Schema Agreement:** Ensure that all nodes reach schema agreement using Cassandra\u2019s existing schema propagation mechanisms. After a table is created, all other nodes must synchronize their schema information to reflect the new table structure consistently.\n\n### Implementation Steps:\n\n- **Step 1:** Introduce a distributed lock service (e.g., ZooKeeper or C* LWT).\n- **Step 2:** Modify the table creation workflow to acquire a global lock before proceeding.\n- **Step 3:** Implement conflict resolution and cleanup procedures.\n- **Step 4:** Test the locking mechanism extensively to ensure no performance degradation or deadlocks.\n- **Step 5:** Roll out the changes in a phased manner to ensure stability and backward compatibility.\n\n### Benefits:\n- This decision prevents concurrent table creation, eliminating the risk of data loss due to schema conflicts.\n- Provides a robust way to handle schema changes in a distributed environment.\n\n### Risk:\n- Introducing global locks may impact the performance of schema operations.\n- Potential for deadlocks or stale locks if the conflict resolution is not robust.\n\nBy implementing distributed mutual exclusion for table creation, we can significantly reduce the risk of data loss in Cassandra due to concurrent schema operations.","GenTime":"2024-07-29 00:29:42"}
{"File Name":"atlasdb\/0015-batch-asynchronous-post-transaction-unlock-calls.md","Context":"## Context\\nAs part of the AtlasDB transaction protocol, write transactions acquire locks from the lock service. They typically\\nacquire two types of locks:\\n- An *immutable timestamp lock*, which AtlasDB uses as an estimate of the oldest running write transaction. The\\nstate of the database at timestamps less than the lowest active immutable timestamp lock is considered immutable, and\\nthus eligible for cleanup by Sweep.\\n- *Row locks* and *cell locks* (depending on the conflict handler of the tables involved in a write transaction) for\\nrows or cells being written to. These locks are used to prevent multiple concurrent transactions from simultaneously\\nwriting to the same rows and committing.\\nTransactions may also acquire additional locks as part of AtlasDB's pre-commit condition framework. These conditions\\nare arbitrary and we thus do not focus on optimising these.\\nAfter a transaction commits, it needs to release the locks it acquired as part of the transaction protocol. Releasing\\nthe immutable timestamp lock helps AtlasDB keep as few stale versions of data around as possible (which factors into\\nthe performance of certain read query patterns); releasing row and cell locks allows other transactions that need to\\nupdate these to proceed.\\nCurrently, these locks are released synchronously and separately after a transaction commits. Thus, there is an\\noverhead of two lock service calls between a transaction successfully committing and control being returned to\\nthe user.\\nCorrectness of the transaction protocol is not compromised even if these locks are not released (though an effort\\nshould be made to release them for performance reasons). Consider that it is permissible for an AtlasDB client to\\ncrash after performing `putUnlessExists` into the transactions table, in which case the transaction is considered\\ncommitted.\\n\n## Decision\n","Decision":"Instead of releasing the locks synchronously, release them asynchronously so that control is returned to the user very\\nquickly after transaction commit. However, maintaining relatively low latency between transaction commit and unlock\\nis important to avoid unnecessarily blocking other writers or sweep.\\nTwo main designs were considered:\\n1. Maintain a thread pool of `N` consumer threads and a work queue of tokens to be unlocked. Transactions that commit\\nplace their lock tokens on this queue; consumers pull tokens off the queue and make unlock requests to the lock\\nservice.\\n2. Maintain a concurrent set of tokens that need to be unlocked; transactions that commit place their lock tokens\\nin this set, and an executor asynchronously unlocks these tokens.\\nSolution 1 is simpler than solution 2 in terms of implementation. However, we opted for solution 2 for various reasons.\\nFirstly, the latency provided by solution 1 is very sensitive to choosing `N` well - choosing too small `N` means that\\nthere will be a noticeable gap between transaction commit and the relevant locks being unlocked. Conversely, choosing\\ntoo large `N` incurs unnecessary overhead. Choosing a value of `N` in general is difficult and would likely require\\ntuning depending on individual deployment and product read and write patterns, which is unscalable.\\nSolution 2 also decreases the load placed on the lock service, as fewer unlock requests need to be made.\\nIn our implementation of solution 2, we use a single-threaded executor. This means that on average the additional\\nlatency we incur is about 0.5 RPCs on the lock service (assuming that that makes up a majority of time spent in\\nunlocking tokens - it is the only network call involved).\\n### tryUnlock() API\\n`TimelockService` now exposes a `tryUnlock()` API, which functions much like a regular `unlock()` except that the user\\ndoes not need to wait for the operation to complete. This API is only exposed in Java (not over HTTP).\\nThis is implemented as a new default method on the `TimelockService` that delegates to `unlock()`; usefully, remote\\nFeign proxies calling `tryUnlock()` will make an RPC for standard `unlock()`. This also gives us backwards\\ncompatiblity; a new AtlasDB\/TimeLock client can talk to an old TimeLock server that has no knowledge of this endpoint.\\n### Concurrency Model\\nIt is essential that adding an element to the set of outstanding tokens is efficient; yet, we also need to ensure that\\nno token is left behind (at least indefinitely). We thus guard the concurrent set by a (Java) lock that permits both\\nexclusive and shared modes of access.\\nTransactions that enqueue lock tokens to be unlocked perform the following steps:\\n1. Acquire the set lock in shared mode.\\n2. Read a reference to the set of tokens to be unlocked.\\n3. Add lock tokens to the set of tokens to be unlocked.\\n4. Release the set lock.\\n5. If no task is scheduled, then schedule a task by setting a 'task scheduled' boolean flag.\\nThis uses compare-and-set, so only one task will be scheduled while no task is running.\\nFor this to be safe, the set used must be a concurrent set.\\nThe task that unlocks tokens in the set performs the following steps:\\n1. Un-set the task scheduled flag.\\n2. Acquire the set lock in exclusive mode.\\n3. Read a reference to the set of tokens to be unlocked.\\n4. Write the set reference to point to a new set.\\n5. Release the set lock.\\n6. Unlock all tokens in the set read in step 3.\\nThis model is trivially _safe_, in that no token that wasn't enqueued can ever be unlocked, since all tokens that can\\never become unlocked must have been added in step 3 of enqueueing, and unlocking a lock token is idempotent modulo\\na UUID clash.\\nMore interestingly, we can guarantee _liveness_ - every token that was enqueued will be unlocked in the absence of\\nthread death. If an enqueue has a successful compare-and-set in step 5, then the token must be in the set\\n(and is visible, because we synchronize on the set lock). If an enqueue does _not_ have a successful compare-and-set,\\nthen some thread must already be scheduled to perform the unlock, and once it does the token must be in the relevant\\nset (and again must be visible, because we synchronize on the set lock).\\nTo avoid issues with starving unlocks, we use a fair lock scheme. Once the unlocking thread attempts to acquire the set\\nlock, enqueues that are still running may finish, but fresh calls to enqueue will only be able to acquire the set lock\\nafter the unlocking thread has acquired and released it. This may have lower throughput than an unfair lock,\\nbut we deemed it necessary as 'readers' (committing transactions) far exceed 'writers' (the unlocking thread) -\\notherwise, the unlocking thread might be starved of the lock.\\n### TimeLock Failures\\nIn some embodiments, the lock service is provided by a remote TimeLock server that may fail requests. There is retry\\nlogic at the transport layer underneath us.\\nPreviously, running a transaction task would throw an exception if unlocking row locks or the immutable timestamp\\nfailed; we now allow user code to proceed and only emit diagnostic logs indicating that the unlock operation failed.\\nThis is a safe change, as throwing would not make the locks become available again, and user code cannot safely\\nassume that locks used by a transaction are free after it commits (since another thread may well have acquired them).\\nIn practice, locks will be released after a timeout if they are not refreshed by a client. This means that not\\nretrying unlocks is safe, as long as we do not continue to attempt to refresh the lock. AtlasDB clients automatically\\nrefresh locks they acquire; we ensure that a token being unlocked is synchronously removed from the set of locks\\nto refresh *before* it is put on the unlock queue.\\n","tokens":355,"id":3096,"Prediction":"## Decision\nWe will implement an asynchronous lock release mechanism for the locks acquired during the AtlasDB transaction protocol. This means that instead of releasing the immutable timestamp lock and the row\/cell locks synchronously after a transaction commits, we will dispatch these release operations asynchronously.\n\nThe main steps involved in this decision are as follows:\n\n1. **Asynchronous Lock Release Implementation:** \n    - Modify the lock release calls to be executed asynchronously. This can be accomplished by leveraging a thread pool or an asynchronous task execution framework.\n    - Ensure that the asynchronous tasks for lock releases are idempotent and retryable, so that if there is a failure in releasing a lock, the operation can be retried without adverse effects.\n\n2. **Monitoring and Alerting:**\n    - Implement monitoring mechanisms to track the state of lock releases. This monitoring should help detect if locks are not being released timely, which might impact performance and highlight potential issues.\n    - Set up alerting systems that notify the development or operations team if the buildup of unreleased locks exceeds a defined threshold, indicating potential problems that require immediate attention.\n\n3. **Performance Considerations:**\n    - Assess the performance impact of the asynchronous lock release mechanism to ensure that it results in overall gains in transaction throughput and reduces latency for the end-user.\n    - Conduct load testing to ensure that the asynchronous lock release mechanism scales effectively with the number of concurrent transactions.\n\n4. **Robustness and Fault Tolerance:**\n    - Ensure that the asynchronous task execution framework is robust against failures, including scenarios where AtlasDB clients might crash or network issues might prevent immediate lock release.\n    - Design a fallback mechanism where, in case of persistent issues in releasing locks asynchronously, a periodic background process is used to clean up and release any lingering locks to maintain system health.\n\nBy adopting asynchronous lock releases, we aim to reduce the overhead associated with synchronous lock release operations post-transaction, thereby optimizing the overall transaction performance. Moreover, we maintain robust monitoring and fault tolerance measures to ensure the correctness and efficiency of the AtlasDB transaction protocol.","GenTime":"2024-07-29 00:29:51"}
{"File Name":"atlasdb\/0009-load-and-read-streams-in-same-transaction.md","Context":"## Context\\nReading streams from within the same transaction that calls loadStream() must\\nbe supported. There are a number of benefits to doing this rather than reading\\nstreams outside the transaction.\\n1. If the transaction that calls loadStream() is run with retry, you risk\\nleaking resources because multiple streams may be created that never get\\nreturned to the caller (so the caller has no opportunity to close them).\\nThe stream can not only be read (entirely) inside the transaction, but\\nalso closed inside the transaction. This makes it safe to run with retries\\n(since the retries can also close the stream they load).\\n2. When the actual stream data is read outside of the transaction, that read\\ncan fail (for example if the stream was then deleted). If instead the read is\\nperformed inside the transaction, any failed reads would also cause the\\ntransaction to fail, which could be desired. For example, if as a result of\\nreading the stream you want to perform some writes based on the read data, it\\nmakes sense to perform all of that in a single transaction.\\n\n## Decision\n","Decision":"Reading from a stream inside the same transaction that calls loadStream()\\nshould be the preferred method unless you have a specific reason that the reads\\nmust be performed later outside the transaction. Reading from a stream should\\nbe supported both inside the same transaction where it was loaded and outside\\n(after the transaction commits or aborts).\\n","tokens":229,"id":3099,"Prediction":"## Decision\nWe will support reading streams from within the same transaction that calls `loadStream()`. This means that:\n\n1. **Stream Lifecycle Management**: Streams will now be fully managed within the transaction, including their creation, reading, and closure. This ensures that multiple stream instances do not leak and that they are properly closed if retries occur.\n\n2. **Transactional Integrity**: By reading the stream data within the transaction, we ensure that any read failures also fail the transaction, preserving integrity. This allows the calling process to make decisions based on the read data within a single transactional context.\n\n### Implementation Plan\n\n- **API Changes**: Modify the `loadStream()` function to ensure it can return streams that are safely readable within the transaction scope. \n- **Retry Logic**: Enhance the transaction retry logic to handle stream resources properly, ensuring they are closed correctly in case of failures and retries.\n- **Error Handling**: Implement error handling to ensure transactional failures occur if stream read operations fail, triggering any necessary rollback mechanisms.\n\n### Benefits\n\n- **Resource Management**: Prevents resource leaks by ensuring streams are closed within the transaction.\n- **Consistency**: Maintains consistency by ensuring all operations (read and subsequent writes) happen within the same transactional scope.\n- **Robustness**: Increases robustness by tying the lifecycle of the streams to the transaction, allowing for safer retry mechanisms.\n\nThis decision ensures reliable and consistent handling of stream data within transaction boundaries, maximizing resource management and transaction integrity.","GenTime":"2024-07-29 00:29:59"}
{"File Name":"compliantkubernetes\/0008-use-hostnetwork-or-loadbalancer-for-ingress.md","Context":"## Context and Problem Statement\\nMany regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an [Ingress controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) and [cert-manager](https:\/\/github.com\/jetstack\/cert-manager). As of February 2021, Compliant Kubernetes comes by default with [nginx-ingress](https:\/\/kubernetes.github.io\/ingress-nginx\/), but [Ambassador](https:\/\/www.getambassador.io\/docs\/latest\/topics\/running\/ingress-controller\/) is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?\\n## Decision Drivers\\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\n\n## Decision\n","Decision":"* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\nChosen options:\\n1. Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes:\\n* Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.\\n* Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.\\n* SafeSpring falls in this category, since it is missing load balancers.\\n* If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.\\n2. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.\\nAdditional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until *after* we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:\\n```\\n*.$BASE_DOMAIN     60s A 203.0.113.123\\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\\n```\\n203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by [kops](https:\/\/github.com\/kubernetes\/kops\/blob\/d5d08a43034dd4c7242cf1faa020cf9a8c3965e2\/upup\/pkg\/fi\/cloudup\/dns.go#L41) and should not feel astonishing.\\n### Positive Consequences\\n* We make the best of each cloud provider.\\n* Obeys principle of least astonishment.\\n* We do not add a load balancer \"just because\".\\n### Negative Consequences\\n* Complexity is a bit increased, however, this feels like essential complexity.\\n","tokens":254,"id":3104,"Prediction":"## Decision\n\nTo address the problem of ensuring encrypted traffic and handling diverse deployment environments, we will implement the following decision:\n\n### Utilize NodePort Service for Ingress Controller\n\n**Reasoning:**\n\n1. **Principle of Least Astonishment**: Using a NodePort service is a standard Kubernetes approach familiar to many administrators and developers. This aligns with the Principle of Least Astonishment by using a concept that most Kubernetes users would expect.\n\n2. **Support for Hybrid Cloud and Bare-Metal Deployments**: By choosing a NodePort service, we can support hybrid cloud setups, including bare-metal environments, which might not have Kubernetes-controlled load balancers. This ensures a broader range of deployment scenarios are covered, increasing flexibility and adaptability.\n\n3. **Simpler Configuration**: NodePort services require less complex configuration compared to integrating external load balancers. This simplifies the setup and management of the Ingress controller, making it easier for administrators to deploy and maintain.\n\n4. **Compatibility with Ambassador**: NodePort services are compatible with both nginx-ingress and Ambassador, ensuring that our planned shift to Ambassador as an alternative ingress controller is feasible without changing the underlying mechanism.\n\n5. **Autonomy in BYO VM Deployments**: With NodePort, we avoid dependency on the cloud provider's load balancer and network configurations. This is particularly beneficial in Bring-Your-Own VM setups where such integrations can be restrictive or unavailable.\n\n### Implementation Steps:\n\n1. **Configure Ingress Controller Deployment**:\n    - Ensure that the Ingress controller (nginx-ingress or Ambassador) is deployed with NodePort services.\n    - Configure appropriate NodePort ranges as per Kubernetes best practices.\n\n2. **TLS Termination**:\n    - Leverage cert-manager to automatically handle TLS certificate generation and renewal.\n    - Configure the Ingress resources to use the certificates managed by cert-manager to encrypt traffic.\n\n3. **Monitoring and Maintenance**:\n    - Implement monitoring to ensure that the NodePort services are operating correctly.\n    - Regularly review and update configurations and certificates to maintain security and compliance.\n\nBy adopting NodePort for traffic to arrive at the Ingress controller, we ensure a consistent, simple, and adaptable solution that meets various deployment needs while maintaining compliance through encrypted traffic over the public Internet.","GenTime":"2024-07-29 00:30:22"}
{"File Name":"compliantkubernetes\/0004-plan-for-usage-without-wrapper-scripts.md","Context":"## Context and Problem Statement\\nWe frequently write wrapper scripts. They bring the following value:\\n1. They bind together several tools and make them work together as a whole, e.g., `sops` and `kubectl`.\\n2. They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.\\n3. They enforce best practices, e.g., encrypt secrets consumed or produced by tools.\\nUnfortunately, wrapper scripts can also bring disadvantages:\\n1. They make usages that are deviating from the \"good way\" difficult.\\n2. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.\\n3. They add overhead when adding new features or supporting new use-cases.\\n4. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the [Law of Leaky Abstractions](https:\/\/www.joelonsoftware.com\/2002\/11\/11\/the-law-of-leaky-abstractions\/).\\n## Decision Drivers\\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\n\n## Decision\n","Decision":"* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.\\nThis decision applies for new wrapper scripts. We will not rework old wrapper scripts.\\n### Positive Consequences\\n* The operations team can encode standard operating procedures and scale ways of working.\\n* Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.\\n* Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.\\n### Negative Consequences\\n* There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.\\n","tokens":301,"id":3109,"Prediction":"## Decision\n\nWe will continue to use wrapper scripts but implement the following measures to mitigate their disadvantages:\n\n1. **Documentation and Transparency**:\n   - Maintain comprehensive and up-to-date documentation for all wrapper scripts. This includes a detailed explanation of what each script does, the tools it invokes, the parameters it accepts, and example use-cases.\n   - Ensure that the logic of the wrapper scripts is transparent and that it is easy to trace which underlying tools and commands are being executed. This will help lower the adoption barrier and make it easier for users accustomed to the underlying tools to follow the invocation flow.\n\n2. **Modularity and Extensibility**:\n   - Refactor wrapper scripts to be modular, allowing for individual components to be reused or replaced easily. This modularity will facilitate easier addition of new features or the support of new use-cases without requiring a complete overhaul of the script.\n   - Design the scripts with extensibility in mind, making it possible to introduce new functionality in a plug-and-play manner, minimizing disruption to the existing users.\n\n3. **Error Handling and Best Practices Enforcement**:\n   - Enhance error handling to provide meaningful error messages and suggestions for resolution. This will make operations more resilient to human error.\n   - Continue to enforce best practices around security, configuration management, and operational procedures through the wrapper scripts. Make sure that these enforcement checks are clearly documented and configurable where appropriate, to allow for flexibility while maintaining standards.\n\n4. **Training and Onboarding**:\n   - Develop training materials and onboarding sessions focused on getting newcomers up to speed with the wrapper scripts, the underlying tools, and why the scripts exist.\n   - Include practical examples and walkthroughs to help new users understand the value that the wrapper scripts provide and how to effectively use them in real-world scenarios.\n\n5. **Community Feedback and Continuous Improvement**:\n   - Actively seek feedback from the user community regarding the wrapper scripts. Use this feedback to continuously improve the scripts, making them more user-friendly and aligned with the needs of the users.\n   - Implement a regular review process to assess the usage and performance of the wrapper scripts, ensuring they remain relevant and efficient as the underlying tools and operational requirements evolve.\n\nThis approach will help us leverage the benefits of wrapper scripts while addressing the potential drawbacks, ultimately making operations more straightforward, predictable, and scalable while maintaining flexibility and agility.","GenTime":"2024-07-29 00:30:50"}
{"File Name":"compliantkubernetes\/0013-configure-alerts-in-omt.md","Context":"## Context and Problem Statement\\nAlerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full.\\nTerminology differs across tooling and organizations, but one generally cares about:\\n* P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;\\n* P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;\\n* P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.\\nOther priorities (e.g., P4 and below) are generally used for informational purposes.\\nDealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when.\\n\"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.\\nUnder-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\".\\nOver-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored.\\nHence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.\\nWhere should alerting be configured, so as to quickly converge to the optimal alerting level?\\n## Decision Drivers\\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\n\n## Decision\n","Decision":"* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\nChosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie).\\nConfiguration of alerts happens in the OMT.\\n### Positive Consequences\\n* Clear separation of concerns.\\n* Alerting does not require per-customer configuration of Compliant Kubernetes.\\n* Leverages existing tools and processes.\\n* We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.\\n### Negative Consequences\\n* Does not capture alerting know-how in Compliant Kubernetes.\\n* Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.\\n","tokens":445,"id":3110,"Prediction":"## Decision\n\nWe will use a centralized Alert Management System (AMS) to handle alert configuration, notifications, and escalation processes. This system will be integrated with our monitoring tools and will provide a flexible and intuitive interface for managing alerts.\n\n### Detailed Decisions:\n\n1. **Centralized Configuration**:\n    - All alert configurations will be managed centrally within the AMS. This includes defining alert priorities (P1, P2, P3), notification methods (SMS, email, etc.), and escalation workflows.\n    - The AMS will support silencing, un-silencing, and re-prioritizing alerts in real-time to quickly adapt to changing conditions.\n\n2. **Integration with Monitoring Tools**:\n    - The AMS will integrate with existing monitoring solutions to fetch alert data and manage notifications. This ensures minimal disruption to current workflows while leveraging existing monitoring investments.\n\n3. **Notification Flexibility**:\n    - The AMS will provide comprehensive options for configuring notification rules, including:\n        - Who should be notified based on alert priority and type.\n        - The order of notification channels (e.g., initial SMS followed by email if unacknowledged).\n        - Custom notification schedules to match the on-call rotation and working hours.\n        - Integration with communication tools (e.g., Slack, Teams) for real-time alerting.\n\n4. **Escalation Management**:\n    - The AMS will include robust escalation policies, allowing for the automatic escalation of alerts if the initial notification is not acknowledged within a specified time frame.\n    - Multiple levels of escalation will be supported, ensuring that critical alerts get the necessary attention even if the primary on-call person is unavailable.\n\n5. **User-Friendly Interface**:\n    - The AMS will have an intuitive and easy-to-use interface, enabling on-call teams to quickly adjust alert settings and respond to alerts.\n    - Detailed dashboards and reports will be available to monitor alerting effectiveness and fine-tune configurations.\n\n6. **Adherence to SLAs**:\n    - Configuration in the AMS will ensure compliance with SLA requirements by prioritizing and escalating alerts correctly and timely.\n    - It will also provide metrics and analytics to ensure that SLA violations are minimized and addressed promptly.\n\n### Justification\n\nThis centralized approach ensures a systematic and streamlined way of managing alerts, notifications, and escalations. By leveraging existing tools and integrating them into a central system, we reduce the complexity of managing alerts and ensure that critical events receive the appropriate level of attention. The flexibility and comprehensive configuration options of the AMS will help in maintaining the balance between over-alerting and under-alerting, optimizing the on-call team's responsiveness and well-being.","GenTime":"2024-07-29 00:30:55"}
{"File Name":"racing-tips\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3119,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) system to document and manage all significant architectural decisions made during the project. This system will use Markdown files stored in a version-controlled repository to ensure traceability and easy access.\n\n1. **Template Selection:** We will adopt the MADR (Markdown Architectural Decision Records) 2.0.0 template for consistency.\n2. **Repository Setup:** A dedicated directory, `decisions\/`, will be created in our main project repository to store ADR files.\n3. **File Naming Convention:** ADR files will be named using a sequential numbering system followed by a brief, descriptive title, e.g., `0001-record-arch-decisions.md`.\n4. **Review Process:** Each ADR will be subject to a peer review process, where at least two team members must approve the decision before it is finalized.\n5. **Retention Policy:** All ADRs will remain accessible for the duration of the project and for a minimum of one year after project completion to support maintenance and future enhancements.\n6. **Versioning:** We will use Git to manage changes to ADRs, ensuring a complete history of all architectural decisions.\n\nThis approach will help us maintain a clear record of why and how particular architectural decisions were made, providing valuable context for current and future team members.","GenTime":"2024-07-29 00:31:29"}
{"File Name":"publicservice-registry\/0004-sqlstreamstore.md","Context":"## Context\\nSince we decided to use event sourcing, we need a way to store events in our database.\\nIn `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save\/read events were hand-written.\\n**However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.\\nWhile `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.\\nWe currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.\\nAs an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\\n\n## Decision\n","Decision":"We will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.\\n","tokens":264,"id":3123,"Prediction":"## Decision\nWe will use `SqlStreamStore` to store events in our `MSSQL` database. This decision leverages our current database infrastructure and aligns better with our existing hosting capabilities and business-critical requirements.\n\n### Rationale\n1. **Infrastructure Compatibility**: We already use `MSSQL` for other business-critical data, eliminating the need for additional hosting and maintenance overhead that `GetEventStore` would require.\n2. **Catch-up Subscriptions**: `SqlStreamStore` supports catch-up subscriptions, which aligns with our requirement for async event handlers and allows event handlers to autonomously manage their interest in events and perform their own rebuilds.\n3. **Community and Support**: `SqlStreamStore` is an open-source library with a vibrant community, providing stability and support. It has been proven in several production systems, ensuring a level of maturity and reliability for our needs.\n4. **Ease of Debugging**: Continuing to use `MSSQL` allows us to maintain our current debugging practices and tools, ensuring a smoother transition and minimal learning curve for the team.\n\nBy opting for `SqlStreamStore`, we gain the benefits of catch-up subscriptions without introducing additional hosting complexities, thus maintaining our focus on delivering business value efficiently.","GenTime":"2024-07-29 00:31:42"}
{"File Name":"publicservice-registry\/0009-use-more-value-objects.md","Context":"## Context\\nA value object is a small object that represents a simple entity whose equality is not based on identity: i.e. two value objects are equal when they have the same value, not necessarily being the same object. Examples of value objects are objects representing an amount of money or a date range.\\nIn `Wegwijs`, we experienced great value towards type safety from using VOs. We want to bank in even more on the use of VOs.\\n\n## Decision\n","Decision":"Use a Value Object wherever possible.\\n","tokens":96,"id":3124,"Prediction":"## Decision\nContinue to extensively use Value Objects (VOs) throughout the `Wegwijs` application to enhance type safety and maintain consistency. All simple entities that can be represented as value objects will be converted to VOs, ensuring that equality is based on the value rather than object identity. This approach will improve encapsulation, readability, and maintainability of the codebase. \n\nAdditionally, we will implement a common framework or utilities for creating and managing VOs to ensure uniformity and ease of use across different parts of the application. This might also include common validation logic, immutability features, and integration with existing libraries or tools that support VOs.","GenTime":"2024-07-29 00:31:44"}
{"File Name":"publicservice-registry\/0012-use-dutch-resource-names.md","Context":"## Context\\nWhen exposing API endpoints, exposing a UI or exposing resource field names, we have to decide on the language used. Do we cater to the majority of the world by using English, or do we use Dutch because we are creating resources for the Flemish Government?\\n\n## Decision\n","Decision":"We will use Dutch terms because it is an external requirement dictated by the organisation.\\n","tokens":56,"id":3126,"Prediction":"## Decision\nWe will primarily use English for the API endpoints and UI resource field names. This decision is based on several factors:\n\n1. **Global Reach and Integration**: English is the de facto language for global tech communities, which will facilitate easier integration with other software systems, libraries, and services.\n  \n2. **Developer and User Community**: A substantial portion of the developer community and documentation is available in English, ensuring better support and adoption.\n\n3. **Future-Proofing**: By using English, we enable scalability and potential use beyond the Flemish Government, opening doors for broader applications should the need arise.\n\n4. **User Interface Localization**: While the core API and field names will be in English, we will implement front-end localization support, allowing end-users to experience the interface in Dutch or any other preferred language. This will ensure that local users, particularly those in the Flemish Government, can interact with the system in their native language without altering the underlying technology structure.\n\nThis approach balances global standards with local usability, ensuring that our resources remain accessible and user-friendly for the Flemish Government while maintaining compatibility with international best practices.","GenTime":"2024-07-29 00:31:52"}
{"File Name":"publicservice-registry\/0013-handle-eventual-consistency-with-etags.md","Context":"## Context\\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\\n\n## Decision\n","Decision":"We will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\\nAll Projections will update their ProjectionState with this number.\\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\\n","tokens":83,"id":3127,"Prediction":"## Decision\n\nTo ensure that the client can check whether or not its latest update has been processed by the projections, we will implement an acknowledgment mechanism combined with a projection status check.\n\n1. **Acknowledgment Mechanism:**\n   - When the UI sends a request to create or alter a resource, the server will immediately return an acknowledgment that contains a unique operation identifier (UUID).\n   \n2. **Projection Status Check Endpoint:**\n   - We will create a new endpoint (e.g., `\/projection-status`) where the client can query the processing status of the given operation (using the UUID returned in the acknowledgment).\n   - This endpoint will return the status of projections related to the specified operation. Possible statuses may include: \"Pending,\" \"Processing,\" or \"Completed.\"\n\n3. **Polling Mechanism in UI:**\n   - The UI will employ a polling mechanism that periodically sends requests to the `\/projection-status` endpoint using the provided UUID until it receives a \"Completed\" status.\n   - To improve efficiency and reduce server load, an exponential backoff strategy will be used for polling intervals.\n\n4. **Handling Stale Data:**\n   - Until the UI receives confirmation that the projections are up to date, it will display an appropriate loading indicator or a message informing the user that the latest data is being processed.\n   - Optionally, the UI could also display the last known state with a notification that updates are pending, to enhance user experience.\n\n5. **Error Handling:**\n   - If the projection status check returns an error or is taking longer than expected, suitable error messages and possible retry mechanisms will be implemented.\n\nImplementing these mechanisms will ensure that the user is informed about the status of their request and will see up-to-date data once the asynchronous projections have been processed, thereby improving the reliability and transparency of the application.","GenTime":"2024-07-29 00:31:56"}
{"File Name":"publicservice-registry\/0010-do-not-use-clr-type-names-for-event-types.md","Context":"## Context\\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\\n\n## Decision\n","Decision":"Use a dictionary\/map between your message types and the CLR type you want to deserialize your message into.\\n","tokens":57,"id":3129,"Prediction":"## Decision\n\nWe have decided to use well-defined string identifiers for event types instead of relying on CLR type names. This approach will ensure that our event store remains robust and maintainable over time, even if our .NET CLR types undergo changes such as namespace modifications or refactoring.\n\n### Justification\n\n- **Long-term Stability**: Event types identified by string identifiers will not be affected by changes in the implementation details of the event classes.\n- **Ease of Refactoring**: Moving or renaming CLR types can be done without breaking existing event data in the store.\n- **Interoperability**: Using string identifiers can facilitate integration with systems developed in different languages or environments.\n- **Versioning and Evolution**: String identifiers can make it simpler to version event types and manage evolving domain models.\n\n### Implementation Steps\n\n1. **Define Event Type Identifiers**: For each event, define a unique string identifier.\n2. **Update Codebase**: Replace instances where CLR type names are used as event identifiers with the new string identifiers.\n3. **Documentation**: Document the string identifiers corresponding to each event to ensure consistent usage across the team.\n4. **Testing**: Thoroughly test the event store to ensure that the change does not affect existing functionality and that event retrieval continues to work seamlessly.\n\nBy adopting this approach, we can ensure that our event store is resilient to codebase changes and remains compatible with organizational needs and technology evolutions.","GenTime":"2024-07-29 00:32:04"}
{"File Name":"pul_solr\/0001-document-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3131,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) system using Markdown files stored in a dedicated 'decisions' directory within the project repository. Each decision will be documented in a separate file, following the ADR template format, to ensure clarity and consistency. This approach allows for easy tracking, reviewing, and updating of all architectural decisions throughout the project's lifecycle.","GenTime":"2024-07-29 00:32:08"}
{"File Name":"modernisation-platform\/0009-use-secrets-manager-for-secrets.md","Context":"## Context\\nThe Modernisation Platform team and its users need a way to store secrets securely. There are several different methods currently used across the MoJ, including [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/), [Parameter Store](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html), [LastPass](https:\/\/www.lastpass.com\/) and [Git-Crypt](https:\/\/github.com\/AGWA\/git-crypt).\\nThere are also other well known industry solutions such as [HashiCorp Vault](https:\/\/www.vaultproject.io\/). We want to have a consistent solution across the Modernisation Platform.\\n\n## Decision\n","Decision":"We've decided to use [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) for our secrets storage.\\nParameter store can be used to store non secret parameters if needed for environment specific configuration, but the first choice should be using an app_variables.json like [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform-environments\/blob\/main\/terraform\/environments\/sprinkler\/app_variables.json)\\n","tokens":138,"id":3142,"Prediction":"## Decision\n\nThe Modernisation Platform will adopt **AWS Secrets Manager** as the primary solution for secure storage and management of secrets.\n\n### Rationale:\n\n1. **Security**: AWS Secrets Manager offers robust security features including automatic encryption of secrets using AWS KMS (Key Management Service) keys. It also supports fine-grained access control policies to ensure that only authorized users and services can access secrets.\n\n2. **Integration**: AWS Secrets Manager integrates seamlessly with other AWS services that the Modernisation Platform already utilizes, such as AWS Lambda, Amazon RDS, and Elastic Beanstalk. This allows for smoother workflows and less overhead in managing integrations.\n\n3. **Automation and Rotation**: AWS Secrets Manager supports automatic rotation of secrets, which can enhance security by regularly updating sensitive information without manual intervention. It supports Lambda functions to define and manage custom rotation logic.\n\n4. **Consistency**: Adopting a single solution will ensure consistency across the Modernisation Platform, reducing complexity and the risk of mismanaged secrets. This move will streamline the learning curve for team members, operations, and new hires.\n\n5. **Compliance**: AWS Secrets Manager is compliant with various industry standards and regulations, such as SOC, PCI DSS, and ISO\/IEC 27001. This will help the Modernisation Platform adhere to required compliance standards.\n\n6. **Cost-Effectiveness**: While there are costs associated with using AWS Secrets Manager, its features and benefits justify the expenditure. The cost is also predictable and scales according to usage, which aligns with budget planning.\n\n### Implementation Plan:\n\n1. **Assessment**: Conduct an assessment to inventory all current secrets and their storage mechanisms.\n   \n2. **Migration Plan**: Develop a detailed migration plan to transfer existing secrets from alternative storage solutions (Parameter Store, LastPass, Git-Crypt) to AWS Secrets Manager.\n   \n3. **Training**: Provide training sessions and documentation for the team to ensure they are well-versed in using AWS Secrets Manager.\n\n4. **Integration Updates**: Update integration points in the codebase and infrastructure scripts to use secrets from AWS Secrets Manager.\n\n5. **Monitoring and Auditing**: Set up monitoring and auditing for secrets usage, rotation, and access logs to detect and mitigate any security issues swiftly.\n\n6. **Decommissioning Legacy Solutions**: Once the migration is complete, decommission the legacy secrets management solutions to avoid unnecessary complexity and potential security risks.\n\nBy standardizing on AWS Secrets Manager, the Modernisation Platform team will enhance the security, manageability, and consistency of secrets storage across the entire ecosystem.","GenTime":"2024-07-29 00:32:53"}
{"File Name":"modernisation-platform\/0010-terraform-module-strategy.md","Context":"## Context\\nThe Modernisation Platform uses [Terraform](https:\/\/www.terraform.io\/) for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code you can use [Terraform Modules](https:\/\/www.terraform.io\/docs\/language\/modules\/). There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.\\n\n## Decision\n","Decision":"Modules used only by the Modernisation Platform core infrastructure will remain in the [terraform\/modules](https:\/\/github.com\/ministryofjustice\/modernisation-platform\/tree\/main\/terraform\/modules) folder where they are currently located. These modules are mainly single use modules but created to keep the code tidier and easier to maintain. Modules used only by the core which currently have their own repository will remain where they are.\\nModules used by users will have their own repository per module which we link to from the main repo. These modules will be versioned with GitHub releases, and tested with [Terratest](https:\/\/terratest.gruntwork.io\/) against a test AWS account.\\n","tokens":82,"id":3144,"Prediction":"## Decision\n\nWe will adopt a standardized approach to utilizing Terraform modules within the Modernisation Platform to ensure reusability, maintainability, and scalability of our infrastructure code. The key aspects of this decision are outlined below:\n\n1. **Core Module Repository:** We will establish a core repository for shared, reusable Terraform modules. This repository will serve as the single source of truth for common infrastructure components used across different projects within the platform. Adhering to a well-defined versioning scheme will allow teams to reference stable versions of these modules.\n\n2. **Documentation and Examples:** Each module in the core repository will be accompanied by detailed documentation and usage examples. This will include explanations of input variables, output values, and any dependencies. Clear documentation will aid developers in understanding and correctly implementing the modules.\n\n3. **Component-Specific Modules:** For project-specific needs, teams will create their own Terraform modules as submodules within their project repositories. However, if a project-specific module is deemed broadly useful, it will be refactored and contributed back to the core module repository.\n\n4. **Module Testing:** Before integration, all modules must be tested to ensure they function as expected. Automated testing frameworks like [Terratest](https:\/\/terratest.gruntwork.io\/) will be used to validate the correctness and reliability of modules.\n\n5. **Module Registry Utilization:** We will leverage Terraform's public and private module registries to discover and use high-quality, community-vetted modules wherever applicable. Our core repository will also be published in a private registry for easy consumption by internal teams.\n\n6. **Consistent Naming Conventions:** We will adopt and enforce consistent naming conventions across all modules to enhance clarity and discoverability. This includes a standardized format for module directory names, resource names, and variable names.\n\n7. **Security and Compliance:** All modules will undergo regular security reviews to ensure they comply with organizational and regulatory requirements. Sensitive data such as secrets and credentials will never be hard-coded within modules, and will be managed using secure mechanisms like AWS Secrets Manager or HashiCorp Vault.\n\n8. **Lifecycle Management:** We will implement robust lifecycle management practices for our modules, including version control, deprecation policies, and migration strategies. This will facilitate seamless updates and transitions without disrupting existing infrastructure.\n\nBy adopting these practices, we aim to create a modular, consistent, and efficient infrastructure codebase that allows for easy scalability and collaboration across the Modernisation Platform.","GenTime":"2024-07-29 00:33:03"}
{"File Name":"modernisation-platform\/0015-use-aws-image-builder-for-managing-amis.md","Context":"## Context\\nThere is likely to  be a common requirement across Modernisation Platform consumers to utilise the benefits of using customised AMIs, such as managed consistency of configuration and speed of EC2 instance deployment.\\nSome options to tackle AMI generation and management were considered\\n* A custom approach - based on a reference architecture (see <https:\/\/aws.amazon.com\/blogs\/awsmarketplace\/announcing-the-golden-ami-pipeline\/>) that brings together a number of tools and techniques to manage AMIs through their lifecycle. This approach brings with it a fair amount of complexity and arguably extra operational overhead.\\n* [Packer](https:\/\/www.packer.io\/) - a common approach across private and public cloud platforms, using this Hashicorp tool to programmatically build AMIs. Such a solution would also need to manage the distribution of AMIs across accounts and lifecycle management of AMIs\\n* [AWS Image Builder](https:\/\/docs.aws.amazon.com\/imagebuilder\/latest\/userguide\/what-is-image-builder.html) - a managed AWS service incorporating concepts such as pipelines, recipes and components, and even a marketplace of build and test components. Image builder is based on the use of AWS Systems Manager (so no ssh connections and exposed ports). A solution based on this (at the time of writing) would need to also handle the lifecycle management of AMIs (as it the case with Packer)\\n\n## Decision\n","Decision":"We will create an AMI generation and management capability based on AWS Image Builder.\\n","tokens":287,"id":3145,"Prediction":"## Decision\n\nWe have decided to adopt [AWS Image Builder](https:\/\/docs.aws.amazon.com\/imagebuilder\/latest\/userguide\/what-is-image-builder.html) for the creation and management of AMIs within the Modernisation Platform.\n\n### Rationale:\n\n1. **Managed Service**: AWS Image Builder is a managed service that reduces the operational overhead associated with custom AMI creation and management, thereby simplifying maintenance and support.\n2. **Integration with AWS Ecosystem**: Being an AWS service, Image Builder integrates seamlessly with other AWS services like AWS Systems Manager, making it easier to manage and automate the image lifecycle without requiring SSH connections or exposed ports.\n3. **Custom Components and Recipes**: Image Builder facilitates the use of custom recipes and components, enabling us to maintain consistent AMI configurations that meet our requirements.\n4. **Pipeline Support**: The service incorporates pipeline support, allowing us to build, test, and distribute AMIs using a well-defined workflow.\n5. **Security and Compliance**: AWS Image Builder adheres to AWS security best practices, ensuring that our AMIs are built in a secure environment. Additionally, leveraging a managed service helps ensure that we remain compliant with various standards and regulations.\n6. **Scalability**: AWS Image Builder scales effortlessly with our needs, making it a future-proof choice as the Modernisation Platform evolves.\n\nWhile both a custom approach and Packer offer extensive control and flexibility, they come with higher operational overhead and complexity. AWS Image Builder provides a balanced approach by abstracting much of the underlying complexity while still offering the flexibility we need to create and manage our AMIs effectively. This decision aligns with our goal of leveraging managed services to minimize operational burdens and improve system consistency and reliability.","GenTime":"2024-07-29 00:33:09"}
{"File Name":"modernisation-platform\/0006-use-a-multi-account-strategy-for-applications.md","Context":"## Context\\nIn the Modernisation Platform, we want to reduce the blast radius and increase the scalability of how we create, maintain, and support applications in the context of what AWS account(s) they sit within.\\n\n## Decision\n","Decision":"We've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https:\/\/user-guide.modernisation-platform.service.justice.gov.uk\/concepts\/environments\/).\\n","tokens":44,"id":3147,"Prediction":"## Decision\n\nWe will adopt a multi-account strategy within AWS to create, maintain, and support applications. Each application (or tightly coupled group of applications) will be allocated a dedicated AWS account. This approach will involve the following key actions:\n\n1. **Account Isolation**:\n   - Each application will reside in its own AWS account, effectively isolating resources and minimizing the blast radius in the event of a security incident or service disruption.\n  \n2. **Centralized AWS Account Management**:\n   - Utilize AWS Organizations to manage multiple AWS accounts centrally. This will streamline the creation and governance of accounts, provide consolidated billing, and enforce security best practices through SCPs (Service Control Policies).\n\n3. **IAM Role Structuring**:\n   - Implement cross-account IAM roles to facilitate necessary access between accounts. This will ensure that teams can perform their duties while maintaining strong access control standards.\n\n4. **Networking Considerations**:\n   - Establish a common VPC structure and networking policies which can be implemented across accounts to ensure consistent connectivity and security protocols.\n\n5. **Automation and Infrastructure as Code**:\n   - Use Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation to automate the creation and management of these accounts, ensuring consistency and repeatability across environments.\n\n6. **Monitoring and Logging**:\n   - Enable centralized logging and monitoring using AWS CloudWatch, CloudTrail, and a dedicated logging account to aggregate logs from all application accounts for unified monitoring and troubleshooting.\n\n7. **Cost Management**:\n   - Implement AWS Cost Explorer and tagging strategies to monitor and manage costs across these multiple accounts, providing visibility into individual application expenditures.\n\nBy adopting a multi-account strategy, we will enhance the scalability, security, and manageability of our applications, allowing teams to innovate independently while maintaining robust operational oversight and governance standards.","GenTime":"2024-07-29 00:33:19"}
{"File Name":"modernisation-platform\/0011-use-vpc-flow-logs-to-gain-insight-into-network-state.md","Context":"## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n\n## Decision\n","Decision":"[VPC flow flows](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\\n","tokens":65,"id":3150,"Prediction":"## Decision\n\nTo ensure that the network topology and configuration of the Modernisation Platform consistently meet security, compliance, and best-practice requirements, we have decided to adopt a multi-faceted monitoring and testing strategy supported by industry-leading tools and products.\n\n1. **Network Monitoring Tool Selection**: We will use a combination of AWS CloudWatch and Prometheus for real-time monitoring and alerting. AWS CloudWatch will provide comprehensive visibility into the network infrastructure within the AWS ecosystem, enabling metric collection, log monitoring, and automated responses to predefined thresholds. Prometheus, with its powerful querying language and flexible data model, will be used to closely monitor performance metrics and provide long-term trending analyses.\n\n2. **Configuration Management & Compliance Checking**:\n    - **AWS Config** and **HashiCorp Terraform** are chosen for infrastructure as code (IaC) and configuration management. AWS Config will continuously monitor AWS resource configurations and assist in the evaluation of compliance with internal policies and external regulations. HashiCorp Terraform will manage and provision network components to ensure that infrastructure remains in its desired state. \n    - Regular compliance audits will be automated through AWS Config Rules, which can trigger alerts and remediation actions if deviations from the desired configuration are detected.\n\n3. **Network Security Testing**:\n    - **AWS GuardDuty** will be employed for threat detection to continuously monitor for malicious activity and unauthorized behavior. This will be complemented by periodic vulnerability assessments using tools like **Nessus** to identify and remediate potential security gaps within the network configuration.\n    - **AWS Firewall Manager** and **AWS Shield** will provide managed firewall policies and DDoS protection, ensuring that the network remains fortified against external threats.\n\n4. **Best-Practices Audit**: Regular network audits will be conducted using **AWS Trusted Advisor**, which will provide insights and recommendations on performance improvement, cost optimization, and adherence to best practices. This will keep the network configuration aligned with industry standards.\n\n5. **Performance and Load Testing**:\n    - **Apache JMeter** and **AWS Load Balancer** services will be used for performance testing across different segments of the network. These tools will help identify bottlenecks and ensure that the infrastructure can handle peak loads without compromising performance.\n\nBy integrating these tools and products into our monitoring and testing ecosystem, we can achieve comprehensive visibility and control over the network topology and configuration. This multi-layered approach ensures that security, compliance, and best-practice standards are consistently met and maintained.","GenTime":"2024-07-29 00:33:32"}
{"File Name":"alfa\/adr-003.md","Context":"## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n\n## Decision\n","Decision":"We will use [TypeScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\\n","tokens":195,"id":3153,"Prediction":"## Decision\n\nWe have decided to adopt TypeScript as the primary language for the Alfa code base.\n\n### Rationale\n\n1. **Type Safety:** TypeScript provides a robust type system that helps catch common bugs related to `undefined` or `null` values, and incorrect parameter passing at compile time, which addresses our primary source of past bugs.\n   \n2. **Tooling and Ecosystem:** TypeScript integrates seamlessly with existing JavaScript tools and frameworks while offering additional tooling support. This enhances our development experience through improved code completion, navigation, and refactoring capabilities.\n\n3. **Compatibility:** TypeScript is a superset of JavaScript, meaning we can incrementally adopt it in our existing JavaScript code base. It compiles down to plain JavaScript, which is necessary for the code to run in environments like web browsers.\n\n4. **Maintainability:** With a growing and complex code base, the ability to manage and scale our applications will benefit from TypeScript\u2019s static type checking. This will make the code more understandable and maintainable for current and future developers.\n\n5. **Community and Support:** TypeScript has strong community support and is widely adopted in the industry. Many libraries and frameworks now include TypeScript types, making integration easier and more reliable.\n\n### Implementation\n\n1. **Incremental Adoption:** Begin by converting the most critical and frequently changing parts of the code base to TypeScript. Over time, convert the rest of the code base.\n\n2. **Toolchain Integration:** Integrate TypeScript into our build and development toolchain, ensuring that it compiles to JavaScript seamlessly for both Node.js and browser targets.\n\n3. **Training:** Provide resources and training sessions for the development team to familiarize themselves with TypeScript and its best practices.\n\n4. **Continuous Integration:** Update our CI pipeline to include TypeScript compilation and type-checking steps to ensure that type safety guarantees are enforced consistently.\n\n### Alternatives Considered\n\n- **Flow:** We considered using Flow for type checking JavaScript, but TypeScript has a broader ecosystem support and richer features.\n- **Pure JavaScript with Enhanced Tooling:** We considered staying with plain JavaScript and enhancing tooling around it to catch common bugs. However, the maturity and comprehensive nature of TypeScript's type system make it a more robust choice for the long term.\n\nBy adopting TypeScript, we aim to improve the reliability, maintainability, and overall quality of the Alfa code base while ensuring it remains compatible with browser environments.","GenTime":"2024-07-29 00:33:46"}
{"File Name":"alfa\/adr-002.md","Context":"## Context\\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https:\/\/github.com\/GoogleChrome\/accessibility-developer-tools), [aXe by Deque](https:\/\/github.com\/dequelabs\/axe-core), and [HTML_CodeSniffer by Squiz](https:\/\/github.com\/squizlabs\/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https:\/\/wicg.github.io\/aom\/).\\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\\n\n## Decision\n","Decision":"We will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\\n","tokens":437,"id":3156,"Prediction":"## Decision\n\nTo address the non-deterministic nature of browser-based accessibility assessments and ensure reproducibility of results, we will implement a hybrid approach that combines both browser-based and server-side assessments. The key components of this approach are:\n\n### Hybrid Testing Infrastructure\n1. **Controlled Environment for Replication**\n   - Develop a server-side rendering and testing environment that can reliably reproduce the exact circumstances in which an assessment is made. This will include a headless browser setup within a containerized environment (such as Docker) that ensures consistent network conditions, deterministic script execution, and controlled request headers.\n\n2. **Separation of Analysis Responsibilities**\n   - Perform initial data gathering and static analysis using the server-side environment. This will involve extracting the DOM structure, CSSOM, and any other relevant static resources needed for accessibility analysis.\n   - Utilize browser-based testing primarily for dynamic, behavior-based evaluations that cannot be replicated server-side. These tests will be minimal and structured to isolate dynamic elements which are prone to change.\n\n### Deterministic Assessment Engine\n1. **Standardized Testing Pipeline**\n   - Create a standardized pipeline for processing each web page that sequentially captures and analyzes the necessary data in a consistent manner. By defining and adhering to strict testing stages, we can reduce variability in the assessment process.\n\n2. **Persistent Artifacts for Each Assessment**\n   - Store detailed logs, captured states of the DOM, CSSOM, and any intermediate accessibility trees for every assessment. This will facilitate a comprehensive audit trail that can be used to investigate discrepancies and answer customer queries accurately.\n\n### Enhanced Communication and Support\n1. **Transparent Reporting**\n   - Develop a reporting system that clearly delineates which parts of the assessment were performed in a server-side environment and which in the browser. This transparency will help users understand the reliability of different parts of the results.\n  \n2. **Effective Customer Support Tools**\n   - Build a suite of tools that enables customer support to easily replicate the customer\u2019s environment and reproduce reported issues. This will include the ability to replay a recorded state of the web page assessment within the controlled server-side environment.\n\nBy implementing this hybrid approach, we aim to mitigate the inherent non-determinism of browser-based accessibility assessments, enhance the reproducibility of results, and improve the overall reliability and customer satisfaction of our accessibility tools.","GenTime":"2024-07-29 00:33:58"}
{"File Name":"hmpps-interventions-service\/0002-represent-validation-errors-at-field-level.md","Context":"## Context\\nFor any user interface or client relying on our API, we need to define how we represent what was wrong with\\ninvalid client requests.\\n\n## Decision\n","Decision":"We will use field-level error validation.\\nWe will use meaningful codes per field.\\nExample:\\n```json\\n{\\n\"status\": 400,\\n\"error\": \"validation error\",\\n\"message\": \"draft referral update invalid\",\\n\"validationErrors\": [\\n{\\n\"field\": \"serviceUser.crn\",\\n\"error\": \"FIELD_CANNOT_BE_CHANGED\"\\n}\\n]\\n}\\n```\\n","tokens":31,"id":3159,"Prediction":"## Decision\n\nWe will standardize the representation of errors for invalid client requests using the following approach:\n\n1. **HTTP Status Codes:** Utilize appropriate HTTP status codes to indicate the type of error. For example,\n   - `400 Bad Request` for general client-side errors.\n   - `401 Unauthorized` for authentication-related issues.\n   - `403 Forbidden` for authorization-related issues.\n   - `404 Not Found` for non-existent resources.\n   - `409 Conflict` for resource conflict issues, etc.\n\n2. **Error Response Structure:** Return a structured JSON response that includes the following fields:\n   - `status`: The HTTP status code.\n   - `error`: A brief and clear textual summary of the error (e.g., \"Validation Error\").\n   - `message`: A detailed message explaining what was wrong, which can be useful for debugging and user feedback.\n   - `path`: The request path that led to the error.\n   - `timestamp`: The time at which the error occurred.\n\n   Example:\n   ```json\n   {\n       \"status\": 400,\n       \"error\": \"Bad Request\",\n       \"message\": \"Email address is required\",\n       \"path\": \"\/api\/v1\/users\",\n       \"timestamp\": \"2023-10-01T12:45:30Z\"\n   }\n   ```\n\n3. **Validation Error Details:** For validation errors, include an additional `details` field to provide information about specific validation issues:\n   - `fieldErrors`: An array of objects where each object contains:\n     - `field`: The field name that caused the validation error.\n     - `issue`: A description of what is wrong with the field.\n\n   Example:\n   ```json\n   {\n       \"status\": 400,\n       \"error\": \"Validation Error\",\n       \"message\": \"Invalid input data\",\n       \"path\": \"\/api\/v1\/users\",\n       \"timestamp\": \"2023-10-01T12:45:30Z\",\n       \"fieldErrors\": [\n           {\n               \"field\": \"email\",\n               \"issue\": \"Email address is required\"\n           },\n           {\n               \"field\": \"password\",\n               \"issue\": \"Password must be at least 8 characters\"\n           }\n       ]\n   }\n   ```\n\n4. **Consistent Fallback:** Ensure that all endpoints consistently follow this error formatting to maintain uniformity across the API. This will also aid clients in reliably parsing and presenting error information to end-users.\n\nBy implementing this structured approach, we aim to provide clarity and actionable insights to API consumers whenever an invalid request is made, fostering a better developer experience and more robust client applications.","GenTime":"2024-07-29 00:34:10"}
{"File Name":"heroes-board-game\/0002-tests-definition-and-scope.md","Context":"## Context and Problem Statement\\nAcceptance tests are embedded into game server application.\\nShould they be written one layer above (UI)?\\n## Decision Drivers\\n* Acceptance tests should be meaningful for end users\\n\n## Decision\n","Decision":"* Acceptance tests should be meaningful for end users\\nRedefine current acceptance tests and treat them like _component_ tests according to [this definition](https:\/\/www.simpleorientedarchitecture.com\/defining-test-boundaries\/).\\n* Good, because it promotes different tests for different audiences\\n* Good, because it sets a clear terminology\\n* Good, because end users don't care about layers below UI\\n","tokens":44,"id":3166,"Prediction":"## Decision\nAcceptance tests will be written one layer above the game server application, specifically at the UI layer.\n\n## Rationale\n* **End-User Perspective**: By moving the acceptance tests to the UI layer, we ensure they reflect the user experience and validate the features and behaviors that end users directly interact with. This makes the tests more meaningful and relevant for end users.\n* **Separation of Concerns**: Keeping acceptance tests at the UI layer helps maintain a clear separation of concerns. It allows server-side logic to be tested independently through unit and integration tests, while acceptance tests focus on end-to-end user interactions.\n* **Comprehensive Validation**: UI-level acceptance tests provide comprehensive validation of the system as they cover multiple components interacting together, ensuring the game server's integration with the UI functions correctly from the user's standpoint.","GenTime":"2024-07-29 00:34:35"}
{"File Name":"tul_cob\/0001-use-feature-flags-to-toggle-features.md","Context":"## Context\\nThere are features that exist in the codebase we are not yet ready to release in production. We would like to use feature flags to toggle the availability of certain features, which will help prevent development and production branches from drifting.\\n\n## Decision\n","Decision":"We've decided to implement very simple feature flags that can be toggled with environment variables.\\n","tokens":49,"id":3171,"Prediction":"## Decision\nWe will implement feature flags within our software to manage the availability of specific features. This will allow us to control the release and roll-out of features without needing to deploy different versions of the codebase. Feature flags will be applied at the application layer, enabling us to toggle features on and off using a configuration management system. This approach will help us to conduct A\/B testing, gather user feedback, and ensure that unready features do not impact production stability. We will choose a feature flagging tool that integrates well with our current technology stack and supports gradual rollouts, targeting specific user segments, and real-time updates.","GenTime":"2024-07-29 00:34:50"}
{"File Name":"service-stac\/2020_10_21_static_asset.md","Context":"## Context\\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\\n\n## Decision\n","Decision":"Because it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\\n","tokens":91,"id":3173,"Prediction":"## Decision\n\nTo ensure efficient and secure serving of static assets for the admin pages, `service-stac` will be configured to use a Reverse Proxy with Nginx for the following reasons:\n\n1. **Separation of Concerns:** Nginx will handle the serving of static files (such as CSS, images, and icons), while `gunicorn` will focus solely on handling dynamic web requests, enhancing the overall performance.\n   \n2. **Security:** By placing a Reverse Proxy (Nginx) in front of `gunicorn`, we can mitigate risks related to slow client connections and Denial of Service (DoS) attacks. Nginx\u2019s ability to queue requests and handle them efficiently will improve the robustness of our application.\n   \n3. **Best Practices:** Serving static assets with Nginx is a widely-accepted practice in the industry. It is optimized for handling static files and can do so more efficiently than `gunicorn`.\n   \n4. **Scalability:** This setup will assist in future scalability, as Nginx is well-suited to distribute HTTP requests across multiple instances of `gunicorn` if needed.\n   \n### Implementation Steps\n\n1. **Install Nginx:** Ensure that Nginx is installed on the server.\n2. **Configure Nginx:** Update the Nginx configuration to:\n    - Serve static files directly.\n    - Proxy pass requests for dynamic content to the `gunicorn` server.\n3. **Update Django Settings:** Modify Django settings to:\n    - Ensure static files are collected and placed in a directory that Nginx can serve from, typically using Django's `collectstatic` command.\n4. **Deploy and Test:** Deploy the updated configuration and rigorously test to confirm that both static and dynamic content are served appropriately and efficiently.","GenTime":"2024-07-29 00:34:58"}
{"File Name":"react-transcript-editor\/2018-10-05-components-comunication.md","Context":"## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n\n## Decision\n","Decision":"* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026 -->\\n","tokens":71,"id":3187,"Prediction":"## Decision\n\nImplement the **Observer Pattern** for internal component communication in the Transcript Editor.\n\n### Rationale:\n1. **Simplicity and Clarity:**\n   - The Observer Pattern provides a clear and straightforward method for components to subscribe to and propagate changes, making the communication flow easier to understand and manage.\n   \n2. **Extensibility:**\n   - This pattern is highly extensible. As new components or features are added, they can easily subscribe to relevant updates without requiring significant changes to the existing codebase. This ensures the system can evolve to meet future use cases and requirements.\n\n3. **Separation of Concerns:**\n   - By using the Observer Pattern, the system can maintain a loose coupling between components. Each component can focus on its specific functionality, promoting better maintainability and scalability.\n\n4. **Consistency and Synchronization:**\n   - It ensures that all components are synchronized and receive the most up-to-date information, which is crucial for maintaining data consistency within the transcript editing process.\n\n5. **Flexibility for Internal Info Accessibility:**\n   - Since components can dynamically subscribe to events, internal state changes can be observed and acted upon only when necessary, making internal info accessible and modifiable without invasive changes.\n\n### Implementation Summary:\n- **Publisher:** Components that have state changes will act as publishers.\n- **Subscribers:** Components that need to respond to these state changes will act as subscribers.\n- **Event Management:** Create a centralized event manager or bus to manage the registration and notification of events.\n\nBy adopting the Observer Pattern, the Transcript Editor will have a robust, flexible, and maintainable method for component communication that aligns well with the identified decision drivers.","GenTime":"2024-07-29 00:35:58"}
{"File Name":"aspan_micro_front_ends\/0002-derived-attributes.md","Context":"## Context\\nDerived attributes are read-only.\\n\n## Decision\n","Decision":"Following attributes are derived from file system:\\n1. name - file name without extension\\n2. contentType - file extension without dot, lowercased\\n","tokens":10,"id":3192,"Prediction":"## Decision\nGiven that derived attributes are read-only, we have decided to adopt a design where derived attributes can only be modified through changes in their dependent attributes. Consequently, these derived attributes will not have any setters or direct methods that allow them to be updated independently. This ensures data consistency and integrity within the system.","GenTime":"2024-07-29 00:36:07"}
{"File Name":"aspan_micro_front_ends\/0004-info-graphql-import.md","Context":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n\n## Decision\n","Decision":"Decision here...\\n","tokens":20,"id":3194,"Prediction":"## Decision\nWe have decided to adopt the import and export definitions feature available in GraphQL SDL (Schema Definition Language) to manage our GraphQL modules. This decision will improve the modularity, reusability, and maintainability of our GraphQL schema definitions, enabling us to build a more scalable and organized codebase.\n\nSpecifically, we will implement the following strategies:\n\n1. **Module-Based Structure:** Organize our GraphQL schemas into distinct, self-contained modules. Each module will encapsulate its own type definitions, resolvers, and related utilities.\n   \n2. **Import\/Export Syntax:** Utilize the `# import` syntax to include type definitions and resolvers from other files, promoting clear separation of concerns. This will allow us to compose larger schemas from smaller, manageable pieces.\n\n3. **Centralized Schema Stitching:** Create a central point in our codebase to stitch together all the modular GraphQL SDL files. This centralized stitching process will aggregate all the imports and exports to form the final executable schema.\n\n4. **Code Generation Tools:** Leverage existing tools like `graphql-import` or `graphql-tools` to facilitate the parsing and stitching of our schema definition files, ensuring compatibility and reducing manual errors.\n\n5. **Documentation and Best Practices:** Document the structure and guidelines for creating and managing GraphQL modules. Establish best practices for naming conventions, file organization, and dependency management within our team.\n\nThis approach will ensure that our GraphQL API remains modular, scalable, and easier to maintain while allowing different team members to work on separate modules independently.","GenTime":"2024-07-29 00:36:13"}
{"File Name":"heptaconnect-docs\/2020-04-30-contracts-and-interfaces.md","Context":"## Context\\nThere has to be a guide how to structure code to allow extendability.\\nWe have to ensure that functionality can be exchanged without interfering with adjacent packages within the package hierarchy.\\n\n## Decision\n","Decision":"The approved approach is using the language feature type hierarchy.\\nUsing interfaces, traits and contracts (abstract classes) is a good way to structure and allow replacements by ensuring certain behaviors.\\nWe use interfaces when multiple implementation will exist and are unknown at any time for the package that it is introduced in.\\nFor example, we do not know what kind of storage is used within the portal-base, but it will need some kind of storage.\\nTo supply some basic logic for commonly used interfaces we can provide traits for others to implement them easier.\\nWe use contracts similar to interfaces but use their advantages to contain any logic beforehand.\\nThis enables us to add additional code later with a reduced level of changes that can be non-breaking without removing the replacing option.\\nContracts are best without dependencies that have to be given in the constructor as this forces other implementations to follow this pattern regardless whether they need it.\\n","tokens":39,"id":3203,"Prediction":"## Decision\n\nWe will adopt the following code structuring guidelines to ensure extendability and minimize interference between packages:\n\n1. **Modular Design**: Structure the codebase into well-defined, self-contained modules or packages. Each module should encapsulate related functionalities and expose a clear interface for interaction.\n\n2. **Separation of Concerns**: Adhere to the single responsibility principle within classes and functions, ensuring that each part of the codebase has one clear purpose. This will make it easier to modify or replace individual components without affecting others.\n\n3. **Interface-Based Programming**: Define clear interfaces for each module. Use interfaces or abstract classes to define the contracts for interaction between modules. Implementation details should not be exposed outside the module.\n\n4. **Dependency Injection**: Utilize dependency injection to manage dependencies between modules. This will allow swapping different implementations of a module without changing the depending code.\n\n5. **Event-Driven Architecture**: Implement an event-driven architecture where modules communicate through events. This will decouple the modules and allow easy replacement or enhancement of functionalities.\n\n6. **Configuration and Convention**: Employ a combination of configuration and convention to manage module configurations. This will provide flexibility while maintaining a predictable structure.\n\n7. **Testing and Documentation**: Provide comprehensive tests for each module to ensure that changes or replacements do not introduce regressions. Document the purpose, interfaces, and expected behavior of each module to aid future enhancements.\n\n8. **Versioning and Backward Compatibility**: When making changes to module interfaces, maintain versioning and ensure backward compatibility wherever possible to avoid disrupting dependent modules.\n\nBy following these guidelines, we aim to create a codebase that is both extendable and resilient to changes, facilitating easier maintenance and enhancement over time.","GenTime":"2024-07-29 00:36:48"}
{"File Name":"heptaconnect-docs\/2021-10-29-flow-components-are-not-crud.md","Context":"## Context\\nAt the time of writing we have explorers, emitters and receivers as three main flow components.\\nThey resemble CR and U from the well-known [CRUD](https:\/\/en.wikipedia.org\/wiki\/Create,_read,_update_and_delete).\\nMost APIs are CRUD or [BREAD](http:\/\/paul-m-jones.com\/post\/2008\/08\/20\/bread-not-crud\/) based and therefore match the three named flow components.\\nFor now, emitting and receiving entities can be also used differently as this \"just\" sends data from one portal node and is received by another portal node.\\nEmitters and receivers could send commands instead of entities.\\nAs previously mentioned we do not have a deletion flow component.\\nA receiver could receive an entity with a custom deletion command with any previous version of HEPTAconnect.\\nThis is discouraged but possible.\\nWe have already seen implementations, that receive data but don't write anything to the API the portal resembles.\\nThis is a misuse that is similar to described scenario above.\\nLooking at the other existing flow components we also have webhooks and status reporters.\\nThese are not related to CRUD at all, so we are not limited to CRUD.\\n\n## Decision\n","Decision":"Receivers are not meant to do everything, when it is about receiving a command.\\nReceivers are meant to be used for entities only.\\nGrouping explorers, emitters, receivers and \"deleters\" into a single CRUD flow component enforces structures that probably don't benefit APIs, that do not fall into this pattern.\\nGrouping flow components is not helpful when we do not know the possible groups in beforehand and therefore can't be done right.\\nEvery other transfer needs a new flow component.\\nAs routes connect emitters and receivers they need to learn how to decide which flow components to use on a route.\\nThis is described in a [different ADR](.\/2021-10-30-route-capabilities.md).\\n","tokens":250,"id":3205,"Prediction":"## Decision\n\nGiven the current architecture and the flexibility of our existing flow components (explorers, emitters, and receivers), we will proceed with the following decisions:\n\n1. **Unified Emitting and Receiving Mechanism**: We will enhance the emitting and receiving components to handle both entities and commands explicitly. This allows us to maintain consistency and flexibility while accommodating scenarios beyond CRUD or BREAD operations. Specific command types, including deletion, can be handled in a standardized manner without misusing existing functionalities.\n\n2. **Introduction of a Deletion Flow Component**: To address the absence of a dedicated deletion flow, we will introduce a new flow component specifically for deletion. This will ensure clarity and proper separation of concerns within our system. Implementing a distinct deletion component will reduce the chances of misuse and make the framework more intuitive for developers.\n\n3. **Documentation and Guidance**: We will provide comprehensive documentation and guidelines on the appropriate use of each flow component. This will include best practices for handling commands within emitters and receivers and the proper scenarios for utilizing the new deletion component. The documentation will also cover non-CRUD operations, ensuring developers understand the full range of capabilities our system offers.\n\n4. **Refinement of Non-CRUD Components**: For components such as webhooks and status reporters that fall outside the CRUD paradigm, we will refine their definitions and use cases. This refinement will include clear specifications on how these components should interact with the rest of the system, ensuring they provide value without introducing ambiguity or misuse.\n\nBy implementing these decisions, we aim to enhance the robustness and clarity of our system, making it more versatile and developer-friendly while preventing common pitfalls and misuse scenarios.","GenTime":"2024-07-29 00:36:56"}
{"File Name":"heptaconnect-docs\/2021-10-30-route-capabilities.md","Context":"## Context\\nRoutes define directions for data to flow.\\nThe interpretation or use-case for a flow can be different for various reasons.\\nIn general, we support read-multiple-times write-multiple-times scenarios, and they are very generous in options to work with but often needs to be limited in integrations.\\nLimitations like transferring data only once or transferring data for finding the equivalent on the target are missing but requested.\\nWe need a way to configure route behaviour in core without adding more work to the integrators.\\n\n## Decision\n","Decision":"All limitations (e.g. transferring once) will be implemented as skipping existing steps.\\nThese changes in behaviour can be represented by simple boolean flags.\\nEvery step that is not a limitation will result in further flow components that will get a boolean flag.\\n","tokens":106,"id":3210,"Prediction":"## Decision\n\nWe will introduce a configurable route behaviour system in the core framework to address the diverse requirements of different use-cases for data flow. This system will allow integrators to set route behaviours through configuration, without requiring additional development work on their part. \n\n### Key Elements of the Decision:\n1. **Modular Configuration**: Implement a modular and flexible configuration model that allows for defining specific behaviours per route. This could include options like:\n   - Single-transfer routes\n   - Equality-check routes for target matching\n   - Standard read\/write multiple times routes\n\n2. **Behaviour Profiles**: Introduce predefined behaviour profiles that can be easily applied to routes. These profiles will encapsulate common configurations and can be customized as needed.\n\n3. **Configurator Tool**: Develop an intuitive configuration tool or interface that guides integrators through setting up routes with the desired behaviours, minimizing the potential for errors and ensuring ease of use.\n\n4. **Backwards Compatibility**: Ensure that the addition of configurable route behaviours does not disrupt existing implementations and remains backwards compatible. Existing routes should continue to function as they currently do unless explicitly reconfigured.\n\n5. **Documentation and Guidance**: Provide comprehensive documentation and examples to guide integrators in understanding and applying the new configurable route behaviour system effectively.\n\nBy implementing these configurable route behaviours, we will cater to diverse integration needs while reducing the complexity and overhead for integrators. This approach balances flexibility with ease of use, ensuring that our core framework remains robust and versatile for various data flow scenarios.","GenTime":"2024-07-29 00:37:15"}
{"File Name":"dlp-lux\/0003-Shibboleth.md","Context":"## Context\\nBoth Lux and Curate need to authenticate users to enforce authorization. Emory uses Shibboleth to manage\\nauthentication for University applications. Curate has gone through the lengthy process of requesting access to\\nShibboleth, and implemented user management through it. Part of the request was enumerating the \"attributes\" or\\nmetadata Curate needs to receive when a person logs in. Curate currently has three \"endpoints\", or applications,\\nthat can use its access to Shibboleth (prod, test, and arch).\\nLux also has access to Shibboleth, with the same three configured endpoints, and the same attributes.\\nLux does need to allow new users to sign in, where Curate does not.\\nIn production, neither application needs to allow users to authenticate from another source.\\n\n## Decision\n","Decision":"Lux will not use endpoints from Curate's Shibboleth access, instead using its own config. From the Lux side,\\na configuration option will be available to use database auth instead, for development, and to ease rollout.\\nIf the configuration option is not present, database auth will be used.\\n","tokens":167,"id":3214,"Prediction":"Given the context provided, the decision is as follows:\n\n## Decision\nLux and Curate will both standardize their authentication processes using Emory's Shibboleth system, leveraging the existing infrastructure and access already in place. Both Lux and Curate will use their respective configured endpoints (prod, test, and arch) with the same set of attributes for user management.\n\n### Specific Actions:\n1. **Adoption of Shibboleth for Authentication**:\n   - Lux and Curate will continue to use Shibboleth for user authentication to maintain consistency and leverage the existing access.\n\n2. **Consistent Attribute Set**:\n   - Both applications will utilize the same enumerated attributes for user metadata as configured during Curate's initial request. This ensures a uniform authentication experience and simplifies management.\n\n3. **Endpoint Utilization**:\n   - Both applications will utilize the three configured endpoints (prod, test, and arch) for their respective authentication processes.\n\n4. **User Sign-In Flexibility**:\n   - Lux will implement the functionality to allow new users to sign in, as it has a use case for registering new users, while Curate will not need this capability.\n\n5. **Single Source for Authentication**:\n   - In production environments, both applications will restrict authentication to Shibboleth only, disallowing other authentication sources to maintain security and integrity.\n\n### Justification:\n- By using a shared authentication source, we streamline the authentication process and reduce the overhead of managing multiple systems.\n- Utilizing the same set of attributes and endpoints promotes consistency and simplifies maintenance and support.\n- Allowing Lux to handle new user sign-ins addresses its specific requirements without complicating the authentication flow for Curate.\n- Restricting production authentication to Shibboleth enhances security by relying on a proven and vetted authentication method established by the University.\n\nThis decision ensures that both applications benefit from a consistent, secure, and manageable authentication process while addressing their unique functional requirements effectively.","GenTime":"2024-07-29 00:37:30"}
{"File Name":"dlp-lux\/0002-Blacklight-Version.md","Context":"## Context\\nIn order to facilitate work on dlp-lux, we need to make a decision about which verion of Blacklight to run.\\n\n## Decision\n","Decision":"We will use Blacklight 7.\\nThis allows us to avoid problems deploying the application, and sets us up for the future when Blacklight 7 is\\nsupported by Hyrax.\\nThis decision accepts that minor styling differences may occur between Lux and Curate.\\n","tokens":31,"id":3215,"Prediction":"## Decision\n\nWe have decided to run the latest stable version of Blacklight available at the time of the decision. Specifically, this means upgrading to Blacklight version 7.x. This decision was made based on the following considerations:\n\n1. **Support and Maintenance**: The latest version is actively maintained, meaning any security issues or bugs are likely to be addressed more swiftly than in older versions.\n2. **Features**: The newer version includes several enhancements and new features that can improve our application\u2019s performance, user experience, and development processes.\n3. **Community and Documentation**: Running a widely adopted stable version ensures we have access to community support, resources, and comprehensive documentation, which can facilitate smoother development and troubleshooting.\n4. **Compatibility**: We have assessed that our current infrastructure, including Ruby and Rails versions, is compatible with Blacklight 7.x, ensuring minimal friction in the upgrade process.\n5. **Long-term Viability**: Upgrading now positions us better for future upgrades and keeps us in line with the latest industry practices and standards.\n\nBy choosing to upgrade to the latest stable version of Blacklight, we aim to minimize technical debt and position our project for continued growth and improvement.","GenTime":"2024-07-29 00:37:33"}
{"File Name":"bookit-api\/0006-use-okta-as-identity-provider.md","Context":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n\n## Decision\n","Decision":"* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","tokens":279,"id":3217,"Prediction":"## Decision\nWe have decided to integrate Okta with our application for authentication and authorization needs based on the following key factors:\n\n1. **Scalability and Free Developer Account**: Okta offers a forever free developer account supporting up to 7000 active users per month, which aligns with our anticipated usage well before we will need to reconsider scaling options.\n\n2. **Uniform Authentication and Authorization API**: Using Okta's ability to authenticate against multiple providers and map these users into Okta's system, we can achieve a more uniform and standards-adhering API. This will simplify the integration complexity for the team and provide better maintainability across different authentication providers.\n\n3. **Spring Boot and Security Integration**: Okta has existing integrations with Spring Boot 2.x and Spring Security 5.x, which are our current technology stacks. This will facilitate a smoother implementation process leveraging well-established libraries and tools.\n\n4. **Reduction of Custom Code Using React SDK**: Switching to Okta's React Client SDK allows us to eliminate the need for our custom hand-rolled client code for Microsoft Azure AD, reducing the overall maintenance burden and leveraging a well-supported library.\n\n5. **Admin Capabilities**: Okta provides out-of-the-box administrative screens and dashboards for user and role management. This is a significant value-add, allowing our team to manage admin-level user groups without building custom admin interfaces.\n\n6. **Future-Proofing for Additional Logins**: As our application grows, the integration with Okta allows us to add additional social logins or support direct Active Directory (AD) integrations without major overhaul, supporting our future scalability and flexibility needs.\n\nConsidering these factors, we will proceed with implementing Okta for managing authentication and authorization in our application. This decision will enable a more robust, scalable, and maintainable security solution.","GenTime":"2024-07-29 00:37:43"}
{"File Name":"bookit-api\/0010-jpa-manages-schema.md","Context":"## Context\\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\\n\n## Decision\n","Decision":"* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging\/production databases (we will continue to drop\/recreate all other databases....local, integration).\\n* recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\\n","tokens":71,"id":3221,"Prediction":"## Decision\n\nWe will adopt a more sustainable approach to database management by integrating a dedicated database migration tool, such as Flyway or Liquibase, into our deployment process. This solution will allow us to manage database schema changes more effectively and ensure data integrity across deployments without having to drop and recreate the database each time.\n\n### Key Actions:\n1. **Select Migration Tool:**\n   - Evaluate and choose between Flyway and Liquibase based on our project requirements and team's familiarity.\n   \n2. **Initial Setup:**\n   - Integrate the chosen migration tool into our Spring Boot application.\n   - Convert existing schema.sql and data.sql scripts into migration files compatible with the chosen tool.\n\n3. **Develop Migration Process:**\n   - Establish a process for creating migration scripts that include changes to the schema and seed data.\n   - Devise a versioning strategy for these migration scripts.\n\n4. **Testing:**\n   - Thoroughly test the migration process in development and staging environments before deploying to production.\n   \n5. **Documentation and Training:**\n   - Document the new migration process for the development team.\n   - Conduct training sessions to ensure all team members are proficient with the new approach.\n\n6. **Deployment Configurations:**\n   - Update deployment pipelines to include steps for applying migrations.\n   - Configure the migration tool to run during application startup or as a part of CI\/CD pipeline.\n\nBy implementing these actions, we will avoid dropping and recreating the database on each deployment, ensuring a more stable and scalable solution for database management. This approach will also facilitate handling more complex schema changes and maintaining existing application data across deployments.","GenTime":"2024-07-29 00:37:55"}
{"File Name":"snippets-service\/0005-frequency-capping.md","Context":"## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n\n## Decision\n","Decision":"We decide to implement the Frequency Capping functionality into our\\nplatform to allow Content Managers to limit the number of Impressions,\\nClicks and Blocks per Job.\\nLocal or User Frequency Capping will be handled on the Browser level\\nby the Firefox Messaging Platform. The later supports only Impression\\nFrequency Capping.\\nThe Snippets Service will provide an interface (UI) for the Content\\nManagers to set upper limits on the number of Impressions a Job gets\\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\\nProfile Lifetime. This information is included in the JSON generated\\nfor each Job.\\nFor Global Frequency Capping the Snippets Service will provide an\\ninterface (UI) for the Content Managers to set the limits on total\\nworldwide number of Impressions, Clicks and Blocks per Job.\\nSnippets Service will query Mozilla's Redash for Telemetry data every\\nten minutes and will fetch current impressions, clicks, blocks for\\neach Job with set limits.\\nWhen the reported numbers exceed the set limits then, the Job will be\\nmarked COMPLETE and will be pulled out of the Bundles on the next run\\nof `update_jobs` cron job.\\nThe Frequency Capping functionality is additional to the Date\\nPublishing controls, therefore a Job can end on a specific Date and\\nTime or when its Global Frequency Capping Limits are met.\\n### Monitoring and Handling of Errors\\nSince Global Frequency Capping depends on an external system for\\nMetrics (Redash \/ Telemetry) it is possible that the latest numbers are\\nnot always available to the Snippets Service to make a decision. Such\\ncases include scheduled or unplanned service interruptions or network\\nerrors.\\nIn co-ordination with Snippet Content Owner we decided that for cases\\nwhere the Snippets Service cannot get the latest numbers for more than\\n24 hours, Jobs with Global Frequency Capping will get canceled. The\\ncancellation reason will state that the Jobs where prematurely\\nterminated due to missing metrics.\\nThe cron job responsible for fetching the Data from Telemetry is\\nmonitored by a Dead Man's Snitch.\\n","tokens":208,"id":3229,"Prediction":"## Decision\n\nImplement Frequency Capping in the Snippet Service using a hybrid approach that leverages both server-side and client-side mechanisms.\n\n### Server-Side Component\n1. **Global Frequency Capping Management**:\n   * Develop a backend service to track and manage the total number of impressions served globally.\n   * Maintain a centralized database to log and update global impression counts in real-time.\n   * Implement APIs to query the current count of global impressions for specific content pieces, enabling the server to decide whether to serve the content based on the cap settings.\n\n2. **User Frequency Capping Management**:\n   * Extend the backend service to authenticate and manage user-specific impressions.\n   * Implement user identifiers (e.g., hashed user IDs) to track the number of times each user has interacted with specific content.\n   * Utilize the backend to serve content only if the user\u2019s impression count is below the specified threshold.\n\n### Client-Side Component\n1. **Telemetry Integration**:\n   * Leverage the Firefox Telemetry Pipeline to collect, aggregate, and report impression data back to the server efficiently.\n   * Ensure that the client-side implementation can reliably send metrics to the server without impacting user experience or performance.\n\n2. **Local Storage Management**:\n   * Implement a client-side caching mechanism using local storage or IndexedDB to temporarily store user-specific impression data.\n   * Mitigate edge cases where the user might clear data or switch devices by synchronizing local and server-side data periodically.\n\n### Cross-Functional Considerations\n1. **Performance Optimization**:\n   * Minimize the performance impact on the browser by ensuring that the frequency capping logic operates efficiently both on the server and client sides.\n   \n2. **Privacy and Security**:\n   * Adhere to privacy laws and guidelines by anonymizing user data and ensuring secure communication channels between the client and server.\n   * Provide transparency to users regarding the frequency capping mechanism and offer opt-out options if necessary.\n\n3. **Scalability and Reliability**:\n   * Design the frequency-capping solution to scale with the growing number of users and increasing demand for content impressions.\n   * Ensure high availability and reliability of the frequency capping services to maintain consistent user experience.\n\n### Conclusion\nThis hybrid approach allows the Snippet Service to effectively implement both global and user-specific frequency capping while overcoming previous technical limitations. By combining server-side precision and client-side agility, we can provide a robust and scalable solution that enhances content management and user satisfaction.","GenTime":"2024-07-29 00:38:25"}
{"File Name":"cafebabel.com\/0005-flask-architecture.md","Context":"## Context\\nFlask allows you to do whatever you want on your app\/folders\/templates\/etc\\nand it can quickly become messy.\\nPlus, to deal with configurations within extensions is harder if you do not\\ndeclare the Flask app once and for all then load the configuration.\\n\n## Decision\n","Decision":"An architecture based on https:\/\/github.com\/stephane\/flask-skeleton\\n","tokens":58,"id":3237,"Prediction":"## Decision\n\nTo maintain a clean and organized codebase, we will implement a structured application factory pattern for our Flask application. This will involve creating a dedicated function to initialize and configure the app. We'll follow a modular blueprint approach, separating our application into distinct components with specific roles such as models, views, and templates.\n\n### Steps to Implement:\n\n1. **Application Factory**: Create an `app.py` file containing the application factory function. This function will initialize the Flask app and load the necessary configurations from a configuration file or environment variables.\n\n    ```python\n    from flask import Flask\n    from config import Config\n\n    def create_app(config_class=Config):\n        app = Flask(__name__)\n        app.config.from_object(config_class)\n        \n        with app.app_context():\n            from . import routes\n            app.register_blueprint(routes.bp)\n        \n        return app\n    ```\n\n2. **Configuration Management**: Define configurations in a separate `config.py` file that will be imported by the application factory. This allows easy management and modification of configurations.\n\n    ```python\n    import os\n\n    class Config:\n        SECRET_KEY = os.environ.get('SECRET_KEY') or 'you-will-never-guess'\n        SQLALCHEMY_DATABASE_URI = os.environ.get('DATABASE_URL') or 'sqlite:\/\/\/app.db'\n        SQLALCHEMY_TRACK_MODIFICATIONS = False\n    ```\n\n3. **Blueprints**: Segment the application using blueprints. For instance, create a `routes` directory containing all route definitions in `routes.py`.\n\n    ```python\n    from flask import Blueprint\n\n    bp = Blueprint('main', __name__)\n\n    @bp.route('\/')\n    def index():\n        return \"Hello, World!\"\n    ```\n\n4. **Directory Structure**: Organize the project directory to enhance clarity and maintainability.\n\n    ```\n    my_flask_app\/\n    \u251c\u2500\u2500 app\/\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 routes\/\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 routes.py\n    \u2502   \u2514\u2500\u2500 templates\/\n    \u2502       \u251c\u2500\u2500 base.html\n    \u251c\u2500\u2500 config.py\n    \u251c\u2500\u2500 app.py\n    \u251c\u2500\u2500 requirements.txt\n    \u2514\u2500\u2500 run.py\n    ```\n\n5. **Centralized Configuration Loading**: Initialize the application in `run.py`, ensuring that configurations are loaded once and reused across the application.\n\n    ```python\n    from app import create_app\n\n    app = create_app()\n\n    if __name__ == '__main__':\n        app.run(debug=True)\n    ```\n\nBy organizing the Flask application following these steps, we ensure better maintainability, scalability, and ease of extending the application in the future. This modular approach will significantly simplify managing configurations, adding new features, and debugging.","GenTime":"2024-07-29 00:38:59"}
{"File Name":"html-diff-service\/0002-implement-using-spring-boot.md","Context":"## Context\\nThe HTML Diff Service is born out of need to increase performance of HTML\\ndiffing by not requiring a command line execution in Contently's web\\napplication. Additionally, we wanted to remove the Java dependency from our\\nRails application container to simplify and reduce the threat surface.\\n\n## Decision\n","Decision":"Spring Boot is used to allow for the service to be standalone with support for\\nembedded servers. The decision allows portability to deploy the service without\\na J2EE server.\\n","tokens":60,"id":3249,"Prediction":"## Decision\n\nWe decided to implement a new, dedicated HTML Diff Service using a server-side JavaScript engine (Node.js) instead of relying on command line tools or Java dependencies. This decision was made to meet our performance requirements for HTML diffing and to streamline our application's tech stack. \n\nBy using Node.js for our HTML Diff Service, we achieve the following:\n\n1. **Improved Performance**: Node.js, with its event-driven, non-blocking I\/O model, enhances the performance of HTML diffing tasks compared to the overhead introduced by command line interactions.\n2. **Simplified Tech Stack**: Removing the Java dependency helps in reducing the complexity of the deployment process and minimizes potential security vulnerabilities associated with maintaining an additional runtime environment.\n3. **Ease of Integration**: A Node.js-based service can be easily integrated into our existing Rails application with minimal friction, thanks to RESTful API endpoints or message queues.\n4. **Scalability**: Node.js is well-suited to handle a large number of concurrent requests, which will enable our service to scale effectively as demand grows.\n\nThis decision ensures that our HTML Diffing capabilities are more performant, secure, and maintainable in the long run.","GenTime":"2024-07-29 00:39:46"}
{"File Name":"html-diff-service\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3250,"Prediction":"## Decision\nWe will establish an Architectural Decision Record (ADR) process for documenting all significant architectural decisions made throughout the project. This will ensure that all stakeholders can understand the rationale behind key design choices, as well as the implications and trade-offs associated with each decision.\n\n### Steps to Implement the ADR Process:\n1. **Identify Key Decision Points**: Any architectural decision that has a long-lasting impact on the system or influences other architectural choices will be documented.\n2. **Document the Context and Problem Statement**: For each decision, clearly outline the context and problem being addressed.\n3. **Analyze Alternatives**: Document the alternative solutions considered, including the pros and cons of each.\n4. **Decision Making**: Clearly state the final decision, along with the rationale behind choosing it over other alternatives.\n5. **Impact and Consequences**: Describe the expected impact and any potential consequences of the decision.\n6. **Review and Approval**: Implement a review and approval process to validate the decision with relevant stakeholders.\n7. **Record Keeping**: Store all ADRs in a centralized, accessible repository (e.g., a dedicated directory in the project\u2019s version control system or a document management system).\n\n### Tools and Templates:\n- **Template**: Develop a standard template to be used for every ADR to ensure consistency.\n- **Version Control**: Utilize version control systems (e.g., Git) to manage changes and updates to the ADRs.\n- **Collaboration Tools**: Use collaboration tools (e.g., Confluence, Google Docs) to facilitate the review and approval process.\n\n### Maintenance:\nRegularly review the ADRs to ensure they are up-to-date with the evolving architectural landscape of the project.","GenTime":"2024-07-29 00:39:50"}
{"File Name":"HES_pipeline\/data_storage_and_access.md","Context":"## Context\\nThe raw HES data is large, complex, unweildly and potentially dirty. A way to\\nclean, store and later easily access the data for analysis required, which means\\nthat repeated cleaning and prep of the raw data is avoided.\\nThere are multiple datasets (with different variables), which once cleaned, will\\nbe accompanied by additional derived variables, and additional derived datasets.\\n\n## Decision\n","Decision":"The raw data will be read into a SQLite database through R. R is The Health\\nFoundation's open-source language of choice, and is well equiped to\\nhandle later analysis of said data. An SQLite database is not stored in memory,\\navoiding holding very large datasets an R session. It is a standard database\\nformat which can be easily queried with SQL which can in turn be written within\\nmost other common languagses, including R.\\nIn an SQLite database the database consistents of all the data, and within that\\nthere are tables; here each table corresponds to a HES dataset or a derived\\ndataset.\\n","tokens":83,"id":3253,"Prediction":"## Decision\n\nTo facilitate the efficient handling of the raw HES data, a comprehensive data management system will be implemented using a combination of the following components:\n\n1. **Data Cleaning Pipeline**:\n   - Utilize Extract, Transform, Load (ETL) processes to automate data cleaning and transformation.\n   - Implement data validation checks and cleansing rules to handle missing values, outliers, and inconsistencies.\n\n2. **Data Storage Solution**:\n   - Choose a scalable and high-performance database solution that can manage and store large datasets. Options include:\n     - A relational database management system (RDBMS) such as PostgreSQL for structured data with complex relationships.\n     - A NoSQL database such as MongoDB for more flexible schema configurations or handling unstructured data.\n\n3. **Data Versioning and Auditing**:\n   - Implement a data versioning system to track changes and updates to the cleaned datasets.\n   - Ensure auditability by logging data cleaning operations and transformations for transparency and reproducibility.\n\n4. **Data Access and Query Layer**:\n   - Develop an API layer or use a data warehouse solution (e.g., Amazon Redshift, Google BigQuery) to facilitate easy access and querying of the cleaned datasets.\n   - Ensure the API provides endpoints for accessing both raw and derived variables and datasets, enabling customizable data retrieval for analysis.\n\n5. **Derived Data Management**:\n   - Store derived variables and datasets separately but ensure they are linked to their original raw datasets for easy traceability and reference.\n   - Automate the process of generating derived variables and datasets as part of the ETL pipeline.\n\n6. **Metadata Management**:\n   - Develop a comprehensive metadata management system to document the data schema, cleaning rules, and transformation logic.\n   - Use metadata to enrich the datasets, making them more informative and easier to work with for analysis.\n\n7. **User Interface**:\n   - Consider building or integrating with a user-friendly interface for data scientists and analysts to explore, visualize, and analyze the data without requiring direct database queries.\n\nThis approach ensures the HES data is clean, well-organized, and easily accessible for analysis, while minimizing repetitive cleaning and preparation efforts.","GenTime":"2024-07-29 00:40:01"}
{"File Name":"mediawiki-extensions-Kartographer\/webgl_renderer.md","Context":"## Context\\nIn order to decrease maintenance overhead in the maps infrastructure, the tile server will receive improvements in the vector-tile generation using Tegola. Although Kartotherian already have a vector-tile endpoint, now there is some effort to evolve the platform as a whole, and it includes client-side rendering, that will take advantage of future capabilities supported by Tegola.\\nThe client-side render process will provide decoupling of some server-side components and will make it possible to replace mapnik, which is a maintenance overhead.\\nAfter analyzing all the open-source options in the market, we have reached the decision of using the mapbox-gl-style specification as the next-level in terms of maps rendering. The reason is because it can be easily integrated with the top client-side maps frameworks: maplibre-gl, Leaflet, and OpenLayers.\\nBecause of its flexibility and support across frameworks, it gives us freedom to experiment different engines in the early stages of this work.\\n\n## Decision\n","Decision":"Accepted (02-16-2021)\\n","tokens":198,"id":3286,"Prediction":"## Decision\n\nWe have decided to adopt the mapbox-gl-style specification for client-side maps rendering. This choice is guided by the specification\u2019s compatibility with leading client-side frameworks such as maplibre-gl, Leaflet, and OpenLayers, which affords us the flexibility to interchange and experiment with different rendering engines during the initial stages of implementation.\n\nBy moving to a client-side rendering approach with the mapbox-gl-style specification, we can leverage the enhanced capabilities provided by Tegola for vector-tile generation and simultaneously diminish the dependency on server-side components like mapnik, ultimately reducing maintenance overhead.\n\nThis strategic shift aims to evolve our maps infrastructure into a more modular and maintainable platform that supports future extensibility and robustness across various map rendering scenarios.","GenTime":"2024-07-29 00:41:49"}
{"File Name":"paas-csls-splunk-broker\/ADR001-syslog-http-to-csls-adapter.md","Context":"## Context\\nWe want to provide a reliable method of shipping logs from applications (on\\nGOV.UK Paas) to Splunk so they can take advantage of the log storage, analytics\\nand protective monitoring provided by the GDS Cyber Security team.\\nGDS Cyber Security maintain an [AWS Kinesis][kinesis] based log shipping stream\\nthat accepts log events in the [AWS CloudWatch Logs][cloudwatch] format and\\nqueues them for delivery to Splunk.\\nGOV.UK PaaS supports forwarding log events from an application's stdout and\\nstderr streams in [syslog format][syslog] via [syslog drains][drains].\\nComponents such as [Fluentd][fluentd] are available that process and forwarding\\nlogs from various sources to various targets, but their configuration can\\nunweildly and hard to test.\\nGOV.UK PaaS Tenants can run [sidecar][sidecar] containers to handle custom log\\nshipping or instument their applications with [logging libraries that support\\nmultiple transports][winston], but this brings extra complexity and doesn't\\nmake use of supported logging infrastructure already provided by GOV.UK PaaS.\\n\n## Decision\n","Decision":"We will build an \"adapter\" application to deploy alongside the CSLS logging\\npipeline that accepts requests in the \"syslog over http\" format exported by the\\nGOV.UK PaaS syslog drain system, translates them into the CloudWatch Logs\\nformat and forwards them on to the CSLS Kinesis stream.\\n","tokens":248,"id":3290,"Prediction":"## Decision\n\nWe will implement a solution that leverages the existing GOV.UK PaaS syslog drains to forward application logs in syslog format to the AWS Kinesis-based log shipping stream maintained by GDS Cyber Security.\n\n1. **Utilize GOV.UK PaaS syslog drains:** By utilizing the existing support for syslog drains in GOV.UK PaaS, we can capture and forward stdout and stderr logs from applications with minimal configuration changes to the applications themselves. This takes advantage of the supported logging infrastructure provided by GOV.UK PaaS.\n\n2. **Convert Syslog to CloudWatch Logs format:** We will introduce a component that converts logs from the syslog format to the AWS CloudWatch Logs format. This ensures compatibility with the AWS Kinesis-based log shipping stream.\n\n3. **Forward logs to AWS Kinesis:** The converted logs will be queued and forwarded to the AWS Kinesis stream, ensuring that they are delivered to Splunk for storage, analytics, and monitoring as maintained by the GDS Cyber Security team.\n\n4. **Simplicity and Maintainability:** By avoiding the need for tenants to run sidecar containers or instrument their applications with custom logging libraries, we reduce complexity. This also simplifies testing and maintenance, ensuring a more reliable and manageable logging infrastructure.\n\n5. **Alignment with Existing Infrastructure:** This decision aligns with existing supported logging infrastructure and methodologies within GOV.UK PaaS, ensuring better support and integration.\n\nThis approach provides a balance between leveraging existing infrastructure and fulfilling the need for reliable log shipping to Splunk, ensuring logs are efficiently translated and routed with minimal overhead on application development and maintenance.","GenTime":"2024-07-29 00:41:59"}
{"File Name":"paas-csls-splunk-broker\/ADR007-paas-taking-ownership.md","Context":"## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov\/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n\n## Decision\n","Decision":"The GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\\n","tokens":94,"id":3291,"Prediction":"## Decision\n\nGiven the current setup where the RE Autom8 team and Cyber Security jointly maintain the broker, and it is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, with a limited set of GDS-internal tenants, the following decisions are made:\n\n1. **Repository Management**:\n   - The broker code will be migrated to its own dedicated repository within the `alphagov` GitHub organization. This improves codebase organization and enables better version control and collaboration.\n   - The dedicated repository will be named `alphagov\/re-autom8-broker` to maintain clarity on its purpose and ownership.\n\n2. **Pipeline Segregation**:\n   - The CI\/CD pipeline for this broker will be separated from the Tech Ops multi-tenant Concourse and moved to a dedicated Concourse pipeline. This change ensures that the processes responsible for building and deploying the broker are streamlined and isolated from other operations.\n   - The new pipeline will include stages for building, testing, security scanning, and deployment.\n\n3. **Documentation & Collaboration**:\n   - Comprehensive documentation will be created and included in the new repository. This will cover setup, configuration, deployment practices, and troubleshooting.\n   - Collaboration guidelines and a clear maintenance path will be established between the RE Autom8 team and Cyber Security to ensure continued smooth operation and security compliance.\n\n4. **Regional Configuration and Expansion**:\n   - The broker will continue operating in the Ireland and London regions. However, a review will be performed annually to assess whether additional regions or expansion to more tenants are required or beneficial.\n\n5. **Security Measures**:\n   - Regular security audits will be scheduled with the Cyber Security team to ensure the broker adheres to the latest best practices.\n   - Any vulnerabilities identified will be tracked and remediated promptly, with updates pushed through the dedicated CI\/CD pipeline.\n\nThese decisions aim to enhance the maintainability, security, and scalability of the broker service while ensuring clear ownership and accountability.","GenTime":"2024-07-29 00:42:04"}
{"File Name":"paas-csls-splunk-broker\/ADR004-deploy-broker-as-lambda.md","Context":"## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n\n## Decision\n","Decision":"We will deploy the Broker as an AWS Lambda\\n","tokens":77,"id":3295,"Prediction":"## Decision\n\nDeploy the Broker as an AWS Lambda Function\n\n### Rationale:\n1. **Consistency**: Both the Adapter and the Broker are written in Go and can have similar deployment pipelines if both are deployed as AWS Lambda functions. This would streamline our operational and development processes, ensuring consistency across our architecture.\n  \n2. **Scalability**: AWS Lambda provides automatic scaling capabilities, which means that the Broker can handle varying loads efficiently without manual intervention.\n\n3. **Managed Infrastructure**: Deploying the Broker as a Lambda function leverages AWS-managed infrastructure, reducing our burden on maintenance and allowing the team to focus on core functionalities rather than infrastructure management.\n\n4. **Integration**: Deploying the Broker as a Lambda function promotes seamless integration with the existing Adapter lambda, potentially reducing latency and improving communication between them.\n\n5. **Pipeline Reusability**: We already have a build, test, and deploy pipeline for the Adapter. This can be easily adapted or extended for the Broker, ensuring continuous integration and delivery with minimal additional setup.\n\n6. **Security**: AWS Lambda provides built-in security features like IAM roles, network isolation with VPCs, and environment configurations that will bolster the security posture of the Broker service.\n\nDeploying the Broker as an AWS Lambda function aligns with our existing infrastructure strategy and takes advantage of AWS Lambda's benefits, such as scalability, managed infrastructure, and ease of integration with the current systems.","GenTime":"2024-07-29 00:42:21"}
{"File Name":"linshare-mobile-flutter-app\/0006-download-file-manager.md","Context":"## Context\\nIn LinShare Flutter application, we can download files one by one from \"My Space\", \"Received Shares\" or \"Shared Spaces\" with the library `flutter_downloader` by adding it to the queue :\\n```\\nTaskId enqueue(String url, String saveDir, String header...)\\n```\\nWe can also clear or cancel file queue with dedicated functions.\\n\n## Decision\n","Decision":"We decided that LinShare could currently download a file one by one.\\n","tokens":80,"id":3297,"Prediction":"## Decision\n\nWe will adopt the `flutter_downloader` library for managing file downloads in the LinShare Flutter application. This library provides essential features such as:\n\n1. **Enqueue Functionality**: It allows users to download files one by one from \"My Space\", \"Received Shares\", or \"Shared Spaces\" by adding download tasks to a queue using the `enqueue` method:\n   ```\n   TaskId enqueue(String url, String saveDir, String header...)\n   ```\n\n2. **Queue Management**: Users can clear or cancel the download queue using dedicated functions, which offers flexibility and control over ongoing and pending downloads.\n\nThese capabilities align well with our requirements for efficient and manageable file downloads within the application. By leveraging `flutter_downloader`, we can ensure a seamless and user-friendly download experience.","GenTime":"2024-07-29 00:42:27"}
{"File Name":"linshare-mobile-flutter-app\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3308,"Prediction":"## Decision\nWe will use an Architectural Decision Record (ADR) template to document and archive all significant architectural decisions made throughout this project. Each decision will be captured in a markdown file and stored in a version-controlled repository, ensuring easy access and traceability. The ADR template will include the following sections: Title, Date, Context, Decision, Status, Consequences, and any relevant Supporting Information. This structured approach will help maintain a clear history of architectural choices and rationale, facilitating better maintenance and future decision-making.","GenTime":"2024-07-29 00:42:59"}
{"File Name":"cukeness\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3316,"Prediction":"## Decision\n\nWe will implement an Architectural Decision Record (ADR) process using a combination of Markdown files stored in a version-controlled repository (e.g., GitHub) and an organizational structure organized by decision categories.\n\n### Key Guidelines:\n1. **ADR Template**: Standardize ADRs using a well-defined Markdown template. This will include sections such as Context, Decision, Consequences, and Status.\n2. **Repository Structure**: Create a specific directory within the main project repository named \"docs\/adr\/\" where all ADRs will be stored.\n3. **Naming Convention**: Each ADR will be named with a sequential number followed by a short descriptive title, for example, \"0001-initial-adr-process.md\".\n4. **Review and Approval Process**: Establish a process where ADRs are reviewed by relevant stakeholders and approved before being marked as accepted. This can be managed through pull requests and code reviews in GitHub.\n5. **Version Control**: Use Git for version control to track changes and history of each ADR. This facilitates collaboration and maintains a clear history of architectural decisions.\n6. **Categories**: Organize ADRs by categories such as \"architecture\", \"security\", \"performance\", etc., to facilitate easy navigation.\n7. **Status Tracking**: Maintain the status of each ADR (e.g., proposed, accepted, deprecated) within the document to provide clear visibility on the decision lifecycle.\n8. **Documentation**: Supplement ADRs with additional documentation when necessary, linking to detailed design documents, diagrams, or any related artifacts.\n9. **Adoption and Training**: Conduct a training session for the project team to ensure everyone understands how to create, review, and manage ADRs.\n\n### Implementation Steps:\n1. **Setup Repository**: Create the \"docs\/adr\/\" directory in the version-controlled repository.\n2. **Define Template**: Create a standard ADR template and add it to the repository.\n3. **Initial ADR**: Document this decision as the first ADR (\"0001-initial-adr-process.md\").\n4. **Educate Team**: Organize a training session to explain the ADR process to the team.\n\n### Tools and Platforms:\n- **Version Control**: GitHub (or equivalent)\n- **Documentation**: Markdown\n- **Review Workflow**: GitHub Pull Requests\n\nBy following this ADR process, we aim to ensure that all architectural decisions are well-documented, transparent, and accessible to the entire development team, thereby fostering better communication and more informed decision-making within the project.","GenTime":"2024-07-29 00:43:32"}
{"File Name":"react-template\/0003-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n\n## Decision\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":125,"id":3321,"Prediction":"## Decision\n\nWe will adopt Prettier as our code formatter for maintaining a consistent code style across all projects. Prettier will be integrated into our development workflow to automatically format code, ensuring uniformity and reducing the need for manual code style reviews. This decision will include the following actions:\n\n1. **Installation**: We will install Prettier as a development dependency in all our repositories.\n2. **Configuration**:\n   - Create a shared Prettier configuration file (`.prettierrc`) to be used across all projects.\n   - Optionally, create a `.prettierignore` file to exclude specific files or directories from being formatted.\n3. **Integration with Tools**:\n   - Set up Prettier to run as a pre-commit hook using tools like Husky to automatically format code before every commit.\n   - Integrate Prettier with code editors (e.g., VSCode, WebStorm) and IDEs to format code on save.\n4. **Documentation**: Update project documentation to instruct developers on setting up and using Prettier locally.\n5. **Initial Format**: Perform an initial formatting of existing codebases to conform to the Prettier style guidelines.\n6. **Continuous Integration (CI)**: Add checks to ensure code is properly formatted according to Prettier rules during the CI process.\n\nBy making this decision, we aim to streamline code reviews, enhance code readability, and foster a unified coding standard across our teams.","GenTime":"2024-07-29 00:43:46"}
{"File Name":"react-template\/0005-use-jest.md","Context":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n\n## Decision\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":3323,"Prediction":"## Decision\n\nWe will adopt Jest as our test framework for the project. Jest is highly compatible with both React and TypeScript and is the recommended standard test framework for React applications. Its features, such as a built-in test runner, mocking capabilities, and snapshot testing, make it an excellent fit for our requirements. Additionally, the rich ecosystem and active community support provide greater confidence that we will have access to extensive resources and plugins to meet our testing needs.","GenTime":"2024-07-29 00:43:52"}
{"File Name":"react-template\/0014-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n\n## Decision\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":3328,"Prediction":"## Decision\nWe will integrate Dependabot into our GitHub repository to manage dependency updates automatically. Dependabot will be configured to check for updates on a daily basis and create pull requests for any new versions of dependencies. This will include updates for both direct and transitive dependencies to ensure our project remains secure and up-to-date. The PRs generated by Dependabot will be reviewed by our development team to assess compatibility and perform necessary testing before merging.","GenTime":"2024-07-29 00:44:05"}
{"File Name":"knot-documentation\/ARC-002-fog-cloud-sync.md","Context":"## Context\\nOne of the main questions that appear when discussing the new fog service development\/integration was how the device contextual representation and data should be synchronized between fog and cloud. Before, when we were using the Meshblu we decided to send all the messages (device registration, schema, and data) to the cloud independently of the message type, data relevance at time or cloud connectivity state. This solution was simple for that moment but was increasing the internal message queues size and obviously isn't optimal mainly when dealing with a constrained environment. Therefore, our goal is to start with a simple but optimal approach in the first stage of the new fog service integration.\\n\n## Decision\n","Decision":"* Retry to register and update the device's schema on the cloud five times and if the error persists retry with ten minutes interval.\\n* Retry to publish only the last device's data.\\n* Retry to execute only the last commands sent to the device, avoiding to store a lot of pending commands on the message queue (which could lead to inconsistencies).\\n","tokens":132,"id":3337,"Prediction":"## Decision\nWe will implement a context-aware synchronization strategy for device data between fog and cloud services. This strategy involves the following steps:\n\n1. **Priority-Based Message Filtering**: Only messages deemed critical will be sent to the cloud immediately. Other less critical messages will be buffered in the fog layer.\n   \n2. **Batch Processing**: Non-critical data will be aggregated and sent in batches to reduce the number of communications needed, thus optimizing bandwidth usage and reducing congestion in the internal message queues.\n\n3. **Conditional Sync Triggers**: Device data synchronization will be contingent on a set of predefined conditions, such as cloud connectivity status and data relevance\/time-sensitivity. The system will therefore defer less relevant data syncing during periods of poor cloud connectivity.\n\n4. **Local Processing**: Wherever possible, data will be processed locally at the fog layer to reduce the amount of data sent to the cloud. This includes local analytics and decision-making that can be handled without cloud intervention.\n\nThis approach aims to optimize resource utilization and maintain an efficient and reliable synchronization process that is adaptive to varying conditions of data relevance and cloud connectivity.","GenTime":"2024-07-29 00:44:32"}
{"File Name":"knot-documentation\/ARC-003-amqp-exc-name.md","Context":"## Context\\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\\nThe goal is to define names that make it clear what this Exchange is used for.\\n\n## Decision\n","Decision":"* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\\n* The reference for choosing the queue name will be the client.\\n* The queue where BabelTower publishes messages will be called 'fogOut'.\\n* The queue where BabelTower consumes messages will be called 'fogIn'.\\n* The queue where Connector publishes messages will be called 'connOut'.\\n* The queue where Connector consumes messages will be called 'connIn'.\\n","tokens":131,"id":3339,"Prediction":"## Decision\nThe Exchange names will be defined based on the role and function they serve within the overall architecture. The naming convention will aim for clarity and consistency to ensure that the purpose and usage of each Exchange is easily understandable by all team members.\n\n### Naming Convention:\n1. **[ServiceName].[Purpose]Exchange**: The Exchange name will begin with the name of the service it relates to, followed by the specific purpose it serves within that service. This will make it immediately clear where the exchange fits within the system's architecture.\n\n### Examples:\n1. **Connector.MessageRoutingExchange**: For an exchange specifically responsible for routing messages within the Connector service.\n2. **Connector.DataSyncExchange**: For an exchange that handles data synchronization tasks related to the Connector service.\n3. **BabelTower.NotificationExchange**: For an exchange used by BabelTower for sending out notifications.\n4. **BabelTower.LoggingExchange**: For an exchange dedicated to logging messages generated by BabelTower.\n\n### Rationale:\n- **Clarity**: This naming scheme ensures that the function of each Exchange is explicit, reducing confusion and improving maintainability.\n- **Consistency**: Having a standardized naming convention helps in managing and navigating various parts of the system more efficiently.\n- **Scalability**: As the system grows, new exchanges following this convention can be added with ease and without ambiguity.\n\nBy adhering to this naming convention, we align with the architectural goals and ensure that the roles of the Exchanges are immediately apparent, fostering better communication and understanding among developers and operational staff.","GenTime":"2024-07-29 00:44:40"}
{"File Name":"plant-shop\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3349,"Prediction":"## Decision\nWe will implement a standardized process for documenting Architectural Decision Records (ADRs) using Markdown files stored in a dedicated directory within our project's repository. Each ADR will include structured sections such as Context, Decision, Status, Consequences, and Alternatives Considered. This will ensure consistent and traceable documentation of all architectural decisions throughout the lifecycle of the project.","GenTime":"2024-07-29 00:45:16"}
{"File Name":"mediawiki-extensions-Wikispeech\/0001-store-synthesized-audio-in-mediawiki-extension.md","Context":"## Context\\nThe original implementation of Wikispeech stored the synthesized audio\\nas files in a folder within the Speechoid service (in the\\nwikispeech-server sub-service). The paths to these files, together\\nwith the related metadata were then passed on as a response to the\\nMediaWiki extension.\\nThis implementation had a few identified drawbacks: Wikimedia\\ninfrastructure expects files to be stored in [Swift] rather than as\\nfiles on disk, supporting this would require implementing Swift\\nstorage in the Speechoid service.  There is a desire to keep the\\nSpeechoid service stateless, persistent storage of synthesized files\\nwithin the service runs counter to this.  The utterance metadata was\\nnot stored, requiring that each sentence always be re-synthesized\\nunless cached together with the file path.\\nWhile Wikimedia requires Swift many other MediaWiki installations\\nmight not be interested in that. It is therefore important with a\\nsolution where the file storage backend can be changed as desired\\nthrough the configs.\\nDue to [RevisionDelete] none of the content (words) of any segment\\nanywhere should be stored anywhere, e.g. in a table, since these must\\nthen not be publicly queryable, and to include mechanisms preventing\\nnon-public segments from being synthesized.\\nWe have an interest in storing the utterance audio for a long time to\\navoid the expensive operation of synthesizing segments on demand, but\\nwe still want a mechanism that flush stored utterances after a given\\nperiod of time. If a user makes a change to a text segment, it is\\nunlikely that the previous revision of that segment is used in another\\narticle and could thus be instantly flushed. There is also the case\\nwhere we want to flush to trigger re-synthesizing segments when a word\\nis added to or updated in the phonetic lexicon, as that would improve\\nthe resulting synthesized speech.\\nRe-use of utterance audio across a site (or many sites) is desirable,\\nbut likely to be rare (largely limited to headings and shorter\\nphrases). What will likely be more common is re-use of utterance audio\\nacross multiple revisions of the same page. If a single segment is\\nedited then all other segments, and their corresponding audio, remain\\nvalid. For this reason utterance audio should not be tied to a given\\npage or revision.\\n\n## Decision\n","Decision":"Files are only temporarily stored within Speechoid.\\nWhen a segment is synthesized, or when the audio is retrieved, a check\\nmust be performed to ensure it corresponds to a page revision which\\nhas not been suppressed through RevisionDelete. A segment is\\nrepresented by a hash to satisfy RevisionDelete requirements on public\\ntables. The segment hash should only be constructed from its\\ncontents. For the sake of RevisionDelete the link to the synthesised\\naudio should never be exposed to the end user.\\nThe MediaWiki extension parses the response from Speechoid, fetches\\nthe synthesized audio and stores this as a file using the provided\\n[FileBackend] functionality. The corresponding utterance metadata is\\nstored as a JSON file. Both files share the same base filename.\\nAn expiry date is attached to each stored Speechoid response to allow\\nlexicon updates to propagate and for the flushing of outdated\\nsegments.\\n","tokens":488,"id":3351,"Prediction":"## Decision\n\n1. **Adopt Swift Storage as the Primary Storage Backend**: To align with Wikimedia infrastructure requirements and ensure compatibility, the synthesized audio files will be stored using Swift. This will keep compliance and leverage the storage benefits provided by the Swift system.\n\n2. **Abstract Storage Backend Configuration**: Implement an abstraction layer for file storage where the storage backend can be easily configured and swapped based on the needs of different MediaWiki installations. This will be achieved by introducing a configurable storage interface, allowing installations to choose between Swift or other file storage solutions as per their requirements.\n\n3. **Stateless Speechoid Service**: Maintain the stateless nature of the Speechoid service by ensuring that no stateful data (such as audio files or metadata) is stored within the service itself. The necessary data will be either passed through temporary mechanisms or managed by external persistent storage systems.\n\n4. **Metadata Storage and Management**: Introduce a separate persistent storage mechanism for the utterance metadata. This will prevent the need to re-synthesize sentences unless necessary, thus optimizing performance. Metadata will be stored in a way that does not violate the constraints of non-public queryable data.\n\n   - **Avoidance of Storing Sensitive Content**: Ensure that no sensitive content (such as actual words of segments) is stored in publicly queryable tables, in compliance with RevisionDelete. Implement mechanisms to manage and restrict access to non-public segments.\n\n5. **Cache Management and Expiry**: Implement a dynamic caching mechanism that stores synthesized utterances based on usage patterns. Introduce automatic flushing capabilities for outdated or no longer needed utterances:\n\n   - Enable automatic flushing of utterances when the corresponding text segment is edited and is no longer relevant.\n   - Provide configuration options to set a retention period for utterance audio, after which it will be automatically flushed.\n   - Introduce a re-synthesizing trigger when updates occur in the phonetic lexicon to ensure the most accurate and updated synthesized speech.\n\n6. **Re-Usability of Utterance Audio**: Structure the storage of utterance audio to be reusable across multiple revisions of the same page and across different pages when applicable. This will minimize unnecessary re-synthesizing and improve the efficiency of the system.\n\n   - **Granular Storage and Retrieval**: Implement a granular approach where each segment of audio can be independently stored and retrieved. This will allow for the recombination of valid segments across page revisions and multiple uses.\n\nThis decision is aimed at optimizing the storage and management of synthesized audio in Wikispeech, ensuring flexibility, performance efficiency, and compliance with Wikimedia infrastructure requirements.","GenTime":"2024-07-29 00:45:26"}
{"File Name":"ethnicity-facts-and-figures-publisher\/0002-use-heroku-run-detached-for-static-site-build.md","Context":"## Context\\nAt the moment, we have a set of scheduled tasks on Heroku that run every 10 minutes, hour, or day. These range from building the static site (every 10 minutes), to reporting stalled builds (every hour), to refreshing the database from production data (in dev\/staging, every night). The main job here, building the static site, has started to fail because it occasionally runs longer than 10 minutes. The Heroku scheduler will only allow a scheduled instance to live for as long as the interval on the task. Heroku is therefore killing the instance before it can finish, leading to the incomplete builds and frequent reports of 'stalled' (in this instance, killed) builds.\\nWe need to schedule static site builds in a way that removes this strict time constraint so that they will always finish building.\\n\n## Decision\n","Decision":"We will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.\\nThe command we will use is `heroku run:detached -a <app_name> .\/manage.py build_static_site`.\\nPros\\n----\\n* Very easy to implement.\\n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).\\nCons\\n----\\n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.\\n* We have to expose a Heroku API key and install the Heroku CLI buildpack on our main Publisher app.\\n","tokens":172,"id":3361,"Prediction":"## Decision\n\nWe have decided to migrate the scheduled tasks, particularly the static site builds, from Heroku Scheduler to an AWS SageMaker training job. The decision is motivated by the need to overcome the time constraints imposed by Heroku and ensure task completion without interruptions.\n\n### Detailed Plan\n\n1. **Transition Scheduled Tasks to AWS SageMaker:**\n   - **Static Site Builds:** Move the static site build task, which currently runs every 10 minutes, to an AWS SageMaker training job. SageMaker does not impose the same execution time constraints, allowing the task to run until completion whether it takes 10 minutes or longer.\n   - **Other Tasks:** Strategically evaluate other tasks (reporting stalled builds, refreshing the database) and consider whether they benefit from a similar transition or can remain on Heroku Scheduler with appropriate adjustments.\n\n2. **Configuration and Execution:**\n   - **Build Configuration:** Configure the AWS SageMaker environment to handle the static site build process efficiently. This likely involves setting up the appropriate Docker containers, necessary environment variables, and any build dependencies.\n   - **Scheduling Mechanism:** Use AWS EventBridge (formerly CloudWatch Events) to define the same intervals (10 minutes, hourly, nightly) for scheduled tasks. EventBridge will trigger the SageMaker jobs according to the defined schedule without time constraints.\n\n3. **Monitoring and Alerts:**\n   - **Monitoring:** Implement monitoring and logging for the SageMaker jobs using AWS CloudWatch to track execution times, successful completions, and any potential errors. \n   - **Alerts:** Set up alerting mechanisms to notify the development team of build failures or unusual execution times to facilitate quick troubleshooting.\n\n4. **Gradual Rollout and Testing:**\n   - **Initial Migration:** Start with migrating the static site build job and run it simultaneously on both Heroku and SageMaker to ensure correctness and stability.\n   - **Full Transition:** Once confidence is gained with the SageMaker solution, deprecate the Heroku Scheduler jobs gradually and migrate other tasks as needed.\n\n### Rationale\n\n- **Scalability:** AWS SageMaker provides a scalable environment that can accommodate tasks of varying execution times without imposing strict limits, ensuring consistent task completion.\n- **Reliability:** Removing the strict execution time limit reduces the risk of incomplete builds and the associated downstream issues.\n- **Flexibility:** AWS's suite of tools, including SageMaker, EventBridge, and CloudWatch, offers a robust and flexible mechanism for scheduling, executing, and monitoring tasks.\n\nBy transitioning to AWS SageMaker, we not only resolve the immediate issue of build failures but also set a solid foundation for future scalability and reliability of our scheduled tasks.","GenTime":"2024-07-29 00:46:04"}
{"File Name":"dos-server\/adr-14-persistent-file-links.md","Context":"## Context\\nDOS returns internally generated unique identifiers for files it ingests.\\nThe URLs are roughly of the form (at the time of this ADR, plain integer identifiers are returned):\\nhttps:\/\/dos-stage.mitlib.net\/file?id=123e4567-e89b-12d3-a456-556642440000\\nOnce files are ingested, the identifiers are stored in ArchivesSpace.\\nThis may result in a situation where if URLs to files need to change (e.g., if\\nthe AWS bucket name for DOS changes), all links in ArchivesSpace need to be updated for\\nall ingested files.\\nAlthough DOS can be made to generate persistent identifies, it is desirable to externalize\\nthis functionality for reuse and portability considerations.\\nThe Handle System can be used for this purpose. Handles are already being used in\\nDome and DSpace, and there is organizational familiarity with the system.\\n\n## Decision\n","Decision":"Handle System will be used to generate persistent links. DOS will use the\\nHandle server API to generate handles. Handles will be returned when the\\nrelevant DOS end point is invoked.\\n","tokens":187,"id":3369,"Prediction":"## Decision\n\nWe will implement the use of the Handle System for generating persistent identifiers (PIDs) for files ingested into the Digital Object Store (DOS). \n\n### Rationale:\n\n1. **Persistence and Stability:** Handles provide a stable and persistent way to reference files, mitigating the problem of URL changes due to modifications in infrastructure (e.g., changes in AWS bucket names). \n\n2. **Organizational Familiarity:** Our organization already has experience and infrastructure in place for managing Handles, as seen with Dome and DSpace. This familiarity can reduce the learning curve and promote a smoother integration.\n\n3. **Portability and Reuse:** The externalization of the PID generation to the Handle System enhances reusability and portability, ensuring that the generated identifiers are not tightly coupled with DOS\u2019s internal workings.\n\n4. **Consistency Across Systems:** Using Handles across various systems provides consistency in how digital assets are referenced, making it easier to manage and link disparate systems and repositories.\n\n### Implementation Steps:\n\n1. **Handle Prefix Registration:** Ensure the appropriate Handle prefix is registered for DOS, distinct from those used for Dome and DSpace to avoid collisions.\n\n2. **Integrate Handle System:** Update the file ingestion workflow in DOS to integrate with the Handle System for PID generation.\n\n3. **Update ArchivesSpace:** Modify the integration with ArchivesSpace to store the newly generated Handles instead of the existing temporary URLs.\n\n4. **Migration of Existing References:** Develop a script or utility to update existing file references in ArchivesSpace to their corresponding new Handles.\n\n5. **Testing and Validation:** Conduct thorough testing to ensure the new identifiers work as expected and validate that references in ArchivesSpace are correctly resolved.\n\n6. **Documentation and Training:** Provide updated documentation and training for staff to ensure they are familiar with the new process and the benefits of using Handles.\n\nBy adopting Handles, we ensure long-term stability and consistency in referencing our digital objects, making our system more robust and easier to manage.","GenTime":"2024-07-29 00:46:29"}
{"File Name":"dos-server\/adr-11-exception-monitoring.md","Context":"## Context\\nAn application is needed for real time production error discovery and reporting. Sentry is currently being\\nused by DLS for various applications.\\n\n## Decision\n","Decision":"We will use Sentry for application exception monitoring.\\n","tokens":30,"id":3370,"Prediction":"## Decision\nWe will integrate Sentry into the application for real-time production error discovery and reporting. \n\n### Rationale:\n1. **Consistency**: As Sentry is already used across various applications by DLS, it helps maintain a consistent error monitoring and reporting infrastructure.\n2. **Efficiency**: Integrating with an already approved and tested tool minimizes the time required for setup and configuration.\n3. **Features**: Sentry provides robust capabilities for real-time error tracking, which meets the primary requirement of the application.\n4. **Familiarity**: The team is already familiar with Sentry, which can reduce the learning curve and accelerate the development process.\n5. **Integration**: Sentry offers seamless integration with various frameworks and languages, ensuring smooth incorporation into our existing tech stack.\n\n### Implementation:\n- Configure Sentry to capture and report errors at designated checkpoints within the application.\n- Set up appropriate alert thresholds and notification channels to ensure timely error reporting to development and operations teams.\n- Conduct testing to ensure Sentry is capturing and reporting errors correctly, and fine-tune configurations as necessary.\n- Train the team on how to interpret Sentry reports and logs for effective error resolution.\n\n### Expected Outcomes:\n- Improved visibility into production errors in real-time.\n- Faster response and resolution times for critical issues.\n- Enhanced reliability and user experience of the application.","GenTime":"2024-07-29 00:46:32"}
{"File Name":"dos-server\/adr-1-metadata.md","Context":"## Context\\nDigital objects need to have associated metadata for various use cases (refer to\\nthe requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural.\\nTo avoid \"duplication\" of descriptive metadata, it is desired that DOS not store descriptive metadata.\\n\n## Decision\n","Decision":"Descriptive metadata will not be stored by DOS.\\n","tokens":59,"id":3372,"Prediction":"## Decision\n\nThe system will be designed to reference external sources for descriptive metadata, rather than storing this information within the Digital Object Store (DOS) itself. This will be achieved by utilizing unique identifiers to link each digital object in the DOS to its corresponding descriptive metadata stored in an external metadata repository.\n\n### Key Implementation Points:\n1. **Unique Identifiers**: Each digital object in the DOS will have a unique identifier that will serve as a key to retrieve its descriptive metadata from an external source.\n2. **Metadata Repository**: A reliable and scalable external metadata repository will be identified or established to store the descriptive metadata. Possible options may include utilizing existing systems like DOI (Digital Object Identifier) systems, institutional repositories, or dedicated metadata databases.\n3. **Administrative and Structural Metadata**: While descriptive metadata will be externally referenced, the DOS will continue to store administrative and structural metadata directly within the system to ensure efficient management and retrieval.\n4. **APIs and Interfaces**: APIs or suitable interfaces will be developed to facilitate seamless integration and retrieval of descriptive metadata from the external repository based on the unique identifiers.\n5. **Performance Considerations**: The system will be designed to ensure minimal performance impact from external metadata retrieval, potentially through caching mechanisms or pre-fetching strategies.\n\n### Rationale:\n- **Avoiding Duplication**: By not storing descriptive metadata within the DOS, the system avoids the redundancy and potential inconsistencies that could arise from having multiple copies of the same metadata.\n- **Enhanced Metadata Management**: Storing descriptive metadata externally allows it to be managed, updated, and versioned separately from the digital objects, leading to better metadata quality and consistency.\n- **Flexibility**: The decision allows the DOS to remain flexible and adaptable to changes in descriptive metadata schemas or standards, as updates can be handled within the external repository without affecting the core DOS system.\n\n### Implementation Steps:\n1. Identify and evaluate potential external metadata repositories.\n2. Develop the schema for unique identifiers and ensure it aligns with existing standards.\n3. Design and implement the API or interface for metadata retrieval.\n4. Update the DOS architecture to use the unique identifiers for metadata linkage.\n5. Test the integration thoroughly to ensure reliability and performance.","GenTime":"2024-07-29 00:46:39"}
{"File Name":"dos-server\/adr-2a-java.md","Context":"## Context\\nAs DOS will be an important component of our collections platform,\\nthe software stack (web server, application container, programming language, database)\\nDOS uses to realize core functionality and to fulfill non-functional requirements\\nof scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and\\nmature ecosystem libraries can help meet the functional requirements easily. The Spring\\nframework also makes it relatively easy to create RESTful web services.\\n\n## Decision\n","Decision":"The core service will rely on Java and Spring framework as the basic stack for implementing core services.\\n","tokens":114,"id":3374,"Prediction":"## Decision\nWe have decided to adopt the following technology stack for the DOS (Digital Order System) component of our collections platform:\n\n1. **Programming Language**: Java\n   - Justification: Java is a mature and widely-used programming language that offers strong performance, reliability, and scalability. It is known for its extensive ecosystem, robust libraries, and community support, ensuring stability and long-term maintenance.\n\n2. **Framework**: Spring Framework\n   - Justification: The Spring framework is a proven and highly adopted framework within the Java ecosystem. It simplifies the development of RESTful web services and includes built-in support for various non-functional requirements such as security, transaction management, and dependency injection.\n\n3. **Web Server**: Apache Tomcat\n   - Justification: Apache Tomcat is a widely adopted, reliable, and performant web server that is well-suited for Java web applications. It is also supported by the Spring framework, ensuring seamless integration and operational stability.\n\n4. **Application Container**: Docker\n   - Justification: Docker will be used to containerize the application, providing consistency across development, testing, and production environments. Containers ensure the scalability and portability of the application, which is essential for meeting our performance and reliability requirements.\n\n5. **Database**: PostgreSQL\n   - Justification: PostgreSQL is a powerful, open-source relational database system known for its reliability, feature robustness, and performance. It also supports complex queries and indexing and has a strong community, which ensures continuous improvement and support.\n\nBy selecting this stack, we aim to ensure that the core functionality and non-functional requirements (scalability, performance, reliability, security, and backward compatibility) of the DOS are robustly met.","GenTime":"2024-07-29 00:46:46"}
{"File Name":"support-rota\/0004-add-rollbar-for-application-monitoring.md","Context":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n\n## Decision\n","Decision":"Use Rollbar to collect and manage our application errors.\\n","tokens":187,"id":3388,"Prediction":"## Decision\n\nWe will continue to use Rollbar as our primary tool for capturing and managing application errors, with the following refinements to address the current issues and improve overall efficiency:\n\n1. **Standardization of Environment Names**:\n    - We will standardize the environment naming conventions across all projects to ensure consistency between Rollbar and our Rails environments.\n    - The standardized naming convention will follow the format: `<environment>-<project>`. For example, `production-myapp`, `staging-myapp`.\n    \n2. **Configuration Management**:\n    - We will explicitly specify the `ROLLBAR_ENV` variable in our configuration files to ensure that the correct environment is used in Rollbar.\n    - Detailed documentation will be provided for configuring `ROLLBAR_ENV` to help teams easily set up and modify it as needed.\n\n3. **Integration with Other Tools**:\n    - We will review and document the use of other error tracking tools like Sentry for certain projects as necessary.\n    - When a different tool is required, a clear justification must be provided to ensure that it's the best fit for the project's needs.\n    - An integration layer can be considered to unify error tracking across multiple tools if needed for cohesive reporting.\n\n4. **Regular Review Process**:\n    - We will establish a bi-annual review process to reassess our error tracking solution, ensuring that Rollbar or any other tool in use continues to meet our evolving needs.\n\nBy implementing these steps, we aim to streamline our error tracking process, improve consistency across projects, and ensure that our debugging practices are efficient and effective.","GenTime":"2024-07-29 00:47:34"}
{"File Name":"support-rota\/0003-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n\n## Decision\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":3389,"Prediction":"## Decision\nWe will adopt a supported gem for managing environment variables centrally while providing safer access and the ability to stub values in our test suite. After evaluating the current options available, we have decided to use the dotenv and envyable gems together:\n\n1. **Dotenv**: This gem will be responsible for loading environment variables from a `.env` file into the ENV hash in a development and test context. This makes it easy to manage environment-specific variables without needing to set them locally or in a deployment pipeline.\n\n   - Benefits:\n     - It's widely adopted in the community and regularly maintained.\n     - It simplifies the setup for new developers by standardizing how environment variables are loaded.\n     - Provides a clear and central place for environment variables in the project.\n\n2. **Envyable**: This gem complements dotenv by adding a YAML-based configuration file that defines the required environment variables and their default values. This gem will allow us to:\n   - Define which environment variables are mandatory and provide default values.\n   - Fail fast during application startup if required environment variables are missing or misconfigured, providing clear error messages.\n   - Provide a straightforward way to stub environment variables in tests by leveraging its structure.\n\nBy combining Dotenv and Envyable, we will achieve:\n- Centralized management of environment variables.\n- Clear error reporting for missing or misconfigured variables during application startup.\n- The ability to define defaults and enforce required variable checks.\n- Flexibility to easily stub environment variables during testing without needing to mutate the ENV hash directly.\n\n### Implementation Plan\n1. **Add and configure Dotenv**:\n   - Add `dotenv-rails` to the Gemfile for development and test groups.\n   - Create a `.env` file to store environment variables required for development and testing.\n\n2. **Add and configure Envyable**:\n   - Add `envyable` to the Gemfile.\n   - Create a `config\/env.yml` with the required environment variables and their default values.\n\n3. **Setup initialization**:\n   - Configure the application initialization process to load environment variables via Dotenv.\n   - Integrate Envyable to enforce the required environment variables during the application startup process.\n\n4. **Update Tests**:\n   - Update the test suite to stub environment variables by accessing the configuration instead of directly manipulating the ENV hash.\n\n### Sample `config\/env.yml`\n```yaml\ndevelopment:\n  FOO: 'default_value'\n  BOX_ID: '1234'\n  \ntest:\n  FOO: 'test_value'\n  BOX_ID: '5678'\n  \nproduction:\n  FOO: \n  BOX_ID:\n```\n\n### Sample `config\/initializers\/envyable.rb`\n```ruby\nEnvyable.load(Rails.root.join('config', 'env.yml'), Rails.env)\n```\n\nBy following this decision, we ensure our application handles environment variables in a robust, maintainable, and test-friendly manner.","GenTime":"2024-07-29 00:47:40"}
{"File Name":"mario\/0008-use-one-s3-bucket-per-source.md","Context":"## Context\\nEach data source will need to upload one or more files to S3 in order to trigger processing. S3 events, which will drive Lambda execution (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), are configured at the bucket level. We may or may not have much control over the environment which is sending data to S3, for example, if it came directly from a vendor. At minimum we must be able to specify a bucket, but we should not assume we will have much more control than this.\\nEach data source will also need different processing. This implies the need to identify which source a data file came from.\\n\n## Decision\n","Decision":"Use one S3 bucket per data source.\\n","tokens":137,"id":3416,"Prediction":"## Decision\n\nWe will use a dedicated S3 bucket per data source to handle file uploads, enabling more straightforward management and triggering of events. For each data source:\n\n1. **Separate S3 Buckets**: Assign a unique S3 bucket. This helps in isolating data files, easing the identification of data sources, and customizing processing.\n2. **Bucket Naming Conventions**: Implement a naming convention that includes the data source identifier in the bucket name. For instance, `data-source-A-bucket`, `data-source-B-bucket`, etc.\n3. **Lambda Configuration**: Configure S3 events to trigger the appropriate AWS Lambda function based on the bucket. Each bucket's event will invoke a specifically designed Lambda that includes the logic for processing data files from that particular source.\n4. **Access Control**: Establish proper access controls and permissions for each bucket to ensure that only authorized systems or users can upload data files. If necessary, include IAM roles that vendors can assume for uploading files.\n5. **Environment Flexibility**: This structure supports flexibility in the environment, as each data source is isolated with its own configurations. There is no need for the sending environment to adhere to a strict format or protocol beyond basic S3 upload capabilities.\n6. **Monitoring and Logging**: Implement logging within each Lambda function to keep track of all processing activities per data source. This provides visibility into operations and aids in troubleshooting any issues related to data ingestion or processing.\n\nThis decision ensures modularity, security, and clarity in the handling and processing of data from multiple sources, while accommodating varying levels of control over the data upload process.","GenTime":"2024-07-29 00:49:17"}
{"File Name":"saas-plaform-tenant-identity-provider\/0002-use-a-hosted-identity-solution-over-a-self-hosted-one.md","Context":"## Context\\nWe need an identity provider for our platform. This could be a host (SaaS) solution vs a self hosted solution. While the Cost & Lockin is higher at scale with a SaaS the self hosted has a higher Management Effort & Less Security & Expensive Bootstraping. From an evolutionary architecture perspective it is not core to value generation and does not change rapidly[source](https:\/\/www.youtube.com\/watch?v=8bEsNT7jdC4&t=112s&index=57&list=WL) - identity will mostlikly the same (OpenIDConnect, Login page ...) but it will be hard to change in the future if you decided for one provider.\\n\n## Decision\n","Decision":"We will use a SaaS solution as we don't have the skill to host our own solution in the efficient quality. And from a evolutionary architecture perspective the system is commondity or support.\\n","tokens":142,"id":3421,"Prediction":"## Decision\n\nWe will use a SaaS identity provider for our platform.\n\n### Rationale\n\n1. **Core Value Focus**:\n   - Identity management is not core to our value generation. By using a SaaS solution, we can focus our resources and efforts on building features that directly contribute to our platform's unique value proposition.\n\n2. **Management and Maintenance**:\n   - SaaS solutions generally offer lower management effort. They provide regular updates, security patches, and compliance measures, reducing the burden on our team to maintain and secure the identity service.\n\n3. **Security and Compliance**:\n   - Reputable SaaS identity providers typically offer robust security measures and compliance certifications that would be expensive and complex to achieve with a self-hosted solution.\n\n4. **Scalability and Flexibility**:\n   - While a hosted solution may incur higher costs at scale and potentially lead to vendor lock-in, the shifting and evolving user requirements can be better managed without having to deal directly with infrastructural burdens of a self-hosted solution.\n\n5. **Future Changes**:\n   - Given that identity solutions are crucial and relatively stable components, initially choosing a SaaS provider allows us to adopt industry standards (like OpenIDConnect). Although it might be challenging to change providers in the future, the initial integration and operation with a reliable SaaS provider will likely provide a smoother user experience and operational stability.\n\nBy selecting a SaaS solution, we mitigate bootstrap costs, reduce management effort, enhance security, and allow our team to concentrate on delivering core platform functionalities.","GenTime":"2024-07-29 00:49:34"}
{"File Name":"mediawiki-extensions-Popups\/0008-enable-minification-of-bundle-with-uglifyjs.md","Context":"## Context\\nSources are currently minified by JSMin producing a bundle bigger than\\nnecessary. We want to produce a JS bundle as small as possible to ship less\\ncode to clients.\\nGiven we have a pre-compilation step in Popups, we can leverage node based\\ntooling without penalty, so using UglifyJS to minify the code is an option.\\nWe performed some analysis to see the differences in asset size between the two\\ntools. See:\\n* [Minifying assets with uglifyjs][1]\\nResults with the current codebase as of today 12th of May 2017 are that uglify\\nproduces a 40% smaller bundle, and a 25% smaller bundle when using gzip.\\nThis results are stable through the last 3 months, the same test has been\\nperformed multiple times. See [Compressed JS comparison][2]\\n\n## Decision\n","Decision":"Webpack's production mode has been enabled for `npm run build`, and thus the\\ncompiled bundle will be minified in production with UglifyJS.\\nSee [Enable production settings for the production bundle][3]\\n","tokens":189,"id":3428,"Prediction":"## Decision\nWe will switch our JS minification tool from JSMin to UglifyJS.\n\n**Rationale:**\n1. **Bundle Size Reduction:** UglifyJS consistently produces a smaller bundle compared to JSMin. Results show a 40% reduction in bundle size and a 25% smaller bundle when compressed with gzip.\n2. **Stable Performance:** The tests over the last three months have consistently shown these reductions, indicating that UglifyJS performs reliably with our current codebase.\n3. **Infrastructure Compatibility:** The existing pre-compilation step in Popups allows us to leverage node-based tooling without any additional overhead or penalty, making UglifyJS a viable option.\n\n**Expected Benefits:**\n1. **Reduced Load Times:** A smaller JS bundle will lead to faster download and load times for clients, improving overall user experience.\n2. **Better Performance:** Lower payload size will reduce bandwidth usage and enhance performance, especially on slower networks.\n3. **Future Scalability:** With a more efficient minification tool in place, future additions to the codebase will also benefit from reduced bundle sizes.","GenTime":"2024-07-29 00:49:55"}
{"File Name":"k8s-vagrant-centos-cluster\/0002-why-not-use-kubeadm.md","Context":"## Context\\nkubeadm can be used to setup Kubernetes cluster with apiserver, etcd, controller, scheduler...\\n\n## Decision\n","Decision":"Setup Kubernetes cluster with apiserver, etcd, controller, scheduler without using kubeadm and docker\\n","tokens":26,"id":3440,"Prediction":"## Decision\n\nWe have decided to use `kubeadm` to set up and manage our Kubernetes cluster.\n\n### Reasons for the Decision:\n1. **Simplicity and Ease of Use**: `kubeadm` streamlines the complex process of setting up Kubernetes components such as the API server, etcd, controllers, and schedulers, making it accessible even to users who may not have extensive experience with Kubernetes.\n\n2. **Flexibility**: While `kubeadm` provides a straightforward setup path, it retains the flexibility required for advanced configurations, allowing us to fine-tune and customize our Kubernetes clusters as per our specific requirements.\n\n3. **Community and Support**: `kubeadm` is widely adopted and actively maintained by the Kubernetes community, ensuring that we have access to a wealth of resources, community support, and continuous updates in line with the latest Kubernetes releases.\n\n4. **Alignment with Best Practices**: Using `kubeadm` allows us to follow Kubernetes best practices and recommendations, ensuring a robust and reliable cluster setup.\n\n5. **Interoperability**: `kubeadm` is designed to work with a range of infrastructure setups, promoting interoperability and reducing dependencies on specific cloud providers or environments.\n\n### Implementation Plan:\n1. **Preparation**: Ensure that all nodes meet the system requirements for Kubernetes and have the necessary prerequisites installed, including Docker and other dependencies.\n2. **Initialization**: Use `kubeadm init` to initialize the master node, followed by `kubeadm join` commands to add worker nodes to the cluster.\n3. **Configuration**: Apply required network configurations, install a network plugin, and set up role-based access controls (RBAC) and other necessary security measures.\n4. **Validation**: Run tests and validation scripts to confirm that the cluster is successfully set up and all components are functioning correctly.\n5. **Documentation**: Document the entire setup process and configurations to ensure future maintainability and replication ease.\n\nBy choosing `kubeadm`, we aim to leverage its powerful yet straightforward orchestration capabilities to accelerate our Kubernetes deployment while adhering to industry standards and best practices.","GenTime":"2024-07-29 00:50:33"}
{"File Name":"community\/dr-017-Application-Integration-without-Wormhole-Connector.md","Context":"## Context\\nThe Wormhole Connector is a part of the Application Integration. It is used by external systems deployed inside customers' private datacenter to establish a secure connection tunnel to the Kyma platform.\\n\n## Decision\n","Decision":"The decision is to get rid of the Wormhole Connector at this stage of the project. It can be an optional component of the Application Connector but a secure tunnel. For now, the Application Connector should focus on stability and on providing the core business features.\\n","tokens":42,"id":3448,"Prediction":"## Decision\nWe have decided to implement a highly secure and scalable Wormhole Connector utilizing a combination of TLS encryption, mutual authentication, and API gateway for enforcing security policies. This implementation will also leverage modern cloud-native infrastructure to ensure high availability and resilience.\n\n#### Rationale:\n1. **Security**: By using TLS encryption and mutual authentication, we ensure data integrity and confidentiality as well as verify the identities of both communicating parties.\n2. **Scalability**: Leveraging cloud-native infrastructure like Kubernetes ensures we can scale the Wormhole Connector horizontally to accommodate varying loads.\n3. **Resilience**: The cloud-native approach also aids in building a more resilient system with features like auto-scaling, health checks, and automated failover mechanisms.\n4. **Policy Enforcement**: The integration of an API gateway provides a centralized point to enforce security policies such as rate limiting, IP whitelisting, and request authentication\/authorization.\n\n#### Implementation Steps:\n1. Establish a secure configuration for TLS and mutual authentication.\n2. Set up an API gateway to handle incoming connections and enforce security policies.\n3. Deploy the Wormhole Connector on a Kubernetes cluster ensuring auto-scaling and resilience mechanisms are in place.\n4. Conduct thorough security testing and performance benchmarking to validate the setup.\n5. Provide detailed documentation and support for external systems to integrate seamlessly with the Wormhole Connector.","GenTime":"2024-07-29 00:51:01"}
{"File Name":"community\/dr-012-Prettier_as_JavaScript_code_formatter.md","Context":"## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https:\/\/prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n\n## Decision\n","Decision":"The decision is to use Prettier as the only code formatter for JavaScript-based projects.\\n","tokens":301,"id":3451,"Prediction":"## Decision  \nWe will adopt Prettier as the official code formatter for the project.\n\n### Key Points:\n1. **Enforcement of Unified Code Style:** Prettier will ensure consistent code formatting across all parts of the project, enhancing readability and maintenance.\n\n2. **Scope of Formatting:** Prettier will be configured to apply to JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON files, which are used throughout our codebase.\n\n3. **Integration with Development Tools:**\n   - **IDEs:** Prettier will be integrated as a plugin in the commonly used IDEs within the team.\n   - **Pre-commit Hook:** Prettier will be set up as a pre-commit hook to automatically format code before it is committed to the repository.\n   - **Standalone CLI:** The CLI version of Prettier will be available for any ad-hoc formatting needs.\n\n4. **Configuration and Customization:** \n    - Prettier\u2019s configuration will be stored in a `.prettierrc` file at the root of the project.\n    - The configuration will adhere to the standard settings with minimal customization specific to our project requirements.\n\n5. **Documentation and Onboarding:**\n    - Documentation will be provided to ensure all team members understand how to effectively use Prettier.\n    - New team members will be onboarded with training on the use of Prettier within their development environment.\n\n### Expected Outcomes:\n- **Code Consistency:** All code contributions will adhere to a uniform style, reducing the cognitive load on reviewers and maintainers.\n- **Reduced Noise in Diffs:** Changes will focus on substantive modifications rather than stylistic differences, making code reviews more efficient.\n- **Efficiency:** Developers will save time by not having to manually format code, allowing them to focus on writing quality code and logic.\n\n### Next Steps:\n1. Set up the Prettier configuration file and define the project's formatting rules.\n2. Integrate Prettier into the CI\/CD pipeline to enforce formatting on all pull requests.\n3. Update development environment setup guides to include Prettier integration.\n4. Conduct a one-time reformat of the existing codebase to align with Prettier\u2019s formatting rules.\n5. Provide training sessions and documentation for all team members on using Prettier.","GenTime":"2024-07-29 00:51:10"}
{"File Name":"community\/0007-github-actions.md","Context":"## [Context](https:\/\/docs.google.com\/document\/d\/1zRy8OCZ_JJpbs6scwrvx5PYkWwaWbR-5DLuq3AujhBA\/edit)\\nLibero products need automated builds for pull requests and release candidates.\\nTravis CI has been [acquired](https:\/\/techcrunch.com\/2019\/01\/23\/idera-acquires-travis-ci\/) by a private equity firm and has an [uncertain future](https:\/\/twitter.com\/ReinH\/status\/1098663375985229825). We also have run into performance bottlenecks of [5 concurrent jobs](https:\/\/travis-ci.com\/plans) in the open source offer.\\nGithub Actions is a Github-native general workflow system that can provide CI\/CD capabilities; it has been in general availability [since November](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/). It offers fully managed, sandboxed environments and a per-repository limit to concurrency.\\n\n## Decision\n","Decision":"We will create Github Actions builds for all new projects.\\nWe will port existing Travis CI builds of maintained projects onto Github Actions, on an as-needed basis.\\n","tokens":222,"id":3456,"Prediction":"## Decision\n\nWe will transition our CI\/CD pipelines from Travis CI to GitHub Actions for the following reasons:\n\n1. **Future Stability**: The acquisition of Travis CI by a private equity firm introduces uncertainty about its future. GitHub Actions, as a GitHub-native solution, appears to have a more stable and predictable future.\n\n2. **Performance and Scalability**: GitHub Actions offers more flexible concurrency limits, which can help overcome the bottlenecks we face with Travis CI's 5 concurrent job limitation. This will enhance our ability to handle more extensive and parallelized build and test workflows.\n\n3. **Integration and Ecosystem**: As a native GitHub feature, GitHub Actions seamlessly integrates with our existing GitHub repositories, making configuration and management easier. Additionally, it supports a rich ecosystem of pre-built actions and community contributions.\n\n4. **Cost and Resource Efficiency**: GitHub Actions likely provides a more cost-effective CI\/CD solution, considering that the managed environments are part of the existing GitHub infrastructure and may align better with our workflow requirements.\n\nWe will initiate a phased migration plan, starting with less critical repositories to minimize disruption. This will include updating CI\/CD configuration files, testing new workflows, and training the team on GitHub Actions where required.","GenTime":"2024-07-29 00:51:25"}
{"File Name":"community\/dr-013-Allowed_UI_technology_in_the_Console_UI.md","Context":"## Context\\nThe Console UI is the administration view of Kyma and is a core part of the product.\\nTo simplify the development process in Kyma, the number of used programming languages and frameworks is limited. It makes the collaboration easier and allows the developers to specialize in a chosen technology.\\nFollowing this principle, the Go language is used as the only programming language for the back-end functionality of the core Kyma features.\\nBy design, the Console UI can be extended using plugins written in any framework, developed either internally or by customers or partners. The interface between the Console UI and the plugins is based on HTML and JavaScript but the approach is to provide libraries for the most popular and well-adapted UI frameworks, and gather knowledge and experience in using them.\\n\n## Decision\n","Decision":"The decision is to allow the [React](https:\/\/reactjs.org) and [Angular](https:\/\/angular.io) frameworks in combination with the JavaScript and TypeScript programming languages to build the core views in the Kyma Console UI.\\n","tokens":159,"id":3460,"Prediction":"## Decision\n\nTo align with Kyma's principle of simplifying the development process by limiting the number of used programming languages and frameworks, we will adopt the following decisions for the Console UI:\n\n1. **Back-End Development with Go**: \n   - The back-end functionality of the Console UI will exclusively utilize the Go programming language. This approach ensures consistency with the core Kyma features and leverages existing expertise within the development team.\n\n2. **Front-End Interface Based on HTML and JavaScript**: \n   - The front-end interface between the Console UI and its plugins will be based on standard HTML and JavaScript. This choice facilitates flexibility and broader adoption for both internal and external developers.\n\n3. **Preferred Libraries for UI Frameworks**:\n   - We will provide and recommend libraries for well-supported, popular UI frameworks such as React, Vue.js, and Angular. This support will encourage best practices, enhance maintainability, and streamline the onboarding process for developers.\n    \n4. **Documentation and Best Practices**:\n   - Comprehensive documentation and best-practice guidelines will be developed and maintained to support internal teams, customers, and partners in creating plugins. This will include examples, code snippets, and detailed instructions for leveraging the provided libraries.\n\n5. **Plugin Extensibility and Flexibility**:\n   - While the primary UI framework recommendations will be provided, flexibility will be ensured so that plugins can be developed using other frameworks if necessary. This ensures that the console remains adaptable to a wide range of developer preferences and needs.\n\nBy adhering to these decisions, the development process for the Console UI in Kyma will maintain consistency, encourage expertise within specific technologies, and support a diverse ecosystem of plugins.","GenTime":"2024-07-29 00:51:43"}
{"File Name":"community\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3466,"Prediction":"## Decision\n\nTo facilitate the recording of architectural decisions, we will implement an Architectural Decision Record (ADR) process. This process will include the following steps:\n\n1. **Template Selection**: We will use a well-defined ADR template that includes sections such as Context, Decision, Status, Consequences, etc. A common template will ensure consistency.\n   \n2. **Tooling**: ADRs will be documented using Markdown files stored in a version control system (e.g., Git). This step ensures that all decisions are versioned, traceable, and easily accessible to the team.\n\n3. **Frequency and Timing**: ADRs will be recorded at key decision points within the project lifecycle. This includes initial project setup, significant changes to architecture, and after major design meetings.\n\n4. **Review and Approval**: Each ADR will be reviewed by a lead architect and approved by relevant stakeholders to ensure that the decision is well thought out and agreed upon.\n\n5. **Communication**: Approved ADRs will be communicated to the entire development team during regular meetings and stored in an easily accessible repository for reference.\n\nBy adopting this ADR process, we aim to improve decision-making transparency, preserve architectural knowledge, and ensure alignment across the project team.","GenTime":"2024-07-29 00:52:04"}
{"File Name":"gp-finder\/0006-use-prometheus-for-exposing-metrics.md","Context":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n\n## Decision\n","Decision":"We will use Prometheus to monitor and alert on the state of the application.\\n","tokens":86,"id":3469,"Prediction":"## Decision\n\nWe will implement an application performance monitoring (APM) solution to continuously gather key performance indicators (KPIs) from our application. This monitoring system will be integrated directly into the application to avoid the overhead and inefficiencies associated with log scraping. The selected APM tool will:\n\n1. **Provide Real-Time Monitoring:** Allow us to capture real-time data on how the application is performing.\n2. **Track Key Metrics:** Collect specific KPIs such as response time, error rates, transaction volumes, database query performance, and server resource usage.\n3. **Offer Alerting Mechanisms:** Include customizable alerting capabilities to notify the team of any anomalies or performance degradation.\n4. **Enable Root Cause Analysis:** Provide detailed insights to help troubleshoot and pinpoint issues efficiently.\n5. **Support Scaling:** Be capable of handling the scale we anticipate as our application user base grows.\n6. **Ease of Integration:** Be relatively straightforward to integrate with our existing tech stack without causing significant performance overhead or requiring substantial modifications.\n\nBy implementing this APM solution, we aim to ensure continuous monitoring of application health, enabling a proactive approach to maintaining optimal performance and delivering a reliable, high-quality experience to our users. This decision supports our goal of ensuring the application\u2019s health is sound and facilitates rapid response to any issues that may arise.","GenTime":"2024-07-29 00:52:16"}
{"File Name":"gp-finder\/0002-use-express-web-framework.md","Context":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum. The decision of using Express and Node.js\\nwhere made at (platform level)[https:\/\/github.com\/search?q=topic%3Aconnecting-to-services+org%3Anhsuk&type=Repositories],\\nand the team had experience in creating these types of applications.\\n\n## Decision\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":96,"id":3470,"Prediction":"## Decision\n\nGiven the context, the decision taken is to use the Express framework coupled with Node.js for the development of the web application. This decision leverages the team's existing experience with Express and Node.js, streamlining development and ensuring that effort is focused on domain-specific problems rather than on addressing low-level language issues.\n\nThis choice aligns with the precedent set at the platform level and should expedite the development process due to the familiarity of the team with these technologies. Additionally, Express provides the necessary abstraction over low-level language features, enabling rapid development of robust web applications while adhering to best practices in web development.","GenTime":"2024-07-29 00:52:18"}
{"File Name":"gp-finder\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3475,"Prediction":"## Decision\n\nWe will implement an Architectural Decision Record (ADR) system for this project to document all significant architectural decisions. This system will serve as a structured approach to capturing the context, decision, and consequences of these decisions, ensuring a clear and traceable history of architectural changes.\n\n### Key Points of the Decision:\n1. **Template Usage**: We will adopt a standardized ADR template that includes sections for context, decision, consequences, and any alternatives considered.\n2. **Version Control Integration**: ADRs will be stored in our version control system (e.g., Git) alongside the codebase to ensure they are versioned and maintainable.\n3. **Regular Review**: ADRs will be regularly reviewed and updated as necessary throughout the project lifecycle to reflect any changes in decisions or new information.\n4. **Team Collaboration**: All team members will be encouraged to contribute to and consult ADRs to maintain coherence and alignment across the project.\n\nBy implementing this ADR system, we aim to provide transparency, facilitate better communication among team members, and ensure informed decision-making throughout the development process.","GenTime":"2024-07-29 00:52:36"}
{"File Name":"devops-challenge\/0010-feat-ci-cd-with-circleci.md","Context":"## Context\\nTime to work on the CI\/CD solution.\\nI need a job manager to build, test and deploy the apps to the kubernetes\\ncluster.\\nI know there is a lot of documentation about jenkins, but I have been working\\nin the last years with other solutions like:\\n* SolanoCI (now closed)\\n* CircleCI\\nSo I need to see if there is a way to use my knowledge in CircleCI or not.\\nBesides, CircleCI has a free plan very useful for testing.\\nI know there is a jenkins-x product, but I don't have a clear idea about\\nit.  I installed it and spent couple of hours to make it work (jx) but I got\\nan error downloading kops, and I couldn't go further.\\n\n## Decision\n","Decision":"Try first with CircleCI to see if I can deploy the apps to kubernetes.\\nAs I'm using a monorepo, all the apps are within the same repository, so we\\ncannot separate the building process of each microservice.  All them will be\\nbuild and deployed as one.\\nBut if nothing has change in the app, then the building process will be faster.\\nTesting will be peformed on all apps secuentially, and some tests could be\\nperformed on all services without implementing mockups.\\nThis way, when a event is received by the CI, a script will be executed to\\nexecute the actions on each app.\\n","tokens":168,"id":3482,"Prediction":"## Decision\n\nBased on the context provided, the decision is to move forward with CircleCI for the CI\/CD implementation for the following reasons:\n\n1. **Familiarity and Expertise**: The team has substantial experience with CircleCI, which will reduce the learning curve and troubleshooting time compared to Jenkins or Jenkins-X.\n2. **Free Plan Availability**: CircleCI offers a free plan which is beneficial for testing purposes, potentially reducing costs during the initial phases of the project.\n3. **Ease of Integration**: CircleCI provides a straightforward setup process that matches the team's existing skill set, allowing for quicker onboarding, setup, and iterations compared to the issues encountered with Jenkins-X.\n4. **Documentation and Community Support**: While Jenkins has more documentation, the team's current and past experience with CircleCI outweighs the potential need to rely solely on documentation, coupled with the stability and support CircleCI offers.\n\nThe decision to use CircleCI aligns with the current expertise, potentially reducing setup times, allowing for quicker deployments, and making use of cost-effective plans for early development and testing phases.","GenTime":"2024-07-29 00:52:55"}
{"File Name":"devops-challenge\/0009-doc-about-monorepo.md","Context":"## Context\\nMicroservices are complex because they are small and can use different languages\\nand different stacks to run: java, spring, jakarta, python, flask, tornado, ruby\\nrails, sinatra, mysql, postgresql, mongodb, redis...\\nThere are so heterogeneous that sometimes we need to set a common way to work with\\nthem.  Each microservice has a different life-cycle, some are update more\\nfrequently, others are not.\\nUsually, when apps differs in their life-cycle speed, incompatibilities will come\\nup.  Those incompatibilities can make the system get down.  And keeping an\\nupdated and acurated matrix of compatibilities is a pain.\\nA way to minimize this risks is to put all code in the same repository, but not as\\na monolithic application, but in separate folders.  This has some advantages\\nlike:\\n- all code is tested at the same time\\n- transversal refactors can be easy\\n- compatibility matrices are simplified\\n\n## Decision\n","Decision":"To use a monorepo to keep all code together and deploy it at the same time.\\n","tokens":210,"id":3483,"Prediction":"## Decision\nAdopt a mono-repository (monorepo) approach for managing all microservices' code.\n\nRationale:\nGiven the complexity and heterogeneity of managing multiple microservices with different tech stacks, a mono-repo setup can significantly simplify operations. By consolidating all microservices' code into a single repository but maintaining them in separate folders, we can achieve the following benefits:\n\n1. **Unified Testing:** All code can be tested simultaneously, ensuring that changes in one service do not inadvertently break another. This facilitates a more robust and integrated CI\/CD pipeline.\n   \n2. **Simplified Refactoring:** Transversal refactors become more manageable because all services are within the same repository. Developers can make concurrent changes across multiple services more efficiently and with less risk of introducing inconsistencies.\n   \n3. **Compatibility Management:** A single repository allows for a more streamlined and simplified approach to managing compatibility between different services. This can help mitigate the risks associated with differing life-cycle speeds and reduce downtime due to incompatibilities.\n   \n4. **Dependency Management:** Shared libraries and common dependencies can be managed more easily, promoting reusability and consistency across services.\n\n5. **Single Source of Truth:** A monorepo provides a single source of truth for all microservice code, which aids in better version control, rollback, and audit trails.\n\nImplementation Plan:\n1. Structure the monorepo with clear and distinct folders for each microservice.\n2. Implement a robust CI\/CD pipeline to handle the unified testing and deployment processes.\n3. Use tools and strategies (e.g., Bazel, Lerna) that support monorepo management to maintain performance and organization.\n4. Establish coding standards and guidelines to ensure that all teams follow consistent practices within the monorepo framework.","GenTime":"2024-07-29 00:53:00"}
{"File Name":"devops-challenge\/0001-record-architecture-decisions.md","Context":"## Context\\nThe goal of this challenge is to implement a support plaform for a\\nmicroservices architecture with several requirements:\\n* is scalable on load\\n* is fully automated\\n* identifies Single Point of Failures\\n* implements resilience and fault tolerance mechanisms\\nWe need to record the architectural decisions made on this project as\\na way to document all the thinking and building process.  This will help\\nothers to answer their wondering questions about why and how this solution was\\nimplemented.\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":112,"id":3488,"Prediction":"## Decision\nTo address the requirements and ensure the effective implementation of the microservices support platform, we will adopt the following architectural decisions:\n\n1. **Scalable Infrastructure**: We will leverage cloud-native solutions in our infrastructure. This includes using Kubernetes for container orchestration, which allows automatic scaling of microservices in response to load changes. Additionally, adopting a managed database service will ensure that our data storage solutions can scale automatically based on demand.\n\n2. **Full Automation**: Implement continuous integration and continuous deployment (CI\/CD) pipelines using tools like Jenkins, GitLab CI, or GitHub Actions. Infrastructure as Code (IaC) using Terraform or AWS CloudFormation will automate the provisioning and management of the infrastructure.\n\n3. **SPoF Identification**: Incorporate monitoring and alerting tools such as Prometheus and Grafana to continuously monitor the health of the system and identify potential single points of failure. This will enable proactive identification and remediation of any potential risks.\n\n4. **Resilience and Fault Tolerance**: Integrate resilience patterns such as circuit breakers (using libraries like Hystrix or Resilience4j), retries with exponential backoff, and fallbacks for service communications. Additionally, use service mesh technologies like Istio to provide enhanced observability, traffic management, and fault tolerance features.\n\n5. **Documentation of Decisions**: Maintain an Architectural Decision Record (ADR) repository where all decisions are documented with the reasoning behind each choice. This repository will be version-controlled and regularly updated to ensure it reflects the current architecture and decisions.\n\nBy implementing these decisions, we ensure our microservices architecture is scalable, automated, fault-tolerant, and well-documented, thereby meeting the project requirements comprehensively.","GenTime":"2024-07-29 00:53:15"}
{"File Name":"git-en-boite\/0016-handle-concurrent-git-operations","Context":"## Context\\nConcurrent git operations can fail because git is locking the repo. The current implementation is waiting for jobs to finish, but multiple workers in different containers can still take jobs in the queue and proccess them in parallel.\\n\n## Decision\n","Decision":"Reduce worker concurrency to 1 by using a single container (for now).\\n","tokens":47,"id":3497,"Prediction":"## Decision\n\nTo handle concurrent git operations and avoid repository locking issues, we will implement a distributed locking mechanism using a centralized lock manager. Redis will be employed as the lock manager due to its simplicity, performance, and widespread support.\n\n### Steps Involved:\n\n1. **Redis Integration**: \n   - Add Redis as a dependency in the project.\n   - Ensure Redis server is set up and configured appropriately.\n\n2. **Lock Implementation**:\n   - Implement a locking mechanism using Redis's `SETNX` (SET if Not eXists) command or its Redlock algorithm for distributed environments.\n   - Each git operation will first attempt to acquire a lock before proceeding. If the lock is not available, the operation will wait and retry after a short interval.\n\n3. **Lock Acquisition and Release**:\n   - Before starting a git operation, a worker will attempt to set a lock key in Redis with a short expiration time to avoid deadlocks.\n   - Once the operation is complete, the worker will release the lock by deleting the key from Redis.\n\n4. **Retry Mechanism**:\n   - Implement a retry mechanism with exponential backoff for acquiring the lock if it's not immediately available.\n   \n5. **Graceful Handling**:\n   - Ensure that if a worker crashes or an operation fails, the lock will eventually expire, allowing other workers to proceed with their operations.\n\n### Benefits:\n- **Reduced Failures**: Prevents concurrent git operations from causing repository locking issues.\n- **Scalability**: Allows multiple workers to manage jobs in a queue efficiently.\n- **Resilience**: Handles lock contention gracefully, reducing the likelihood of operation failures.\n\n### Impact:\n- There will be an increased complexity in the codebase due to the addition of distributed lock management.\n- Redis introduces an additional component to monitor and maintain.\n- Slight performance overhead due to lock acquisition and release, but it's negligible compared to the benefits of having a robust locking mechanism.\n\n### Alternatives Considered:\n- Using file-based locks within the repository: Not feasible in a distributed containerized environment.\n- Centralized job processing: Would require significant rewrites of the job handling logic and reduce the benefits of distributed processing.\n\nBy integrating Redis for distributed locking, we aim to robustly manage concurrent git operations across multiple distributed workers, ensuring smooth and reliable processing.","GenTime":"2024-07-29 00:53:54"}
{"File Name":"git-en-boite\/0009-prefer-dugite-over-raw-git-commands-in-tests.md","Context":"## Context\\nThe build failed because the version of git used in CircleCI was not what we expected.\\n\n## Decision\n","Decision":"Use `GitProcess.exec` even in tests that need to lean on the git process.\\n","tokens":22,"id":3504,"Prediction":"## Decision\nUpgrade the version of Git used in the CircleCI configuration to match the expected version. This will involve updating the CircleCI configuration file (usually `.circleci\/config.yml`) to specify the desired Git version explicitly. Ensure that this version is consistent with the development and production environments to avoid any discrepancies. Additionally, set up automated tests to verify the Git version during the build process to prevent similar issues in the future.","GenTime":"2024-07-29 00:54:18"}
{"File Name":"git-en-boite\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3505,"Prediction":"## Decision\nWe will implement a structured Architectural Decision Record (ADR) for documenting our architectural decisions throughout the project. \n\nThe ADR will follow a standardized format:\n1. **Title:** A descriptive title for the decision.\n2. **Context:** A detailed explanation of the circumstances surrounding the decision.\n3. **Decision:** A clear and concise statement of the decision made, including alternative options considered.\n4. **Consequences:** Implications of the decision, both positive and negative.\n\nThis structured approach ensures consistency, traceability, and clarity in documenting architectural choices, ultimately enhancing communication among the team and aiding in future decision-making processes.","GenTime":"2024-07-29 00:54:20"}
{"File Name":"caia\/0011-pin-mbtest-library-to-v2.5.1.md","Context":"## Context\\nOn March 22, 2021, it was discovered that the \"caia\" Jenkins builds were\\nfailing. All the failing tests were failing with the following error, related\\nto a \"get_actual_requests\" method call:\\n```\\nTypeError: 'generator' object is not subscriptable\\n```\\nThe \"caia\" build was last successful in Jenkins on October 7, 2020. No builds\\nwere performed again until March 22, 2021, as there was no development work\\nbeing done on the project.\\nBuilds were made on March 22, 2021 because of a move to the\\n\"GitHub organization\" pipeline in LIBITD-1880, which triggered rebuilds in all\\nexisting projects.\\nWhen the last successful build was made in October, the \"mbtest\" library\\n([https:\/\/github.com\/brunns\/mbtest](mbtest)) was at v2.5.1. In v2.5.2, the\\n\"src\/mbtest\/server.py\" file was modified, changing the \"get_actual_requests\"\\nmethod signature (see [this commit e398f2f1f32420](mbtest_commit)). from:\\n```\\ndef get_actual_requests(self) -> Mapping[int, JsonStructure]:\\n```\\nto\\n```\\ndef get_actual_requests(self) -> Iterable[Request]:\\n```\\nThe change from a Mapping to an Iterable is the cause of the error in the tests.\\n\n## Decision\n","Decision":"The simplest solution for the moment is to \"pin\" the version of the \"mbtest\"\\nlibrary to v2.5.1 in the \"setup.py\" file. This will preserve the current\\nbehavior, until further \"caia\" development warrants additional testing.\\n","tokens":315,"id":3507,"Prediction":"## Decision\n\nAfter thorough analysis, the decision is made to pin the \"mbtest\" library to version 2.5.1 for the \"caia\" Jenkins builds to restore build stability. Given the timeline, it's clear that the upgrade to \"mbtest\" version 2.5.2 introduced a breaking change in the \"get_actual_requests\" method, causing the 'TypeError: 'generator' object is not subscriptable' error in the test suite.\n\n### Rationale:\n\n1. **Continuous Integration Stability:** Ensuring that our CI pipeline remains stable and predictable is crucial. By pinning to version 2.5.1, we align the build process with the last known stable version, providing immediate relief from the failing tests.\n  \n2. **Project Timeline:** Since there was a significant gap (October 2020 to March 2021) in development and builds for the \"caia\" project, it's practical to focus on restoring the previous stable state to minimize disruption and facilitate ongoing or new development efforts.\n\n3. **Future Proofing:** To manage dependencies effectively, we will update our dependency documentation to reflect this pinning decision. Additionally, any plans to upgrade to a newer version of \"mbtest\" will require thorough review and adjustment of the existing test suites to accommodate the changes in \"get_actual_requests\".\n\n### Implementation Steps:\n\n1. **Update Dependency Specification:**\n   - Modify the project's dependency file (e.g., `requirements.txt`) to specify \"mbtest==2.5.1\".\n  \n2. **Trigger Jenkins Rebuilds:**\n   - Rebuild the \"caia\" project in Jenkins to ensure the builds pass successfully with the specified version of \"mbtest\".\n\n3. **Document Change:**\n   - Update the project documentation and ADR documentation to reflect the decision and the context for future reference and audits.\n\n4. **Plan for Future Upgrades:**\n   - Consider scheduling a task to evaluate and refactor the test suite to be compatible with later versions of \"mbtest\". This might include type-checking and revisiting the usage of subscriptable generator objects.\n\nBy following this approach, we ensure the immediate return to a stable build environment while preparing for future upgrades in a controlled manner.","GenTime":"2024-07-29 00:54:31"}
{"File Name":"tracking-consent-frontend\/0007-use-eslint-for-linting-and-formatting.md","Context":"## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n\n## Decision\n","Decision":"* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\nChosen option: ESLint + Airbnb\\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https:\/\/github.com\/airbnb\/javascript)\\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\\nit was overcome with config updates.\\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\\nchose to use Typescript for Tracking Consent but we usually use Javascript.\\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\\nthis is also a common feature which most IDEs have.\\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\\nwere committing.\\n### Positive Consequences\\n* Code style within a file will be consistent\\n* Files within a project will be consistent with each other\\n* When widely applied this Javascript\/Typescript files between projects will be consistent\\n* IDE will reformat for us which takes away the need for us to do it ourselves\\n* Code reviews will focus less on code format and more on the details of the changes made\\n* Builds will fail if format isn't followed which guarentees consistency\\n### Negative Consequences\\n* Builds will fail if format isn't followed which can disrupt focus\/workflow (mitigated by pre-commit hooks)\\n","tokens":133,"id":3521,"Prediction":"## Decision\n\nWe decided to standardize our JavaScript formatting by adopting Prettier, a widely-used code formatter with a set of established rules that minimize configuration overhead and personal preference conflicts. \n\n### Decision Rationale\n1. **Prettier Compatibility**: Prettier integrates well with a variety of IDEs and code editors, helping ensure consistency across the team's development environments.\n2. **Reduction in Manual Formatting Effort**: By using Prettier's autoformatting capabilities, we cut down on the manual effort involved in formatting code.\n3. **Consistency in Codebase**: Prettier helps enforce a uniform style throughout the codebase, making the code easier to read and maintain.\n4. **Focus on Code Quality**: Automated formatting allows the team to focus code reviews on logical and architectural issues rather than style debates.\n5. **Reduced Merge Conflicts**: Consistent formatting helps in reducing the surface for merge conflicts, as the code will be uniformly styled across all contributors' commits.\n\n### Implementation\n1. **Prettier Configuration**: We will create a `.prettierrc` configuration file with our preferred settings. For simplicity, we will start with Prettier\u2019s default settings and adjust as necessary based on team feedback.\n2. **IDE Integration**: Ensure each team member configures their IDE or code editor to use Prettier for autoformatting on file save, or provide setup instructions for enabling this integration.\n3. **Pre-Commit Hook**: Implement a pre-commit hook using Husky to automatically format code with Prettier before it gets committed to the repository. This ensures that any code that gets committed adheres to the agreed-upon style.\n4. **CI\/CD Integration**: Integrate Prettier with our CI\/CD pipeline to enforce style checks, ensuring that any pull request meets our formatting standards.\n\n### Maintenance\n1. **Periodic Review**: Regularly review the Prettier configuration to ensure it meets the evolving team needs and project requirements.\n2. **Team Training**: Provide training or documentation about Prettier usage and configuration to all team members to ensure smooth adoption.\n\nBy adopting Prettier, we aim to streamline our development process, reduce formatting-related overhead, and maintain a consistent code style across our JavaScript codebase.","GenTime":"2024-07-29 00:55:11"}
{"File Name":"tracking-consent-frontend\/0005-use-data-attribute-for-language.md","Context":"## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https:\/\/github.com\/hmrc\/assets-frontend\/blob\/97c638289e23bee255ac30724a8572c6efa96817\/assets\/patterns\/help-users-when-we-time-them-out-of-a-service\/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n\n## Decision\n","Decision":"* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings\/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui\/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n","tokens":409,"id":3526,"Prediction":"## Decision\nWe will remove the dependency on reading the PLAY_LANG cookie in the tracking consent banner and instead leverage a `data-language` attribute for language determination. Here\u2019s the detailed course of action:\n\n1. **Use `data-language` Attribute**: We will introduce a `data-language` attribute to be used in the tracking consent banner code. This attribute can be set server-side based on the user's preferred language.\n\n2. **Support for Classic Services**: For services written in Java or other frameworks that do not set the PLAY_LANG cookie, the `data-language` attribute can be easily incorporated, ensuring that the banner can support Welsh and other language translations without relying on PLAY_LANG.\n\n3. **Deprecate Cookie-based Language Detection**: We will deprecate the client-side reading of the PLAY_LANG cookie for language determination in favor of the `data-language` attribute, which aligns with the approach used in other govuk-frontend and hmrc-frontend components.\n\n4. **Communicate and Document the Change**: We will communicate this change to all relevant teams and provide documentation on how to set the `data-language` attribute to ensure a smooth transition. This will include example implementations and best practices.\n\n5. **Non-Breaking Change Implementation**: This change will be implemented in a non-breaking manner. Existing services that do not yet adopt the `data-language` attribute will continue to function using the current mechanism, but a warning or deprecation notice will be issued.\n\n6. **ZAP Tests Compliance**: By moving away from the PLAY_LANG cookie approach, we avoid the security concerns and the need for exemptions in ZAP tests, ensuring that all services remain compliant without additional security risks.\n\nBy adopting this decision, we aim to standardize language preferences across all frontend components consistently while addressing the immediate need to support classic services and maintaining timely deployment. This solution streamlines our approach and ensures future scalability and easier maintenance.","GenTime":"2024-07-29 00:55:31"}
{"File Name":"tracking-consent-frontend\/0002-use-data-attribute-for-gtm-container.md","Context":"## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n\n## Decision\n","Decision":"* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\nChosen option: \"Use a data attribute\", because based on the benefits listed below the team\\nbelieves this is the best way forward.\\n### Positive Consequences\\n* The Javascript bundle creation process is simplified.\\n* The Scala Play routing is simplified\\n* Only one endpoint needs to be managed in production\\n* Future additional containers can be supported more easily\\n* Service developers only have to configure a single URL to tracking consent and use\\na data attribute to configure the container.\\n* The central common configuration repo (owned by a separate team) only requires a single URL to\\ntracking consent defining for each environment, rather than one for each GTM container.\\n### Negative Consequences\\n* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPT\\ntag when integrating.\\n","tokens":78,"id":3529,"Prediction":"## Decision\nAfter evaluating the requirements and decision drivers, the decision is to configure the GTM (Google Tag Manager) container via data attributes rather than separate bundles. \n\n### Rationale\n\n1. **Simplicity for Service Developers**: Data attributes provide a more straightforward and intuitive way for service developers to integrate and manage tracking consent configurations. This reduces the learning curve and development effort required, aligning with the goal to keep things simple.\n   \n2. **Improved Operability**: Using data attributes, it is easier to manage and update configurations directly within HTML, enabling quicker adjustments and better real-time operability. This approach avoids the need for complex builds and deployments associated with separate bundle configurations.\n\n3. **Consistency**: Data attributes ensure that all configuration details are encapsulated within the HTML, maintaining consistency and reducing the risk of configuration mismatches or errors that could occur with separate bundles.\n\nBy choosing data attributes for GTM container configuration, we anticipate a smoother integration process, enhanced operability, and overall simplicity which directly benefits service developers and operational efficiency.","GenTime":"2024-07-29 00:55:44"}
{"File Name":"rotc\/0002-use-aws-as-example-cloud-platform.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3546,"Prediction":"## Context\nThe current system architecture is monolithic, which has led to difficulties in scaling and maintaining the application due to tightly coupled components. As the user base has grown, performance issues have become more frequent, and deploying new features or updates often requires a complete system restart, causing downtime that affects user satisfaction. Furthermore, different teams working on various parts of the application have encountered coordination challenges due to the shared codebase and overlapping dependencies.\n\n## Decision\nTo address these issues, we will transition from a monolithic architecture to a microservices architecture. This will involve decomposing the existing application into smaller, loosely coupled services that can be developed, deployed, and scaled independently. Each microservice will encapsulate a specific business capability and will communicate with other services through well-defined APIs, typically using HTTP\/REST or messaging protocols.\n\nThis approach aims to improve system scalability, enhance maintainability, and reduce the impact of changes or failures in any single part of the application. It will also allow different teams to work more autonomously, as each team can focus on a specific set of services with minimal dependencies on others. The migration will be executed incrementally to minimize disruption, starting with the most critical and frequently updated parts of the application.","GenTime":"2024-07-29 00:56:43"}
{"File Name":"blueprint\/0003-feature-selection-policy.md","Context":"## Context and Problem Statement\\nOpen Data Hub has release v0.8.0, some of the Elyra features required by Thoth Station experiments are\\nnot part of this ODH release. This would require to update certain components to the HEAD of main branch of ODH upstream\\nproject.\\n## Decision Drivers\\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\n\n## Decision\n","Decision":"* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\nChosen option: \"deploy specific versions of components\", because this will give the most efficient deployment to\\nOperate First operators and users.\\n### Positive Consequences\\n* operators can gain a maximum of experience, enabling feedback on component versions that might have not been tested\\nby the upstream project\\n* users get the feature set they need to get the most out of an Operate First deployment\\n### Negative Consequences\\n* additional deployment\/manifest customizations that are not valuable to upstream project, as they are out of scope\\nfor them\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":90,"id":3551,"Prediction":"## Decision\n\nWe have decided to update the necessary components to the HEAD of the main branch of the Open Data Hub (ODH) upstream project to access the required Elyra features for Thoth Station experiments. This decision is driven by the need to ensure that our users have access to the most current software components that meet their requirements.\n\nTo minimize operational complexity, we will:\n\n1. **Assess Compatibility:** Rigorously assess and test the compatibility of the HEAD components with the existing ODH v0.8.0 release to mitigate any potential integration issues.\n2. **Incremental Updates:** Implement the updates incrementally to allow for faster identification and resolution of any issues that arise.\n3. **Documentation and Training:** Provide comprehensive documentation and training for the operational team to manage the divergences from the standard release more effectively.\n4. **Monitoring and Support:** Enhance monitoring and support mechanisms to quickly address any unexpected problems caused by the updates.\n5. **Feedback Loop:** Establish a feedback loop with users to gather insights and continuously improve based on their experiences with the updated components.\n\nIn doing so, we aim to balance the operational complexity introduced by diverging from an official release with the user need for up-to-date features, ensuring a smooth and reliable experience for our user base.","GenTime":"2024-07-29 00:57:04"}
{"File Name":"blueprint\/0019-org-management.md","Context":"## Context and Problem Statement\\nManaging GitHub organizations via UI is restricted to few individuals only and does not offer any review mechanisms. The same apply to repository creation, organization membership management and label management. This ADR focuses on bringing transparency into this aspect of housekeeping within a community and aims to provide a way to individual contributors, so they can influence and propose changes themselves.\\nADR seeks a way to manage GitHub organization as a code, declaratively via GitOps as a natural extension of the core paradigm of Operate First movement.\\n## Decision Drivers\\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\n\n## Decision\n","Decision":"Improve transparency, provide auditing and reviews and allow individual contributors to propose changes.\\nChosen option: _\"1. Prow's Peribolos\"_, because it's a declarative solution that is widely embraced in upstream communities like Kubernetes.\\n### Positive Consequences\\n* Repositories can be created declaratively.\\n* Even external users can request themselves to be added to the organization via a pull request.\\n* Teams can be created and maintained declaratively.\\n* Labels can be centrally managed in a unified way as well as updated per repository bases.\\n* Solution can be easily automated via post-submit Prow jobs.\\n### Negative Consequences\\n* Using a declarative configuration may not be as straightforward as clicking buttons in the UI.\\n* Some users can still do manual changes to repositories they own, these changes are undone by the automation.\\n","tokens":128,"id":3554,"Prediction":"## Decision\n\nWe will adopt a GitOps approach to manage the GitHub organization, repository creation, organization membership management, and label management. This will involve:\n\n1. **Infrastructure as Code (IaC) Tools**: Utilizing tools such as Terraform or Pulumi to manage GitHub resources declaratively. These tools will allow us to define and version control the organizational setup, ensuring that changes are reviewed before being applied.\n\n2. **Repository for GitHub Management**: Creating a dedicated repository (e.g., `github-org-management`) where configuration files defining the state of the GitHub organization, its repositories, members, and labels will reside. This repository will be set up to use a Continuous Integration\/Continuous Deployment (CI\/CD) pipeline.\n\n3. **CI\/CD Pipeline Integration**: Implementing a CI\/CD pipeline using GitHub Actions or similar services. The pipeline will validate changes to the configuration files, check for compliance with organizational policies, and apply the changes to the GitHub organization upon approval.\n\n4. **Pull Request Workflow**: Instituting a pull request (PR) workflow where individual contributors can propose changes by submitting pull requests to the `github-org-management` repository. Each PR will be subject to peer review and approval processes defined by the community.\n\n5. **Review & Approval Process**: Establishing a transparent review and approval process for changes, which includes:\n   - Automated checks for policy compliance.\n   - Manual review by designated maintainers or a governance team.\n   - Auditable logs of all changes made to the GitHub organization.\n\n6. **Automated Auditing**: Incorporating automated auditing tools to generate reports and logs for changes made to the GitHub organization. These reports will be accessible to the community to maintain transparency and accountability.\n\nThe above decision aligns with the core paradigm of the Operate First movement and ensures that individual contributors can influence and propose changes while maintaining a high level of transparency, auditing, and review.","GenTime":"2024-07-29 00:57:20"}
{"File Name":"adr-tools\/0005-help-comments.md","Context":"## Context\\nThe tool will have a `help` subcommand to provide documentation\\nfor users.\\nIt's nice to have usage documentation in the script files\\nthemselves, in comments.  When reading the code, that's the first\\nplace to look for information about how to run a script.\\n\n## Decision\n","Decision":"Write usage documentation in comments in the source file.\\nDistinguish between documentation comments and normal comments.\\nDocumentation comments have two hash characters at the start of\\nthe line.\\nThe `adr help` command can parse comments out from the script\\nusing the standard Unix tools `grep` and `cut`.\\n","tokens":64,"id":3566,"Prediction":"## Decision\n\nThe `help` subcommand will be designed to extract usage documentation directly from comments within the script files. This approach ensures that the documentation is easily accessible both when reading the code and when using the `help` subcommand. To facilitate this, we will adopt a structured comment format for usage documentation within the scripts. This format will be consistent and easily parseable by the `help` subcommand.\n\nAdvantages:\n1. **Consistency:** By keeping documentation within the script files, we ensure that the usage information stays up-to-date with the code changes.\n2. **Convenience:** Developers and users can access the usage documentation from both the command line and the source code.\n3. **Single Source of Truth:** Reduces the risk of discrepancies between code and documentation.\n\nTo implement this, we will:\n1. Define a clear and standardized comment format for usage documentation.\n2. Include a parser in the `help` subcommand to extract and display these comments.\n3. Ensure all current and future scripts adhere to this documentation standard.\n\nThis decision will improve maintainability and user experience by coupling code and documentation more closely.","GenTime":"2024-07-29 00:58:20"}
{"File Name":"adr-tools\/0004-markdown-format.md","Context":"## Context\\nThe decision records must be stored in a plain text format:\\n* This works well with version control systems.\\n* It allows the tool to modify the status of records and insert\\nhyperlinks when one decision supercedes another.\\n* Decisions can be read in the terminal, IDE, version control\\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\n\n## Decision\n","Decision":"browser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n","tokens":114,"id":3568,"Prediction":"## Decision\n\nWe will use a Markdown format for storing Architectural Decision Records (ADRs):\n\n1. **Version Control Compatibility**: Markdown files are plain text files, and they integrate seamlessly with version control systems like Git.\n2. **Readability in Various Environments**: Markdown files can be viewed directly in terminals, IDEs, and version control browsers, providing a good balance between human readability and machine processing.\n3. **Formatting Capabilities**: Markdown supports various formatting options such as lists, code blocks, headers, and more. This ensures that users can include necessary formatting in their ADRs to improve comprehension.\n4. **Hyperlinks and Status Updates**: We can use tools to modify Markdown files to update statuses and insert hyperlinks when decisions supersede others.\n5. **Enhanced Readability and Printable Format**: Markdown files can be converted to other formats (such as HTML or PDF) using tools like Pandoc, providing a more readable and printable version of the decision records.\n\nThis approach leverages the best of both worlds: the simplicity and compatibility of plain text with the enhanced formatting capabilities of Markdown.","GenTime":"2024-07-29 00:58:25"}
{"File Name":"adr-tools\/0009-help-scripts.md","Context":"## Context\\nCurrently help text is generated by extracting specially formatted comments from the top of the command script.\\nThis makes it easy for developers of the tool:  documentation and code is all in one place.\\nBut, it means that help text cannot include calculated values, such as the location of files.\\n\n## Decision\n","Decision":"Where necessary, help text can be generated by a script.\\nThe script will be called _adr_help_<command>_<subcommand>\\n","tokens":63,"id":3571,"Prediction":"## Decision\n\nThe team has decided to separate the help text documentation from the command script code and implement a dynamic help text generation module. This module will allow calculated values, such as file locations, to be included in the help text.\n\n1. **Implementation of a Help Text Configuration File**: A separate configuration file (e.g., JSON or YAML) will be created to store static help text. This file can be easily edited and maintained, allowing for changes without modifying the source code.\n\n2. **Development of a Dynamic Help Text Generator**: A new module will be introduced that reads the configuration file and augments the help text with dynamic information, such as calculated file paths or other runtime data. This module will ensure that the help text is up-to-date with real-time values.\n\n3. **Integration with Command Scripts**: Command scripts will be updated to invoke the dynamic help text generator when the help option is triggered. This ensures that the displayed help text includes both static and dynamic content.\n\n4. **Documentation and Training**: Comprehensive documentation will be written to guide developers on how to update the help text configuration file and use the dynamic generator module. Training sessions will be scheduled to ensure all team members are comfortable with the new process.\n\nBy adopting this decision, the help text will remain user-friendly and informative, retaining the ease of documentation while enabling dynamic and context-specific content. This approach balances the need for maintainable code with the necessity for more informative and adaptable help documentation.","GenTime":"2024-07-29 00:58:35"}
{"File Name":"konfetti\/0004-use-lazy-loading.md","Context":"## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n\n## Decision\n","Decision":"We will use a lazy evaluation approach, similar to [implemented in Django](https:\/\/github.com\/django\/django\/blob\/master\/django\/conf\/__init__.py#L42)\\n","tokens":37,"id":3572,"Prediction":"## Decision\n\nWe have decided to implement a lazy-loading mechanism for configuration settings using `konfetti`. This approach will only load the necessary configuration values as they are needed, rather than loading all settings at application startup or test initiation. By doing so, we will minimize the side effects and overhead associated with fully configuring settings when only a subset is required for specific tests. This ensures a more modular and efficient testing process, allowing developers to run tests without the need to configure the entire project settings.","GenTime":"2024-07-29 00:58:37"}
{"File Name":"court-case-service\/0005-guiding-principles.md","Context":"## Context\\nThe implementation of [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) raised questions about what is the appropriate approach for serving new data to the prepare-a-case app, and in particular stressed the need for error handling by `prepare-a-case` where graceful degradation from partial failures is needed. The question was raised as to whether `court-case-service` should be coupled closely with `prepare-a-case` and is notionally a dedicated gateway for it or whether it should be treated as a general purpose API which may be consumed by multiple consumers.\\n\n## Decision\n","Decision":"1. All data stored by `court-case-service` is considered its own domain and should be treated as a general purpose api\\n2. All endpoints composing data on behalf of `prepare-a-case` are not intended for consumption by other clients and should provide all data needed on a given `prepare-a-case` page as efficiently as possible\\n3. The exception to point 2 is the case where errors in the retrieval of certain pieces of data are expected and need to be isolated. In this case data to be isolated should be served as a separate endpoint which will fail fast. `prepare-a-case` can then handle it simply as a failed HTTP call. See [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) for an example of this.\\n","tokens":129,"id":3579,"Prediction":"## Decision\nWe have decided to treat `court-case-service` as a general-purpose API designed to serve multiple consumers rather than coupling it closely with the `prepare-a-case` application. \n\n### Rationale:\n1. **Scalability:** By maintaining `court-case-service` as a general-purpose API, we allow for future scalability and the possibility of integrating other consumers without the need for significant refactoring.\n   \n2. **Separation of Concerns:** Decoupling `prepare-a-case` from `court-case-service` aligns with the architectural principle of separation of concerns. Each service should be responsible for a specific functionality and not be overly dependent on another.\n\n3. **Flexibility:** Treating `court-case-service` as an independent API ensures flexibility in its development and evolution. Any changes or enhancements can be made without the constraint of a single consuming application.\n   \n4. **Error Handling:** The responsibility of handling partial failures and ensuring graceful degradation should be managed within the `prepare-a-case` app. This approach offers finer control for `prepare-a-case` to handle specific errors and degrade gracefully based on its use-case requirements.\n   \n### Implementation:\n- **Error Handling:** `prepare-a-case` will implement comprehensive error-handling mechanisms to manage partial failures when interfacing with `court-case-service`.\n- **API Design:** `court-case-service` will continue to be designed with RESTful principles, ensuring it remains accessible and versatile for any consumer needs.\n- **Documentation:** Clear documentation will be provided for `court-case-service` so that any potential future consumers understand its capabilities and limitations.\n\nBy making this decision, we aim to create a more modular, maintainable, and scalable system architecture that can adapt to evolving requirements and integrations.","GenTime":"2024-07-29 00:59:11"}
{"File Name":"api-catalogue\/0001-remove-tech-docs-gem-dependency.md","Context":"## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https:\/\/github.com\/alphagov\/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n\n## Decision\n","Decision":"Remove the TDG as a dependency by vendoring the code relevant to\\nthe API catalogue directly into the project itself.\\n","tokens":228,"id":3586,"Prediction":"## Decision\n\nMove away from the Tech Docs Gem (TDG) and Middleman-based build process to a more flexible and efficient solution.\n\n### Justification:\n\n1. **Flexibility and Customization:**\n   - Since the project isn't purely documentation and the data source has shifted from markdown files to CSV, we need a more adaptable system.\n   - TDG's rigid template structure makes it difficult to implement custom layouts or add page-specific JavaScript.\n\n2. **Performance:**\n   - The current use of TDG to render the Table of Contents is inefficient. Given that every page must be re-rendered multiple times, build times are in excess of a minute, which slows down the development and deployment processes.\n\n3. **Dependency Management:**\n   - TDG imposes specific dependency versions, limiting our ability to control version upgrades. This can hinder our ability to stay on updated, supported versions and quickly implement necessary security patches.\n\n### Next Steps:\n\n1. **Evaluate Alternatives:**\n   - Explore other static site generators or frameworks that better align with our current data structure and functional requirements (consider Jekyll, Hugo, or React-based solutions like Gatsby).\n\n2. **Prototype New Solution:**\n   - Create a prototype to ensure the new solution meets the project\u2019s needs for customization, speed, and dependency management.\n\n3. **Transition Plan:**\n   - Develop a detailed transition plan including data migration, template redesign, testing, and deployment.\n   - Schedule the transition to minimize disruption and ensure thorough validation before decommissioning the current solution.\n\n4. **Documentation and Training:**\n   - Ensure thorough documentation of the new build process and provide training sessions for the development team.\n\nThis decision aims to enhance the flexibility, performance, and maintainability of the API catalogue site, ultimately leading to a better development experience and a more responsive, secure web application.","GenTime":"2024-07-29 00:59:34"}
{"File Name":"libmemory\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3590,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) system to formally document the ongoing architectural decisions of this project. This will involve creating a standardized template for all architectural decisions. Each ADR will include context, decision, consequences, and any other relevant information to ensure clarity and continuity throughout the project's lifecycle.\n\nKey steps include:\n1. Selecting an appropriate ADR template.\n2. Setting up a repository or directory in the project's codebase for storing ADRs.\n3. Encouraging all team members to document their architectural decisions promptly and consistently.\n4. Regularly reviewing and updating ADRs as the project evolves.","GenTime":"2024-07-29 00:59:49"}
{"File Name":"axiomatic\/0001-enabling-configuration-repo-access.md","Context":"## Context and Problem Statement\\nHow should Axiomatic ensure that dir2consul, when run as a Nomad batch job, has access to configuration repos hosted on GitHub?\\n## Decision Drivers\\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\n\n## Decision\n","Decision":"* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\nChosen option: Option 2, Axiomatic Retrieves Deployment Keys\\n### Positive Consequences\\n* Allows us to provide a repeatable solution for configurationn repos in the short term\\n* We will default to using read-only ssh access\\n### Negative Consequences\\n* Requires codifying a key management process and a potential automation effort\\n","tokens":161,"id":3595,"Prediction":"## Decision\n\nAfter considering the context and decision drivers, it was decided to proceed with **Option 2: Utilizing unique SSH keys for each repository and managing keys via a centralized Vault**. This approach aligns with the following considerations:\n\n1. **Avoid Reuse of Deployment Keys:** Since GitHub restricts the reuse of deployment keys across multiple repositories, generating unique SSH keys for each repository ensures compliance with GitHub's policies.\n  \n2. **Cost Management:** Given the lack of appetite for spending money on Service User accounts, this option avoids additional costs associated with Service User licenses.\n  \n3. **Security Management through Vault:** By storing private SSH keys in a centralized Vault, we can maintain a high level of security. This also facilitates key rotation and access control in a manageable and secure manner.\n  \n4. **Automated Key Management:** Though the setup process for creating and managing SSH key pairs, adding public keys to repositories, and storing private keys in Vault is more complex, this can be streamlined through automation using Terraform. This minimizes the operational overhead required for initial setup and key rotation.\n\nWith these considerations in mind, Option 2 provides a balanced solution addressing security, compliance, and cost without being hampered by technical limitations or policy violations associated with the other options (Option 3 and Option 4).","GenTime":"2024-07-29 01:00:10"}
{"File Name":"alfresco-anaxes-shipyard\/0002-docker-registry-for-internal-and-protected-images.md","Context":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n\n## Decision\n","Decision":"We will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).\\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\\nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.\\nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\\n","tokens":130,"id":3598,"Prediction":"## Decision\nWe will adopt Docker Hub as our standard Docker Image Registry for the Alfresco Digital Business Platform due to its capabilities in access control, security scanning, scalability, and global performance. This decision is driven by the following considerations:\n\n1. **Access Control and Security**: Docker Hub provides robust access control features, allowing us to create private repositories and manage user permissions effectively. Its integration with automated security scanning tools ensures our images are continuously monitored for vulnerabilities.\n\n2. **Scalability and Global Performance**: Docker Hub's globally distributed infrastructure ensures high performance and scalability. Users across different regions can access and pull images with reduced latency, which addresses our global performance requirements.\n\n3. **Usability and Maintainability**: Docker Hub is a widely used, well-documented platform with a user-friendly interface. Adopting it will reduce the onboarding time for our teams and external users (partners, customers).\n\n4. **Resource Constraints**: Given our IT team's current bandwidth, Docker Hub's managed service model alleviates the need for additional infrastructure deployment and maintenance, allowing us to focus more on core development activities.\n\nBy standardizing on Docker Hub, we align with industry best practices while meeting our immediate operational needs and setting a solid foundation for future growth.","GenTime":"2024-07-29 01:00:16"}
{"File Name":"alfresco-anaxes-shipyard\/0004-k8s-cluster-creation.md","Context":"## Context\\nWe need to recommend a tool to those customers that do not have an existing Kubernetes cluster.\\nA tool called minikube can be used on laptops for development and evaluation purposes. For a production ready stack we need to provide a way for customers to provision a cluster in AWS using a mechanism a less technically literate person can follow.\\nA list of candidate solutions can be found [here](https:\/\/github.com\/kubernetes\/community\/blob\/master\/sig-aws\/kubernetes-on-aws.md).\\n\n## Decision\n","Decision":"We will be informing customers that we use kops internally and it's the cluster upon which we perform all our testing.\\nWe have chosen kops as it's the tool the most closely fulfils our current and future requirements, it\u2019s produced by the same team that build Kubernetes and it\u2019s been [endorsed](https:\/\/aws.amazon.com\/blogs\/compute\/kubernetes-clusters-aws-kops) by AWS.\\n","tokens":101,"id":3600,"Prediction":"## Decision\n\nAfter evaluating the candidate solutions listed in the referenced GitHub repository and considering the necessity for ease of use for less technically literate customers, we have decided to recommend **Amazon EKS (Elastic Kubernetes Service)** for our customers to provision production-ready Kubernetes clusters in AWS.\n\n### Rationale:\n\n1. **User-Friendly**: Amazon EKS provides a managed Kubernetes service, which simplifies the process of provisioning and managing Kubernetes clusters. This makes it accessible to users with limited technical expertise.\n\n2. **Integration with AWS Services**: EKS integrates seamlessly with other AWS services, offering enhanced scalability, security, and management of applications.\n\n3. **Managed Service**: As a managed service by AWS, EKS handles the complexities of setting up, managing, and maintaining the Kubernetes control plane, reducing the operational burden on customers.\n\n4. **Security and Compliance**: Amazon EKS conforms to AWS's security and compliance standards, providing a robust security framework out-of-the-box.\n\n5. **Community and Support**: Being a part of the AWS ecosystem, Amazon EKS has extensive documentation and a large community, which can be incredibly beneficial for new users.\n\n6. **Scalability**: EKS automatically adjusts the scaling based on the application's needs, enabling customers to efficiently manage workloads without manual intervention.\n   \nIn summary, Amazon EKS offers a blend of simplicity, integration, and reliability, making it the most suitable candidate for customers requiring a production-ready Kubernetes cluster with minimal technical overhead.","GenTime":"2024-07-29 01:00:26"}
{"File Name":"alfresco-anaxes-shipyard\/0006-helm-chart-repository.md","Context":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to decide on a Helm Chart Repository that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Helm Charts. When we looked for a solution, we considered that this is going to be a public-facing customer repository. We also took into consideration the need for a custom domain for it.\\nThe criterias we looked for when we investigated each option are: if it uses AWS and Quay, if it is a PaaS\/Cloud Storage solution, if the project is mature enough for our use case, if it is a personal project or if it is maintained by a company and if it requires IT support (if it does can it be avoided by altering the pipeline).  We considered that authentication and SSL is not needed.\\nWe had several options that we considered. The full list of pros and cons for each option can be found [here](https:\/\/issues.alfresco.com\/jira\/secure\/attachment\/97743\/DEPLOY-150%20Helm%20Chart%20Repos.xlsx). More comments on the investigation we have done can be found [here](https:\/\/issues.alfresco.com\/jira\/browse\/DEPLOY-150).\\n\n## Decision\n","Decision":"We will use Github Pages to store the Helm Charts. The reasons why we elected this solution are: it is a cloud storage solution, it is mature project that has been actively mantained, doesn't require IT support and it offers an easy, well-known pipeline.\\n","tokens":266,"id":3602,"Prediction":"## Decision\n\nAfter thorough investigation and consideration of our requirements for a Helm Chart Repository that will be utilized by Alfresco engineering teams, other internal groups within the organization, and external customers and partners, we have decided on adopting **JFrog Artifactory** as our preferred solution.\n\nHere are the key reasons why JFrog Artifactory was selected:\n\n1. **PaaS\/Cloud Storage Solution**: JFrog Artifactory offers a robust, cloud-based storage solution which aligns with our preference for a PaaS model.\n   \n2. **Maturity and Reliability**: JFrog Artifactory is a mature project backed by a reputable company with a solid track record in managing repositories and artifacts. This provides confidence in its reliability and continued support.\n\n3. **Company-backed**: Unlike personal or community-maintained projects, JFrog Artifactory is maintained by an established company, ensuring longevity and professional support.\n\n4. **Avoidance of IT Support Requirements**: The solution structure allows for integration into our CI\/CD pipeline without extensive reliance on dedicated IT support, which is in line with our desire for minimal IT intervention.\n\n5. **AWS and Quay Integration**: JFrog Artifactory supports integration with AWS and Quay, which matches our existing infrastructure and future expansion plans.\n\n6. **Custom Domain Capability**: It supports hosting under a custom domain, fulfilling one of our primary requirements for a seamless customer-facing experience.\n\nAlthough alternatives were considered (the detailed pros and cons for each are documented in the attached investigation), JFrog Artifactory best meets our criteria and operational requirements. This decision will be documented and implemented immediately so that we can proceed with our containerized deployment efforts efficiently.\n\nWe will start by setting up a pilot repository instance to test and finalize integration before broad rollout to all stakeholders. Further guidelines and SOPs will be prepared for internal teams and external partners on how to publish and consume Helm Charts using the new JFrog Artifactory instance.","GenTime":"2024-07-29 01:00:31"}
{"File Name":"alfresco-anaxes-shipyard\/0008-applying-extensions-to-docker-images.md","Context":"## Context\\nOne of the main advantages of Docker images is their immutability. This means once an image has been tested and verified there is a high confidence it will work as intended in other environments as it doesn't change.\\nAlfresco allows the core product to the enhanced via external modules in the form of [AMPs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-amps.html) or [simple JARs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-jar-files.html).\\nThis results in two big problems, how do we release containers with every combination of AMP available and how do customers apply their own extensions?\\nWe have three options; apply extensions at build time (thus retaining the immutability advantage), apply extensions at runtime using a mechanism that doesn't change the contents of the container or apply extensions as the container initializes (breaking immutability).\\nApplying extensions at build time means we will be forcing customers to build their own images depending on which official and custom extensions they require.\\nHowever, there are a number of disadvantages of applying extensions at runtime; The immutability advantage is lost (the WAR file is changed), an extension could fail to apply and prevent the container from starting, it has the potential to introduce performance issues as the AMP or JAR has to be fetched and applied and finally it opens a potential security hole as code can be applied to a container via a simple environment variable.\\nWe investigated potential approaches to keep the main container immutable by using volumes and initContainers but this increases the complexity of the solution and doesn't resolve all the issues outlined above.\\n\n## Decision\n","Decision":"We will proceed with the build time option as there are a number of disadvantages to applying extensions at runtime, customers are already used to applying their own extensions and they're having to learn a new deployment mechanism anyway.\\nWe will release a small number of images with and without common AMPs applied.\\n","tokens":351,"id":3604,"Prediction":"## Decision\n\nBased on the context, the decision is to apply extensions at build time. \n\n### Justification:\n\n1. **Immutability Maintenance:** By applying extensions at build time, we maintain the immutability of the Docker image, which ensures consistency across different environments and deployments. This is a significant advantage that aligns with standard DevOps practices and improves the reliability of the deployment.\n\n2. **Stability and Reliability:** Applying extensions at build time ensures that potential issues with extensions (such as conflicts or errors during application) are detected early during the build phase rather than at runtime. This avoids runtime failures that can be difficult to diagnose and resolve.\n\n3. **Performance Considerations:** Extensions applied at build time do not introduce performance overhead during runtime since all necessary components are already integrated into the image. This avoids any performance penalties associated with fetching and applying extensions during container initialization.\n\n4. **Security:** By not allowing extensions to be applied dynamically at runtime via environment variables, we significantly reduce the security risks associated with unauthorized or malicious code being introduced into the container.\n\n5. **Customer Flexibility:** While this approach means customers may need to build their own Docker images when they need specific combinations of official and custom extensions, we can support this process by providing clear documentation, tooling, and templates to simplify image creation.\n\n### Implementation:\n\n1. **Custom Image Creation:** Provide a mechanism (e.g., Dockerfile templates, custom build scripts) for customers to build their own Docker images that include desired AMPs or JARs at build time.\n\n2. **Documentation:** Offer comprehensive documentation to guide users through the process of building their own images with necessary extensions.\n\n3. **Build Pipelines:** Optionally, provide pre-configured build pipelines or CI\/CD templates to assist customers in automating the image-building process.\n\nBy choosing to apply extensions at build time, we prioritize the reliability, stability, and security of the Docker images, while also providing adequate support to help customers manage their specific extension needs efficiently.","GenTime":"2024-07-29 01:00:40"}
{"File Name":"cloud-sdk-js\/0021-odata-url-builder.md","Context":"## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET \/People('scottketchum')\/Friends`\\n- getting \"raw value\" of a property `\/People('scottketchum')\/$value`\\n\n## Decision\n","Decision":"- Implement A for now as a powerful workaround.\\n- Proposal B\/C\/variant will be a `2.0` task, where it seems C might be the winner and we might review the decision later as they close to each other.\\nAt least, implement it as a separate task so we have a workaround for custom URL.\\n### Proposal B\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends, 'scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend) \/\/ single item can continue linking\\n.navigationProp(People.BestFriend); \/\/ single item can continue linking\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Better fluent API (compared to `asChildOf`) with builder pattern.\\n- Can be extended for supporting problem 5-7.\\n- Typed.\\n##### Cons:\\n- Lots of effort to build the new structure, which seems to be a `2.0` task.\\n### Proposal C\\nBasically, the same idea but with different API in terms of reaching single items.(e.g., \"getByKey\" and 1-to-1 navigation properties)\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends) \/\/ multi item can call \"key\" to become a single item\\n.key('scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n### Proposal C variants\\n```\\n\/\/frank\\nPeople.requestBuilder()\\n.getByKey('key') \/\/ xxxRequestBuilder\\n.toFriend('abc')\\n.toBestFriend()\\n.toFriends()\\n.getBuilder()\/\/create\\n\/\/marika\\nPeople.requestBuilder()\\n.key('scottketchum')\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination)\\n```\\n#### Pros and cons:\\nSame as `Proposal B`, but with more methods instead of overloading functions with more parameters.\\n### Proposal D\\nUse the similar API like `asChildOf`\\n```ts\\n\/\/ \/People(personKey)\/Friends\\nFriends.requestBuilder().getAll().asChildOf(person, People.Friends);\\n```\\n```ts\\n\/\/ \/People(personKey)\/Friends(friendKey)\\nFriends.requestBuilder().getByKey(friendKey).asChildOf(person, People.Friends);\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Consistent with `asChildOf` for generating same URL.\\n- Medium complexity\\n- Typed.\\n##### Cons:\\n- Cannot be extended for supporting problem 5-7, so we need to find solution for them.\\n- The ugly API `asChildOf` is used with additional use cases.\\n- Different order: `Entity -> NavigationProp` (url) V.S. `NavigationProp -> Entity` (API usage)\\n#### Decision:\\nNot chosen due to the lack of extension and confusing API.\\n### previous docs\\nFind related discussion [here](..\/implementation-documentation\/api-improvements.md)\\n","tokens":61,"id":3630,"Prediction":"## Decision\n\nWe have decided to enhance the current request builder APIs to support OData-specific requests, such as querying navigation properties and retrieving the raw value of a property. The following actions will be taken:\n\n1. Extend the request builder library to include methods that can construct and handle navigation property queries, i.e., `GET \/People('scottketchum')\/Friends`. This will be achieved by:\n   - Adding functionality to recognize and process navigation property paths.\n   - Updating URL generation logic to correctly format these types of requests.\n   - Ensuring response handling accommodates the structure of navigation property results.\n\n2. Add support for raw value retrieval requests, such as `\/People('scottketchum')\/$value`, by:\n   - Implementing a method to append `\/$value` to the entity path in the request builder.\n   - Modifying the parsing and processing logic to correctly handle responses with raw scalar values rather than structured JSON objects.\n\nThese enhancements will ensure that our APIs are compliant with OData protocols and are capable of supporting more advanced querying scenarios. This decision aligns with our goal of providing robust and flexible API solutions to meet diverse client needs.","GenTime":"2024-07-29 01:02:28"}
{"File Name":"adrflow\/5-Utility_Context_Lazily_Initialized.md","Context":"# 5 Utility Context Lazily Initialized\\n## Context\\nThe [synchronous utils](..\/..\/src\/adr_util_sync.js) are intended to be used by commands to work on ADRs, with the given context of the project, e.g. the [adr marker file](1-ADR_directory_identified_by_marker_file.md).\\nSince the context is mandatory (the ADR directory, existing ADRs), it is initialized at the beginning, in the object's constructor.\\nSeveral commands (scripts) already include that script, and initialize the object on the script loading, as part of the `require` statement.\\nThis resulted in loading the context whenever a command is loaded.\\nThis might have performance implications, but the more immediate functional lacuna is that this then assumes that the context *can* be loaded every time.\\nOf course, this assumption breaks - when running the `init` command, where by definition there is no context to load - the `init` command creates that.\\nThis resulted in trying to run the `init` command, on an existing directory, in an error.\\n\n## Decision\n","Decision":"The ADR project context will be loaded lazily, only when needed.\\nThis assumes the `init` command doesn't require it, and hence will not invoke any commands that require that context (e.g. the existing files).\\nSo any command that is loaded can still create the context object and create an instance of, but the actual context will be loaded easily.\\nThis assumption seems safe enough since `init` command indeed creates the context (the `.adr` file).\\nAnd this will probably hold for similar commands.\\nCommands that will require the ADR context, will continue to use it as today, and it will be loaded lazily.\\nWe need of course to make sure that the context isn't created\/calculated every time it is called; e.g. searching for the ADR dir every time one of the utility functions is called. In other words - memoize the context-creating functions.\\n","tokens":227,"id":3635,"Prediction":"## Decision\nWe will refactor the synchronous utility context to be lazily initialized rather than being loaded at the time of the script inclusion. This means that the context initialization will be deferred until it is explicitly required, eliminating the need for it to be present at the time of the object\u2019s construction.\n\n### Implementation Steps:\n\n1. **Refactor Constructor:**\n   - Modify the constructor of the utility context class so that it does not perform any context initialization.\n   \n2. **Lazy Initialization Method:**\n   - Implement a method in the utility context class that initializes the context only when it is first accessed. This method will handle all necessary setup for the context.\n\n3. **Modify Command Scripts:**\n   - Ensure that all command scripts call the lazy initialization method of the utility context class before they begin operations that require context. This can be done at the beginning of each command\u2019s execution logic.\n\n4. **Handle `init` Command:**\n   - Special handling will be added in the `init` command to bypass context initialization since this command is responsible for creating the context.\n\n### Benefits:\n- **Performance:** \n  - Loading the context only when needed can potentially reduce the initial startup time of commands.\n  \n- **Flexibility:** \n  - Commands that don\u2019t need context at startup, especially the `init` command, will no longer face errors due to the absence of context.\n\n- **Segregation of Concerns:** \n  - Separating context initialization from object construction promotes cleaner and more maintainable code structure.\n\n### Potential Drawbacks:\n- **Increased Complexity:** \n  - Introducing lazy initialization may add some complexity to the codebase, as additional checks will need to be performed to ensure the context is initialized before use.\n\nBy adopting this decision, we enhance the robustness and flexibility of our command utility scripts, ensuring they can handle scenarios where the context may not be available at startup, particularly benefiting use cases like the `init` command.\n","GenTime":"2024-07-29 01:02:51"}
{"File Name":"adrflow\/8-Remove_ADR_Util_Sync_Module.md","Context":"supersedes [5](5-Utility_Context_Lazily_Initialized.md)\\nmentions [6](6-Centralize_Definition_of_Filename.md)\\n## Context\\nThe ADR Util module grew as an attempt to centralize a list of utility functions that require the context of the ADR file and directory.\\nThe resulting module was very central to the logic of the entire program, but resulted in a module that had too much differing concerns (too many responsibilities\/reasons to change).\\nThis has eventually resulted in tangled code, and sometimes to (unintentional) circular dependencies, e.g. as when trying to isolate the handling of links.\\nAt the same time, the usage of the shared context - ADR files and directory - hasn't proved that useful in most case.\\nIn the cases where it should be used, it can be defined and cached.\\n\n## Decision\n","Decision":"Divide the utilities defined in this module to other, more focused modules (under `core` directory):\\n1. `files.js`: handles all file reading and writing\\n2. `adrobj.js`: handles all ADR logical-level operations. Essentially linking the ID to the file.\\n3. `links.js`: handles logic around linking ADRs\\nIn cases where caching the ADR files is needed, e.g. in `adrobj.js`, it is done in that module.\\n","tokens":180,"id":3637,"Prediction":"## Decision\nRefactor the ADR Util module to separate the different concerns into distinct, smaller modules to better adhere to the Single Responsibility Principle (SRP). Each new module will handle a specific aspect of the utility functions, such as file operations, link handling, and context caching. This will reduce the risk of circular dependencies and make the codebase easier to maintain and extend. Specifically:\n\n1. **File Operations Module:** A dedicated module to handle file-related utilities, such as reading, writing, and managing ADR files and directories.\n2. **Link Handling Module:** A specific module to manage the creation, updating, and verification of links within ADR documents.\n3. **Context Caching Module:** A utility to define and cache ADR context in cases where shared context proves necessary.\n\nThis refactoring will streamline the codebase, isolate responsibilities, and improve maintainability by reducing the centralization of diverse utilities in a single module.","GenTime":"2024-07-29 01:02:56"}
{"File Name":"corona-hackathon\/0010-branching-strategy.md","Context":"## Context\\nWe have to make a decision on the branching strategy for development.\\n\n## Decision\n","Decision":"[Git Flow](https:\/\/danielkummer.github.io\/git-flow-cheatsheet\/) it will be.\\n","tokens":17,"id":3653,"Prediction":"## Decision\nWe have decided to adopt the Gitflow branching strategy for our development process. \n\n### Justification:\n1. **Separation of Concerns**: Gitflow provides a clear distinction between development and production-ready code, allowing for more organized and maintainable project management.\n2. **Feature Development**: Individual features or tasks can be developed in isolated branches, reducing the risk of destabilizing the main codebase.\n3. **Environment Management**: Different branches for development, testing, and production environments facilitate easy deployments and hotfixes.\n4. **Process Alignment**: With planned releases, Gitflow aligns well with our sprint-based development cycle and continuous integration practices.\n\n### Implementation Steps:\n1. **Main Branch** (`main`): This will store the official release history.\n2. **Develop Branch** (`develop`): This is where the latest development changes accumulate.\n3. **Feature Branches** (`feature\/*`): Branches created off of `develop` for new features.\n4. **Release Branches** (`release\/*`): Support preparation of a new production release, coordinating final changes and testing.\n5. **Hotfix Branches** (`hotfix\/*`): Branches created off of `main` for quick fixes directly in production.\n\nThis strategy will be reviewed periodically to ensure it continues to meet our needs and will be adjusted based on team feedback and evolving project requirements.","GenTime":"2024-07-29 01:03:52"}
{"File Name":"corona-hackathon\/0011-postgres.md","Context":"## Context\\nWe have to make a decision on how\/where to host the DB.\\n\n## Decision\n","Decision":"We use a freely managed [Postgres on Heroku](https:\/\/www.heroku.com\/postgres)\\n","tokens":19,"id":3657,"Prediction":"## Decision\n\nAfter evaluating several options for hosting the database, we have made the decision to use a managed cloud-based database service. Specifically, we will host our database on Amazon RDS (Relational Database Service) for the following reasons:\n\n1. **Scalability**: Amazon RDS provides easy scalability options, both vertically and horizontally, which is essential for accommodating future growth and varying loads.\n2. **Reliability and Uptime**: Amazon RDS offers high availability and durability through Multi-AZ deployments with automatic failover. This ensures minimal downtime and data loss.\n3. **Performance**: Amazon RDS includes built-in optimization features such as automated backups, automated software patching, and read replicas, which enhance performance and operational efficiency.\n4. **Security**: With RDS, we can take advantage of advanced security features including encryption at rest and in transit, VPC (Virtual Private Cloud) isolation, and fine-grained access control via IAM (Identity and Access Management).\n5. **Cost-efficiency**: Using a managed service reduces the TCO (Total Cost of Ownership) by offloading database maintenance tasks to AWS, allowing our team to focus on core application features rather than infrastructure management.\n6. **Integration**: Amazon RDS integrates seamlessly with other AWS services that our application might use, such as Amazon EC2, AWS Lambda, and Amazon S3, enhancing overall system coherence and performance.\n\nThis decision will be re-evaluated periodically to ensure that it continues to meet our evolving needs, and we remain open to considering other solutions if circumstances change.","GenTime":"2024-07-29 01:04:09"}
{"File Name":"corona-hackathon\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3663,"Prediction":"## Decision\n\nWe have decided to implement the use of Architectural Decision Records (ADRs) as our standard practice for documenting key architectural decisions made throughout the project. ADRs will provide a structured approach to capturing the context, options considered, the decision itself, and the consequences of that decision. This will enhance transparency, improve collaboration among team members, and serve as a valuable reference for future decision-making processes.\n\n* **Format**: ADRs will follow the standard Markdown template to ensure consistency and ease of use.\n* **Storage**: All ADRs will be stored in a version-controlled repository alongside the project codebase to ensure accessibility and traceability.\n* **Review Process**: Each ADR will be reviewed and approved by the architecture team before being finalized.\n* **Updates**: ADRs will be labeled with a unique identifier and version number, and they will be updated as necessary to reflect any changes or new insights.\n\nThis decision aims to foster a culture of thoughtful decision-making and maintain a comprehensive record of our architectural evolution throughout the project's lifecycle.","GenTime":"2024-07-29 01:04:37"}
{"File Name":"nhsuk-prototype-kit-version-one\/0001-use-node-and-express.md","Context":"## Context\\nThe prototype kit application will need to operate similarly to a live service, depending on the level of fidelity required for testing. This means that a server is required. To make the prototype kit easy to use for different levels of coding experience we want to implement auto routing (or route matching) for template files, the simplest way to achieve this this dynamically is to have an application running on a server.\\n\n## Decision\n","Decision":"We will use Node.js with the Express framework to run a Node.js server for the prototype kit on both local development environments and production environment when published on Heroku. Node.js is written in javascript and is therefore one the most common languages between designers who have some coding experience and software engineers. It is also the easiest runtime to install on both Mac and PC.\\n","tokens":83,"id":3681,"Prediction":"## Decision\nGiven the need for the prototype kit application to operate similarly to a live service and support automatic routing for template files, we will deploy a Node.js server with Express.js framework. This solution will allow us to implement dynamic auto routing efficiently with minimal setup, broad compatibility, and ease of use for developers with varying levels of coding experience.\n\n### Justification:\n1. **Server Requirement**: Node.js is well-suited to handle the server-side operations needed for the prototype kit.\n2. **Auto Routing**: Express.js offers robust support for routing, which simplifies the implementation of auto routing mechanisms.\n3. **Developer Experience**: Node.js and Express.js provide straightforward setups and are widely adopted, which means ample learning resources and community support.\n4. **Performance**: Node.js is asynchronous and efficient for handling I\/O operations, meeting the performance needs of our application.\n5. **Flexibility and Scalability**: As requirements grow, we can easily scale the application within the Node.js ecosystem. \n\nWith this decision, we expect to achieve a smooth development and testing process, accommodating varied levels of fidelity for prototyping and enabling developers of different skill levels to contribute effectively.","GenTime":"2024-07-29 01:06:01"}
{"File Name":"nhsuk-prototype-kit-version-one\/0003-use-npm-scripts-and-gulp-for-running-tasks.md","Context":"## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n\n## Decision\n","Decision":"We will use a mixture on NPM scripts and [Gulp](https:\/\/gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\\n","tokens":49,"id":3682,"Prediction":"## Decision\n\nTo streamline the process of setting up the prototype kit and ensure consistency, we will implement an automated build script using a task runner like Gulp or a build tool like Webpack. This tool will handle the following tasks:\n\n1. **Installing Dependencies**: The build script will automate the installation of necessary dependencies using a package manager such as npm or Yarn.\n2. **File Management**: It will move required files from dependencies into the appropriate app file structure, ensuring everything is organized correctly.\n3. **Running the Application**: The build tool will handle starting the development server or compiling the application, making it easy to run with a single command.\n\nUsing a task runner or build tool will not only reduce the manual effort involved but also ensure repeatability and reduce human error, helping to get the prototype kit up and running more efficiently. Additionally, clear documentation and instructions will be provided to guide users through using the build script.","GenTime":"2024-07-29 01:06:03"}
{"File Name":"smarthub\/0002-use-lerna-for-package-versioning.md","Context":"## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n\n## Decision\n","Decision":"Migrate code base to monorepo structure and use `lerna` for versioning management.\\n","tokens":39,"id":3691,"Prediction":"## Decision\nTo address the complexity of manually updating dependent packages within the Smarthub SDK, we will automate the dependency management process. This can be achieved through the implementation of a robust dependency management tool such as Lerna (for JavaScript\/Node.js projects) or a similar tool that is appropriate for the given technology stack. \n\n### Execution Steps:\n1. **Dependency Management Tool Selection**:\n   - Evaluate and select a suitable dependency management tool based on the technology stack and project requirements (e.g., Yarn Workspaces, Lerna, Poetry for Python, etc.).\n\n2. **Tool Configuration**:\n   - Configure the chosen tool to handle the specific needs of the Smarthub project. This includes setting up the correct scopes, versioning strategies, and workflows.\n  \n3. **Integration with CI\/CD Pipeline**:\n   - Integrate the dependency management tool with the existing Continuous Integration\/Continuous Deployment (CI\/CD) pipeline to ensure automated dependency updates and versioning as part of the build process.\n\n4. **Documentation and Training**:\n   - Update project documentation to reflect the new dependency management process.\n   - Provide training for the development team to ensure they understand how to work with the new process.\n\n5. **Monitoring and Maintenance**:\n   - Establish monitoring to ensure the dependency updates are applied correctly.\n   - Regularly review and maintain the dependency management configurations to align with the evolving project requirements.\n\nBy automating the dependency management, we aim to reduce manual overhead, mitigate the risk of human error, and ensure that the Smarthub project dependencies are consistently and correctly maintained.","GenTime":"2024-07-29 01:06:42"}
{"File Name":"smarthub\/0003-create-backend-client-lib.md","Context":"## Context\\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\\n\n## Decision\n","Decision":"Create client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\\n","tokens":29,"id":3692,"Prediction":"## Decision\nTo address the issue of direct REST calls being scattered across various components and complicating unit testing, we will introduce an abstraction layer in the form of a dedicated API client service. This service will centralize all REST API interactions and expose them through a well-defined interface.\n\n### Justification:\n\n1. **Improve Testability**:\n    - The abstraction will allow for easier mocking and stubbing of REST interactions during unit tests, making tests more isolated and reliable.\n  \n2. **Decouple System Components**:\n    - By centralizing API interactions, we minimize the coupling between the system components and the external data source, fostering better maintainability and scalability.\n\n3. **Enhance Readability and Maintainability**:\n    - Having a single, dedicated service for API interactions will improve code readability and make it easier to maintain and update the REST call logic in the future.\n\n4. **Facilitate Error Handling**:\n    - Centralizing API calls allows for consistent error handling and logging across the entire application.\n\n### Implementation Steps:\n\n1. **Create API Client Service**:\n    - Develop a new service module responsible exclusively for handling all REST API operations.\n    - Define clear and concise methods that encapsulate the logic for different API endpoints.\n\n2. **Refactor Existing Components**:\n    - Refactor existing system components to use the new API client service instead of making direct REST calls.\n    - Replace all instances of direct API calls with method calls to the new service.\n\n3. **Update Unit Tests**:\n    - Update unit tests to mock the API client service instead of the direct REST interactions.\n    - Ensure tests are restructured to take advantage of the improved testability offered by the abstraction layer.\n\n4. **Documentation and Training**:\n    - Document the usage of the API client service to guide developers on best practices.\n    - Provide training or workshops if necessary to get the development team up to speed with the new approach.\n\nBy implementing this decision, the system will be better structured, more maintainable, and easier to test, ultimately leading to higher software quality and faster development cycles.","GenTime":"2024-07-29 01:06:47"}
{"File Name":"saas-platform-frontend\/0003-use-javascript-over-typescript.md","Context":"## Context\\nI need to decide in which language I implement the frontend.\\n\n## Decision\n","Decision":"I use Javascript.\\n","tokens":16,"id":3715,"Prediction":"## Decision\n\nAfter considering the options and based on the context provided, the decision has been made to implement the frontend using React.js.\n\n### Reasons for Decision:\n1. **Popularity and Community Support**: React.js is one of the most popular frontend frameworks, which ensures strong community support and a wealth of resources.\n2. **Component-Based Architecture**: React\u2019s component-based architecture supports reusable components, leading to more maintainable and scalable code.\n3. **Performance**: React provides good performance with its virtual DOM and efficient update mechanism, making the application faster.\n4. **Ecosystem**: A vast ecosystem of libraries and tools that integrate well with React, such as Redux for state management and React Router for handling routing, simplifies development.\n5. **Flexibility**: React is unopinionated about how to handle different aspects of app development, offering flexibility to choose libraries or tools as needed.\n6. **Industry Adoption**: Significant adoption by top-tier companies ensures long-term viability and ongoing development of the framework.\n7. **Learning Curve**: React has a moderate learning curve, which is manageable for developers with experience in modern JavaScript.\n\nOverall, React.js aligns well with the project\u2019s needs for performance, scalability, and maintainability.","GenTime":"2024-07-29 01:07:43"}
{"File Name":"saas-platform-frontend\/0006-use-antdesign-as-the-ui-framework.md","Context":"## Context\\nTo speed up the UI development we need to select a UI Framework that has a good community as well as good functionality.\\n\n## Decision\n","Decision":"We use Ant Design as the UI Framework.\\n","tokens":28,"id":3718,"Prediction":"## Decision\nWe have decided to adopt the **React** library as our UI framework. React is well-supported by a large and active community, ensuring that any issues we encounter can be quickly resolved with community assistance. Additionally, React offers robust functionality, including efficient component-based architecture and a virtual DOM for improved performance. These features will enable us to speed up the UI development process while maintaining high standards of reliability and performance.","GenTime":"2024-07-29 01:07:56"}
{"File Name":"saas-platform-frontend\/0002-use-the-stack-from-react-the-complete-guide-to-keep-up-to-date-with-react.md","Context":"## Context\\nI need to decided with which tools I build my react app.\\n\n## Decision\n","Decision":"I build the react app using [`React - The Complete Guide (incl Hooks, React Router, Redux)`](https:\/\/www.udemy.com\/react-the-complete-guide-incl-redux\/).\\n","tokens":17,"id":3721,"Prediction":"## Decision\n\n### JavaScript Framework: React\n- **Justification**: React is a popular and widely-used JavaScript library for building user interfaces, especially single-page applications. It allows for component-based architecture, which promotes reusability and maintainability.\n\n### State Management: Redux\n- **Justification**: Redux is a predictable state container for JavaScript apps. While there are other state management solutions available, Redux provides a robust and scalable approach to managing application state, especially for larger applications with complex state logic.\n\n### Build Tool: Webpack\n- **Justification**: Webpack is a powerful module bundler for JavaScript applications. It offers extensive customization and optimization capabilities, making it a suitable choice for building React apps. It also has a rich ecosystem of plugins that can enhance its functionality.\n\n### Package Manager: Yarn\n- **Justification**: Yarn is a fast, secure, and reliable dependency management tool. While npm is also a viable option, Yarn\u2019s features for performance and security make it a preferred choice for many React developers.\n\n### Styling: Styled-Components\n- **Justification**: Styled-components is a popular library for writing CSS-in-JS. It enables the styling of React components using tagged template literals, making it easier to manage and localize styles.\n\n### Testing: Jest and React Testing Library\n- **Justification**: Jest is a comprehensive testing framework that works seamlessly with React. React Testing Library complements Jest by providing utilities to test React components by mimicking user interactions.\n\n### Routing: React Router\n- **Justification**: React Router is a widely-used routing library for React applications, providing a declarative way to handle navigation. It is well-maintained and integrates smoothly with the React ecosystem.\n\n### Code Quality: ESLint and Prettier\n- **Justification**: ESLint is a powerful linter for identifying and fixing problems in JavaScript code. Prettier is an opinionated code formatter that ensures consistent code styling. Together, they help maintain clean and readable code.\n\n### Version Control: Git\n- **Justification**: Git is the most widely-used version control system. It offers distributed version control, making it easier for teams to collaborate. GitHub or GitLab can be used as a remote repository hosting service.\n\n### Continuous Integration: GitHub Actions\n- **Justification**: GitHub Actions provides built-in CI\/CD capabilities within GitHub repositories. It allows for automating testing and deployment workflows directly in the repository, ensuring continuous integration and delivery.\n\nBy selecting these tools, we aim to create a robust, maintainable, and scalable React application with industry-standard practices.","GenTime":"2024-07-29 01:08:13"}
{"File Name":"saas-platform-frontend\/0005-use-storybook-to-build-self-contained-components-but-not-for-uis.md","Context":"## Context\\nWe need a workflow to build our appliation and components.\\n\n## Decision\n","Decision":"We use Storybook only for building new self contained components.\\n","tokens":16,"id":3722,"Prediction":"## Decision\n\nWe will adopt a CI\/CD pipeline to streamline our build and deployment processes. This will be accomplished using Jenkins as our primary tool for automating the workflow. Jenkins offers robust support for setting up continuous integration and continuous delivery pipelines, which will help in managing the complexity of our application's build and deployment processes. It integrates well with our current toolset and infrastructure, and has strong community support for plugins and troubleshooting.\n\n### Steps:\n1. **Set Up Jenkins:** Install Jenkins on a dedicated server or use a cloud-based Jenkins service.\n   \n2. **Source Control Integration:** Integrate Jenkins with our source control system (e.g., GitHub, GitLab, Bitbucket) to automatically trigger builds upon code commits.\n\n3. **Automated Testing:** Configure the build pipeline to run unit tests, integration tests, and other necessary validations to ensure code quality.\n\n4. **Build Automation:** Define and script the steps required to build our application and its components. This may include compiling code, packaging binaries, and creating Docker images.\n\n5. **Deployment Pipelines:** Set up deployment pipelines to move builds through different stages such as development, staging, and production environments. These pipelines will include appropriate approval gates and rollback mechanisms.\n\n6. **Monitoring and Notifications:** Implement monitoring and alerts for build status, test results, and deployment outcomes. Use email, Slack, or other communication tools to notify team members of pipeline status and issues.\n\n### Benefits:\n- Improved build consistency and reliability through automation.\n- Faster feedback on code changes, enabling quicker identification and resolution of issues.\n- Enhanced collaboration and communication among team members.\n- Reduced human error and manual intervention in the build and deployment processes.\n- Scalability to handle additional components and increased complexity as the application evolves.\n\nBy adopting this CI\/CD pipeline approach with Jenkins, we ensure a streamlined, efficient, and reliable workflow for building and deploying our application and its components.","GenTime":"2024-07-29 01:08:17"}
{"File Name":"runner\/0276-problem-matchers.md","Context":"# ADR 0276: Problem Matchers\\n**Date** 2019-06-05\\n**Status** Accepted\\n## Context\\nCompilation failures during a CI build should surface good error messages.\\nFor example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply \"tsc exited with exit code 1\".\\nVSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https:\/\/code.visualstudio.com\/docs\/editor\/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.\\nThe problem-matcher concept fits well with \"setup\" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.\\n\n## Decision\n","Decision":"### Registration\\n#### Using `::` command\\n`::add-matcher::path-to-problem-matcher-config.json`\\nUsing a `::` command allows for flexibility:\\n- Ad hoc scripts can register problem matchers\\n- Allows problem matchers to be conditionally registered\\nNote, if a matcher with the same name is registered a second time, it will clobber the first instance.\\n","tokens":227,"id":3734,"Prediction":"## Decision\nWe will implement problem matchers within our CI build system to improve error reporting and debugging efficiency. Specifically, we will:\n\n1. **Adopt VSCode Problem Matchers:** Utilize the problem matcher feature in VSCode to scan CI build output and surface detailed error messages. This will help us transition from generic error messages like \"tsc exited with exit code 1\" to more informative messages that indicate the actual compilation issues.\n\n2. **Integrate Problem Matchers in Setup Actions:** Incorporate the registration of relevant problem matchers into our setup actions, such as `setup-nodejs`. By doing so, we ensure that the appropriate problem matchers are applied automatically for the duration of the job.\n\n3. **Configuration Availability:** Allow users to configure which problem matchers to use via the build configuration files. This will provide flexibility and enable custom error reporting specific to the project's needs.\n\n4. **Documentation and Guidelines:** Provide clear documentation and guidelines on how to define and use problem matchers within the CI environment to leverage their full potential.\n\nThis approach will enable a richer error output experience, making it easier to pinpoint and resolve issues during the CI build process.","GenTime":"2024-07-29 01:09:04"}
{"File Name":"runner\/0274-step-outcome-and-conclusion.md","Context":"## Context\\nThis ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.\\nThis allows downstream a step to run based on whether a previous step succeeded or failed.\\nReminder, currently the steps contains `steps.<id>.outputs`.\\n\n## Decision\n","Decision":"For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:\\n- `success`\\n- `failure`\\n- `cancelled`\\n- `skipped`\\nWhen a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.\\n### Example\\n```yaml\\nsteps:\\n- id: experimental\\ncontinue-on-error: true\\nrun: .\/build.sh experimental\\n- if: ${{ steps.experimental.outcome == 'success' }}\\nrun: .\/publish.sh experimental\\n```\\n### Terminology\\nThe runs API uses the term `conclusion`.\\nTherefore we use a different term `outcome` for the value prior to continue-on-error.\\nThe following is a snippet from the runs API response payload:\\n```json\\n\"steps\": [\\n{\\n\"name\": \"Set up job\",\\n\"status\": \"completed\",\\n\"conclusion\": \"success\",\\n\"number\": 1,\\n\"started_at\": \"2020-01-09T11:06:16.000-05:00\",\\n\"completed_at\": \"2020-01-09T11:06:18.000-05:00\"\\n},\\n```\\n","tokens":63,"id":3738,"Prediction":"## Decision\n\nWe have decided to add `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context within our software platform.\n\n### Reasoning:\n\n1. **Enhanced Control Flow**: The introduction of `steps.<id>.outcome` and `steps.<id>.conclusion` will provide enhanced control flow capabilities. This will allow downstream steps to conditionally execute based on the success or failure of previous steps.\n  \n2. **Error Handling**: This addition makes it easier to implement robust error handling mechanisms. Conditional execution based on the outcome of previous steps will help in creating more resilient workflows that can handle failures gracefully.\n  \n3. **Maintain Context Consistency**: Aligning with the existing `steps.<id>.outputs`, the new context attributes will maintain semantic consistency and offer a clear and intuitive structure for referencing step results.\n  \n4. **User Demand**: The need for this feature has been identified through user feedback, emphasizing its importance in real-world use cases where conditional step execution is essential.\n  \n### Implementation:\n\n- **`steps.<id>.outcome`** will capture the raw result of a step (e.g., succeeded, failed, skipped).\n- **`steps.<id>.conclusion`** will capture the summary verdict of a step, which could be more descriptive or high-level compared to `outcome`.\n\nThese attributes will be dynamically populated and available throughout the execution of a sequence, allowing users to reference and act upon them as needed.\n\nThis decision aligns with our goal of providing a more flexible and user-friendly experience, ensuring our platform evolves to meet the needs of its users while maintaining simplicity and clarity.","GenTime":"2024-07-29 01:09:23"}
{"File Name":"runner\/0280-command-input-echoing.md","Context":"## Context\\nCommand echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.\\nCommand outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.\\nEcho output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.\\n\n## Decision\n","Decision":"- The only commands that currently echo output are\\n- `remove-matcher`\\n- `add-matcher`\\n- `add-path`\\n- These will no longer echo the command, if processed successfully\\n- All commands echo the input when any of these conditions is fulfilled:\\n1. When such commands fail with an error\\n2. When `::echo::on` is set\\n3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`\\n- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019\/11\/04):\\n- `add-mask`\\n- `debug`\\n- `warning`\\n- `error`\\n- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these \"echo-suppressing\" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit.\\n","tokens":124,"id":3741,"Prediction":"## Decision\n\nImplement an opt-in mechanism for command echoing in user logs:\n\n1. **Default Behavior Change**: \n   - Command echoing will be disabled by default to prevent unnecessary clutter in the user logs.\n   - Users will have to explicitly opt in to enable command echoing.\n\n2. **Conditional Echoing for Errors**:\n   - Command outputs will be automatically echoed if there are any errors during command processing. This ensures that users have sufficient context for troubleshooting and understanding the cause of the failure.\n\n3. **User Control Commands**:\n   - Introduce new commands `::echo::on` and `::echo::off` to give users explicit control over echoing behavior at any point in their process.\n\n4. **Environment Variable Dependency**:\n   - By default, command echoing will be enabled if the `ACTIONS_STEP_DEBUG` secret is enabled. If this secret is not enabled, then echoing will be disabled by default.\n\n5. **Documentation and Communication**:\n   - Update the user documentation to reflect these changes and ensure that users are aware of how to control command echoing through the new commands and environment variable settings.\n\nThese changes will help in maintaining cleaner user logs, while still providing necessary information for debugging and troubleshooting when errors occur.","GenTime":"2024-07-29 01:09:32"}
{"File Name":"optimint\/adr-005-serialization.md","Context":"## Context\\nAll the basic data types needs to be efficiently serialized into binary format before saving in KV store or sending to network.\\n\n## Decision\n","Decision":"`protobuf` is used for data serialization both for storing and network communication.\\n`protobuf` is used widely in entire Cosmos ecosystem, and we would need to use it anyways.\\n","tokens":27,"id":3746,"Prediction":"## Decision\n\nWe will utilize Protocol Buffers (protobuf) as the serialization framework for our basic data types.\n\n### Justification:\n1. **Efficiency**: Protocol Buffers are designed with efficiency in mind, providing both compact data representation and fast serialization and deserialization times which are crucial for performance in both storage and network transmission.\n2. **Flexibility**: Protocol Buffers support a wide range of data types and allow for extensibility, accommodating changes over time without breaking existing implementations.\n3. **Interoperability**: Protobuf is language-agnostic and can generate code for multiple programming languages (e.g., Java, C++, Python), facilitating consistent data handling across different parts of our system.\n4. **Schema Definition**: Protobufs use a robust schema definition language, which allows us to precisely define our data structures. This helps in maintaining the integrity and versioning of data formats.\n5. **Maintenance**: Given its wide adoption and strong community support, using Protocol Buffers ensures long-term sustainability and availability of resources for troubleshooting and enhancements.\n","GenTime":"2024-07-29 01:09:50"}
{"File Name":"tech-events-calendar\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3753,"Prediction":"## Decision\n\nWe will implement an Architectural Decision Record (ADR) process to document all key architectural decisions made throughout the project's lifecycle. This process involves:\n\n1. **Template Selection:** Adopting a standardized template for ADRs to ensure consistency.\n2. **Tooling:** Using a version-controlled repository (e.g., Git) to store ADRs, ensuring they are easily accessible and editable by team members.\n3. **Ownership:** Assigning a specific person or role to be responsible for maintaining and updating ADRs.\n4. **Review Process:** Implementing a peer review process for each ADR before it is finalized to ensure thorough consideration and consensus.\n5. **Integration with Workflow:** Ensuring ADRs are written as part of the development workflow, especially when changes to the architecture are being considered. \n6. **Periodic Review:** Scheduling regular reviews of existing ADRs to reevaluate past decisions in the context of new information or evolving project requirements.\n\nBy formalizing this process, we aim to enhance communication, provide clear justifications for architectural decisions, and ensure that the rationale behind our architecture is well documented and understood by all team members.","GenTime":"2024-07-29 01:10:26"}
{"File Name":"mat-process-utils\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3763,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) process for documenting the architectural decisions made throughout the project. This process will involve:\n\n1. **Template Standardization**: Adopting a standardized template for ADRs to ensure consistency and comprehensiveness. Each ADR will include sections for context, decision, rationale, and potential consequences.\n\n2. **Central Repository**: Storing all ADRs in a central, version-controlled repository (e.g., a dedicated folder in the project's Git repository) to ensure transparency and accessibility for all team members.\n\n3. **Decision-making Process**: Establishing a formal process for proposing, discussing, and finalizing architectural decisions. This will involve:\n   - **Proposal**: Team members can propose an ADR by creating a new document in the repository.\n   - **Review and Discussion**: The proposed ADR will be reviewed and discussed in designated meetings or via asynchronous communication tools.\n   - **Approval**: Decisions will be finalized through consensus or majority vote, with the decision and its rationale clearly documented in the ADR.\n\n4. **Versioning and Updates**: Ensuring ADRs remain up-to-date by revisiting and amending them as necessary, particularly when significant changes occur in the project's scope, requirements, or technology stack.\n\n5. **Visibility and Training**: Ensuring all team members are aware of the ADR process and understand how to create, review, and update ADRs. This may involve training sessions and ongoing reminders.\n\nBy formalizing the process of recording architectural decisions, we aim to improve project documentation, facilitate better decision-making, and enhance team collaboration and alignment.","GenTime":"2024-07-29 01:11:08"}
{"File Name":"unit-e-project\/ADR-0007.md","Context":"## Context\\nWe need a way to provide initial supply. At the moment it is not possible to spend the genesis block coinbase because\\neven if the block is added to the index, its transactions are not added to the txdb.\\n\n## Decision\n","Decision":"We want to change the code so that we are able to create an initial supply but we MUST only use the coinbase of the\\ngenesis block and MUST NOT resort to further `imports` in subsequents blocks like for example Particl did.\\nAll the coins of the initial supply MUST be minted in the coinbase transaction of the genesis block.\\n","tokens":50,"id":3764,"Prediction":"## Decision\n\nWe will implement a custom functionality to allow the spending of the genesis block coinbase. This will involve the following steps:\n\n1. **Modify Genesis Block Handling:**\n   - Update the block validation logic to ensure the genesis block and its transactions are included in the index and transaction database (txdb).\n\n2. **Transaction Inclusion:**\n   - Adjust the transaction processing algorithm to account for the inclusion of genesis block transactions in the txdb. This ensures that these transactions can be referenced and spent like any other transaction in the blockchain.\n\n3. **Supply Distribution Mechanism:**\n   - Design and integrate a mechanism to distribute initial supply from the genesis block coinbase. This could be through predefined wallet addresses or through a decentralized allocation mechanism.\n\n4. **Backward Compatibility and Security:**\n   - Ensure backward compatibility with the current state of the blockchain network. This includes extensive testing to confirm that existing nodes and wallets can handle the updated genesis block handling without issues.\n   - Conduct a thorough security audit to ensure that the modifications do not introduce vulnerabilities or unintended behaviors.\n\n5. **Documentation and Communication:**\n   - Update all relevant documentation to reflect the changes in the genesis block handling.\n   - Clearly communicate the changes to stakeholders, developers, and users to ensure smooth transition and understanding.\n\nBy implementing these steps, we will enable the initial supply to be provided from the genesis block coinbase, ensuring that the genesis block transactions are properly indexed and spendable within the blockchain network.","GenTime":"2024-07-29 01:11:12"}
{"File Name":"unit-e-project\/ADR-0002.md","Context":"## Context\\nWe have to maintain a consistent naming in the codebase for roles in the Esperanza protocol.\\nAt the moment we have plenty of examples where there is confusion especially when mixing terms like `staking`, `miner`, `proposer`.\\n\n## Decision\n","Decision":"We outline two clear actors in the protocol:\\n- The `PROPOSER`: which solves the function of proposing new blocks through his stake. At the moment words like `staking`, `miner`, `mining` are associated with the same function.\\n- The `VALIDATOR`: which takes active part in the finalisation process with the voting.\\nWe therefore MUST replace:\\n- all the instances of `miner` with `proposer`.\\n- all the instances of `mining` with `proposing`.\\n- all the instances of `staking` with `proposing`.\\nBut we MUST NOT replace the instances of the word `stake` since this indicates the actual deposit made in order to propose.\\nThe only exception to this is `stakethread.h` that MUST be renamed to `proposerthread.h`.\\n","tokens":52,"id":3768,"Prediction":"## Decision\nWe will establish a clear and consistent convention for naming roles in the Esperanza protocol to prevent confusion and ensure code maintainability. The terms will be standardized as follows:\n\n1. **Staker**: Use this term exclusively to refer to entities that lock their tokens in the protocol to earn rewards.\n2. **Proposer**: This will be used for entities that produce new blocks in the protocol.\n3. **Miner**: To avoid ambiguity, we will phase out the use of this term in the context of the Esperanza protocol. If necessary, we will clearly define its scope or replace it entirely with more specific terminology.\n\nAdditionally, to enforce these conventions:\n- Update the current codebase to align with these naming conventions.\n- Educate the development team about these terms and their specific meanings.\n- Implement code review checks to ensure compliance with the established naming convention moving forward.\n\nThis decision aims to increase clarity, reduce confusion, and improve the overall quality and coherence of the codebase.","GenTime":"2024-07-29 01:11:16"}
{"File Name":"elife-xpub\/0004-code-structure.md","Context":"## Context\\nAs the code rapidly grows we need a common structure and approach that will keep the code tidy and coherent.\\nThere are various aspects, the initial concern was the data access separation. The issue #140 initially talked about how this could be separated in terms of an ORM (Object-Relational Mapping) or a DAL (Data Access Layer).\\nHowever, this discussion also soon covered the structure of the code in general and how to structure it for ease of development and best practise.\\n### Rejected Options\\nWe considered the following options:\\n* The Pubsweet Way - This was to define a base class \"Model\" and extend this for each entity. This was rejected as its another self made ORM but and has validations that we don't want.\\n* Other ORMs : Waterline, sequelize, bookshelf - General purpose ORMs were rejected based on the experience that the time spent going up the learning curve and sifting through the docs when things go wrong is greater then simply rolling your own.\\n* Query Builders : Knex.js -These were not entirely ruled out, and were deemed compatible with a good code structure that would allow the database access to be encapsulated in a way that would allow a query builder to be used where necessary.\\n\n## Decision\n","Decision":"In summary the server-side code will be structured below the `server` folder as follows:\\n* **entities** - This folder will contain subfolders relating to named entities in the system. See [\"Data Model Specification\"](https:\/\/docs.google.com\/document\/d\/1KU-DLMNhPxjQF2j8HVlJvenvttPLxgtbTHo8Sy_PNRc\/).\\n* **entities\/\\<entity\\>** - The example below shows `Mansuscript` as an example entity, this folder will contain a common set of files that describe the entity's behaviour. Each of these have a particular purpose explained below:\\n* index.js - The main business logic, e.g. for `Manuscript` this could contain `getAuthor()`\\n* typedefs.js - Contains the GraphQL types pertinent to this entity.\\n* resolvers.js - Contains the GraphQL interface specified in terms of `Query` and `Mutation` 's for this entity. These in general should map onto the exposed functions from index.js. For example, `Manuscript` may contain a `Query` for `allManuscripts` and a `Mutation` for `createManuscript`.\\n* data-access.js - Contains the functions for saving to and loading from the database. All references to the database and SQL that is written should remain private to this file.\\n* **manuscript-state-machine** - It is anticipated that this is where the most orchestration of the entites will take place. This has been created as a separate folder so that it can expand to implement the state transitions currently defined for the manuscript.\\n### Example\\n```\\nserver\\n|\\n\u251c\u2500\u2500 entities\\n|   |\\n\u2502   \u251c\u2500\u2500 manuscript\\n\u2502   \u2502   \u251c\u2500\u2500 typedefs.js\\n\u2502   \u2502   \u251c\u2500\u2500 resolvers.js\\n\u2502   \u2502   \u251c\u2500\u2500 data-access.js\\n\u2502   \u2502   \u2514\u2500\u2500 index.js\\n|   |\\n\u2502   \u2514\u2500\u2500 <entity>\\n\u2502       \u251c\u2500\u2500 typedefs.js\\n\u2502       \u251c\u2500\u2500 resolvers.js\\n\u2502       \u251c\u2500\u2500 data-access.js\\n\u2502       \u2514\u2500\u2500 index.js\\n|\\n\u2514\u2500\u2500 manuscript-state-machine\\n\u2514\u2500\u2500 index.js\\n```\\n","tokens":257,"id":3776,"Prediction":"## Decision\n\nAdopt a Data Access Layer (DAL) approach with structured code organization.\n\n### Rationale\nGiven the current size and projected growth of the codebase, a well-defined structure is essential for maintaining code manageability and coherence. The chosen approach should provide:\n\n1. **Ease of Development**: Developers should be able to work with a clear, consistent structure that minimizes confusion and promotes efficient work.\n2. **Separation of Concerns**: A clear separation between different parts of the code, making it easier to manage and understand.\n3. **Flexibility and Scalability**: The ability to grow and evolve as project requirements change.\n4. **Compatibility**: The structure should be compatible with existing and potential tools like query builders.\n\n### Implementation Plan\n\n1. **Define the DAL**: \n   - Abstract database interactions into a data access layer. This will serve as a middle layer, facilitating communication between the application and the database.\n   - Implement the DAL in such a way that it can encapsulate calls to a query builder like Knex.js, allowing flexibility in querying the database as needed.\n\n2. **Organize Code Structure**:\n   - **Models**: Define data models representing the structure of different entities in the application.\n   - **Repositories**: Create repository classes that use the DAL to perform CRUD (Create, Read, Update, Delete) operations and other database interactions. This encapsulates all data-related logic in a single layer.\n   - **Services**: Implement service classes that contain business logic and utilize the repositories for data operations.\n   - **Controllers**: Define controllers that handle HTTP requests, use the services to process them, and send appropriate responses.\n\n3. **Documentation and Conventions**:\n   - Establish and document coding conventions and project guidelines to ensure that all team members understand and follow the structure consistently.\n   - Provide examples and templates for creating new models, repositories, services, and controllers to streamline development.\n\n### Benefits\n\n- **Standardized Structure**: All parts of the application will follow a well-defined, consistent structure, making it easier to navigate and understand.\n- **Improved Maintainability**: Clear separation of concerns reduces complexity, making the code easier to maintain and extend.\n- **Enhanced Collaboration**: With clear guidelines and a common structure, team members can work more effectively and collaboratively.\n\nBy adopting this approach, we aim to keep our codebase organized, manageable, and prepared for future growth and complexity.","GenTime":"2024-07-29 01:11:55"}
{"File Name":"TANF-app\/001-ADR-template.md","Context":"## Context\\nWe need to record the architectural decisions made on this project. This context section should include the history and driving reason(s) on why a decision needed to be made.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":37,"id":3784,"Prediction":"## Context\nWe need to record the architectural decisions made on this project. This context section should include the history and driving reason(s) on why a decision needed to be made.\n\nOur project is expanding to serve a significantly larger user base. As a result, the current monolithic architecture is causing performance bottlenecks and is limiting our ability to scale effectively. Additionally, the development team is growing, and it's becoming more challenging to maintain and update the system without affecting other parts of the application. There is a clear need to improve both performance and maintainability to ensure we can continue to innovate and serve our users effectively.\n\n## Decision\nWe have decided to transition from a monolithic architecture to a microservices architecture. This decision is driven by the need to improve system scalability, enhance performance, and enable more autonomous and efficient development by allowing teams to work on independent services. Each service will handle a specific business capability, communicate via well-defined APIs, and be developed and deployed independently. This approach will help mitigate the current performance issues, provide better fault isolation, and support more agile and streamlined development processes.","GenTime":"2024-07-29 01:12:34"}
{"File Name":"TANF-app\/008-deployment-flow.md","Context":"## Context\\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.\\nSince the recent changes to our [Git workflow](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https:\/\/github.com\/HHS\/TANF-app\/blob\/837574415af7c57e182684a75bbcf4d942d3b62a\/docs\/Architecture%20Decision%20Record\/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.\\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\n\n## Decision\n","Decision":"Additionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\nDeploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role                                             | Deploys when ...                                  |\\n-------------------|-----------------|----------------------|--------------------------------------------------|---------------------------------------------------|\\nDev                | Tanf-Dev        | Vendor & Gov      | Deploy code submitted for gov review                | Relevant github label assigned as shown below     |\\nDevelop            | Tanf-Staging    | Vendor & Gov      | Deploy code once gov-approved                       | Code merged to `raft-tech\/TANF-app:develop` |\\nStaging            | Tanf-Staging    | Gov               | Deploy code once gov-approved                       | Code merged to `HHS\/TANF-app:main` |\\nProduction         | Tanf-Prod       | Gov               | Deploy code tested in staging & ready for prod      | Code merged to `HHS\/TANF-app:master`                |\\n### Gitflow and Deployments\\nWe will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](.\/018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS\/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech\/TANF-app:develop` will be deployed to our staging sites.\\n### Dev deployments\\nWithin the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:\\n| Dev Site | Frontend URL | Backend URL | Purpose                                          |\\n| -------- | -------- | -------- |--------------------------------------------------|\\n| A11y | https:\/\/tdp-frontend-a11y.app.cloud.gov | https:\/\/tdp-backend-a11y.app.cloud.gov\/admin\/ | Space for accessibility testing                  |\\n| QASP | https:\/\/tdp-frontend-qasp.app.cloud.gov | https:\/\/tdp-backend-qasp.app.cloud.gov\/admin\/ | Space for QASP review                            |\\n| raft | https:\/\/tdp-frontend-raft.app.cloud.gov | https:\/\/tdp-backend-raft.app.cloud.gov\/admin\/ | Space for Raft review                          |\\n","tokens":396,"id":3789,"Prediction":"## Decision\n\nTo address the issues outlined in the context, the following changes and strategies will be implemented:\n\n1. **Create Additional Spaces and Environments**:\n    - **TANF-Dev**: Continue using the existing `tanf-dev` space for the initial development and integration testing.\n    - **TANF-Staging**: Enhance the current `tanf-staging` space by creating multiple dedicated environments within this space. This approach will facilitate parallel development and feature-specific testing.\n        - **Feature Branch Environments**: Introduce temporary environments for specific feature branches. These environments will be instantiated dynamically based on the active development needs and can be torn down once their usefulness has expired.\n        - **Release Staging Environment**: A stable environment dedicated for staging impending releases and conducting release-specific testing as per ADR 018.\n    - **TANF-Prod**: Use the existing `tanf-prod` environment solely for production deployments.\n\n2. **Adjust Deployment Workflow**:\n   - Align the updated environments with the new Git workflow. Deployments will now be structured as follows:\n       - Development branches will deploy to dynamically created feature branch environments within `tanf-dev`.\n       - The consolidated code in the main branch will be deployed to a dedicated environment in `tanf-staging` for final approval and testing.\n       - Once approved in `tanf-staging`, the code will be promoted to `tanf-prod`.\n\n**Benefits**:\n- **Reduced Environmental Crowding**: By creating multiple environments, different features and fixes can be developed and tested independently, reducing the risks of interference and facilitating parallel development processes.\n- **Improved Release Management**: Dedicated staging environments ensure thorough testing and quality assurance before releases, aligning with industry best practices and simplifying the overall deployment flow.\n- **Greater Flexibility**: The creation of temporary feature branch environments provides flexibility to the development team to test their changes without compromising or disrupting other ongoing work.\n\n**Action Steps**:\n- Implement the necessary configurations in Cloud.gov to support the creation and management of multiple environments within `tanf-staging`.\n- Update deployment scripts and continuous integration (CI) pipelines to support environment-specific deployments.\n- Communicate the changes to all team members and conduct training sessions if necessary to ensure a smooth transition to the updated deployment strategy.","GenTime":"2024-07-29 01:12:54"}
{"File Name":"TANF-app\/010-assign-superuser.md","Context":"## Context\\nUsually in Django we would assign the first superuser through the CLI, but because this will not\\nbe available in production, we will need another method.\\n\n## Decision\n","Decision":"The Django Admin provides the easiest way to assign superuser status, so for most users that will\\nbe the method employed. However, we still need a method for creating the first superuser, so that\\nuser will be able to assign others as needed. We will assign this with a data migration using a\\nusername defined in environment variables.\\n","tokens":35,"id":3792,"Prediction":"## Decision\n\nTo address the need for creating a superuser in production where CLI access is not available, we will implement a Django data migration script that checks for the existence of a superuser and creates one if it does not exist. This will be achieved by adding a custom data migration in one of our Django apps. \n\n### Steps\n1. **Create a data migration** that runs on deployment, which will:\n   - Check if a superuser already exists.\n   - If no superuser is found, create one with predefined credentials.\n   \n2. **Ensure credentials are securely managed** by:\n   - Storing default superuser credentials in environment variables.\n   - Using Django settings to read these environment variables.\n\n3. **Implement logic** in the data migration to use the credentials from environment variables to create the superuser.\n\nHere is an example of what the implementation might look like:\n\n1. Create a data migration file in one of your Django apps:\n   ```bash\n   python manage.py makemigrations your_app_name --empty\n   ```\n\n2. Edit the generated migration file to include the following:\n   ```python\n   from django.db import migrations\n   from django.contrib.auth.models import User\n   from django.conf import settings\n\n\n   def create_superuser(apps, schema_editor):\n       User = apps.get_model('auth', 'User')\n       if not User.objects.filter(is_superuser=True).exists():\n           User.objects.create_superuser(\n               username=settings.SUPERUSER_USERNAME,\n               email=settings.SUPERUSER_EMAIL,\n               password=settings.SUPERUSER_PASSWORD\n           )\n\n\n   class Migration(migrations.Migration):\n       dependencies = [\n           ('your_app_name', 'previous_migration_file_name'),\n       ]\n\n       operations = [\n           migrations.RunPython(create_superuser),\n       ]\n   ```\n\n3. Add the default superuser credentials to your environment and read them in your Django settings:\n   ```python\n   # settings.py\n   import os\n\n   SUPERUSER_USERNAME = os.environ.get('DJANGO_SUPERUSER_USERNAME', 'admin')\n   SUPERUSER_EMAIL = os.environ.get('DJANGO_SUPERUSER_EMAIL', 'admin@example.com')\n   SUPERUSER_PASSWORD = os.environ.get('DJANGO_SUPERUSER_PASSWORD', 'supersecurepassword')\n   ```\n\n4. Deploy the changes and ensure the environment variables are set in your production environment. When the data migration runs, it will create the superuser if none exists, using the credentials provided.\n\nThis approach ensures that the superuser is automatically created in a secure and repeatable manner during deployment without needing CLI access in the production environment.","GenTime":"2024-07-29 01:13:15"}
{"File Name":"TANF-app\/011-buildpacks.md","Context":"## Context\\nCurrently, our frontend and backend apps are running in Docker containers on Cloud.gov. The deployment process involves building the docker containers at [DockerHub](https:\/\/dockerhub.com). Because there is very little security documentation on DockerHub, the path to getting an ATO with this process would be very difficult. There are other options that may be easier to document, but none of them offer the benefits of buildpacks, which have already been Fed Ramped and documented.\\n\n## Decision\n","Decision":"Our recommendation is to move to Cloud.gov buildpacks at this time. They are already Fed Ramped, [shift responsibility to Cloud.gov](https:\/\/cloud.gov\/docs\/technology\/responsibilities\/) and ensure tightened security.\\n","tokens":99,"id":3793,"Prediction":"## Decision\n\nWe have decided to move our build process from DockerHub to the Cloud.gov buildpack service. The following reasons guide this decision:\n\n1. **Security and Compliance**: Cloud.gov buildpacks have been FedRAMP authorized, easing the process of achieving an Authority to Operate (ATO). This will help us meet regulatory requirements more efficiently compared to the current approach with DockerHub which lacks sufficient security documentation.\n\n2. **Documentation**: Since Cloud.gov buildpacks are already well-documented and supported, this will facilitate smoother audits and compliance checks. This comes in contrast to DockerHub, where the absence of adequate documentation poses significant barriers.\n\n3. **Simplicity and Integration**: Utilizing buildpacks aligns more closely with our existing Cloud.gov infrastructure. This should simplify our build and deployment processes, potentially reducing complexity and points of failure.\n\n4. **Operational Efficiency**: Moving to buildpacks will integrate seamlessly with our deployment pipeline in Cloud.gov, providing a consistent, stable, and secure build environment. This shift will also potentially decrease overall maintenance and operational overhead.\n\nTransition Plan:\n- **Assessment**: Evaluate the current Docker-based deployment process to identify all necessary steps to replicate in a buildpack-based approach.\n- **Migration**: Begin transitioning individual services to Cloud.gov buildpacks in stages, starting with non-critical services to minimize risk.\n- **Testing**: Thoroughly test each service after migration to ensure that there are no disruptions or regressions in functionality.\n- **Documentation**: Update all internal documentation to reflect the new build and deployment process.\n- **Training**: Provide necessary training and resources for the development and operations teams to ensure a smooth transition.\n\nBy making this change, we aim to maintain the benefits of containerization while enhancing our security posture and simplifying our path to achieving compliance.","GenTime":"2024-07-29 01:13:20"}
{"File Name":"TANF-app\/015-terraform-service-provisioning.md","Context":"## Context\\nIn [this issue](https:\/\/github.com\/raft-tech\/TANF-app\/issues\/609) we propose tasks to automate the provisioning of Cloud.gov-brokered services via our CI pipeline.\\nThis project plans to use Cloud.gov-brokered S3 buckets for TANF data file storage (sensitive PII data) and for Django Admin static assets (public), and a Cloud.gov-brokered RDS PostgreSQL service. Currently, there are no scripts to provision and secure these S3 buckets and RDS instance, and no dev documentation on how to deploy them, secure them, and verify that they are configured correctly. Additionally, this same initial provisioning must be done again for all additional target environments.\\n\n## Decision\n","Decision":"[Terraform](https:\/\/www.terraform.io\/) is a tool for building, changing, and versioning infrastructure safely and efficiently, and was proposed as a solution for managing our persistent Cloud.gov-brokered infrastructure. We can closely model what is done in [another HHS project](https:\/\/github.com\/HHS\/Head-Start-TTADP) and create per-environment infrastructure configurations which are leveraged by Terraform in CircleCI, with environment-specific settings read directly from Cloud.gov during our CI process. Note that this workflow was a [recommendation from Cloud.gov](https:\/\/www.youtube.com\/watch?v=86wfgNK_0o4), and they themselves use Terraform to provision their own infrastructure.\\n","tokens":148,"id":3794,"Prediction":"## Decision\nWe will implement automated scripts within our Continuous Integration (CI) pipeline to provision and secure Cloud.gov-brokered S3 buckets and RDS PostgreSQL services. \n\n### Detailed Plan:\n1. **Automation Scripts**: Develop and integrate scripts into the CI pipeline that:\n   - Create and configure S3 buckets using CloudFormation templates or similar Infrastructure as Code (IaC) tools.\n   - Create and configure RDS PostgreSQL instances, ensuring best practices for security and performance are adhered to.\n\n2. **Security Enhancements**:\n   - Enforce server-side encryption (SSE) for all S3 buckets storing sensitive PII data.\n   - Implement bucket policies to restrict access to the necessary IAM roles.\n   - Configure RDS to require SSL connections for data in transit, and ensure encryption at rest.\n   - Regularly update and review IAM policies to follow the principle of least privilege.\n\n3. **Documentation**:\n   - Document the provisioning process in the developer documentation to ensure all team members can deploy and secure these resources in additional environments.\n   - Include clear instructions on verifying proper configuration and security settings.\n\n4. **Environmental Consistency**:\n   - Ensure that the same provisioning scripts can be applied to all target environments (e.g., development, staging, production) to maintain consistency and reduce the risk of configuration drift.\n\n5. **Verification and Monitoring**:\n   - Integrate checks within the CI pipeline to verify that the provisioning and security settings are correctly applied.\n   - Set up monitoring and alerting for the S3 buckets and RDS instances to quickly identify and respond to any issues.\n\nThis approach will not only streamline the provisioning and configuration of our infrastructure components but also ensure that security best practices are consistently applied across all environments.","GenTime":"2024-07-29 01:13:25"}
{"File Name":"TANF-app\/012-antivirus-strategy.md","Context":"## Context\\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no fool proof way of doing this and a malicious user would be able to spoof a virus as a text file.\\n\n## Decision\n","Decision":"Instead of using a signed URL and sending the file directly to S3, we will instead send the file to the backend and scan it with Clam AV before sending it to S3. In the event there is a virus, we will destroy the file on the backend immediately and return an error to the frontend.\\nBy using the [ClamAV REST server](https:\/\/github.com\/ajilaag\/clamav-rest) implementation we are able to scan files for viruses and malicious behavior. Additionally, Anti-Virus definitions are kept up to date automatically by use of the included [freshclam](https:\/\/www.clamav.net\/documents\/signature-testing-and-management#freshclam) tool which automatically downloads and updates an internal database of virus signatures using the official ClamAV source.\\nIn addition to this, the frontend is able to reliably determine if a file is a binary file. The client will check submitted files for this and immediately return an error to the user. These files will not be sent to the backend.\\n","tokens":72,"id":3797,"Prediction":"## Decision\n\nGiven that verifying file types based on headers is not reliable and exposes the system to potential security risks, we have decided to implement an additional security layer in our file upload process. We will utilize a multi-step approach to ensure that malicious files are not uploaded into our S3 buckets:\n\n1. **File Scanning Service**: Introduce a third-party or custom-built file scanning service to thoroughly inspect each file for malware and verify its type before allowing it to be uploaded to S3.\n   \n2. **Staging Area**: Initially upload files to a secure, isolated staging area (another S3 bucket or temporary storage) where they will be scanned for security threats. Only after successful validation should the files be moved to the main S3 bucket.\n\n3. **Content Validation**: Employ deeper content validation methods beyond file headers (e.g., using magic number checks or content-based validations) within the scanning service to ensure the file type matches the expected format.\n\n4. **User Feedback**: Provide immediate feedback to the user if their file fails the security checks, detailing why the upload was rejected and guiding them on acceptable file types and formats.\n\n5. **Logging and Monitoring**: Implement logging and monitoring mechanisms to track all upload attempts, successful uploads, and rejected files. This adds another layer of security by allowing for auditing and analysis of potential attack patterns.\n\nBy moving forward with this multi-step approach, we aim to enhance the security of our file upload process, protecting our system and users from malicious files while still achieving our operational goals.","GenTime":"2024-07-29 01:13:36"}
{"File Name":"register-trainee-teachers\/0004-non-integer-trainee-ids.md","Context":"## Context\\nWe have been using trainee IDs in the URL as per standard Rails convention\/operation.\\nIt was felt that we should use non-integer IDs for a number of reasons:\\n* remove predictability\\n* interoperability with other systems without depending on DB IDs\\n\n## Decision\n","Decision":"We chose to use option 3 as it met the needs we had with the minimum of effort and avoided the really long URLs that\\noption 1 would have caused.\\n","tokens":57,"id":3802,"Prediction":"## Decision\nWe will transition from using integer-based trainee IDs in the URL to using universally unique identifiers (UUIDs). This change aligns with several strategic goals:\n\n1. **Enhanced Security and Privacy**: By using UUIDs, we reduce the predictability of trainee IDs, thereby enhancing security and privacy.\n2. **Interoperability**: UUIDs provide a standardized way to uniquely identify trainees across different systems, making our platform more interoperable.\n3. **Independence from Database IDs**: Decoupling the trainee IDs from internal database identifiers allows us to future-proof our architecture and makes it easier to integrate with other services or migrate databases without breaking references.\n\nTo implement this decision, we will:\n\n- Modify our database schema to include a UUID column for trainee IDs.\n- Update our Rails models and controllers to use UUIDs instead of integer IDs.\n- Ensure all API endpoints and internal logic are compatible with UUIDs.\n- Implement a migration strategy to transition existing data to the new format without disrupting ongoing operations.\n- Add necessary validations and formats to ensure UUID correctness.\n\nThis approach will provide a more robust, secure, and scalable identification system for our trainees.","GenTime":"2024-07-29 01:13:57"}
{"File Name":"govuk-design-system-architecture\/001-javascript-for-less-capable-browsers.md","Context":"## Context\\nBefore GOV.UK Frontend, our projects used jQuery for DOM interactions, events and data manipulation.\\nWe\u2019re taking a step back from jQuery due to its lack of support for the browsers we support, its large file size, lack of security updates and from conversations with the community.\\n\n## Decision\n","Decision":"We\u2019re now writing standard ES5 JavaScript instead, that we polyfill where necessary.\\nThis means that in places where we would have previously used [`$.each`](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [`.forEach`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.\\nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).\\nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.\\nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only ships the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.\\nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.\\nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).\\nHere is an [example of polyfilling `addEventListener`](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/docs\/polyfilling.md).\\nAny polyfills included in GOV.UK Frontend will be tested to work in supported browsers and devices, including assistive technology. Any community discussion and documentation around potential bugs or downsides will also be considered before deciding to include a polyfill in GOV.UK Frontend.\\n","tokens":62,"id":3806,"Prediction":"## Decision\n\nWe have decided to completely remove jQuery from our projects in favor of using vanilla JavaScript (ES6 and beyond). This decision is motivated by the need to maintain compatibility with the browsers we support, reduce the file size of our front-end codebase, ensure better security, and align with modern web development practices. \n\nTo achieve this, we will:\n\n1. **Audit Existing Code:**\n   - Conduct a thorough audit of current projects to identify all instances where jQuery is used.\n   - Document these instances to prioritize and plan the migration process.\n\n2. **Replace jQuery with Vanilla JavaScript:**\n   - Remove jQuery and refactor the existing codebase to use vanilla JavaScript equivalents.\n   - Ensure that our JavaScript code adheres to ES6 standards or higher, leveraging modern features such as `querySelector`, `addEventListener`, `fetch`, and Promises where applicable.\n\n3. **Polyfills and Browser Compatibility:**\n   - Evaluate and implement polyfills as necessary to maintain compatibility with older browsers still in use by our audience.\n   - Use tools like Babel and core-js to transpile modern JavaScript based on our targeted browser support.\n\n4. **Update Documentation and Training:**\n   - Provide updated documentation and guidelines for developers transitioning from jQuery to vanilla JavaScript.\n   - Offer training sessions or resources to help team members get up-to-speed with modern JavaScript practices.\n\n5. **Ongoing Maintenance and Improvements:**\n   - Establish a process for regular code reviews to ensure adherence to the new standards.\n   - Monitor and optimize the performance of JavaScript code continuously to achieve the best user experience.\n\n6. **Community Engagement:**\n   - Engage with the community to share our learnings and gather feedback on our transition from jQuery to vanilla JavaScript.\n   - Participate in discussions to stay updated with best practices and emerging technologies.\n\nBy adopting this approach, we aim to create more maintainable, performant, and secure front-end code, aligning with our broader goals for the sustainability and future-proofing of our projects.","GenTime":"2024-07-29 01:14:17"}
{"File Name":"adr-viewer\/0005-distinguish-amendments-to-records-with-colour.md","Context":"## Context\\nArchitecture Decision Records may be `amended` rather than `superseded` if e.g. only a small part of the decision changes.\\n\n## Decision\n","Decision":"Amended records, although not officially supported as a distinct flag in `adr-tools`, should be distinguished from records that are either Accepted or Superseded by.\\n","tokens":33,"id":3813,"Prediction":"## Decision\n### Decision to Amend Architecture Decision Records (ADRs) Instead of Superseding Them\n\nIn the interest of maintaining continuity and avoiding unnecessary complexity in our documentation, we will adopt a policy that allows for the amendment of existing Architecture Decision Records (ADRs) rather than superseding them, when only minor changes are required.\n\n### Rationale\n1. **Clarity** - Amending an existing ADR ensures that the decision history remains clear and easy to follow. Superseding ADRs can create confusion regarding which decisions are current.\n2. **Efficiency** - Amending an ADR is more efficient than creating a new one for small changes. This reduces redundancy in our documentation.\n3. **Traceability** - By detailing changes within a single ADR, we retain a clear trace of how decisions have evolved over time, which is valuable for historical context.\n\n### Guidelines for Amendments\n1. **Scope** - Amendments should be limited to minor changes such as clarifications, small adjustments, and updates that do not fundamentally alter the original decision.\n2. **Documentation** - Any amendments made to an ADR should be dated and clearly noted within the document. This can include a brief summary of what has changed and why.\n3. **Review** - All amendments should go through the same review and approval process as the original ADR to ensure consistency and quality of decisions.\n\nBy adopting this approach, we will maintain more streamlined and comprehensible ADR documentation, while ensuring that our decision-making process remains transparent and well-documented.","GenTime":"2024-07-29 01:14:45"}
{"File Name":"adr-viewer\/0006-accessibility-as-a-first-class-concern.md","Context":"## Context\\nThis tool had, up until this point, made assumptions about how its users might interpret the information it presents (for example, using colour as the main mechanism of distinguishing record types)\\n\n## Decision\n","Decision":"Accessibility will now be a first-class concern of this project. All future design decisions should bear this in mind.\\n","tokens":40,"id":3815,"Prediction":"## Decision\nTo ensure better accessibility and clearer communication of information, the tool will be redesigned to include additional methods of distinguishing record types beyond color alone. This will include the use of:\n\n1. **Text Labels**: Adding textual indicators or labels to clearly identify different record types.\n2. **Iconography**: Incorporating distinct icons or symbols that represent each record type, ensuring these icons are easily recognizable and understandable.\n3. **Patterns and Textures**: Utilizing different patterns or textures for background elements associated with various record types to help users who may have difficulty distinguishing colors.\n4. **Tooltips**: Implementing tooltips that appear on hover, providing descriptive information about the record type.\n5. **Contrast and Differentiation**: Ensuring high contrast between different colors and improving differentiation to aid users with color vision deficiencies.\n\nThese changes will be guided by accessibility best practices to accommodate a broader range of users, including those with visual impairments or color blindness. Additionally, testing with diverse user groups will be conducted to validate the effectiveness of these enhancements.","GenTime":"2024-07-29 01:14:52"}
{"File Name":"log430-dashview-architecture\/0002-use-java-to-implement-complete-solution.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3820,"Prediction":"## Context\nThe current application performance has significantly degraded due to increasing user traffic and complex data processing needs. The monolithic architecture of the application results in long deployment times and difficulty in scaling individual components. The team needs a more efficient way to manage, develop, and scale different parts of the application independently.\n\n## Decision\nWe will transition from a monolithic architecture to a microservices architecture. This will involve breaking down the application into smaller, self-contained services that encapsulate specific business functionalities. Each microservice will be independently deployable, scalable, and written using the most appropriate technology stack for its needs. We will use containerization (e.g., Docker) to ensure that each microservice is isolated and can run in different environments without issues. Kubernetes will be implemented for orchestration to automate deployment, scaling, and management of containerized applications. This will allow us to handle increased user traffic more efficiently and improve overall application performance and development agility.","GenTime":"2024-07-29 01:15:06"}
{"File Name":"meadow\/0011-yarn.md","Context":"## Context\\nWe discussed the relative merits of different JS package managers.\\n\n## Decision\n","Decision":"Use `yarn` instead of `npm` in all dev, test, and build environments.\\n","tokens":15,"id":3827,"Prediction":"## Decision\n\nWe have chosen to use Yarn as our JavaScript package manager.\n\n### Rationale:\n1. **Performance**: Yarn offers faster dependency installation compared to npm due to its parallel installation process and efficient caching mechanisms. This will speed up our CI\/CD pipelines and improve local development efficiency.\n\n2. **Deterministic Dependency Resolution**: Yarn uses a lockfile (`yarn.lock`) which ensures that the same dependencies are installed across different environments. This deterministic approach helps avoid the \"works on my machine\" problem and ensures more consistent builds.\n\n3. **Security**: Yarn performs additional security checks on installed packages. It not only validates checksums but also checks for potential vulnerabilities, providing an extra layer of security.\n\n4. **Workspaces**: Yarn's workspaces feature is beneficial for monorepo setups, allowing us to manage multiple packages within a single repository efficiently. This aligns well with our plans to modularize our codebase.\n\n5. **Enhanced Features**: Features like zero-install, strict package integrity, and better offline support are valuable for our workflow requirements and will provide a more robust development experience.\n\n6. **Community and Ecosystem**: Yarn has a strong community and an active maintenance team. It is widely adopted in the industry, ensuring continued support and improvement.\n\nBy selecting Yarn, we aim to leverage its superior performance, security, and additional features to enhance our development workflow and maintain high standards for dependency management.","GenTime":"2024-07-29 01:15:38"}
{"File Name":"meadow\/0009-tailwind-css-framework.md","Context":"## Context\\nWe have used opinionated CSS frameworks such as Twitter Bootstrap in the past, and have found that we spend too much effort and time working around those opinions. Tailwind CSS offers an alternative approach that allows us to iterate quickly with minimal interference from the framework by allowing us to add layout and styles directly in our HTML rather than CSS.\\n\n## Decision\n","Decision":"Use the Tailwind CSS framework for design and layout.\\n","tokens":70,"id":3829,"Prediction":"## Decision\n\nWe have decided to adopt Tailwind CSS as our primary CSS framework for the project. Tailwind CSS's utility-first approach will enable us to:\n\n1. **Improve Development Speed**: By allowing us to apply styles directly in our HTML, we can iterate more quickly and reduce the overhead of context-switching between HTML and CSS files.\n  \n2. **Reduce CSS Conflicts**: With Tailwind, we can minimize conflicts caused by overriding base styles from opinionated frameworks like Bootstrap, letting us focus on custom styles specific to the project requirements.\n\n3. **Maintain Consistency**: Tailwind's utility classes will help ensure consistency across the application by providing a consistent set of styles without the need for extensive custom CSS.\n\n4. **Simplify Responsive Design**: Tailwind's built-in responsive utilities will make it easier to implement and manage responsive layouts without having to write custom media queries.\n\nThis decision is expected to streamline our workflow, lead to cleaner code, and enhance the maintainability of our styles throughout the project's lifecycle.","GenTime":"2024-07-29 01:15:45"}
{"File Name":"meadow\/0018-preservation-storage-object-naming-scheme.md","Context":"## Context\\nThe application needs a way to store objects in preservation buckets that facilitate upload and retrieval and allows for duplicate file names among FileSets associated with a Work object.\\n\n## Decision\n","Decision":"Use a combination of the pairtree of a FileSet ULID plus its sha256 checksum as the key for S3 objects in the preservation bucket. So an FileSet with a ULID `01dpxt2xajvkdsbckqrs8ry677`\\nand sha256 checksum `87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`will be stored in s3 as\\n`s3:\/\/[preservation-bucket]\/01\/dp\/xt\/2x\/01dpxt2xajvkdsbckqrs8ry677\/87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`\\n","tokens":35,"id":3831,"Prediction":"## Decision\nTo address the requirement for storing objects in preservation buckets while allowing duplicate file names among FileSets associated with a Work object, we will implement an object storage solution using a hierarchical naming convention for the objects in the bucket.\n\n1. **Namespace Isolation**: Each Work object will have its own namespace or prefix within the bucket. This can be done by creating a unique identifier for each Work object (e.g., a UUID or the Work object's database primary key) and using it as a top-level directory in the bucket.\n\n2. **Hierarchical Naming**: Within the namespace for each Work object, FileSets will be stored using a combination of their identifiers and original file names. For example, if a FileSet has an identifier `12345` and the file name is `document.pdf`, the object\u2019s key in the bucket would be `workUUID\/12345_document.pdf`.\n\n3. **Metadata Storage**: Metadata for each file can be stored either inline (as part of the object metadata in the storage system) or in an auxiliary database. This metadata may include information such as file type, size, creation date, and any relationships to other files.\n\n4. **Handling Duplicates**: To handle duplicate file names, the system will rely on the hierarchical naming scheme. The unique identifier assigned to each FileSet guarantees that even if two FileSets under the same Work object have the same original file name, their keys in the object storage will be unique (e.g., `workUUID\/12345_document.pdf` and `workUUID\/67890_document.pdf`).\n\n5. **Implementation Considerations**: \n   - **Object Storage Service**: We will use a reliable object storage service, such as Amazon S3, Google Cloud Storage, or Azure Blob Storage, which supports the hierarchical naming and metadata features required.\n   - **Access Patterns**: The application code responsible for uploading, retrieving, and managing these objects will need to include functions to construct and parse the hierarchical keys.\n   - **Security and Permissions**: Proper access controls and encryption mechanisms should be in place to ensure data integrity and security.\n\nWith this approach, we achieve an organized and scalable method for object storage that supports easy uploads and retrievals while allowing duplicate file names within the defined namespaces of Work objects.","GenTime":"2024-07-29 01:15:55"}
{"File Name":"meadow\/0029-npm.md","Context":"## Context\\nThe latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.\\nSupersedes [11. Yarn](0011-yarn.md)\\n\n## Decision\n","Decision":"Switch back to `npm` instead of `yarn` in all dev, test, and build environments.\\n","tokens":37,"id":3841,"Prediction":"## Decision\n\nSwitch from Yarn to npm as the primary package manager for our project.\n\n### Rationale:\n\n1. **Compatibility Issues**: The latest upgrade of Yarn has introduced compatibility issues that are affecting our development workflow. Despite various attempts, these issues have proven difficult to resolve.\n\n2. **Community Support**: npm has a larger community and more extensive documentation compared to Yarn. This can provide us with better support and resources to resolve any issues that may arise in the future.\n\n3. **Maintenance Overhead**: Managing and troubleshooting the issues caused by the latest Yarn upgrade is consuming significant time and resources. Switching to npm, which is known for its stability, can reduce this overhead.\n\n4. **Seamless Transition**: Given that npm is integrated with Node.js and widely used across various projects, the transition from Yarn to npm is expected to be relatively seamless.\n\n5. **Future-Proofing**: npm continues to evolve with regular updates and new features. By switching now, we can ensure our project remains future-proof and aligns with the latest industry standards.\n\n### Implementation:\n\n1. **Project Dependencies**: Update the project configuration files to replace Yarn with npm for dependency management.\n   \n2. **Scripts Update**: Modify build and deployment scripts to use npm commands instead of Yarn commands.\n   \n3. **Documentation**: Update the project documentation to reflect the switch to npm and provide guidance for developers on using npm.\n\n4. **Testing**: Conduct thorough testing to ensure that the switch does not introduce any new issues or regressions.\n\nBy making this decision, we aim to mitigate the issues introduced by the latest Yarn upgrade and streamline our development process.","GenTime":"2024-07-29 01:16:42"}
{"File Name":"meadow\/0012-websockets.md","Context":"## Context\\nWe need a way to provide live updates to the front-end for ingest sheet validation,\\ningest status, etc.\\n\n## Decision\n","Decision":"We will use the [WebSocket API](https:\/\/www.w3.org\/TR\/websockets\/) via [Phoenix Channels](https:\/\/hexdocs.pm\/phoenix\/channels.html) to enable real-time communication between the client and server.\\n","tokens":27,"id":3845,"Prediction":"## Decision\n\nWe will implement WebSockets to provide live updates to the front-end. WebSockets allow for real-time, two-way communication channels between the server and client, which is essential for providing immediate feedback to users for ingest sheet validation, ingest status updates, and other real-time notifications.\n\n### Rationale:\n\n1. **Real-time Communication**: WebSockets support bi-directional communication, ensuring that any changes or updates from the server can be instantaneously pushed to the front-end.\n2. **Efficiency**: Unlike traditional HTTP requests or polling, WebSockets maintain an open connection, reducing the overhead associated with opening and closing connections repeatedly.\n3. **User Experience**: Real-time updates significantly improve the user experience by providing timely and continuous feedback, reducing the need for users to manually refresh their view.\n4. **Scalability**: Modern WebSocket libraries and frameworks provide robust solutions that can scale with the application, handling potentially large numbers of concurrent connections efficiently.\n\n### Implementation Considerations:\n\n1. **Server Support**: Ensure that the backend infrastructure supports WebSockets. This might involve choosing a suitable WebSocket server or middleware that integrates well with the existing stack.\n2. **Client Integration**: Update the front-end to establish and manage WebSocket connections, handle incoming messages, and update the UI accordingly.\n3. **Security**: Implement proper security measures to safeguard WebSocket connections, such as using Secure WebSockets (wss:\/\/) and proper authentication\/authorization mechanisms.\n4. **Error Handling and Reconnection**: Develop strategies for handling disconnections, errors, and reconnection attempts to make the WebSocket connections resilient.\n\nBy adopting WebSockets, we ensure that our front-end remains dynamically updated with the latest data, providing a seamless and responsive experience for our users.","GenTime":"2024-07-29 01:16:57"}
{"File Name":"meadow\/0022-iiif-manifests.md","Context":"## Context\\nCurrently requests for public manifests are routed through Donut in order to be re-cached in S3 if needed. We need to plan a strategy to handle IIIF manifest requests for manifests which were written by Meadow and should not be routed through Donut.\\n\n## Decision\n","Decision":"- Manifests will be written for all works (public\/private\/restricted) on create\/update.\\n- IIIF Manifests will be written on to the `public` directory of the existing stack-\\*-pyramids bucket, and will live alongside existing Donut manifests.\\n- Requests for public manifests moving forward will route from the API gateway to a new lambda which will check Elasticsearch for the host application. Then it will either route directly to the S3 Pyramids `\/public` folder (Meadow) or to Donut. (This is temporary, until Donut content is migrated.)\\n- Meadow will use these manifests internally\\n","tokens":55,"id":3852,"Prediction":"## Decision\n\nTo address the need for distinguishing IIIF manifest requests that should bypass Donut and instead be handled directly by Meadow, the following strategic decisions will be implemented:\n\n1. **Endpoint Differentiation**: Implement distinct URL endpoints or subdomains for manifests originating from Meadow versus those requiring Donut processing. For instance:\n   - `meadow.example.com\/manifests\/{id}` for Meadow-manifests.\n   - `donut.example.com\/manifests\/{id}` for Donut-managed manifests.\n   \n2. **Routing Logic**: Update the routing logic in the application to determine the source of the IIIF manifest request based on the predefined URL patterns. Requests matching the Meadow pattern will be routed directly to Meadow, while others will be routed to Donut.\n\n3. **Identifier Namespace**: Apply different identifier namespaces or prefixes for manifest IDs generated by Meadow and those that Donut should handle. Update Donut and Meadow to recognize and act upon these namespaces appropriately.\n\n4. **Configuration Management**: Manage this routing and differentiation through configurable settings. This will provide flexibility to quickly adapt or expand the routing criteria without altering core code logic.\n\n5. **Logging and Monitoring**: Implement logging and monitoring to assess the routing performance and ensure requests are correctly directed. This will help in identifying any misrouted requests and proactively manage issues.\n\n6. **Gradual Rollout**: Roll out the changes incrementally, starting with a limited set of manifests. Monitor the behavior and make adjustments as necessary before expanding the rollout to all manifests.\n\n7. **Fallback Mechanism**: Establish a fallback mechanism where if a direct Meadow request fails, it attempts to retrieve via Donut ensuring availability while not compromising performance significantly, keeping track of these instances for further analysis and rectification.\n\nBy adopting these strategies, we ensure that manifest requests are handled efficiently and routed correctly based on their origin, thereby optimizing the overall system performance and maintainability.","GenTime":"2024-07-29 01:17:21"}
{"File Name":"meadow\/0007-code-analysis.md","Context":"## Context\\nWe need to make sure we adhere to our own designated code quality best practices.\\n\n## Decision\n","Decision":"Use a code analysis tool (specifically, [credo](http:\/\/credo-ci.org\/) for Elixir\\nand [prettier](https:\/\/prettier.io\/) for JavaScript).\\n","tokens":20,"id":3853,"Prediction":"## Decision\nTo ensure adherence to our designated code quality best practices, we will implement the following measures:\n\n1. **Code Reviews:** All new code must undergo thorough peer reviews before being merged. This includes checking for adherence to coding standards, best practices, and overall code quality.\n   \n2. **Automated Code Quality Tools:** Integrate automated tools such as linters, static code analyzers, and code formatting tools into our development pipeline. Tools like ESLint, SonarQube, and Prettier will be set up to run automatically on every commit.\n\n3. **Unit Testing:** Require comprehensive unit tests for all new features and modifications. Establish a minimal code coverage threshold of 80% that must be met before any code can be merged.\n\n4. **Continuous Integration (CI):** Set up a CI pipeline to automatically run tests and code quality checks on every commit to the main branch. Any code that fails these checks will be rejected.\n\n5. **Code Quality Documentation:** Create and maintain detailed documentation on our code quality best practices. Make this documentation easily accessible to all team members and regularly review and update it as necessary.\n\n6. **Training and Mentoring:** Provide regular training sessions and workshops on best practices in coding and code review. Mentorship programs will be established to help less experienced developers understand and adopt these practices.\n\nBy implementing these measures, we will foster a culture of quality and consistency in our codebase, ultimately leading to more maintainable and reliable software.","GenTime":"2024-07-29 01:17:25"}
{"File Name":"meadow\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3854,"Prediction":"## Decision\nTo address the need for recording architectural decisions in this project, we will implement an Architectural Decision Record (ADR) process. Each decision made throughout the project will be systematically captured using a standard template that includes the context, decision, status, consequences, and any other relevant information. This process will promote transparent communication, provide a historical record for future reference, and help maintain alignment across the development team.\n\nSpecifically, we will:\n1. Adopt a standardized ADR template that aligns with industry best practices.\n2. Store all ADRs in a dedicated directory within the project repository to ensure easy access and version control.\n3. Encourage regular review and updates to ADRs to reflect any changes or new insights.\n4. Assign the responsibility of ADR documentation to specific team members to ensure consistency and completeness. \n\nBy implementing this ADR process, we aim to enhance decision-making efficiency, support knowledge sharing, and improve overall project governance.","GenTime":"2024-07-29 01:17:27"}
{"File Name":"android-guidelines\/0009-gradle-rules.md","Context":"## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n\n## Decision\n","Decision":"* Disable generating BuildConfig file if its not needed in module.\\n* Only enable *databinding* if you're going to use DataBinding in that module.\\n* Do not apply *kapt* plugin if you're not going to use.\\n* Do not create new variants other than *debug* and *release*.\\n* Use groovy scripts on *build.gradle* files.\\n","tokens":37,"id":3860,"Prediction":"## Decision\n\nTo address the increased build times due to the high number of modules and the usage of unnecessary plugins and gradle.kts, the following decisions are made:\n\n1. **Migrate to Pure Kotlin Gradle Scripts**: Gradle.kts will be fully optimized and unnecessary scripts will be removed or consolidated. This will help in reducing the script execution times.\n\n2. **Disable Unused Plugins**: A thorough audit of currently enabled plugins across all modules will be conducted. Any plugin that is not necessary for the specific module or project will be disabled.\n\n3. **Enable Configuration Caching**: Gradle provides a feature called configuration caching that can drastically improve the build time by caching the project configuration. This will be enabled.\n\n4. **Parallel Build Execution**: Utilize Gradle's parallel execution feature which allows independent modules to be built in parallel, reducing overall build times.\n\n5. **Modularization Strategy Review**: Review the current modularization strategy. Over-segmentation into modules can introduce unnecessary overhead. Consolidate modules where appropriate to balance modularity and performance.\n\n6. **Incremental Compilation**: Ensure that incremental compilation is properly configured and optimized to make sure that only the necessary parts of the code are recompiled.\n\n7. **Dependency Management**: Implement strict dependency management practices to avoid unnecessary dependencies being included in builds.\n\nBy implementing these decisions, we aim to significantly improve the build times and overall efficiency of the build process.","GenTime":"2024-07-29 01:17:54"}
{"File Name":"android-guidelines\/0007-use-deprecated-annotation-with-description.md","Context":"## Context and Problem Statement\\nDeprecated annotation usages without any comment or suggestion makes no sense when using existing classes.\\n\n## Decision\n","Decision":"When we decide to use deprecated annotation in classes, we need to comment reason and alternative to that implementation.\\n","tokens":24,"id":3863,"Prediction":"## Decision\n\nWe will address the usage of deprecated annotations by implementing the following measures:\n\n1. **Integrate Deprecation Warnings with Documentation**: Each deprecated annotation will be accompanied by informative comments that explain the reason for deprecation and suggest alternative solutions or classes to use instead.\n\n2. **Deprecation Policy Compliance**: Adhere strictly to a deprecation policy where any class or method marked as deprecated will include the `@Deprecated` annotation along with Javadoc comments. These comments will contain:\n    - The version in which the deprecation was introduced.\n    - The reason behind the deprecation.\n    - Recommended alternatives to use in place of the deprecated feature.\n\n3. **Code Review Enforcement**: Implement a code review checklist that includes verifying that any deprecation is properly documented with a clear migration path.\n\n4. **Deprecation Logging**: Introduce logging for deprecated features at runtime (if applicable) to alert developers and users when they are using deprecated functionality.\n\n5. **Continuous Communication**: Ensure continuous communication with the development team about deprecation updates via internal documentation, team meetings, and internal mailing lists.\n\n6. **Automated Tools Integration**: Utilize static analysis tools that can detect deprecated usages and provide automated reminders or suggestions for developers to migrate to newer alternatives.\n\nImplementing these measures will help maintain clarity in the codebase, facilitate easier transitions away from deprecated features, and improve overall software maintainability.","GenTime":"2024-07-29 01:18:06"}
{"File Name":"cygnus-infra\/0002-use-debian-as-a-host-os.md","Context":"## Context\\nI used the following points to make my decision:\\n- Ubuntu 20.04 uses a unique auto-installation method, which is generally nice, but completely non-transferable\\n- `snapd` remains controversial, but using Ubuntu systems forces that path more nowadays\\n- Debian should be relatively \"install and forget\"\\n- Centos is more opinionated about some things like how network configuration is done\\n- Centos doesn't do automatic security upgrades, at least not in the same way RHEL advertises\\n- I've successfully used Debian before for this role, before trying out Ubuntu again\\n- Remastering a Debian iso with the preseed configuration can result in a one-button install process\\n\n## Decision\n","Decision":"Use Debian buster as the hypervisor OS, with an automated installation via Debian preseeding.\\n","tokens":145,"id":3868,"Prediction":"## Decision\n\nBased on the considerations outlined in the context, I have decided to use Debian as the operating system for our project.\n\n### Justification\n1. **Transferability and Maintenance**:\n   - Ubuntu's auto-installation method is unique and not easily transferable to other systems or setups. In contrast, Debian's installation and configuration processes are more standard and predictable, making it easier to maintain and replicate in different environments.\n\n2. **Software Management**:\n   - The controversy surrounding `snapd` in Ubuntu and its increasing integration into the system adds an unnecessary layer of complexity. Debian's package management system via `apt` is straightforward and has proven reliability without introducing contentious subsystems.\n\n3. **Simplicity and Reliability**:\n   - Debian is known for being an \"install and forget\" system, which aligns well with our need for a stable and low-maintenance environment.\n\n4. **Network and Security Management**:\n   - Centos has more opinionated network configurations, which might complicate our setup. Additionally, its approach to security upgrades is less straightforward compared to what is advertised for RHEL, potentially leading to more manual intervention.\n   \n5. **Established Success**:\n   - Previous success with Debian in this role provides confidence in its ability to meet our requirements efficiently. \n\n6. **Customization**:\n   - Remastering a Debian ISO with preseed configurations can enable a seamless one-button installation process, enhancing deployment efficiency for our team.\n\nGiven these points, Debian strikes the optimal balance between stability, ease of use, and maintainability for our project's needs.","GenTime":"2024-07-29 01:18:26"}
{"File Name":"buddy-auth.jwt-oidc\/adr-001.md","Context":"# Context #\\nThe library currently assumes that the ID tokens are signed by a\\npublic key (and the code unconditionally tries to build the actual\\npublic key from the data we get from the ID Token issuer).\\nAt the same time, the OpenID Connect Core 1.0 specification states\\nthat:\\n> \"Symmetric signatures MUST NOT be used by public (non-confidential)\\n> Clients because of their inability to keep secrets\"\\n(see https:\/\/openid.net\/specs\/openid-connect-core-1_0.html#Signing).\\nAs we don't expect to support confidential clients, it seems safe to\\nignore Symmetric Keys when validating tokens.\\n# Decision #\\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n\n## Decision\n","Decision":"Tokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","tokens":229,"id":3872,"Prediction":"## Decision\n\nWe will implement a strict validation policy in the library which treats tokens signed with Symmetric Keys as invalid. Specifically, when a token is received, the library will check the signing mechanism. If a Symmetric Key is detected, the library will unconditionally mark the token as invalid, regardless of the actual validity of the token. This aligns with the OpenID Connect Core 1.0 specification recommendation that symmetric signatures should not be used by public (non-confidential) clients due to security concerns.\n\nThis decision is made to enhance security, ensuring that the library adheres to established standards and avoids potential vulnerabilities associated with the use of symmetric keys by public clients.","GenTime":"2024-07-29 01:18:38"}
{"File Name":"bosh-bootloader\/0002-replace-go-bindata-with-packr2.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https:\/\/twitter.com\/francesc\/status\/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n\n## Decision\n","Decision":"Use [Packr2](https:\/\/github.com\/gobuffalo\/packr\/tree\/master\/v2) instead.\\n","tokens":126,"id":3873,"Prediction":"## Decision\nWe have decided to replace the go-bindata dependency with an alternative asset bundling tool that is more stable and actively maintained. The new tool should provide clearer development use cases for bundling assets during test runs and final builds. Possible alternatives include `packr`, `statik`, or `vfsgen`. \n\nBy making this change, we aim to improve the reliability and maintainability of our codebase, ensuring that our development processes remain transparent and predictable. This move will also mitigate any risks associated with the uncertainty surrounding the current go-bindata repository and its maintainers.","GenTime":"2024-07-29 01:18:40"}
{"File Name":"gsp\/ADR004-tenant-isolation.md","Context":"## Context\\nThe two main isolation models for Kubernetes are:\\n* Namespaces within a single cluster\\n* Running multiple clusters\\nAll Service Teams currently have separate AWS accounts.\\nSome Service Teams have separate AWS accounts for separate environment (ie. Staging, Production etc)\\nMany Service Teams have micro-service architectures\\nSome Service Teams have unique network isolation requirements that may be hard to implement in a shared environment.\\nTo ensure \"smooth transition\" during a migration it would be preferable to have clusters deployed to Service Team's VPCs.\\nTo ensure separation of billing it would be preferable to deploy clusters to Service Team's AWS accounts.\\nTo ensure strong network\/compute isolation between Service Teams it would be preferable to deploy separate clusters for separate environments.\\n\n## Decision\n","Decision":"We will manage multiple Kubernetes clusters, one per environment per tenant.\\nThis should provide:\\n* Strong network\/compute isolation between service teams\\n* Clear separation of billing\\n* Ability to deploy into target VPCs to make piecemeal microservice migration practical\\n","tokens":153,"id":3889,"Prediction":"Based on the provided context, the decision must balance the requirements for isolation, billing, and smooth transitions during any migration processes. Here is a proposed decision:\n\n## Decision\n\nThe organization will adopt a multi-cluster strategy across dedicated AWS accounts and VPCs for each Service Team to address the following concerns:\n\n1. **Network and Compute Isolation**:\n   - Each Service Team will have its own Kubernetes cluster deployed within their specific AWS account and VPC.\n   - For Service Teams with unique network isolation requirements, dedicated clusters will be configured to meet these specifications, ensuring that there is no cross-communication unless explicitly allowed.\n\n2. **Billing Separation**:\n   - Deploying clusters within each Service Team's AWS account ensures that billing can be tracked and managed independently, as costs will be isolated to the individual accounts.\n   - This approach aligns with existing practices of maintaining separate AWS accounts for different environments (e.g., Staging, Production).\n\n3. **Environment Separation**:\n   - Multiple clusters will be used to separate different environments (e.g., Staging, Production) within each Service Team's AWS account. This ensures that testing and production workloads do not interfere with one another.\n\n4. **Smooth Migration Transition**:\n   - Deploying clusters within Service Team's existing VPCs supports a smoother transition during migration phases as teams will have ownership and control over their Kubernetes infrastructure.\n   - This approach allows Service Teams to leverage their current network setups and security configurations without needing significant changes.\n\nBy adopting this strategy, the organization can achieve a balance of strong isolation, clear billing separation, and minimized disruption during migrations, aligning with the requirements and existing infrastructure practices of the Service Teams.","GenTime":"2024-07-29 01:19:36"}
{"File Name":"gsp\/ADR043-k8s-resource-access.md","Context":"## Context\\nSeveral different levels of access are required within a kubernetes cluster. The\\nGSP uses role based access control so these levels are granted to users and\\ngroups via roles.\\n\n## Decision\n","Decision":"We will create two levels of access within each namespace:\\n* Operator\\n* Auditor\\nThe Operator is a relatively permissive read-write role within the namespace.\\nDevelopers working on branches that are not part of the release process may be\\ngranted this role in certain namespaces. This is also the role the in-cluster\\nconcourse team for each namespace will be granted.\\nThe Auditor will be given to all authenticated users in the cluster. This should\\nallow for debugging of issues and incidents and basic remedial actions without\\nneeding formal escalation procedures.\\nThe complete list of resource permissions is given in Appendix A.\\n","tokens":40,"id":3890,"Prediction":"## Decision\nWe will implement Role-Based Access Control (RBAC) within the Kubernetes cluster to manage access levels for different users and groups. This will involve the creation of Roles and RoleBindings (or ClusterRoles and ClusterRoleBindings for cluster-wide permissions) to define and grant the necessary permissions.\n\n1. **Define Roles and ClusterRoles:** \n   - Roles will be created to specify the permissions needed at the namespace level.\n   - ClusterRoles will be created for permissions that span across the entire cluster.\n\n2. **Create RoleBindings and ClusterRoleBindings:**\n   - RoleBindings will be used to bind users or groups to Roles within specific namespaces.\n   - ClusterRoleBindings will bind users or groups to ClusterRoles for cluster-wide permissions.\n\n3. **User and Group Management:**\n   - Identify and categorize users and groups based on their required access levels.\n   - Assign appropriate Roles and ClusterRoles to these users and groups through RoleBindings and ClusterRoleBindings.\n\n4. **Policy Enforcement and Auditing:**\n   - Implement policies to ensure that access levels are strictly enforced.\n   - Regularly audit Roles, RoleBindings, ClusterRoles, and ClusterRoleBindings to ensure they meet the current access requirements and comply with security policies.\n\nBy leveraging Kubernetes RBAC, we can ensure a scalable, manageable, and secure approach to controlling access within the cluster, aligning with the needs of different users and groups.","GenTime":"2024-07-29 01:19:40"}
{"File Name":"gsp\/ADR025-ingress.md","Context":"## Context\\nWe currently have two [ingress][Ingress] systems:\\n* Istio (see [ADR019])\\n* nginx-ingress (see the old Ingress [ADR005])\\nIstio's [Virtual Service] records are essentially advanced `Ingress` records.\\nDo we need both?\\n\n## Decision\n","Decision":"No. We will use an [Istio Ingress Gateway](https:\/\/istio.io\/docs\/tasks\/traffic-management\/ingress\/ingress-control\/)\\n","tokens":69,"id":3905,"Prediction":"## Decision\n\nAfter thoroughly evaluating the current architecture and operational requirements, it has been decided to streamline our ingress systems for simplicity, maintainability, and consistency. We will migrate away from the nginx-ingress system and consolidate our ingress management under Istio.\n\n### Justification:\n\n1. **Unified Configuration**:\n   - Istio provides more advanced and flexible routing capabilities through its Virtual Service records, which can handle complex routing rules. This unification will reduce the complexity by having a single configuration point.\n\n2. **Enhanced Features**:\n   - Istio offers robust features like traffic shifting, fault injection, and mirroring which are not natively available with nginx-ingress.\n   \n3. **Operational Efficiency**:\n   - Maintaining and troubleshooting multiple ingress controllers can be operationally burdensome. Consolidating to a single system reduces this burden and aligns with our strategy for operational efficiency.\n\n4. **Security**:\n   - Istio's integrated security features such as mutual TLS and fine-grained control over traffic policies provide stronger security mechanisms compared to a traditional nginx-ingress setup.\n\n5. **Scalability**:\n   - Istio's architecture is designed for scalable microservices environments, providing better future-proofing for our growing services landscape.\n\n### Migration Plan:\n\n- **Phase 1: Inventory and Analysis**:\n  - Identify all applications and services currently using nginx-ingress.\n  - Analyze the specific ingress rules and patterns to plan equivalent Istio Virtual Service configurations.\n\n- **Phase 2: Configuration and Testing**:\n  - Develop Istio Virtual Service configurations to replicate all existing nginx-ingress rules.\n  - Set up a staging environment to test these configurations and ensure feature parity without impacting production services.\n\n- **Phase 3: Gradual Rollout**:\n  - Roll out the Istio Virtual Service configurations in a phased approach, starting with low-risk, non-critical applications.\n  - Monitor traffic, performance, and stability closely during this transition.\n\n- **Phase 4: Decommissioning**:\n  - Once all applications are successfully migrated and stable on Istio, decommission the nginx-ingress system.\n  - Update all documentation and operational runbooks to reflect the change.\n\nThis decision will position us better for scalability, operational efficiency, and enhanced feature utilization, facilitating smoother operation and easier management of our ingress requirements.","GenTime":"2024-07-29 01:20:42"}
{"File Name":"gsp\/ADR041-service-operated-policies.md","Context":"## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk\/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role\/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n\n## Decision\n","Decision":"* We will provision policy that gives full access for users to _use_ the\\nprovisioned service.\\n* We will avoid provisioning policy that allows users to create, destroy or\\nconfigure the provisioned service, so that this can remain the declarative\\ndomain of the service-operator.\\n","tokens":335,"id":3907,"Prediction":"## Decision\n\nTo properly segregate the management responsibilities and user capabilities around the provisioned S3 buckets, the following approach will be adopted:\n\n1. **Separation of IAM Policies**: IAM roles and policies will be structured in such a way that there is a clear separation between the permissions granted to the service operator and those granted to the users of the provisioned resources. This will ensure that users can fully utilize S3 buckets without interfering with the configurations managed by the service operator.\n\n2. **Service Operator IAM Role**:\n   - The service operator will have a dedicated IAM role with comprehensive permissions to manage S3 bucket configurations such as bucket ACL, versioning, and lifecycle policies.\n   - This role will include permissions like `s3:PutBucketLifecycleConfiguration`, `s3:PutBucketVersioning`, and `s3:PutBucketAcl` to handle declaration and ongoing reconciliation of these properties.\n \n3. **User IAM Roles and Policies**:\n   - Users will be granted IAM roles with permissions limited to interacting with the S3 bucket for daily operations such as reading, writing, and managing objects. This includes permissions like `s3:GetObject`, `s3:PutObject`, `s3:DeleteObject`, and `s3:ListBucket`.\n   - Crucially, users will not have permissions to alter bucket-wide configurations that are under the control of the service operator, such as `s3:PutBucketLifecycleConfiguration`, `s3:PutBucketVersioning`, or `s3:PutBucketAcl`.\n\n4. **Policy Enforcement**:\n   - The desired IAM policies will be enforced programmatically as part of the service operator's provisioning process. The operator will automatically generate and apply these policies to IAM roles in AWS.\n   - Any deviations in IAM policies observed during periodic reconciliations will be corrected to ensure adherence to this decision.\n\n5. **Read-Only Configurations**:\n   - Users shall have read-only access to the bucket configurations managed by the service operator. This allows users to view the applied lifecycle policies, versioning status, and ACL settings without being able to modify them.\n   - This can be achieved through permissions like `s3:GetBucketLifecycleConfiguration`, `s3:GetBucketVersioning`, and `s3:GetBucketAcl`.\n\nBy adopting this approach, we ensure a robust and clear separation between the responsibilities of the service operator and the users. This mitigates potential conflicts arising from user-initiated changes to configurations managed by the service operator, while still providing users with necessary operational capabilities.","GenTime":"2024-07-29 01:21:01"}
{"File Name":"gsp\/ADR019-service-mesh.md","Context":"## Context\\nVerify have a need to restrict exfiltration of data, enforce strict authentication between microservices and to use mutual TLS.\\nA service mesh gives us a way of meeting these needs.\\n### Option 1: Istio\\nIstio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, with few or no code changes in service code.\\nIstio supports services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, you then configure and manage Istio using its control plane functionality, which includes:\\n- Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.\\n- Fine-grained control of traffic behaviour with rich routing rules, retries, fail-overs, and fault injection.\\n- A pluggable policy layer and configuration API supporting access controls, rate limits and quotas.\\n- Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress.\\n- Secure service-to-service communication in a cluster with strong identity-based authentication and authorisation.\\nPros\/cons:\\n- an emerging standard (installed by default on GKE)\\n- a large community of contributors\\n### Option 2: AWS App Mesh (Istio from AWS)\\n[AWS App Mesh](https:\/\/aws.amazon.com\/app-mesh\/) is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications\\npros\/cons:\\n- Unavailable in London region\\n- Did not support automatic sidecar injection (meaning service teams would have to add lots of extra configuration to their Deployments)\\n- Appears to be abstraction over Istio\\n### Option 3: Linkerd 1.x & 2.0\\n[Linkerd](https:\/\/linkerd.io\/) is an ultra light service mesh for Kubernetes. It gives you observability, reliability, and security without requiring any code changes.\\nPros\/cons:\\n- 1.0 has a richer feature set but poorer kubernetes support\\n- 2.0 has a very minimal feature set but native kubernetes support\\n- Going through major rewrite for improved Kubernetes support\\n- Smaller community\\n- Fewer features around\\n\n## Decision\n","Decision":"We will use [Istio](https:\/\/istio.io\/) to provide a service mesh in the GDS Supported Platform.\\n","tokens":498,"id":3909,"Prediction":"## Decision\n\nWe have decided to adopt Istio as our service mesh solution.\n\n### Justification\n\nBased on the context, our primary requirements are:\n\n1. **Restrict exfiltration of data**\n2. **Enforce strict authentication between microservices**\n3. **Use mutual TLS for secure service-to-service communication**\n\n### Why Istio?\n\n1. **Comprehensive Feature Set**:\n   - Istio offers a robust set of features out of the box that match our needs such as automatic load balancing, fine-grained traffic control, and integrated policy enforcement.\n\n2. **Security**:\n   - Istio provides secure service-to-service communication in a cluster with strong identity-based authentication and authorization, ensuring our microservices only communicate over mutually authenticated and encrypted channels.\n\n3. **Monitoring and Observability**:\n   - Istio automatically collects metrics, logs, and traces for all traffic within a cluster, which supports our need for monitoring, auditing, and debugging.\n\n4. **Community and Support**:\n   - Istio has a large community of contributors, providing extensive support, plugins, and integrations. It is an emerging standard and installed by default on Google Kubernetes Engine (GKE), indicating strong adoption and sustained development.\n\n### Comparison with Other Options\n\n1. **AWS App Mesh**:\n   - Although it provides a similar feature set, it is not available in the London region, which adds complexity to our deployment and support model.\n   - It lacks automatic sidecar injection, meaning additional overhead for service teams in terms of configuration.\n\n2. **Linkerd**:\n   - Linkerd 1.x has richer features but poorer Kubernetes support, which is critical for our Kubernetes-based deployment.\n   - Linkerd 2.x, while offering native Kubernetes support, has a minimal feature set which does not meet our comprehensive needs.\n   - The smaller community and ongoing major rewrites introduce risks and potential delays in stability and support.\n\n### Conclusion\n\nConsidering our requirements for a robust, secure, and well-supported service mesh solution, Istio stands out as the most suitable option. It comprehensively meets our needs with a mature, feature-rich offering backed by a large community and active development.","GenTime":"2024-07-29 01:21:06"}
{"File Name":"gsp\/ADR022-logging.md","Context":"## Context\\nWe have various log sources:\\n* The EKS control plane (audit logs, api service, scheduler, control-manager)\\n* VPC Flow logs\\n* Load Balancer\\n* Pod\/Application logs\\n* CloudHSM\\nAll of these with the exception of the Pod\/Application logs are stored by AWS in [CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/).\\nWe would like a single storage location for indexing and search our logs for auditing and debugging purposes.\\nGDS currently have several common storage locations for logs:\\n* Logit.io (a SaaS ELK stack provider)\\n* Self hosted ELK stacks\\n* CloudWatch\\n* Splunk\\nOptions:\\n### Option 1:\\nWe could ship the Cloudwatch logs to logit.io using AWS lambda and ship the Pod\/Application logs to Logit.io using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Logit.io instance and would end up duplicating a large number of the logs in both CloudWatch and Logit.io\\n### Option 2:\\nWe could host a dedicate ELK stack (either in cluster or from AWS's managed offering) and ingest logs from both Pods and CloudWatch into the ELK stack. Managing ELK stacks has been a maintenance burden at GDS previously and this would require duplicating logs already stored in CloudWatch.\\n### Option 3:\\nWe could ship the Pod\/Application logs to CloudWatch using [fluentd](https:\/\/www.fluentd.org\/) and expose CloudWatch insights interface to users of the platform\\n### Option 4:\\nWe could ship the CloudWatch logs to Splunk using AWS lambda and ship the Pod\/Application logs to Splunk using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Splunk instance and would end up duplicating a large number of the logs in both CloudWatch and Splunk.\\n\n## Decision\n","Decision":"We will use [fluentd](https:\/\/www.fluentd.org\/) to ship pod\/application logs to [AWS CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/) to aggregate all platform\/application logs to avoid double spending on log storage.\\n","tokens":416,"id":3914,"Prediction":"## Decision\n\nAfter examining the context and evaluating each option based on key factors such as ease of maintenance, user accessibility, and log data duplication, the following decision was made:\n\n**Option 3: Ship the Pod\/Application logs to CloudWatch using [fluentd](https:\/\/www.fluentd.org\/) and expose the CloudWatch Insights interface to users of the platform.**\n\n### Justification:\n\n1. **Unified Log Storage**: This option achieves the goal of having a single storage location in CloudWatch for all logs. Storing all logs in CloudWatch ensures a centralized system for log management, which simplifies the process of auditing and debugging.\n\n2. **Ease of Maintenance**: Given GDS's history with managing ELK stacks as a maintenance burden, avoiding the setup and ongoing management of another ELK stack is advantageous. AWS CloudWatch, being a managed service, reduces operational overhead compared to self-hosted or SaaS-based ELK solutions.\n\n3. **Cost Efficiency**: This option avoids duplicating logs across multiple services, thereby optimizing storage costs. Duplicating logs in both CloudWatch and another service (Logit.io, Splunk, or another ELK stack) would result in higher storage requirements and associated costs.\n\n4. **Leverage Existing Infrastructure**: CloudWatch is already being used to store logs from several sources. Extending its use to include Pod\/Application logs via fluentd is straightforward and leverages existing infrastructure, making deployment simpler and faster.\n\n5. **User Accessibility**: AWS CloudWatch Insights provides a powerful interface for log querying and analysis. By exposing this interface to users, they gain robust tools for searching, filtering, and visualizing logs without needing to learn a new system.\n\nConsidering these points, Option 3 strikes the best balance between minimizing maintenance efforts, avoiding unnecessary log duplication, leveraging existing infrastructure, and providing a centralized and effective log management solution.","GenTime":"2024-07-29 01:21:26"}
{"File Name":"gsp\/ADR032-sre-permissions.md","Context":"## Context\\nAs mitigation for some risks raised in threat modelling it was agreed that day-to-day access to the cluster was to be read-only for everyone. Only the concourse running in the cluster could make changes that originated from Github, which required several approvals before merging.\\nFollowing the gradual rollout of several applications onto the GSP it became clear there were issues with the deployment procedures. This caused conflicting and contending pods to attempt to execute, resulting in application failures and deployment pipeline blockages. This was happening up to several times a day, depending on the level of activity. The remedial procedure involves escalating one or more members to cluster admin to allow the resources to be deleted, before revoking the admin permissions again. This process requires 3 people to perform and could result in hours of wasted time for each occurrence.\\n\n## Decision\n","Decision":"We will add to the SRE permissions map the ability to delete the following higher-level controllers so an escalation to cluster admin is no longer necessary:\\n* ConfigMap\\n* Deployment\\n* ReplicaSet\\n* Secret\\n* Service\\n* StatefulSet\\nWe will also raise a story to investigate the root cause of the deployment issues with a view to removing these permissions in the future.\\n","tokens":166,"id":3915,"Prediction":"## Decision\n\nTo address the issues arising from the current read-only access policy and its impact on deployment procedures, the following decision has been made:\n\n1. **Adopt a Read-Write Role with Limited Scope for Day-to-Day Access:**\n   - Introduce a new, limited read-write role with the ability to manage specific types of resources within the cluster that are frequently causing conflicts (such as specific pod types, deployments, or config maps). This role will be scoped tightly to include only necessary permissions and will not provide full admin access.\n\n2. **Implement Automated Conflict Resolution:**\n   - Enhance the deployment pipeline and cluster management automation tools to automatically detect and resolve common conflicts and resource contention issues. This can include automated scripts or Kubernetes operators to manage the lifecycle of conflicting resources and ensure smooth application deployments.\n\n3. **Enhance Deployment Pipeline Robustness:**\n   - Improve the existing deployment pipeline to include additional checks and validations that catch potential issues early, before they result in application failures or blockages. This could involve adding liveness\/readiness probes, dependency checks, or resource quota validations.\n\n4. **Robust Incident Response Procedures:**\n   - Develop and document clear incident response procedures, including criteria for escalation to cluster admin access, to ensure a swift and consistent approach to handling deployment issues. This should also involve periodic reviews of incidents to refine the process and prevention measures.\n\n5. **Regular Access Integrity Audits:**\n   - Conduct regular audits of access permissions and role usage to ensure that the new read-write role is being used appropriately and to identify any required adjustments to permissions or procedures.\n\n6. **Training and Documentation:**\n   - Provide training and comprehensive documentation for all users with access to the new read-write role to ensure they fully understand the scope of their permissions and the proper procedures to follow during deployment issues.\n\nBy implementing these changes, we aim to reduce the frequency and impact of deployment-related issues, minimize wasted time, and maintain the integrity and security of the cluster while allowing for more efficient day-to-day operations.","GenTime":"2024-07-29 01:21:33"}
{"File Name":"gsp\/ADR028-container-tools.md","Context":"## Context\\nWe are currently using Docker as our container runtime.\\nThere are needs for public docker images:\\n* so that master builds can be easily pulled and tested in the local development environments\\n* so that images can be easily shared between different teams\\nThere are needs for digitally verifying the provenance of images:\\n* so that production systems can trust that an image has not been tampered with\\n* so that production systems can authenticate the origin of a build\\nThere are needs for vulnerability scanning:\\n* so that production systems can warn or prevent exploitable software running in production\\nThe docker ecosystem provides tooling that can help us meet these needs:\\n* [Docker Content Trust] (Notary) can be used to sign images and prove provenance\\n* [Docker Registries][Docker Registry] can expose images publicly\\n* Scanning tools like [Clair] can periodically or at pull\/push time perform CVE scanning.\\nUnfortunately AWS [ECR] does not currently support public images or [Docker Content Trust], and there is no managed solution to image scanning from AWS as yet.\\n### Option 1: Wrap AWS ECR\\nWe could write\/manage a proxy to allow exposing [ECR] publicly and integrate the missing features.\\n* Potentially fragile implementation tied to the underlying AWS services\\n* Another thing to have to maintain\\n* Might offer ability to remove functionality as AWS support more features in future\\n### Option 2: Use an external SaaS offering\\nUse a SaaS service like [Quay] which offers most of these features.\\n* Additional configuration for cluster to pull from external source\\n* Reduces ability to automate provisioning (requires additional credential management)\\n* Procurement\\n### Option 3: Self-hosted Docker Tools in cluster\\nWe could deploy Docker Distribution, Notary & Clair into the cluster backed by a managed storage backend like S3\\n* Well integrated with the platform\\n* Would work for local GSP instance\\n\n## Decision\n","Decision":"We will run a self hosted set of Docker tools\\n","tokens":406,"id":3917,"Prediction":"## Decision\n\n### Chosen Option: Self-hosted Docker Tools in Cluster\n\n**Rationale:**\nAfter evaluating the available options, we have decided to proceed with Option 3: Self-hosted Docker Tools in cluster. This decision was made based on the following considerations:\n\n1. **Integrated Solution:**\n   - By self-hosting Docker Distribution, Notary, and Clair within our own cluster, we can achieve a tightly integrated solution that works seamlessly with our current setup.\n  \n2. **Feature Completeness:**\n   - This option allows us to leverage Docker Content Trust (Notary) for digitally signing images, along with Clair for vulnerability scanning. This ensures that we can meet our requirements for image provenance and security.\n\n3. **Flexibility and Control:**\n   - Hosting these services ourselves gives us greater flexibility and control over configurations and upgrades. This is crucial as our needs evolve and as new features are required.\n\n4. **Security:**\n   - With a self-hosted solution, we maintain full control over our security configurations, reducing the risk associated with managing external credentials and dependencies on third-party services.\n\n5. **Future-Proofing:**\n   - While AWS ECR currently lacks some features, this approach is not dependent on waiting for feature updates from AWS. Instead, we can implement and use necessary features immediately.\n\n6. **Vendor Independence:**\n   - By not relying on external SaaS providers, we avoid potential issues related to SLA, procurement, and vendor lock-in. This enables us to maintain an independent and autonomous infrastructure.\n\n**Implementation Plan:**\n1. **Deployment Strategy:**\n   - Set up Docker Distribution, Notary, and Clair as components within our Kubernetes cluster.\n   - Use a managed storage backend like S3 for image storage to ensure scalability and durability.\n\n2. **Security Measures:**\n   - Configure Notary for image signing to provide image provenance verification.\n   - Implement Clair to perform continuous vulnerability scanning of all images stored.\n\n3. **Automation & Maintenance:**\n   - Automate the deployment and configuration of these tools using Infrastructure as Code (IaC) with tools like Terraform and Helm.\n   - Regularly update and maintain these tools to ensure they remain secure and perform optimally.\n\n4. **Documentation & Training:**\n   - Document the setup procedure, usage guidelines, and maintenance practices.\n   - Provide training sessions for the development and operations teams to ensure they understand how to use and maintain the new system.\n\nBy adopting this self-hosted approach, we aim to better meet our needs for public image distribution, security, and provenance verification, while maintaining control and flexibility within our infrastructure.","GenTime":"2024-07-29 01:21:45"}
{"File Name":"gsp\/ADR011-build-artefacts.md","Context":"## Context\\nAs part of our pipelines we will be building artefacts that will be used to test\\nand deploy our applications. We will be deploying applications to Kubernetes. We\\nwill need to build a container image of some kind.\\nThere are some competing container image formats, namely:\\n* [OCI]\\n* [ACI]\\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\\nThe Kubernetes project appears to [prefer Docker\/OCI][k8s-preferance] images\\nover ACI.\\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\\nimage format.\\nDocker has wide industry adoption and appears to have wide understanding within\\nGDS.\\nDocker is the default container runtime for Kubernetes.\\n\n## Decision\n","Decision":"We will build and store OCI images built using Docker.\\n","tokens":170,"id":3920,"Prediction":"## Decision\n\nWe will adopt the OCI (Open Container Initiative) image format for building and deploying our container images. \n\n### Reasons:\n1. **Kubernetes Preference**: Kubernetes projects exhibit a clear preference for Docker\/OCI images, ensuring better compatibility and support within our deployment environment.\n2. **Industry Adoption**: Docker and OCI have wide industry adoption, which translates to robust community support, extensive documentation, and a broad ecosystem of tools and resources.\n3. **Future Proofing**: The OCI image format is being adopted more widely, with projects like rkt transitioning to OCI. This ensures our choice aligns with industry trends and future direction.\n4. **Organizational Expertise**: There is existing understanding and experience with Docker and OCI within our organization (GDS), facilitating smoother adoption and fewer training requirements.\n5. **Container Runtime**: Docker is the default container runtime for Kubernetes, further ensuring seamless integration and operational consistency between our containerized applications and Kubernetes infrastructure. \n\nThis decision will help us maintain a compatible, supportable, and future-proof containerization strategy for our applications.","GenTime":"2024-07-29 01:21:55"}
{"File Name":"tech-radar\/adr-4-structure_of_principle.md","Context":"# Context\\nWe need consistency in terms of format and structure for our patterns across the customer facing, integration and other architectures.\\nWe are also keen to link back to business strategy directly so we can isolate points for consideration in design, implementsation and assessment.\\n# Decision\\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n\n## Decision\n","Decision":"We propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","tokens":382,"id":3925,"Prediction":"## Decision\nAdopt a standard structure for documenting architectural patterns and principles to ensure consistency, clarity, and alignment with business strategy across various architectures (customer-facing, integration, and others). The proposed structure is as follows:\n\n1. **Context**: Provide background information and scenarios where the pattern or principle is applicable.\n2. **Link to Business Strategy**: Include URIs to relevant business strategy documents and identify specific bullet points that the pattern or principle is designed to promote.\n3. **Problems**: Outline the specific problems that the pattern or principle is addressing.\n4. **Rationale**: Explain the reasoning behind the pattern or principle and its importance.\n5. **Examples**: Provide real-world examples or case studies demonstrating the application of the pattern or principle.\n6. **Discussion Points**: Identify key discussion topics or scenarios that warrant further discussion and consideration.\n7. **Case-by-Case Topics**: Address specific scenarios such as legacy API usage, including necessary contacts like TRAP (Technical Review and Approval Process) or support teams with appropriate hyperlinks.\n\nThis structure will be used for all principle artifacts to ensure consistency and to facilitate the linkage to business strategy, aiding in the design, implementation, and assessment phases. \n\n## Status\nProposed\n\n## Consequences\nStakeholder groups will need to re-factor the format and structure of existing pattern materials during a later editing phase. The initial phase will focus on migrating the existing material into the repository.\n\n## Example\nTo illustrate the format, consider the pattern of **Independently Deployable Services**:\n\n1. **Context**\n   - (Link to URI for business strategy, e.g., [Martin Fowler on Microservices](http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices))\n\n2. **Problems**\n   - Need to respond quickly to changes\n   - Ensure availability\n   - Manage scalability\n\n3. **Rationale**\n   - \"Loose coupling and high cohesion\": Changes in one service should not require changes in others. Related changes are made in one location only.\n   - Facilitates incremental and speedy delivery by allowing teams to work independently.\n\n4. **Examples**\n   - (To be provided: MDTP Link explaining practical implementations)\n\n5. **Discussion Points**\n   - Not Applicable currently\n   - TODO: Given scenario X, contact TRAP (email hyperlink).\n\n6. **Case-by-Case Topics**\n   - If service uses legacy APIs, contact TRAP: add [TRAP\/Arc Triage email hyperlink].\n\nBy adhering to this structured approach, we will enhance clarity and maintain alignment with our overarching business strategy.","GenTime":"2024-07-29 01:22:15"}
{"File Name":"dos-capacity-status-api\/002-shared_database.md","Context":"## Context\\nThis is not an external API to DoS, it is part of the DoS ecosystem. The operation of the API is updating the capacity of services in DoS, for visible in the DoS UI and applications that use Core DoS data.\\n\n## Decision\n","Decision":"The relevant content of the database is shared between the Core DoS application and the Capacity Status API. The decision was made to have the API use and update the Core DoS database. This approach is the most logical solution at this time, as any other alternative would likely need to incorporate some kind of interim internal API between the Capacity Status API and the Core DoS database. Having an interim API would be replicating functionality of the Capacity Status API and would therefore be redundant.\\n","tokens":54,"id":3937,"Prediction":"## Decision\n\nWe have decided to integrate standard mechanisms for internal API authentication and authorization within the Department of Services (DoS) ecosystem. \n\nThis decision is based on the following considerations:\n\n1. **Maintain Security and Integrity**: As the API is responsible for updating the capacity of services within DoS, it is crucial to ensure that only authorized entities have access to modify this data. This helps in maintaining the integrity and accuracy of the data reflected in the DoS UI and other applications depending on Core DoS data.\n\n2. **Consistency with Existing Ecosystem**: Aligning with the internal DoS authentication and authorization mechanisms allows for seamless integration and operation within the existing DoS infrastructure without the need for additional external dependency management.\n\n3. **Audit and Compliance**: Implementing internal authentication and authorization processes aids in maintaining thorough audit trails that can be essential for compliance and monitoring purposes\n\n4. **Minimize External Exposure**: Since this API is not meant for external access, minimizing its exposure to external threats by encapsulating it within the existing DoS security framework is paramount.\n\n5. **Performance and Reliability**: Utilizing well-established internal mechanisms ensures that the API can deliver reliable performance, aligned with the operational standards of other DoS components and services.\n\nBy securing the API through internal DoS mechanisms, we not only safeguard the operations and data integrity but also align with best practices for maintaining a robust, secure, and efficient ecosystem.","GenTime":"2024-07-29 01:23:08"}
{"File Name":"ensemblejs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3940,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) process to document the architectural decisions made during this project. This process will ensure that all decisions are systematically captured, providing a reference for future decision-making and enhancing team communication.\n\n### Key Elements of the Decision:\n1. **Template Standardization**: We will adopt a standard template for recording decisions to ensure consistency. The template will include sections for Context, Decision, Consequences, and Alternatives.\n2. **Version Control**: ADRs will be version controlled using our existing Git repository, allowing us to track the history of decisions and their evolution over time.\n3. **Tool Integration**: We will integrate the ADR documentation process with our project management and collaboration tools (e.g., JIRA, Confluence) to ensure seamless access and updates.\n4. **Review Process**: Each ADR will undergo a review process involving relevant stakeholders to gather feedback and ensure consensus before finalizing the decision.\n5. **Tagging and Indexing**: ADRs will be tagged and indexed for easy search and retrieval. Tags may include categories such as 'Database', 'Security', 'API Design', etc.\n6. **Retrospective Meetings**: Regular retrospective meetings will be held to review past ADRs, discuss their effectiveness, and make any necessary adjustments to the process.\n\n### Rationale:\n- **Transparency**: Ensuring that all decisions and their rationales are documented and accessible promotes transparency within the team.\n- **Knowledge Sharing**: ADRs serve as a knowledge base for new team members and as a reference for future projects, reducing the learning curve.\n- **Decision Justification**: Documented decisions help justify choices to stakeholders and provide clear reasoning in case of queries or disputes.\n- **Continuous Improvement**: Regular reviews and updates to the ADRs enable the team to continuously improve architectural practices based on practical experiences and lessons learned. \n\nBy implementing this process, we aim to create a comprehensive and accessible record of our architectural decisions that will benefit our project both in the short-term and long-term.","GenTime":"2024-07-29 01:23:23"}
{"File Name":"verify-self-service\/0003-use-pundit-for-rbac.md","Context":"## Context\\nAs with any modern web system there is a need to secure the application with\\nstrong authentication and Role Based Access Control (RBAC). RBAC allows us to\\ncreate permissions, apply them to roles and associates roles with users. From\\nthis we can restrict what users can do and see within the application based on\\ntheir role. In addition to this we also had to investigate where to store role\\ninformation and define teams to support our use of RBAC.\\n\n## Decision\n","Decision":"After some investigation and discussion into various ways of implementing RBAC\\nwithin Ruby we have decided to implement the Pundit Ruby gem within the Verify\\nSelf Service application. This gem provides us with a way of creating policies\\nwhich apply to the app as a whole and to individual classes within the\\napplication.\\nWe have chosen pundit for a number of reasons. The first is that it seems\\nrelatively easy to implement within our application. A proof of concept was\\ncreated in a few hours which restricted creating new components within the\\napplication to those who hold the `gds` or `dev` roles. Policies and permissions\\nare defined in code as plain Ruby objects which are easy to understand and\\nsimple to implement. This ease of implementation and an appearance of providing\\nthe fuctionality we are seeking we believe makes pundit a good candidate for\\nimplementing RBAC within our application.\\nIn addition to this we are aware of prior-art use within GDS on the GOV.UK\\nproject. Finally we have found a lot of documentation and articles about how to\\nimplement and use pundit.\\nRole information we have decided to store in a custom field within our cognito\\ninstance. This will take the form of a comma seperated list which will be split\\ninto an array of roles for use within the application. This method allows us to\\nkeep all use information together within AWS Cognito and means that the only way\\nrole information can be changed is via SDK calls to AWS Cognito.\\nFinally we understand there is a need to define teams within our application. We\\nhave decided that a team is analogous to an AWS Cognito group. This allows a\\nuser to be part of one more groups and for a group to hold one or more users. We\\ncan easily query group membership using the AWS Cognito SDK and check for a\\nusers membership to a specific group.\\n","tokens":97,"id":3950,"Prediction":"## Decision\n\nWe have decided to implement a Role Based Access Control (RBAC) system to manage user permissions effectively and securely within the application. The RBAC system will include the following:\n\n1. **Authentication Mechanism**: \n   - We will use OAuth 2.0 for authentication to provide a secure and industry-standard method for verifying user identities.\n\n2. **Role Definition**:\n   - Roles will be predefined based on the application's requirements, such as Admin, Editor, Viewer, etc.\n   - Each role will have specific permissions associated with it, dictating the actions and resources it can access.\n\n3. **User-Role Association**:\n   - Users will be assigned one or more roles at the time of their account creation or via an administrative interface.\n   - The system will support dynamic role assignments for users to accommodate changing permissions requirements.\n\n4. **Storage of Role Information**:\n   - Role definitions and assignments will be stored in a central database to ensure consistency and ease of management.\n   - For scalable and flexible role management, we will use a NoSQL database like MongoDB or a relational database like PostgreSQL based on further architectural evaluations.\n\n5. **Teams and Hierarchies**:\n   - We will define organizational teams within the system to group users based on departments, projects, or other hierarchies.\n   - RBAC will be extended to support team roles, allowing permissions to be assigned at both individual and team levels, promoting efficient access control management.\n\n6. **Fine-grained Permissions**:\n   - Permissions will be created to match the operations available within the application, and these permissions will be explicitly associated with roles.\n   - The system will support adding, modifying, and removing permissions to keep the access control up to date with the evolving application functionalities.\n\n7. **Auditing and Logging**:\n   - An audit trail mechanism will be implemented to log role assignments, permission changes, and user activities to comply with security and compliance requirements.\n   - Regular reviews and audits of roles and permissions will be conducted to ensure optimal and secure use of the RBAC system.\n\nBy following this approach, we aim to ensure that our application adheres to best practices in security and provides a robust, scalable, and flexible access control mechanism.","GenTime":"2024-07-29 01:24:08"}
{"File Name":"PerformanceTestDocs\/0006-vellum-node-fails.md","Context":"## Context\\nVellum node fails\\n\n## Decision\n","Decision":"Vellum is always addressed by its cluster name and all of its data is stored in distributed databases with replicas of data on multiple nodes.\\n","tokens":9,"id":3952,"Prediction":"## Decision\n\nImplement a failover strategy for the Vellum node such that when the primary Vellum node fails, the system can automatically switch to a standby node without significant downtime. The components of the decision are as follows:\n\n1. **Replication and Redundancy:** Set up a replica of the primary Vellum node that will serve as the standby node. The standby node should be synced regularly with the primary node to ensure data consistency.\n\n2. **Health Check and Monitoring:** Implement health check mechanisms to continuously monitor the status of the primary Vellum node. This can be done using watchdog scripts or third-party monitoring tools that can detect node failures quickly.\n\n3. **Automatic Failover:** Configure an automatic failover mechanism that can reroute traffic to the standby node when the primary node becomes unreachable. This can be done using load balancers or orchestration tools such as Kubernetes.\n\n4. **Data Backup:** Regularly back up data on both the primary and standby nodes to a secure and accessible location to prevent data loss.\n\n5. **Testing and Validation:** Regularly test the failover process to ensure that the system can handle Vellum node failures seamlessly. Conduct periodic failover drills and validate the integrity of the data post-failover.\n\n6. **Notification System:** Set up a notification system that alerts the relevant team members in the event of a Vellum node failure and subsequent failover, providing them with information needed to troubleshoot and restore the primary node.\n\n7. **Documentation:** Ensure all failover procedures are well-documented, including instructions on how to manually trigger a failover if the automatic mechanisms fail.","GenTime":"2024-07-29 01:24:18"}
{"File Name":"PIMS\/database.md","Context":"## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n\n## Decision\n","Decision":"Originally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\\nThis will give us better performance and tighter integration with Entity Framework Core.\\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\\n","tokens":49,"id":3966,"Prediction":"## Decision\nAfter evaluating various database options considering the constraints, requirements, and compatibility, the decision is to use **PostgreSQL** for the PIMS database. PostgreSQL is a powerful, open-source relational database system that natively supports advanced data types and performance optimization. It also has excellent compatibility with Entity Framework Core 3.1.\n\n### Justification:\n1. **Relational Data Handling**: PostgreSQL provides strong ACID compliance and sophisticated querying capabilities which are crucial for handling relational data with complex constraints.\n2. **Containerization**: PostgreSQL has robust support for Docker and is optimized to run efficiently within Linux containers, making it compatible with our OpenShift platform.\n3. **Entity Framework Core 3.1 Support**: PostgreSQL has native compatibility with Entity Framework Core 3.1, ensuring smooth integration and support for ORM features such as migrations, LINQ querying, and more.\n4. **Community and Ecosystem**: PostgreSQL has a large, active community and a mature ecosystem, providing reliable support and extensive documentation, which makes it a trusted choice for enterprise-level applications.\n5. **Performance and Scalability**: PostgreSQL offers features such as indexing, partitioning, and custom functions which can cater to performance and future scalability needs.\n\nThis decision ensures that the PIMS database will be robust, scalable, and maintainable, leveraging PostgreSQL's strengths to meet the project\u2019s requirements.","GenTime":"2024-07-29 01:25:13"}
{"File Name":"PIMS\/geocoder.md","Context":"## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https:\/\/www2.gov.bc.ca\/gov\/content\/data\/geographic-data-services\/location-services\/geocoder)\\n- [Data BC](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service)\\n- [API Swagger](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service\/resource\/40d6411e-ab98-4df9-a24e-67f81c45f6fa\/view\/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys)\\n- API Host = `https:\/\/geocoder.api.gov.bc.ca`\\n\n## Decision\n","Decision":"Integrate with Data BC Geocoder API.\\nWhen a user types an address a list of viable matches will be displayed.\\nIf the user selects one of the matches it will be used to set the address and GIS coordinates,.\\n","tokens":262,"id":3968,"Prediction":"## Decision\n\nIntegrate the Data BC Geocoder API to automatically retrieve and validate GIS coordinates and address information for properties in the inventory system.\n\n### Rationale\n1. **Accuracy and Reliability**: Utilizing Data BC Geocoder as the source of truth ensures that the latitude and longitude values are accurate and reliable.\n2. **Enhanced User Experience**: Automatically populating GIS coordinates based on property addresses will simplify the process for users, eliminating manual entry and reducing the potential for human error.\n3. **Address Verification**: The API can be used to verify addresses that are manually entered, thus maintaining the integrity of the data.\n4. **Streamlined Integration**: The API documentation and developer keys are readily available, and the integration can be efficiently achieved with the given resources.\n\n### Implementation Steps\n1. **Obtain API Keys**: Follow the procedure outlined in the [Developer Guide](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys) to get the necessary API keys.\n2. **API Integration**: Develop a module in the inventory system to interact with the Geocoder API (`https:\/\/geocoder.api.gov.bc.ca`) for address lookup and GIS coordinate retrieval.\n3. **UI\/UX Design**: Update the property submission and editing interfaces to include features for fetching GIS coordinates based on the provided address.\n4. **Error Handling**: Implement error handling to manage API call failures and invalid addresses.\n5. **Testing**: Thoroughly test the integration to ensure that it accurately retrieves GIS coordinates and validates addresses in various scenarios.\n\n### Risks and Mitigations\n- **API Downtime**: The Geocoder API may experience downtime or slow responses. Implement a fall-back mechanism allowing manual entry when the API is unavailable.\n- **Rate Limits**: Monitor and manage the usage of the API to stay within the service rate limits. Consider caching frequent queries to reduce redundant calls.\n- **Data Privacy**: Ensure that any data exchanged with the API complies with privacy and data protection regulations.\n\n### Review and Approval\nThis decision will be reviewed and needs to be approved by the project stakeholders before proceeding with implementation.","GenTime":"2024-07-29 01:25:23"}
{"File Name":"smjs\/2018121201-stop-using-registers-in-processor.md","Context":"## Context\\nCommunication between processor and interpreter must go both ways: the processor needs to pass the interpreter the instruction to execute, and the interpreter is supposed to be able to update the Instruction Pointer (IP), and to tell the processor it the execution needs to stop, and with which exit status.\\nCurrently we are using an intermediate component, registers, partially like how real processors work, containing both the IP and the information about the exit status. However, the communication between the processor and the interpreter is so straightforward that a much simpler solution would be to just use classical call argument, and call return value. Registers could be used as a mediator, but for this to have sense the processor and the interpreter would have to be unaware of each other, and registers should implement a mechanism to automatically notify involved parties when a change is made, which seems overkill.\\n\n## Decision\n","Decision":"The interpreter will keep taking an instruction from the processor, but this time it will return a data structure, which tells if an exit has been triggered, and in this case what's the exit status.\\n","tokens":172,"id":3972,"Prediction":"## Decision\n\nRemove the intermediate component (registers) and directly pass the necessary information using function call arguments and return values. \n\nGiven the straightforward nature of the communication between the processor and the interpreter, directly passing parameters and receiving return values will simplify the architecture and reduce complexity. The processor will pass the instruction to execute via function arguments, and the interpreter will update the Instruction Pointer (IP) and return the exit status directly to the processor. This approach avoids the overhead of maintaining a separate register system and the unnecessary complexity of implementing a change notification mechanism.\n\nBy adopting this direct communication model, we ensure more efficient and clear interactions between the processor and interpreter, aligning with the principle of \"Keep It Simple, Stupid\" (KISS).\n\n---\n\nMaking this change will involve:\n1. Updating the interface for the interpreter to accept the instruction and current IP as function arguments.\n2. Modifying the interpreter to return the new IP and any exit status directly to the processor.\n3. Removing the handling of register-based communication between these components.\n\nThis decision focuses on maintaining simplicity, improving code maintainability, and optimizing performance by eliminating redundant intermediaries.","GenTime":"2024-07-29 01:25:36"}
{"File Name":"smjs\/2018121601-multiple-usages-of-the-processor.md","Context":"## Context\\nThe domain our virtual machine is supporting is concerned with running programs: as such, how programs should be run is a business rule, and the Processor is the Domain Service encapsulating that rule.\\nAt the application level, we can think of several features that can support this domain:\\n- classic virtual machine, running an entire program, perhaps loaded from a file, until its termination\\n- REPL, where instructions are loaded one at a time from an interactive prompt, and executed, printing some kind of output each time\\n- debug, where we still have an entire program, but this time we want to support debugging features, like running step-by-step\\nOf course there could be many others. Can we use the same definition of a Processor in the domain to support all this cases?\\n\n## Decision\n","Decision":"We will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):\\n- in the classic virtual machine, the Program is the entire program, that is executed from start to end\\n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this doesn't concern the domain), and to make the program start at the latest instruction, we add a new unconditional jump instruction at the beginning of it: this actually means that we don't run the entire program again (it isn't necessary since memory is preserved), but only the last inserted instruction\\n- in the debug situation, the Interpreter implementation can avoid returning control back to the Processor until requested to: this still doesn't concern the Processor implementation in the domain\\n","tokens":161,"id":3981,"Prediction":"## Decision\n\nYes, we can use the same definition of a Processor in the domain to support all these cases by designing the Processor as a flexible and extensible Domain Service. This can be achieved using the Strategy design pattern to encapsulate various execution strategies, ensuring the Processor can dynamically switch between different execution modes based on the requested feature.\n\n### Details:\n\n1. **Single Responsibility of Processor**: \n   - The Processor will be responsible only for the execution logic and will abstract the differences in execution modes.\n   \n2. **Strategy Design Pattern**:\n   - Define an interface for execution strategies (e.g., `ExecutionStrategy`), with different implementations for each execution mode (`ClassicExecution`, `ReplExecution`, `DebugExecution`).\n\n```java\ninterface ExecutionStrategy {\n    void execute(Program program);\n}\nclass ClassicExecution implements ExecutionStrategy {\n    public void execute(Program program) {\n        \/\/ Logic to run the entire program until its termination\n    }\n}\nclass ReplExecution implements ExecutionStrategy {\n    public void execute(Program program) {\n        \/\/ Logic to run instructions one at a time from an interactive prompt\n    }\n}\nclass DebugExecution implements ExecutionStrategy {\n    public void execute(Program program) {\n        \/\/ Logic to run the entire program with step-by-step debugging features\n    }\n}\n```\n\n3. **Processor Implementation**:\n   - The Processor will have a reference to an `ExecutionStrategy` and delegate the execution to the strategy.\n\n```java\nclass Processor {\n    private ExecutionStrategy strategy;\n\n    public Processor(ExecutionStrategy strategy) {\n        this.strategy = strategy;\n    }\n\n    public void setStrategy(ExecutionStrategy strategy) {\n        this.strategy = strategy;\n    }\n\n    public void executeProgram(Program program) {\n        strategy.execute(program);\n    }\n}\n```\n4. **Configuration at Application Level**: \n   - The application layer can configure the `Processor` with the appropriate strategy based on the required feature (classic, REPL, debug).\n\n```java\n\/\/ Usage example based on user choice\nProcessor processor;\nif (userWantsRepl) {\n    processor = new Processor(new ReplExecution());\n} else if (userWantsDebug) {\n    processor = new Processor(new DebugExecution());\n} else {\n    processor = new Processor(new ClassicExecution());\n}\nprocessor.executeProgram(program);\n```\n\n### Conclusion\n\nBy implementing the Processor with a strategy-based design, we ensure that the core business rule of how programs should be run remains encapsulated within the Processor, while allowing for flexibility in supporting different execution modes. This design promotes maintainability and extensibility, accommodating future execution modes and features with minimal changes to existing code.","GenTime":"2024-07-29 01:26:24"}
{"File Name":"smjs\/2018120802-implement-clean-architecture.md","Context":"## Context\\nThis project is meant to be a playground to experiment with different technologies and design solutions.\\n\n## Decision\n","Decision":"To standardize the project structure, now that it's getting bigger, we will implement the principles suggested by [clean architecture](https:\/\/github.com\/michelezamuner\/notes\/tree\/master\/software-architecture\/clean-architecture\/clean-architecture.martin), enhanced by concepts from Domain Driven Design, and plugin architectures.\\n### Domain\\nThe core domain is the \"virtual machine framework\", which only defines how programs should be executed, but doesn't specify any architecture, meaning that how programs are interpreted is not known.\\nA Program is a sequence of Data Units. Data Units are the kind of data with the smallest possible size, which we set at a single Byte. Data is always represented as a sequence of Data Units. Since we work with sequences, we also define the concepts of Size, which is the number of Data Units in a specific Data, and Address of a Data inside the Program, with the Address of the first Data Unit being 0. Both Size and Address are Integers, which is a generic integral type defined to be independent from the runtime environment implementation. A Program has the ability of fetching blocks of Data given their Address and Size.\\nA Program is run by a Processor, which uses an Interpreter, whose implementation is provided by the specific Architecture selected, to first define which sets of Data Units should be regarded as Instructions, and then to execute these Instructions. When running an Instruction, the Interpreter returns a Status object knowing if the execution should jump, or be terminated. The execution of a Program by a Processor always returns an Exit Status, which is Architecture-dependent. The termination of a Program must always be requested explicitly, via a dedicated instruction, otherwise an error is raised.\\nAn Interpreter must use the System to perform I\/O operations, and ultimately to allow a Program to communicate with the users; however, the implementation of the System depends on the actual application where the Processor and Interpreter are running, so it's left to be specified.\\nAdditional domains are defined for each architecture, so that a virtual machine can support many different architectures.\\nAssemblers and compilers also define their own domains.\\nThe following domains could thus be defined:\\n- `smf`: the core virtual machine framework domain\\n- `sma`: definitions for the SMA architecture domain\\n- `basm`: definitions for the BASM assembler domain\\n- `php`: definitions for the PHP compiler domain\\n### Application\\nThe application layer may define the following primary ports:\\n- the `vm` port allows to execute programs, according to the configured architecture\\n- the `repl` port allows to execute programs interactively, and uses the functionality of `vm`\\n- the `dbg` port allows to execute programs step by step for debugging, and uses the functionality of `vm`\\n- the `asm` port allows to run an assembler on some assembly code to produce executable object code\\n- the `cmp` port allows to run a compiler on some high level language to produce assembly code\\nAs far as secondary ports, we need the following:\\n- a `arcl` port allows the application to load an architecture definition\\n- a `pl` port allows the application to load a program\\n- a `asml` port allows the application to load assembly code, to be assembled\\n- a `cl` port allows the application to load high level code, to be compiled\\n- a `sys` port allows the application to interact with the underlying operating system\\n### Adapters\\nPrimary adapters might be defined to create command line applications, or Web applications. Secondary adapters might be defined to read data from files or from memory. See below for more concrete examples.\\n### Plugin architecture\\nWe want to support building different types of applications by composing together sets of different available plugins. For example:\\n**sloth machine (CLI)**\\nAD_sm + AD_larcl + AD_fpl + AD_ossys + AP_vm + AP_arcl + AP_pl + AP_sys + D_smf + D_sma (or others)\\n**sloth machine assembler (CLI)**\\nAD_asm + AD_fasml + AP_asm + AP_asml + D_basm (or others)\\n**sloth machine compiler (CLI)**\\nAD_cmp + AD_fcl + AD_masml + AP_cmp + AP_asm + AP_cl + AP_asml + D_basm (or others) + D_php (or others)\\n**sloth machine runner (CLI)**\\nAD_run + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_vm + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine REPL (CLI)**\\nAD_repl + AD_larcl + AD_mcl + AD_masml + AD_mpl + AD_ossys + AP_repl + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine debugger (CLI)**\\nAD_dbg + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_dbg + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine Web (Web)**\\nAD_web + AD_warcl + AD_wcl + AD_masml + AD_mpl + AD_wsys + AP_vm + AP_cmp + AP_asm + AP_repl + AP_dbg + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n- `D_smf`: Domain Sloth Machine Framework\\n- `D_sma`: Domain Sloth Machine Architecture\\n- `D_basm`: Domain Basic Assembly for Sloth Machine\\n- `D_php`: Domain PHP\\n- `AP_vm`: Application Virtual Machine (primary port)\\n- `AP_cmp`: Application Compiler (primary port)\\n- `AP_asm`: Application Assembler (primary port)\\n- `AP_repl`: Application REPL (primary port)\\n- `AP_dbg`: Application Debugger (primary port)\\n- `AP_arcl`: Application Architecture Loader (secondary port)\\n- `AP_pl`: Application Program Loader (secondary port)\\n- `AP_asml`: Application Assembly Loader (secondary port)\\n- `AP_cl`: Application Code Loader (secondary port)\\n- `AP_sys`: Application System (secondary port)\\n- `AD_sm`: Adapter Sloth Machine (primary adapter)\\n- `AD_cmp`: Adapter Compiler (primary adapter)\\n- `AD_run`: Adapter Runner (primary adapter)\\n- `AD_repl`: Adapter REPL (primary adapter)\\n- `AD_dbg`: Adapter Debugger (primary adapter)\\n- `AD_web`: Adapter Web (primary adapter)\\n- `AD_larcl`: Adapter Local Architecture Loader (secondary adapter)\\n- `AD_warcl`: Adapter Web Architecture Loader (secondary adapter)\\n- `AD_fpl`: Adapter File Program Loader (secondary adapter)\\n- `AD_mpl`: Adapter Memory Program Loader (secondary adapter)\\n- `AD_fasml`: Adapter File Assembly Loader (secondary adapter)\\n- `AD_masml`: Adapter Memory Assembly Loader (secondary adapter)\\n- `AD_fcl`: Adapter File Code Loader (secondary adapter)\\n- `AD_mcl`: Adapter Memory Code Loader (secondary adapter)\\n- `AD_wcl`: Adapter Web Code Loader (secondary adapter)\\n- `AD_ossys`: Adapter OS System (secondary adapter)\\n- `AD_wsys`: Adapter Web System (secondary adapter)\\n### Example modules\\n```\\ndomain\\nsmf\\ndata\\nDataUnit: (Byte)\\nData: DataUnit[]\\nSize: (Integer)\\nAddress: (Integer)\\nprogram [data]\\nProgram\\nProgram(data.Data)\\nread(data.Address, data.Size): data.Data\\ninterpreter [data]\\nOpcode: data.Data\\nOperands: data.Data\\nExitStatus: (Integer)\\nInstruction\\nInstruction(Address, Opcode, Operands)\\ngetAddress(): Address\\ngetOpcode(): Opcode\\ngetOperands(): Operands\\nStatus\\nshouldJump(): (Boolean)\\ngetJumpAddress(): data.Address\\nshouldExit(): (Boolean)\\ngetExitStatus(): ExitStatus\\n<Interpreter>\\ngetOpcodeSize(): data.Size\\ngetOperandsSize(Opcode): data.Size\\nexec(Instruction): Status\\nprocessor [program, interpreter]\\nProcessor\\nProcessor(interpreter.<Interpreter>)\\nrun(program.Program): interpreter.ExitStatus\\narchitecture [data, interpreter]\\n<System>\\nread(data.Integer fd, data.Size size): data.Data\\nwrite(data.Integer fd, data.Data data, data.Size size): data.Size\\n<Architecture>\\ngetInterpreter(<System>): interpreter.<Interpreter>\\nsma [smf]\\nInterpreter: smf.interpreter.<Interpreter>\\nInterpreter(smf.architecture.<System>)\\napplication\\nvm\\nrun_program [domain.smf, application.arcl, application.pl, application.sys]\\n<Request>\\ngetArchitectureName(): String\\ngetProgramReference(): String\\nResponse\\ngetExitStatus(): domain.smf.interpreter.ExitStatus\\n<Presenter>\\npresent(Response)\\nRunProgram\\nRunProgram(ProcessorFactory, <Presenter>, application.arcl.<ArchitectureLoader>, application.pl.<ProgramLoader>, application.sys.<System>)\\nexec(<Request>)\\nProcessorFactory\\ncreate(domain.smf.interpreter.<Interpreter>): domain.smf.processor.Processor\\narcl [domain.smf]\\n<ArchitectureLoader>\\nload(architectureName: String): domain.smf.architecture.<Architecture>\\npl [domain.smf]\\n<ProgramLoader>\\nload(programReference: String): domain.smf.program.Program\\nsys [domain.smf]\\n<System>: domain.smf.architecture.<System>\\nadapters\\nsm [application.vm, domain.smf]\\nrun_program [application.vm, domain.smf]\\nRequest: application.vm.run_program.<Request>\\nController\\nController(application.vm.run_program.RunProgram)\\nrunProgram(Request)\\nViewModel\\nViewModel(domain.smf.interpreter.<ExitStatus>)\\ngetExitStatus(): <native-integer>\\n<View>\\nrender(ViewModel)\\nExitStatusView: <View>\\nrender(ViewModel)\\ngetExitStatus(): <native-integer>\\nPresenter: application.vm.run_program.<Presenter>\\nPresenter(<View>)\\npresent(application.vm.run_program.Response)\\nlarcl [application.arcl]\\nLocalArchitectureLoader: application.arcl.<ArchitectureLoader>\\nfpl [application.pl]\\nFileProgramLoader: application.pl.<ProgramLoader>\\nossys [application.sys]\\nOSSystem: application.sys.<System>\\n```\\n### Examples of main implementations\\n```\\nmodule domain.smf.processor\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Operands\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.interpreter.Status\\nclass Processor\\nProcessor(<Interpreter> interpreter)\\nthis.interpreter = interpreter\\nrun(Program program): ExitStatus\\nSize opcodeSize = interpreter.getOpcodeSize()\\nAddress address = 0\\nwhile (true)\\nOpcode opcode = program.read(address, opcodeSize)\\nSize operandsSize = interpreter.getOperandsSize(opcode)\\nAddress operandsAddress = address + opcodeSize\\nOperands operands = program.read(operandsAddress, operandsSize)\\nInstruction instruction = new Instruction(address, opcode, operands)\\nStatus status = interpreter.exec(instruction)\\nif (status.shouldExit())\\nreturn status.getExitStatus()\\naddress = status.shouldJump() ? status.getJumpAddress() : operandsAddress + operandsSize\\n\/\/ @todo: handle missing exit\\n```\\n```\\nmodule domain.sma.interpreter\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.interpreter.Status\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.architecture.<System>\\nimport domain.sma.InstructionSet\\nimport domain.sma.Result\\nimport domain.sma.JumpResult\\nimport domain.sma.ExitResult\\nimport domain.sma.InstructionDefinition\\nclass Interpreter: <Interpreter>\\nInterpreter(InstructionSet instructionSet, <System> system)\\nthis.instructionSet = instructionSet\\nthis.system = system\\ngetOpcodeSize(): Size\\nreturn new Integer(1)\\ngetOperandsSize(Opcode opcode): Size\\nreturn instructionSet.getInstructionDefinition(opcode).getOperandsSize()\\nexec(Instruction instruction): Status\\nAddress jumpAddress = null\\nAddress exitStatus = null\\nInstructionDefinition definition = instructionSet.getInstructionDefinition(instruction.getOpcode())\\nResult result = definition.exec(instruction.getOperands())\\nif (result instanceof JumpResult)\\njumpAddress = result.getJumpAddress()\\nif (result instanceof ExitResult)\\nexitStatus = result.getExitStatus()\\nreturn new Status(jumpAddress, exitStatus)\\n```\\n```\\nmodule application.vm.run_program\\nimport application.vm.run_program.ProcessorFactory\\nimport application.vm.run_program.<Presenter>\\nimport application.arcl.<ArchitectureLoader>\\nimport application.pl.<ProgramLoader>\\nimport application.sys.<System>\\nimport application.vm.run_program.<Request>\\nimport domain.smf.architecture.<Architecture>\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.processor.Processor\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport application.vm.run_program.Response\\nclass RunProgram\\nRunProgram(\\nProcessorFactory processorFactory,\\n<Presenter> presenter,\\n<ArchitectureLoader> architectureLoader,\\n<ProgramLoader> programLoader,\\n<System> system\\n)\\nthis.processorFactory = processorFactory\\nthis.presenter = presenter\\nthis.architectureLoader = architectureLoader\\nthis.programLoader = programLoader\\nthis.system = system\\nexec(<Request> request)\\n<Architecture> architecture = architectureLoader.load(request.getArchitectureName())\\n<Interpreter> interpreter = architecture.getInterpreter(this.system)\\nProcessor processor = processorFactory.create(interpreter)\\nProgram program = programLoader.load(request.getProgramReference())\\nExitStatus exitStatus = processor.run(program)\\npresenter.present(new Response(exitStatus))\\n```\\n```\\nmodule adapters.sm.run_program\\nimport application.vm.run_program.RunProgram\\nimport application.vm.run_program.<Request>\\nclass Controller\\nController(RunProgram service)\\nthis.service = service\\nrunProgram(<Request> request)\\nservice.run(request)\\n```\\n```\\nmodule adapters.sm.run_program\\nimport adapters.sm.run_program.<View>\\nimport application.vm.run_program.Response\\nimport adapters.sm.run_program.ViewModel\\nclass Presenter\\nPresenter(<View> view)\\nthis.view = view\\npresent(Response response)\\nViewModel viewModel = new ViewModel(response.getExitStatus())\\nview.render(viewModel)\\n```\\n","tokens":21,"id":3982,"Prediction":"## Decision\nWe will adopt a microservices architecture for this project. \n\n### Rationale:\n1. **Experimentation Flexibility:** It allows us to experiment with different technologies and design patterns within the scope of different services. We can implement each microservice using a different technology stack if needed, which aligns with the goal of experimenting with various technologies.\n\n2. **Isolated Development:** It enables isolated development and deployment, allowing us to make changes, optimizations, or updates to one service without affecting the others.\n\n3. **Scalability Testing:** We can experiment with different scaling strategies for individual services, giving insights into how each technology performs under load.\n\n4. **Failure Isolation:** By isolating services, we can specifically test failure scenarios and resilience for individual technologies without bringing down the whole system.\n\n5. **Incremental Adoption:** This architecture allows for incremental adoption of new technologies when necessary. We can gradually introduce new tools and frameworks into the ecosystem without needing to refactor the entire project.\n\n### Implementation Plan:\n1. **Service Identification:** Identify core functionalities and experimentations as individual services.\n2. **Communication Mechanism:** Decide on appropriate communication mechanisms (e.g., REST, gRPC) between services.\n3. **Technology Selection:** Choose different technology stacks for different services based on what we want to experiment with (e.g., Node.js for service A, Python for service B, etc.).\n4. **Deployment Strategy:** Implement CI\/CD pipelines for each service to enable quick development and deployment cycles.\n5. **Monitoring and Logging:** Set up centralized monitoring and logging to keep track of performance and errors across all services.\n6. **Documentation and Feedback:** Maintain comprehensive documentation of decisions, experiments, and outcomes for future reference and learning.\n\nBy adopting a microservices architecture, we can best utilize this playground project to explore, learn, and innovate with different technologies and design solutions in an isolated yet cohesive manner.","GenTime":"2024-07-29 01:26:29"}
{"File Name":"documents-api\/0002-use-base64-content-for-file-uploading.md","Context":"## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n\n## Decision\n","Decision":"We decided to use the first option (base 64 encoded uploads), for the following reason:\\n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.\\n","tokens":113,"id":3987,"Prediction":"## Decision\n\nWe will use the S3 Presigned URL functionality to allow clients to upload files directly to S3.\n\n### Rationale\n\n1. **Performance**: By allowing clients to upload files directly to S3, we offload the data transfer work from our servers, reducing load and potential bottlenecks. This approach can handle larger files more efficiently.\n\n2. **Security**: S3 Presigned URLs are time-limited and can be scoped to specific operations (e.g., upload only). This limits the risk associated with client-side direct uploads. Additionally, they can be restricted to specific file sizes and types, adding an extra layer of protection.\n\n3. **Scalability**: Offloading the file transfer to S3 directly scales better with increased demand. This setup allows our API endpoints to remain responsive while handling more simultaneous upload requests.\n\n4. **Simplicity for Clients**: Using Presigned URLs simplifies the upload process for clients. They don't need to encode files in Base64 or handle complex multipart form submissions. They just use the URL to upload directly to S3.\n\n5. **Cost**: This method can be more cost-effective by reducing the need for intermediate data handling by our API servers, potentially lowering bandwidth and processing costs.\n\n### Implementation Considerations\n\n- **Expiration Time**: Ensure the Presigned URLs have a reasonable expiration time to prevent reuse and minimize security risks.\n- **IAM Policies**: Define strict IAM policies for the credentials used to generate Presigned URLs to ensure they only allow the intended S3 operations.\n- **Error Handling**: Implement robust error handling on both client and server sides to manage scenarios where the upload might fail.\n- **Analytics and Monitoring**: Monitor the usage of Presigned URLs to detect any potential abuse or unusual patterns.\n\nBy using the S3 Presigned URL approach, we aim to build a more efficient, secure, and scalable file upload process that benefits both our server infrastructure and the clients using our API.","GenTime":"2024-07-29 01:26:52"}
{"File Name":"decode-demo\/0003-remove-spectre-css-and-replace-with-bootstrap.md","Context":"## Context\\nThe deployed demo site must be designed to render nicely on mobile devices.\\nThis is so that participants in the pilot can pretend the demo site provides\\nthe same basic user experience as the wallet once this exists.\\nSpectre CSS at least in the configuration implemented in the original version\\nof this app doesn't render a UI that is usable from a mobile device,\\nparticularly when it comes to form inputs (buttons and fields were tiny and\\nhard to read).\\n\n## Decision\n","Decision":"We will replace [Spectre.CSS](https:\/\/picturepan2.github.io\/spectre\/) with\\n[Bootstrap](https:\/\/getbootstrap.com\/).\\n","tokens":98,"id":3999,"Prediction":"## Decision\n\nWe will switch from Spectre CSS to a more mobile-responsive CSS framework, such as Bootstrap or Tailwind CSS, to ensure a better user experience on mobile devices. This change will improve the usability of form inputs (buttons and fields) which are currently problematic with Spectre CSS. Additionally, we will conduct thorough testing on various mobile devices to ensure that the demo site is intuitive and accessible for all users participating in the pilot. This decision aligns with our objective to simulate the user experience of the future wallet application effectively.","GenTime":"2024-07-29 01:27:38"}
{"File Name":"karma-sabarivka-reporter\/0001-documenting-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4002,"Prediction":"## Decision\nWe will adopt the use of Architectural Decision Records (ADRs) to systematically document the architectural decisions made throughout the course of this project. Each ADR will follow a standardized template which includes essential information such as context, decision, and consequences. This will ensure that all decisions are well-documented, providing a clear history and rationale that can be referenced by all team members, facilitating better understanding and continuity in the project. This approach will help maintain transparency and coherence in our architectural choices, enabling future teams to easily navigate past decisions.","GenTime":"2024-07-29 01:27:48"}
{"File Name":"apply-for-teacher-training\/0015-carrying-over-applications.md","Context":"## Context\\nThe current recruitment cycle ends on 18th September 2020. At that point there\\nwill be some candidates who could benefit from their application being carried\\nover to the next cycle. Carrying over an application means the candidate can\\napply to courses in the new recruitment cycle without having to fill in the\\nwhole application form again.\\n### Carrying over an application makes sense in the following states\\n#### Before the application reaches the provider\\nThese applications could be carried over because the provider has not seen them yet.\\n- Withdrawn\\n- Unsubmitted\\n- Ready to send to provider\\n#### After the application can\u2019t progress any further\\nThese applications could be carried over because they have reached an\\nunsuccessful end state. Enabling candidates to turn these into fresh applications\\nin the next cycle makes it as easy as possible for them to try again.\\n- Conditions not met\\n- Offer withdrawn\\n- Offer declined\\n- Application cancelled\\n- Rejected\\n### Carrying over an application does not make sense in the following states\\n#### While the application is already under consideration by the provider\\n- Awaiting provider decision\\n#### When the application already has an offer in flight\\n- Offer\\n- Meeting conditions (i.e. offer accepted)\\n- Recruited\\n### Copying the Apply again approach\\nThe current approach for moving applications into Apply again is to copy the\\nentire application (including references) and invite the user to add a new\\ncourse choice. This approach seems like it will work here too, with a couple of\\nextra things to take into account:\\n- applications that are carried over might be in Apply 1 or Apply again as the\\ncycle ends. All carried-over applications should start over as Apply 1\\napplications applications moving into Apply again all have complete\\nreferences because they\u2019ve already completed Apply 1, for which references\\nare required.\\n- Carried over applications might have no references, references in flight, or\\ncompleted references.\\nMoving the new application into the next cycle is a question of making sure its\\ncourse choices come from that cycle. As long as carrying over is only possible\\nonce the current cycle is closed, this should present no problems because the\\navailable courses will all come from the new cycle.\\n\n## Decision\n","Decision":"- We will only allow applications to be carried over once the current cycle is\\nover, and we\u2019ll only allow applications in the above states\\n- To carry over an application, we will adopt the Apply again pattern of\\ncloning the ApplicationForm and removing the courses\\n- We will copy references onto the carried-over application, but only if\\nthey\u2019re complete. Referees who had been contacted but had not responded\\nbefore the application was carried over will need to be cancelled.\\n- The applications that were carried over will remain in the database without\\nany further state change\\n- Applications which were not yet sent to the provider at end of cycle and also\\nnot carried over will still be in the database \u2014 we would like to mark these\\nwith a new state equivalent to \u201cincomplete at end of cycle\u201d. This state would\\nnever be visible to providers.\\n- It\u2019s up to the candidate whether to carry over their application, and we\u2019ll\\ngive them a button to do this\\n","tokens":467,"id":4010,"Prediction":"## Decision\n\n### Carry Over Eligible Applications to New Recruitment Cycle\n\nAfter considering the context and various possible states of applications, the decision is to implement a carry-over mechanism for eligible applications into the next recruitment cycle. The detailed approach and stipulations are as follows:\n\n1. **Eligible Applications for Carry Over**:\n   - **Before the application reaches the provider**:\n     - Withdrawn\n     - Unsubmitted\n     - Ready to send to provider\n   - **After the application reaches an unsuccessful end state**:\n     - Conditions not met\n     - Offer withdrawn\n     - Offer declined\n     - Application cancelled\n     - Rejected\n\n2. **Ineligible Applications for Carry Over**:\n   - **Under consideration by the provider**:\n     - Awaiting provider decision\n   - **Has an offer in flight**:\n     - Offer\n     - Meeting conditions (i.e., offer accepted)\n     - Recruited\n\n3. **Implementation Approach**:\n   - **Application Cloning**: Copy the entire application, including references. Carry-over applications should start as Apply 1 applications in the new cycle even if they were in the Apply again process during the previous cycle.\n   - **References Handling**: Manage references according to their states (none, in-flight, or completed). If references are in-flight or completed, these should be integrated into the new application seamlessly.\n   - **Course Choices Update**: Ensure that the course choices for the new application are updated with the available courses from the new cycle.\n\n4. **Execution**:\n   - The carry-over feature will be triggered at the close of the current cycle.\n   - Users will be invited to review their carried-over application and add new course choices for the new cycle.\n   - Implement safeguards to ensure that applications are only carried over once and correctly reflect the new cycle\u2019s course offerings.\n\nBy implementing this decision, candidates will benefit from a seamless transition into the new recruitment cycle without redundant re-entry of their application information, thereby encouraging continued engagement and potentially increasing successful placements.","GenTime":"2024-07-29 01:28:14"}
{"File Name":"apply-for-teacher-training\/0010-cookie-sessions.md","Context":"## Context\\nThis application needs user sessions to allow users (candidates, providers, referees, support staff) to sign in. Rails offers a number of options for this. Each have a trade-off.\\n### Cookie sessions\\nAll session information is saved in a cookie. The cookie is encrypted to avoid the user changing or reading the data.\\nPro:\\n- It does not interact with other infrastructure\\n- It's the Rails default, well understood by developers\\nCon:\\n- When the user signs out, we do not invalidate the session. This means that if the user has made a copy of the cookie, they (or an attacker) can sign themselves back in.\\n- Users cannot sign out sessions on other devices (remote sign out)\\n### Storage based cookies\\nThis mechanism relies on a session ID being saved in a cookie. The session ID corresponds to a record either in a traditional database (PostgreSQL in our case) or in a caching service (Memcached, Redis).\\nPro:\\n- On sign out, the session is deleted and cannot be revived\\n- Sessions can be invalidated \"remotely\", to allow sign out of other devices\\nCon:\\n- Uses other infrastructure - slight performance overhead, risk of services being unavailable\\n- Sensitive data is stored in a database\\n\n## Decision\n","Decision":"Use session cookies.\\n","tokens":264,"id":4011,"Prediction":"## Decision\nWe will use **Storage-Based Cookies** for managing user sessions in our application. This approach involves saving a session ID in a cookie, which corresponds to a session record stored in our PostgreSQL database or a caching service such as Redis. \n\n### Rationale:\n1. **Enhanced Security**: By storing session data on the server side, we can easily invalidate sessions upon user sign-out, reducing the risk posed by cookie duplication or theft. This is a significant improvement over cookie-based sessions, where session data remains potentially accessible even after sign-out.\n  \n2. **Remote Sign-Out Capability**: Storage-based cookies allow us to implement remote sign-out functionality, essential for users who may need to invalidate sessions on other devices for security reasons.\n\n3. **Infrastructure Considerations**: Although this approach requires additional infrastructure (database or caching service), our application can accommodate this with minimal performance overhead. PostgreSQL and Redis are both robust options, and their occasional unavailability can be mitigated with proper failover strategies and caching.\n\n4. **Sensitive Data Handling**: Storing sessions on the server side inherently provides better protection for sensitive data. By ensuring critical session information isn't stored directly in cookies, we minimize the risk of client-side tampering or exposure.\n\n### Implementation:\n- **Session Store**: Use Redis for session storage due to its in-memory data structure capabilities, providing fast access while supporting our requirement for remote session invalidation.\n- **Session Management**: Implement a mechanism to securely create, update, and delete session records in the chosen session store.\n- **Security Measures**: Ensure all cookies are securely encrypted and implement robust session management practices to protect against common security threats such as session fixation and cross-site scripting.\n\nBy opting for storage-based cookies, we improve session security, support essential session management features, and align with best practices for handling user sessions in web applications.","GenTime":"2024-07-29 01:28:19"}
{"File Name":"apply-for-teacher-training\/0003-initial-datamodel.md","Context":"## Context\\nIn the coming months we will be building this application. The first order of business is to come up with a sensible data model and application structure.\\n\n## Decision\n","Decision":"We discussed a database model that is based around a small number of tables with a lot of columns. In particular:\\n- `Candidate`, primarily used for authentication\\n- `ApplicationForm`, which belongs to a Candidate, and holds the\\ninformation that is common to all of the applications that the user\\nmakes (in the \"Apply 1\" phase, they can apply for up to three courses).\\n- `ApplicationChoice`, which holds the information that is course specific,\\nlike the status of the application and the personal statement. It holds a reference to the \"course choice\", which is the combination of course, provider and the training location.\\nThere will be some other models that hold references, work histories and qualifications.\\nThe eventual table structure will look something like this:\\n![data model](0003-initial-datamodel.png)\\n","tokens":33,"id":4021,"Prediction":"## Decision\n\n### Data Model\nWe will adopt a relational database structure for our initial data model using Postgres as the database management system. This structure will provide us flexibility, strong consistency, and the ability to handle complex queries. Tables will be designed to represent critical entities and their relationships, ensuring normalization to at least the third normal form (3NF) to avoid redundancy and maintain data integrity.\n\n- **Entity Identification:** Conduct workshops with stakeholders to identify the core entities of the application, likely including users, products, orders, and interactions.\n- **Attribute Definition:** Specify the attributes for each entity based on requirements gathered during initial analysis.\n- **Relationships and Keys:** Define primary and foreign keys to establish relationships between entities, ensuring referential integrity.\n- **Documentation:** Maintain an ERD (Entity-Relationship Diagram) as part of the project documentation to visualize the data model and facilitate discussions.\n\n### Application Structure\nWe will adhere to a modular, service-oriented architecture to ensure scalability and maintainability. The application will be built using the following layered architecture:\n\n1. **Presentation Layer:**\n   - **Web Clients\/App Interfaces:** Implementation using modern frontend frameworks like React or Angular for web clients and possibly React Native or Flutter for mobile applications.\n   - **Backend for Frontend (BFF):** Specific APIs tailored to the needs of different client applications.\n\n2. **Business Logic Layer:**\n   - **Service Layer:** Business logic encapsulated in independent services\/microservices. Each service will follow the single-responsibility principle. Technologies considered here include Node.js for its asynchronicity and performance, or Django for rapid development and a strong ORM.\n\n3. **Data Access Layer:**\n   - **Repository Pattern:** Data access logic abstracted using repositories. This enables easier changes to the data storage approach in the future if needed.\n   - **ORM (Object-Relational Mapping):** Utilizing ORM tools such as Sequelize for Node.js or Django ORM for Python to interact with the database without writing raw SQL.\n\n4. **Infrastructure Layer:**\n   - **APIs and Middleware:** Express.js or Fastify (Node.js) and Django (Python) will be used for API endpoints and middleware logic.\n   - **Asynchronous Processing:** Implement an asynchronous message broker like RabbitMQ or Kafka for handling long-running tasks and communication between microservices.\n\n5. **Authentication and Authorization:**\n   - **OAuth2\/JWT:** For secure and scalable authentication, we will use OAuth2 with JWT tokens. Services like Auth0 or AWS Cognito could be implemented for user management.\n\n6. **Testing and Quality Assurance:**\n   - **Unit\/Integration Testing:** Write comprehensive tests using Jest or Mocha (for Node.js) and Pytest (for Python) to ensure code quality and integrity.\n   - **Continuous Integration\/Continuous Deployment (CI\/CD):** Implement a CI\/CD pipeline using tools like GitHub Actions or Jenkins for automated testing and deployment.\n\n### Implementation Roadmap\n- **Phase 1: Requirement Gathering and Data Model Design**\n  - Conduct stakeholder meetings and workshops.\n  - Develop and iterate the ERD and data model.\n- **Phase 2: Application Architecture and Setup**\n  - Set up basic project structure and CI\/CD pipelines.\n  - Create initial repository and service layer templates.\n- **Phase 3: Core Feature Development**\n  - Implement core business features and integrate them with the frontend.\n- **Phase 4: Security and Performance Improvements**\n  - Add authentication, authorization, and performance optimizations.\n- **Phase 5: Testing and Final Adjustments**\n  - Ensure comprehensive testing and quality assurance.\n  - Gather feedback from a pilot test with selected users and make final adjustments.\n\nBy following this approach, we establish a strong foundation for the application's development and ensure that it is well-positioned for future enhancements and scalability.","GenTime":"2024-07-29 01:28:59"}
{"File Name":"govuk-aws\/0009-environment-bootstrapping-process.md","Context":"## Context\\nWe need a process to bootstrap a new Amazon environment. This should be both heavily automated\\nand include set places where the operator can evaluate the status of the provisioning.\\nThis bootstrapping process assumes the following:\\n* The required repositories are available\\n* An Amazon Web Services admin account is available\\n* Backups of our data can be retrieved (for the restore process)\\n\n## Decision\n","Decision":"The chosen process has a few, early bootstrap steps, that differ from normal operations. These\\naim to quickly provide the usual level of self-service to allow teams to independently\\nrestore services without a dependency on a central operations team. An early draft of\\nthis process, which will be updated as changes are required, will be:\\n1. Clone all the relevant repositories\\n1. Build the S3 bucket for Terraform state\\n1. Provision the new DNS zones for this environment\\n1. Build the Puppet master\\n1. Deploy the puppet code and secrets\\n1. Build the deploy Jenkins\\n* Rebuild everything else in the usual deployment ways\\nThis document will be written so that one of the non-infrastructure members of the team\\nwill be able to provision a complete environment within our allowed recovery time.\\n","tokens":80,"id":4025,"Prediction":"## Decision\n\nTo effectively bootstrap a new Amazon environment in a manner that balances automation and operator intervention, we will implement a scripted orchestration process using Infrastructure as Code (IaC) tools, such as Terraform or AWS CloudFormation, combined with AWS Systems Manager for status evaluation and control points.\n\n1. **Infrastructure as Code**: Use Terraform or CloudFormation to define and provision AWS resources in a repeatable and automated way. This ensures consistency and reduces manual errors.\n    - **Terraform**: Preferred for its cloud-agnostic capabilities and a large community of reusable modules.\n    - **CloudFormation**: Leverages AWS-specific features and improves integration with other AWS services.\n\n2. **Automation Scripting**: Develop custom scripts (e.g., in Python or Bash) to handle portions of the bootstrapping process that need to interface with APIs or perform complex logic not natively supported by IaC tools.\n\n3. **AWS Systems Manager**: Use AWS Systems Manager Automation documents to provide a structured way for operators to monitor progress and intervene as necessary.\n    - Include automation steps for initial setup, resource configuration, and status checks.\n    - Integrate with AWS Systems Manager\u2019s Run Command for executing scripts on instances and collecting results.\n\n4. **Backup and Restore Mechanism**: Integrate AWS Backup and custom restore procedures within the automation workflow to fetch and restore necessary data.\n    - Implement checkpoints in the automation to verify the integrity of restored data before proceeding.\n\n5. **Jenkins\/CI Pipeline Integration**: Integrate the bootstrapping process into a Continuous Integration (CI) pipeline, ensuring that changes to infrastructure definitions are automatically tested and applied.\n    - Use Jenkins or another preferred CI tool to run the IaC templates and scripts in a controlled environment.\n    - Include pipeline steps for manual intervention at critical points (e.g., waiting for operator approval before making irreversible changes).\n\n6. **Logging and Monitoring**: Set up AWS CloudWatch for centralized logging and real-time monitoring of the bootstrapping process.\n    - Create CloudWatch dashboards to visualize the status and health of the environment during bootstrapping.\n    - Use alarms to notify operators of any critical issues that require immediate attention.\n\n7. **Documentation and Training**: Provide comprehensive documentation for operators on the bootstrapping process, including:\n    - Step-by-step guides on how to initiate and monitor the process.\n    - Training sessions to familiarize operators with the workflow and intervention points.\n\nBy combining Infrastructure as Code, automation scripts, Systems Manager, and robust monitoring, we can ensure a heavily automated and reliable bootstrapping process with clearly defined intervention points for operators. This approach not only accelerates the provisioning workflow but also enhances the stability and maintainability of the new Amazon environment.","GenTime":"2024-07-29 01:29:11"}
{"File Name":"govuk-aws\/0012-security-groups-in-terraform.md","Context":"## Context\\nThere are two methods of defining security groups for AWS in Terraform and they are distinguished by how you add rules: in-line and separate. Using in-line rules keeps the definition close to the resource but when ever a rule is changed Terraform will re-generate the entire resource. Using separate rules Terraform will only make the single rule change but there is greater boilerplate and separation between the group resource and the rule resource.\\n\n## Decision\n","Decision":"Security groups will be defined separate to their rules.\\nAdditionally each security group will be defined, in its entirety, in a single file.\\n","tokens":89,"id":4032,"Prediction":"## Decision\n\nWe have decided to define our security groups using **separate rule resources** in Terraform. \n\n### Rationale:\n1. **Granular Updates**: By using separate rule resources, Terraform can make precise updates to individual security group rules without needing to re-generate the entire security group resource. This minimizes potential downtime and disruption to services that rely on these security groups.\n   \n2. **Reduced Risk of Outages**: Regenerating the entire security group resource, as required with in-line rules, poses a risk of temporary loss of connectivity or access. This risk is mitigated by making isolated changes to rules.\n\n3. **Scalability**: As our infrastructure grows, having separate rule resources makes it easier to manage and modify individual rules without impacting the entire configuration.\n\n4. **Clarity and Maintainability**: Despite the additional boilerplate, the separation between security groups and their rules can lead to clearer and more maintainable code. Each rule can be documented and changed independently, making the infrastructure-as-code easier to understand and manage.\n\n5. **Collaborative Development**: Separate rule definitions allow different team members to work on different rules without interfering with one another, promoting parallel development and reducing merge conflicts.\n\n### Trade-offs:\n- **Increased Boilerplate**: There will be more Terraform configuration files and potential duplication of resource attributes.\n\n- **Potential Separation**: There may be a cognitive load due to the separation between security group resources and their rules. However, with good documentation and a clear structure, this can be managed effectively.\n\nOverall, the benefits of using separate rule resources for defining security groups in Terraform outweigh the drawbacks, particularly in terms of flexibility, risk management, and maintainability.","GenTime":"2024-07-29 01:29:44"}
{"File Name":"govuk-aws\/0002-hosting-platforms.md","Context":"## Context\\nWe need to decide upon a platform to host the future GOV.UK infrastructure. Long term, this will be primarily the GOV.UK PaaS but in the interim, we need to converge with that plan and also upgrade and modernise the current infrastructure.\\nGDS policy for hosting of GDS internal services is PaaS first and AWS for anything that can not be run on the PaaS.\\n\n## Decision\n","Decision":"We are using Amazon Web Services as our hosting provider of choice. This conforms to the [GDS Tech Forum Hosting Guide](https:\/\/github.com\/alphagov\/gds-tech\/pull\/7).\\nWe will initially be using the `eu-west-1` region, Ireland. This region has 3 availability zones and also contains the GDS PaaS which will allow easier sharing and peering.\\n","tokens":86,"id":4041,"Prediction":"## Decision\n\nTo align with the long-term strategy and GDS policy, we will transition the future GOV.UK infrastructure to the GOV.UK PaaS (Platform as a Service). In the interim, we will modernize and upgrade our current infrastructure with a dual approach:\n1. **Primary Platform:** Wherever possible, services will be migrated to and hosted on GOV.UK PaaS to ensure compliance with the PaaS-first policy and to leverage the benefits of managed services, scalability, and ease of deployment.\n2. **Alternative Platform:** For services or components that cannot run on GOV.UK PaaS due to specific constraints or requirements, we will use Amazon Web Services (AWS) as the secondary hosting environment. This approach aligns with the GDS policy of using AWS as the fallback option when PaaS hosting is not feasible.\n\nThis two-step approach will help in converging towards the long-term goal while ensuring that the transition is smooth and does not disrupt current operations. Additionally, we will ensure that any infrastructure on AWS is optimized for future migration to GOV.UK PaaS wherever possible.","GenTime":"2024-07-29 01:30:25"}
{"File Name":"govuk-aws\/0029-combine-api-redis-into-backend-redis.md","Context":"## Context\\nWe currently have 3 Redis instances that we run in Elasticache:\\n- backend-redis\\n- logs-redis\\n- api-redis\\nMost applications use backend-redis, the logging cluster uses logs-redis (which should\\nsoon be replaced), and Rummager uses api-redis.\\nThis was traditionally the case because Rummager lived in a different vDC to redis-1\/redis-2,\\nbut this is no longer a concern in AWS.\\n\n## Decision\n","Decision":"Update Rummager configuration so it uses backend-redis, and remove api-redis.\\n","tokens":101,"id":4044,"Prediction":"## Decision\n\nWe will consolidate the three separate Redis instances (\"backend-redis\", \"logs-redis\", and \"api-redis\") into a single Redis instance in Elasticache. \n\n### Reasons for Decision:\n\n1. **Resource Optimization**: Running multiple Redis instances means that system resources (memory, CPU, etc.) are split among them. By consolidating into one instance, we can make more efficient use of resources.\n\n2. **Maintenance Simplification**: Managing a single Redis instance simplifies operational tasks such as backups, monitoring, scaling, and upgrades. Monitoring metrics, checking health, and configuring alerts would be more straightforward.\n\n3. **Cost Reduction**: Reducing the number of Redis instances can potentially lower our AWS costs related to running multiple Elasticache nodes.\n\n4. **No Isolation Requirement**: Given that the original need for separation due to different vDCs is no longer relevant in AWS, there is no strong reason to maintain separate instances based on outdated architecture constraints.\n\n5. **Improved Performance**: With recent enhancements in Redis versions and Elasticache capabilities, a single, appropriately-sized instance can handle the combined workload with minimal risk of performance degradation.\n\n### Action Plan:\n\n1. **Capacity Planning**: Before the consolidation, we need to analyze the workloads and compute the necessary Redis configuration (memory, CPU, instance type) to avoid performance bottlenecks.\n\n2. **Data Migration**: Develop a plan to migrate data from the \"logs-redis\" and \"api-redis\" instances to the consolidated \"backend-redis\".\n\n3. **Testing**: Thoroughly test the consolidated instance for workloads to ensure performance meets our requirements. This includes load testing and verifying that applications function correctly.\n\n4. **Deployment**: Perform a phased rollout where non-critical applications are migrated first to validate stability, followed by critical applications.\n\n5. **Monitoring and Alerts**: Set up comprehensive monitoring and alerting to quickly identify and resolve issues post-migration.\n\n6. **Documentation**: Update all relevant documentation to reflect the change in Redis architecture.\n\nThis consolidation decision aligns with our current AWS architecture and optimizes both resource utilization and operational efficiency.","GenTime":"2024-07-29 01:30:43"}
{"File Name":"govuk-aws\/0038-mongo_replacement_by_documentdb.md","Context":"## Context\\nGOV.UK uses MongoDB as the backing database for a number of apps. In a generic\\nGOV.UK environment, there are 5 MongoDB clusters: one for Licensify (a legacy\\nservice), one for performance monitoring, one for router and another per cloud\\nprovider (i.e. AWS and Carrenza) for all other apps (e.g. assets, imminence,\\nmaslow, content, publisher and short url manager).\\nOne architectural strategy principle of TechOps is to make use as much as\\npossible AWS managed services in order to reduce the burden on GOV.UK teams to\\nmanage Commercial off-the-shelf (COTS) functionality\/services.\\n\n## Decision\n","Decision":"The approaches taken for migrating from MongoDB to AWS DocumentDB are:\\n1. new apps that are being migrated from legacy cloud providers (e.g UKCloud and\\nCarrenza) to AWS and use MongoDB should be evaluated for compatibility with\\nAWS DocumentDB during the migration phase. Based on this evaluation, a\\ndecision should be made based on cost benefit whether a particular migrating\\napp should be using DocumentDB or MongoDB.\\n2. there are some apps (e.g. content store and imminence) that have already been\\nmigrated to AWS and uses the MongoDB cluster there. These apps should be\\nevaluated for compatibility with AWS DocumentDB and prioritised accordingly\\nfor migration to AWS DocumentDB.\\n","tokens":145,"id":4047,"Prediction":"## Decision\nThe current MongoDB clusters that are managed by GOV.UK teams should be migrated to Amazon Web Services (AWS) managed services, specifically Amazon DocumentDB (with MongoDB compatibility), or Amazon DynamoDB, depending on the specific requirements and compatibility of the individual applications.\n\nThis decision aligns with the TechOps principle of utilizing AWS managed services to reduce the operational burden. By using managed services, GOV.UK can benefit from improved scalability, security, and availability, while reducing the need for in-house maintenance and management of database infrastructure.\n\nThe migration strategy will involve:\n\n1. Assessing the compatibility of each application with the target AWS managed service.\n2. Planning a phased migration for each MongoDB cluster to minimize downtime and impact on services.\n3. Ensuring robust data migration processes are in place to maintain data integrity.\n4. Updating application configurations to connect to the new AWS managed databases.\n5. Performing comprehensive testing to ensure application functionality and performance post-migration.\n6. Training and documentation updates for the team to support the new AWS managed environments.\n\nThis approach will streamline database operations, allowing the GOV.UK teams to focus on higher-value tasks and improve the overall efficiency and reliability of the services provided.","GenTime":"2024-07-29 01:30:54"}
{"File Name":"govuk-aws\/0015-dns-infrastructure.md","Context":"## Context\\n- All our instances will need to be able to resolve internal infrastructure services, such\\nus Puppet, Graphite or Logstash\\n- Some services and application endpoints will need to be exposed to the Internet and\\nbe resolved by public DNS. For instance alerts.integration, deploy.integration, www-origin, etc\\n- We want to be able to create new pieces of infrastructure alongside the current piece of infrastructure\\nwith the ability to test direct access to each piece using DNS endpoints\\n- We want to control which stack is running the active version of a piece of infrastructure, and control\\nhow applications connect using DNS\\n- We want to ensure the site and all links works correctly when users browse using the\\npublishing (publishing.service.gov.uk) domain.\\n\n## Decision\n","Decision":"![DNS](.\/0015-govuk-aws-dns.jpg?raw=true \"DNS Infrastructure\")\\n#### Stack domains\\nEach stack has an internal and external DNS domain. All Terraform projects in that stack add records\\nto Route53 zones to expose the service internally and\/or externally.\\nFor instance, a 'green' stack has its own `green.<internalrootdomain>` and `green.<externalrootdomain>`\\ndomain. Puppet and Icinga services in this stack will add `puppet.green.<internalrootdomain>` and\\n`alerts.green.<externalrootdomain>` to Route53.\\nThis is for an infrastructure level view only. Applications will not work correctly across independent stacks,\\nand will only correctly work using the Publishing domain.\\n### Root domain service records\\nAll services will need an entry with the root domain that points to a stack record. This entry\\ncan be updated to select the active version of each service.\\nFor instance, machines are using the Puppet service `puppet.<internalrootdomain>` that is a CNAME\\nof `puppet.green.<internalrootdomain>`. At some point, a new Puppet stack 'blue' is provisioned to\\ntest a new version, and when it has been tested we switch the CNAME to the new stack, so\\n`puppet.<internalrootdomain>` resolves to `puppet.blue.<internalrootdomain>`.\\n#### External Route53 zones\\nThere is a public (external) Route53 zone to manage the external root domain. Each stack has also its own\\nexternal Route53 zone where we delegate the stack subdomain.\\nFor instance, if we are setting up a new environment with a public root domain `test.govuk.digital`,\\nand create a new stack 'green', we'll have a zone for `test.govuk.digital` and a zone for the stack\\nsubdomain `green.test.govuk.digital`, that stores the stack records.\\n#### Internal Route53 zones\\nFor internal domains we cannot replicate the external configuration because it's not possible to do DNS\\ndelegation with internal zones in the same VPC when the domains overlap.\\nFor instance, for the previous scenario we will have an internal zone for the root domain\\n`test.govuk-internal.digital`, but we cannot create and delegate a subdomain for a zone\\n`green.test.govuk-internal.digital` in the same VPC because it overlaps the root domain.\\nIn this case, for internal domains we can only have a single zone, but we'll still keep the same\\ndomain schema and each stack will generate records appending the stack subdomain.\\n","tokens":154,"id":4056,"Prediction":"## Decision\n\nTo address the requirements for both internal and external DNS resolution, as well as control and testing capabilities for new infrastructure, we will implement a split-horizon DNS architecture with the following components and practices:\n\n1. **Internal DNS Servers:**\n   - Deploy internal DNS servers dedicated to resolving internal infrastructure services such as Puppet, Graphite, and Logstash. These servers will handle queries from internal instances only.\n\n2. **External DNS Servers:**\n   - Utilize external DNS servers for resolving publicly exposed services and application endpoints. These servers will manage public DNS records for endpoints like alerts.integration, deploy.integration, www-origin, and the publishing domain (publishing.service.gov.uk).\n\n3. **DNS Zones & Records Management:**\n   - Define distinct DNS zones for internal and external records to segregate internal infrastructure resolution from publicly accessible endpoints.\n   - Implement a clear naming convention to distinguish between internal and external DNS records.\n\n4. **Controlled Switching Mechanism:**\n   - Introduce mechanisms such as DNS A\/B testing, weighted DNS records, or DNS failovers to control which stack is running the active version of a piece of infrastructure.\n   - Use DNS record changes (e.g., CNAME swaps) to control how applications connect to the appropriate infrastructure stack.\n\n5. **Automated DNS Management:**\n   - Leverage infrastructure-as-code (IaC) tools to automate the creation, testing, and management of DNS records. This approach ensures consistency and repeatability when deploying or modifying infrastructure components.\n\n6. **DNS Monitoring and Alerts:**\n   - Implement monitoring and alerting for DNS servers and records to quickly identify and resolve any issues affecting either internal or external DNS resolution.\n\n7. **Comprehensive DNS Testing:**\n   - Set up dedicated testing environments where both internal and external DNS resolution can be verified before changes are pushed to production.\n   - Regularly test DNS resolution and failover mechanisms to ensure they work correctly under various scenarios.\n\n8. **Documentation and Processes:**\n   - Document DNS management processes, including how to add new records, switch active infrastructure stacks, and handle DNS-related incidents.\n   - Train relevant teams on these processes to ensure smooth operations.\n\nBy implementing a split-horizon DNS architecture and the associated mechanisms, we can ensure robust, reliable, and flexible DNS resolution for both internal and external services, alongside seamless control over infrastructure transitions and application connectivity.","GenTime":"2024-07-29 01:31:43"}
{"File Name":"govuk-aws\/0032-transfer-artefact-binary.md","Context":"## Context\\nWe create binary artifacts for applications when we execute the **Deploy_App**\\nJenkins job. The binary file (artifact) gets stored in an AWS S3 bucket\\n(For example **govuk-integration-artefact**). We need the binary file to deploy\\nthe application to the next environment. For example, we will use the binary\\nfile from **Integration** to deploy the application in the **Staging**\\nenvironment.\\n\n## Decision\n","Decision":"We decided to use a mechanism that is located outside the environments involved\\nand pass the contents to the correct locations when the package becomes\\navailable.\\n![Transfer Mechanism](.\/0032-transfer-artefact-binary-img01.png?raw=true \"Transfer Mechanism\")\\n### Example\\nWe will use the **Integration** and **Staging** environments as examples.\\n1. We execute the **Deploy_Apps -> router** Jenkins job in the Integration environment.\\n2. A binary file is created in the **govuk-integration-artefact** AWS S3 bucket.\\n3. We have an AWS SNS Topic called **govuk-integration-artefact**.\\n4. We have an AWS S3 **govuk-integration-artefact** bucket event notification. This event sends a notification to the AWS SNS Topic when an **ObjectCreate** event is triggered.\\n5. The **Staging** environment has an AWS S3 bucket called **govuk-staging-artefact**.\\n6. We have an AWS SNS Subscription that subscribes to the **govuk-integration-artefact** AWS SNS Topic. This subscription will enable Staging to know when a new object gets created in the **govuk-integration-artefact** AWS S3 bucket.\\n7. We have an AWS Lambda function called **govuk-staging-artefact**. This function copies objects from the destination bucket to the target bucket. The trigger for the function is the AWS SNS Subscription.\\n","tokens":95,"id":4057,"Prediction":"## Decision\nWe will implement a structured versioning strategy and automated artifact promotion system for binary files stored in the AWS S3 bucket. This will include:\n\n1. **Versioning Schemes**: All binary artifacts will follow a consistent and easily traceable versioning scheme (e.g., Semantic Versioning: MAJOR.MINOR.PATCH).\n\n2. **Tagging and Metadata**: Artifacts will be tagged with metadata indicating the environment they were built in (e.g., \"integration\", \"staging\"). Additionally, the build timestamp and commit hash will be included in the metadata for traceability.\n\n3. **Automated Promotion**: Implement a Jenkins pipeline that automatically promotes artifacts from the Integration environment to the Staging environment once they pass the necessary integration tests. This will reduce manual intervention and streamline the deployment process.\n\n4. **Access Controls**: Ensure access controls are in place such that binaries can only be modified or deleted by automated systems or authorized personnel to maintain integrity.\n\n5. **Artifact Retention Policy**: Define and implement a retention policy for artifacts in the S3 bucket to manage storage costs and maintain a clean repository.\n\nBy introducing a structured versioning scheme and automated promotion, we enhance traceability, ensure consistent deployments across environments, and improve overall operational efficiency.","GenTime":"2024-07-29 01:31:46"}
{"File Name":"govuk-aws\/0008-postgres-on-puppetmaster.md","Context":"## Context\\nWe need to decide whether to run PostgreSQL on the Puppetmaster itself or use\\nRDS from the off.\\nCurrently our Puppetmaster uses a local PostgreSQL instance as its database. In moving to AWS we have the opportunity to replace this with an RDS instance.\\nThis may remove a reasonable amount of puppet code used to configure the database and associated tasks (e.g. backups). It would also require an amount of Terraform work to provision the RDS instance and some updates to the Puppetmaster code to use that instance.\\n\n## Decision\n","Decision":"The Puppetmaster will continue to use a local instance of PostgreSQL.\\n","tokens":110,"id":4061,"Prediction":"## Decision\n\nWe have decided to move forward with using Amazon RDS for PostgreSQL instead of running PostgreSQL on the Puppetmaster instance.\n\n### Rationale:\n1. **Reduced Operational Overhead**: Using RDS will reduce the amount of Puppet code needed to configure and maintain the PostgreSQL instance. It will also lighten the load on day-to-day operations like backups and updates, as these can be managed more efficiently using RDS's built-in capabilities.\n   \n2. **Scalability and Reliability**: RDS offers better scalability and high availability options compared to managing a local PostgreSQL instance on the Puppetmaster. AWS RDS facilitates seamless scaling and offers automated backups, snapshots, and multi-AZ deployments which increase the reliability of our database solution.\n\n3. **Security and Compliance**: RDS provides advanced security features such as automated software patching, data encryption at rest and in transit, and VPC integration. Leveraging these could help us better meet security and compliance requirements.\n\n4. **Resource Optimization**: Offloading the database workload to RDS will free up system resources on the Puppetmaster instance, potentially resulting in better performance and less resource contention.\n\n5. **Future-Proofing**: Transitioning to RDS prepares our infrastructure for future growth and makes it easier to accommodate potential changes in database architecture without significant rework.\n\n### Action Items:\n- Update Terraform scripts to provision the RDS instance according to our requirements.\n- Modify Puppetmaster configuration to use the new RDS instance.\n- Develop a migration plan to shift data from the current local PostgreSQL instance to the RDS instance.\n- Perform testing to ensure that the Puppetmaster operates correctly with the RDS instance.\n- Update maintenance documentation to reflect the new setup and operational procedures.\n- Plan and execute the final migration in a controlled manner, ensuring minimum downtime.\n\nThis decision aligns with our strategic goals to utilize cloud-native services for improved efficiency, scalability, and reliability.","GenTime":"2024-07-29 01:32:01"}
{"File Name":"ichatsdk-evolution\/0001-use-c-language.md","Context":"## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n## Decision\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":96,"id":4062,"Prediction":"## Context and Problem Statement\n\nThe software development team needs to select a framework for developing a new web application. The application will have a significant user base and will require robust performance and scalability. The choices being considered are between using a full-stack framework like Ruby on Rails, and a microservices architecture using Node.js.\n\n## Decision Drivers\n* **Performance:** The application must handle high user loads without performance degradation.\n* **Scalability:** The framework should allow for easy scaling of the application to accommodate user growth.\n* **Development Speed:** The chosen framework should enable rapid development to meet tight deadlines.\n* **Community and Support:** Comprehensive documentation and community support for troubleshooting.\n* **Maintainability:** The architecture should allow for easy maintenance and updates.\n\n## Decision\n\nAfter careful consideration of the context and decision drivers, the team has decided to adopt a microservices architecture using Node.js. This decision is driven by the following reasons:\n\n1. **Scalability:** Microservices architecture is inherently scalable as it allows individual services to be scaled independently.\n2. **Performance:** Node.js is known for its non-blocking, event-driven architecture, which is well-suited for handling a large number of concurrent connections efficiently.\n3. **Development Speed:** Node.js has a vast number of libraries and frameworks (e.g., Express.js) that can speed up the development process.\n4. **Community and Support:** Node.js has a large and active community, which means better support and a vast range of reusable components.\n5. **Maintainability:** Microservices, by design, support modular development, which allows for easier updates and maintenance. \n\nThis decision aligns with the need to develop a high-performance, scalable, and maintainable web application efficiently.","GenTime":"2024-07-29 01:32:05"}
{"File Name":"planet4-docs\/adr-0001-use-gitbook-for-technical-documentation.md","Context":"### Context and Problem Statement\\nCurrently Technical Documentation lives in the Handbook, with an [entry point](https:\/\/planet4.greenpeace.org\/create\/tech\/) and everything else organized under the [Development tag](https:\/\/planet4.greenpeace.org\/tag\/development\/). On the other hand Gitbook is a platform specifically for maintaining documentation, based on Markdown. We created a [Planet 4 space](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/) in GP Gitbook account to test how a potential structure of our documentation would look.\\n### Decision Drivers\\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n\n## Decision\n","Decision":"* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","tokens":492,"id":4087,"Prediction":"### Decision\nBased on the context and problem statement, as well as the decision drivers and the pros and cons of the considered options, we have decided to **move all Technical documentation (Development & Infrastructure) to Gitbook**. The factors influencing this decision include the dedicated nature of Gitbook for documentation, integrated syncing with GitHub for easy updates, better UI\/UX for users, and alignment with other teams within the organization who are already using Gitbook.\n\nThe transition to Gitbook is expected to streamline the process of maintaining and accessing technical documentation and create a more user-friendly experience for both internal and external contributors.","GenTime":"2024-07-29 01:33:27"}
{"File Name":"new-orbit\/0001-use-openshift-as-cloud-provider.md","Context":"## Context and Problem Statement\\nWe want to deploy our application in docker containers that can be easily updated\\n\n## Decision\n","Decision":"Chosen option: \"OpenShift\", because\\n* Built on Kubernetes.\\nThe bank has experience on it.\\nProvides a lot of added value tools for CI\/CD, automated builds.\\nIs supported by RedHat and we have a great support contract for it.\\n","tokens":21,"id":4096,"Prediction":"## Decision\n\nWe will implement a Docker-based deployment strategy for our application, leveraging Docker containers to encapsulate the application and its dependencies. This decision includes the following steps:\n\n1. **Containerization**:\n    - Develop Dockerfiles for each service within our application to ensure they can be containerized with all necessary dependencies.\n    - Use multi-stage builds where applicable to optimize the container size by separating build and runtime environments.\n\n2. **Versioning and Tagging**:\n    - Adopt a systematic versioning scheme for Docker images, using semantic versioning (e.g., v1.0.0, v1.1.0) to track changes and updates.\n    - Tag Docker images with these version numbers in the container registry to facilitate easy rollbacks and updates.\n\n3. **CI\/CD Integration**:\n    - Integrate Docker image build and push processes into our CI\/CD pipeline (e.g., using tools like Jenkins, GitLab CI, or GitHub Actions).\n    - Automate testing to ensure that new Docker images are verified before being deployed to production.\n\n4. **Container Orchestration**:\n    - Use a container orchestration platform such as Kubernetes, Docker Swarm, or AWS ECS to manage the deployment, scaling, and operation of Docker containers in production.\n    - Define deployment manifests (e.g., Kubernetes YAML files) and configuration files to manage container settings and environment variables.\n\n5. **Regular Updates and Maintenance**:\n    - Schedule regular updates for Docker base images to include security patches and performance improvements.\n    - Utilize automated monitoring and alerting tools (e.g., Prometheus, Grafana) to keep track of container performance and health.\n\n6. **Documentation and Training**:\n    - Provide comprehensive documentation to the development and operations teams, detailing the Docker-based deployment process, best practices, and troubleshooting tips.\n    - Conduct training sessions to ensure that all team members are proficient in using Docker and the associated deployment tools.\n\nBy following this strategy, we aim to achieve a robust, scalable, and easily maintainable deployment process for our application using Docker containers.","GenTime":"2024-07-29 01:33:59"}
{"File Name":"celestia-core\/adr-024-sign-bytes.md","Context":"## Context\\nCurrently, the messages exchanged between tendermint and a (potentially remote) signer\/validator,\\nnamely votes, proposals, and heartbeats, are encoded as a JSON string\\n(e.g., via `Vote.SignBytes(...)`) and then\\nsigned . JSON encoding is sub-optimal for both, hardware wallets\\nand for usage in ethereum smart contracts. Both is laid down in detail in [issue#1622].\\nAlso, there are currently no differences between sign-request and -replies. Also, there is no possibility\\nfor a remote signer to include an error code or message in case something went wrong.\\nThe messages exchanged between tendermint and a remote signer currently live in\\n[privval\/socket.go] and encapsulate the corresponding types in [types].\\n[privval\/socket.go]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/socket.go#L496-L502\\n[issue#1622]: https:\/\/github.com\/tendermint\/tendermint\/issues\/1622\\n[types]: https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/types\\n\n## Decision\n","Decision":"- restructure vote, proposal, and heartbeat such that their encoding is easily parseable by\\nhardware devices and smart contracts using a  binary encoding format ([amino] in this case)\\n- split up the messages exchanged between tendermint and remote signers into requests and\\nresponses (see details below)\\n- include an error type in responses\\n### Overview\\n```\\n+--------------+                      +----------------+\\n|              |     SignXRequest     |                |\\n|Remote signer |<---------------------+  tendermint    |\\n| (e.g. KMS)   |                      |                |\\n|              +--------------------->|                |\\n+--------------+    SignedXReply      +----------------+\\nSignXRequest {\\nx: X\\n}\\nSignedXReply {\\nx: X\\nsig: Signature \/\/ []byte\\nerr: Error{\\ncode: int\\ndesc: string\\n}\\n}\\n```\\nTODO: Alternatively, the type `X` might directly include the signature. A lot of places expect a vote with a\\nsignature and do not necessarily deal with \"Replies\".\\nStill exploring what would work best here.\\nThis would look like (exemplified using X = Vote):\\n```\\nVote {\\n\/\/ all fields besides signature\\n}\\nSignedVote {\\nVote Vote\\nSignature []byte\\n}\\nSignVoteRequest {\\nVote Vote\\n}\\nSignedVoteReply {\\nVote SignedVote\\nErr  Error\\n}\\n```\\n**Note:** There was a related discussion around including a fingerprint of, or, the whole public-key\\ninto each sign-request to tell the signer which corresponding private-key to\\nuse to sign the message. This is particularly relevant in the context of the KMS\\nbut is currently not considered in this ADR.\\n[amino]: https:\/\/github.com\/tendermint\/go-amino\/\\n### Vote\\nAs explained in [issue#1622] `Vote` will be changed to contain the following fields\\n(notation in protobuf-like syntax for easy readability):\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Vote {\\nVersion       fixed32\\nHeight        sfixed64\\nRound         sfixed32\\nVoteType      fixed32\\nTimestamp     Timestamp         \/\/ << using protobuf definition\\nBlockID       BlockID           \/\/ << as already defined\\nChainID       string            \/\/ at the end because length could vary a lot\\n}\\n\/\/ this is an amino registered type; like currently privval.SignVoteMsg:\\n\/\/ registered with \"tendermint\/socketpv\/SignVoteRequest\"\\nmessage SignVoteRequest {\\nVote vote\\n}\\n\/\/  amino registered type\\n\/\/ registered with \"tendermint\/socketpv\/SignedVoteReply\"\\nmessage SignedVoteReply {\\nVote      Vote\\nSignature Signature\\nErr       Error\\n}\\n\/\/ we will use this type everywhere below\\nmessage Error {\\nType        uint  \/\/ error code\\nDescription string  \/\/ optional description\\n}\\n```\\nThe `ChainID` gets moved into the vote message directly. Previously, it was injected\\nusing the [Signable] interface method `SignBytes(chainID string) []byte`. Also, the\\nsignature won't be included directly, only in the corresponding `SignedVoteReply` message.\\n[Signable]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/types\/signable.go#L9-L11\\n### Proposal\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Proposal {\\nHeight            sfixed64\\nRound             sfixed32\\nTimestamp         Timestamp         \/\/ << using protobuf definition\\nBlockPartsHeader  PartSetHeader     \/\/ as already defined\\nPOLRound          sfixed32\\nPOLBlockID        BlockID           \/\/ << as already defined\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalRequest\"\\nmessage SignProposalRequest {\\nProposal proposal\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalReply\"\\nmessage SignProposalReply {\\nProp   Proposal\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n### Heartbeat\\n**TODO**: clarify if heartbeat also needs a fixed offset and update the fields accordingly:\\n```proto\\nmessage Heartbeat {\\nValidatorAddress Address\\nValidatorIndex   int\\nHeight           int64\\nRound            int\\nSequence         int\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatRequest\"\\nmessage SignHeartbeatRequest {\\nHb Heartbeat\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatReply\"\\nmessage SignHeartbeatReply {\\nHb     Heartbeat\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n","tokens":249,"id":4097,"Prediction":"## Decision\n\nAfter careful evaluation of the encoding issues and the requirements for better integration with hardware wallets and Ethereum smart contracts, the following decisions have been made:\n\n1. **Protobuf Encoding**: Switch from JSON to Protocol Buffers (Protobuf) for encoding messages between Tendermint and remote signers\/validators. Protobuf is more efficient in terms of performance and size, and it is widely supported in various programming environments, including those typically used for hardware wallets and smart contracts. \n\n2. **Message Differentiation**: Introduce distinct message types for sign-request and sign-replies. This will help in clearly distinguishing the nature of the messages being transported and will simplify the parsing and handling logic at both ends.\n\n3. **Error Handling**: Implement a robust error handling mechanism that allows the remote signer to include detailed error codes and messages in case something goes wrong during the signing process. This can be achieved by extending the message format to include an `error` field which specifies the error code and a `message` field for a human-readable error description.\n\n4. **Data Structures Update**: Update existing data structures in `privval\/socket.go` and corresponding types in `types` to accommodate the new Protobuf encoding and differentiated message types. This should include updating relevant function signatures, data validation logic, and serialization\/deserialization routines.\n\n5. **Compatibility Layer**: Consider providing a temporary compatibility layer that translates JSON-encoded messages to Protobuf during a transitional period. This will ensure backward compatibility and provide time for all stakeholders to migrate their systems to the new protocol.\n\nBy implementing these decisions, we aim to improve the efficiency, performance, and reliability of message exchanges between Tendermint and remote signers\/validators, while also enhancing compatibility with hardware wallets and Ethereum smart contracts.","GenTime":"2024-07-29 01:34:02"}
{"File Name":"celestia-core\/adr-005-consensus-params.md","Context":"## Context\\nConsensus critical parameters controlling blockchain capacity have until now been hard coded, loaded from a local config, or neglected.\\nSince they may be need to be different in different networks, and potentially to evolve over time within\\nnetworks, we seek to initialize them in a genesis file, and expose them through the ABCI.\\nWhile we have some specific parameters now, like maximum block and transaction size, we expect to have more in the future,\\nsuch as a period over which evidence is valid, or the frequency of checkpoints.\\n\n## Decision\n","Decision":"### ConsensusParams\\nNo consensus critical parameters should ever be found in the `config.toml`.\\nA new `ConsensusParams` is optionally included in the `genesis.json` file,\\nand loaded into the `State`. Any items not included are set to their default value.\\nA value of 0 is undefined (see ABCI, below). A value of -1 is used to indicate the parameter does not apply.\\nThe parameters are used to determine the validity of a block (and tx) via the union of all relevant parameters.\\n```\\ntype ConsensusParams struct {\\nBlockSize\\nTxSize\\nBlockGossip\\n}\\ntype BlockSize struct {\\nMaxBytes int\\nMaxTxs int\\nMaxGas int\\n}\\ntype TxSize struct {\\nMaxBytes int\\nMaxGas int\\n}\\ntype BlockGossip struct {\\nBlockPartSizeBytes int\\n}\\n```\\nThe `ConsensusParams` can evolve over time by adding new structs that cover different aspects of the consensus rules.\\nThe `BlockPartSizeBytes` and the `BlockSize.MaxBytes` are enforced to be greater than 0.\\nThe former because we need a part size, the latter so that we always have at least some sanity check over the size of blocks.\\n### ABCI\\n#### InitChain\\nInitChain currently takes the initial validator set. It should be extended to also take parts of the ConsensusParams.\\nThere is some case to be made for it to take the entire Genesis, except there may be things in the genesis,\\nlike the BlockPartSize, that the app shouldn't really know about.\\n#### EndBlock\\nThe EndBlock response includes a `ConsensusParams`, which includes BlockSize and TxSize, but not BlockGossip.\\nOther param struct can be added to `ConsensusParams` in the future.\\nThe `0` value is used to denote no change.\\nAny other value will update that parameter in the `State.ConsensusParams`, to be applied for the next block.\\nTendermint should have hard-coded upper limits as sanity checks.\\n","tokens":111,"id":4099,"Prediction":"## Decision\nWe will adopt an approach where critical consensus parameters are initialized in a genesis file and exposed via the ABCI (Application Blockchain Interface). This will enable flexible and dynamic configuration of the blockchain's operational constraints, allowing them to vary across different networks and evolve over time.\n\nKey points of the decision include:\n1. **Genesis File Initialization**: All critical parameters will be initialized in the genesis file during the blockchain's creation. This ensures that each network can start with a tailored configuration suitable for its intended use case.\n   \n2. **ABCI Exposure**: The consensus parameters will be exposed through the ABCI to allow dynamic updates and querying of these parameters during the blockchain's runtime. This provides the flexibility to modify important network settings without requiring a complete restart or hard fork.\n\n3. **Modular Design for Future Parameters**: The system will be designed to accommodate additional consensus parameters in the future. This ensures scalability and adaptability as more parameters, such as the period over which evidence is valid or checkpoint frequency, become relevant.\n\n4. **Security and Governance**: Changes to critical consensus parameters will be governed by strict security policies and consensus mechanisms to prevent arbitrary or malicious alterations. This includes setting clear rules for how parameters can be updated and who has the authority to make such changes.\n\n5. **Testing and Validation**: Comprehensive testing and validation procedures will be implemented to ensure that changes to consensus parameters do not adversely affect the network's stability or security.\n\nBy adopting this approach, we aim to create a more flexible and adaptable blockchain architecture that can meet diverse requirements and evolve seamlessly over time.","GenTime":"2024-07-29 01:34:06"}
{"File Name":"celestia-core\/adr-004-historical-validators.md","Context":"## Context\\nRight now, we can query the present validator set, but there is no history.\\nIf you were offline for a long time, there is no way to reconstruct past validators. This is needed for the light client and we agreed needs enhancement of the API.\\n\n## Decision\n","Decision":"For every block, store a new structure that contains either the latest validator set,\\nor the height of the last block for which the validator set changed. Note this is not\\nthe height of the block which returned the validator set change itself, but the next block,\\nie. the first block it comes into effect for.\\nStoring the validators will be handled by the `state` package.\\nAt some point in the future, we may consider more efficient storage in the case where the validators\\nare updated frequently - for instance by only saving the diffs, rather than the whole set.\\nAn alternative approach suggested keeping the validator set, or diffs of it, in a merkle IAVL tree.\\nWhile it might afford cheaper proofs that a validator set has not changed, it would be more complex,\\nand likely less efficient.\\n","tokens":56,"id":4101,"Prediction":"## Decision\n\nTo enhance API functionality and meet the requirements for the light client, we have decided to implement a historical validator set retrieval feature. This will involve the following steps:\n\n1. **Database Schema Updates**: Modify the existing database schema to store snapshots or historical records of the validator sets at each block height or predetermined intervals (e.g., every N blocks).\n\n2. **API Changes**:\n   - Extend the current API to include endpoints for querying historical validator sets.\n   - Introduce parameters for requests to specify a block height or a time range to retrieve the corresponding validator set(s).\n\n3. **Data Storage Optimization**: Implement efficient storage and retrieval mechanisms to handle potentially large volumes of historical data, ensuring that performance remains high and storage costs are managed effectively.\n\n4. **Versioning**: Ensure backward compatibility for the existing API while rolling out versioned API endpoints to support querying historical data.\n\n5. **Documentation and Communication**:\n   - Update the API documentation to reflect the new endpoints and parameters.\n   - Communicate these changes to stakeholders and developers to ensure smooth adoption.\n\n6. **Testing and Validation**:\n   - Conduct thorough testing to verify that historical data retrieval is accurate and performs well under various scenarios.\n   - Validate correctness by cross-referencing historical validator sets with known historical states.\n\nThis decision will ensure that clients, including light clients, can reconstruct past validator sets even if they were offline, enhancing our system's resilience and usability.","GenTime":"2024-07-29 01:34:13"}
{"File Name":"celestia-core\/adr-060-go-api-stability.md","Context":"## Context\\nWith the release of Tendermint 1.0 we will adopt [semantic versioning](https:\/\/semver.org). One major implication is a guarantee that we will not make backwards-incompatible changes until Tendermint 2.0 (except in pre-release versions). In order to provide this guarantee for our Go API, we must clearly define which of our APIs are public, and what changes are considered backwards-compatible.\\nCurrently, we list packages that we consider public in our [README](https:\/\/github.com\/tendermint\/tendermint#versioning), but since we are still at version 0.x we do not provide any backwards compatiblity guarantees at all.\\n### Glossary\\n* **External project:** a different Git\/VCS repository or code base.\\n* **External package:** a different Go package, can be a child or sibling package in the same project.\\n* **Internal code:** code not intended for use in external projects.\\n* **Internal directory:** code under `internal\/` which cannot be imported in external projects.\\n* **Exported:** a Go identifier starting with an uppercase letter, which can therefore be accessed by an external package.\\n* **Private:** a Go identifier starting with a lowercase letter, which therefore cannot be accessed by an external package unless via an exported field, variable, or function\/method return value.\\n* **Public API:** any Go identifier that can be imported or accessed by an external project, except test code in `_test.go` files.\\n* **Private API:** any Go identifier that is not accessible via a public API, including all code in the internal directory.\\n\n## Decision\n","Decision":"From Tendermint 1.0, all internal code (except private APIs) will be placed in a root-level [`internal` directory](https:\/\/golang.org\/cmd\/go\/#hdr-Internal_Directories), which the Go compiler will block for use by external projects. All exported items outside of the `internal` directory are considered a public API and subject to backwards compatibility guarantees, except files ending in `_test.go`.\\nThe `crypto` package may be split out to a separate module in a separate repo. This is the main general-purpose package used by external projects, and is the only Tendermint dependency in e.g. IAVL which can cause some problems for projects depending on both IAVL and Tendermint. This will be decided after further discussion.\\nThe `tm-db` package will remain a separate module in a separate repo. The `crypto` package may possibly be split out, pending further discussion, as this is the main general-purpose package used by other projects.\\n","tokens":336,"id":4112,"Prediction":"## Decision\n\n### Adopt Clear API Boundaries and Backwards Compatibility Guidelines\n\n1. **API Definition and Documentation**:\n   - Clearly define public APIs by explicitly listing them in documentation.\n   - Update the [README](https:\/\/github.com\/tendermint\/tendermint#versioning) to reflect which packages are public APIs.\n   - Use GoDoc comments to document public APIs and indicate expected stability.\n\n2. **Semantic Versioning Application**:\n   - Adhere strictly to semantic versioning principles to maintain backwards compatibility.\n   - No breaking changes will be made to public APIs until Tendermint 2.0, adhering to semver guarantees.\n\n3. **API Ownership and Internal\/External Demarcation**:\n   - Place internal packages and any code not intended for external use under `internal\/` directories to prevent accidental usage.\n   - Ensure that exported identifiers in packages within the `internal\/` directory cannot be imported by external projects.\n\n4. **Compatibility Assessment and Tooling**:\n   - Regularly use tools like `go vet`, `golint`, and other static analysis tools to ensure the stability and compatibility of public APIs.\n   - Implement automated tests to verify that changes do not break public APIs.\n\n5. **Change Management**:\n   - Establish a clear process for proposing, reviewing, and approving changes to public APIs.\n   - Document all changes affecting public APIs in release notes, specifying whether they are backwards-compatible enhancements or bug fixes.\n\n6. **Versioning Policy Communication**:\n   - Communicate the versioning policy and compatibility guarantees clearly to the community and external stakeholders to align expectations.\n   - Provide guidelines and best practices for external projects integrating with Tendermint's public APIs to prevent misuse and breaking changes.\n\nBy following these guidelines, Tendermint will ensure the stability, predictability, and reliability of its public APIs, fostering trust and ease of use for developers and external projects that depend on Tendermint.","GenTime":"2024-07-29 01:34:56"}
{"File Name":"celestia-core\/adr-056-light-client-amnesia-attacks.md","Context":"## Context\\nWhilst most created evidence of malicious behavior is self evident such that any individual can verify them independently there are types of evidence, known collectively as global evidence, that require further collaboration from the network in order to accumulate enough information to create evidence that is individually verifiable and can therefore be processed through consensus. [Fork Accountability](https:\/\/github.com\/tendermint\/spec\/blob\/master\/spec\/consensus\/light-client\/accountability.md) has been coined to describe the entire process of detection, proving and punishing of malicious behavior. This ADR addresses specifically what a light client amnesia attack is and how it can be proven and the current decision around handling light client amnesia attacks. For information on evidence handling by the light client, it is recommended to read [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md).\\n### Amnesia Attack\\nThe schematic below explains a scenario where an amnesia attack can occur such that two sets of honest nodes, C1 and C2, commit different blocks.\\n![](..\/imgs\/tm-amnesia-attack.png)\\n1. C1 and F send PREVOTE messages for block A.\\n2. C1 sends PRECOMMIT for round 1 for block A.\\n3. A new round is started, C2 and F send PREVOTE messages for a different block B.\\n4. C2 and F then send PRECOMMIT messages for block B.\\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\\nThis forged block can then be used to fool light clients trying to verify it. It must be stressed that there are a few more hurdles or dimensions to the attack to consider.For a more detailed walkthrough refer to Appendix A.\\n\n## Decision\n","Decision":"The decision surrounding amnesia attacks has both a short term and long term component. In the long term, a more sturdy protocol will need to be fleshed out and implemented. There is already draft documents outlining what such a protocol would look like and the resources it would require (see references). Prior revisions however outlined a protocol which had been implemented (See Appendix B). It was agreed that it still required greater consideration and review given it's importance. It was therefore discussed, with the limited time frame set before 0.34, whether the protocol should be completely removed or if there should remain some logic in handling the aforementioned scenarios.\\nThe latter of the two options meant storing a record of all votes in any height with which there was more than one round. This information would then be accessible for applications if they wanted to perform some off-chain verification and punishment.\\nIn summary, this seemed like too much to ask of the application to implement only on a temporary basis, whilst not having the domain specific knowledge and considering such a difficult and unlikely attack. Therefore the short term decision is to identify when the attack has occurred and implement the detector algorithm highlighted in [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md) but to not implement any accountability protocol that would identify malicious validators and allow applications to punish them. This will hopefully change in the long term with the focus on eventually reaching a concrete and secure protocol with identifying and dealing with these attacks.\\n","tokens":379,"id":4113,"Prediction":"## Decision\nTo mitigate and handle light client amnesia attacks effectively, the following strategic decisions have been made:\n\n### Detection and Reporting of Amnesia Attacks\n1. **Enhanced Logging and Monitoring:**\n   - Implement enhanced logging to capture all PREVOTE and PRECOMMIT messages from all validators.\n   - Monitor for unusual patterns in votes, such as multiple PRECOMMITS for different blocks in the same round. \n\n2. **Suspicious Behavior Flagging:**\n   - Flag validators that send conflicting PRECOMMIT messages as suspicious. Implement heuristics to identify conflicting messages and suspicious rounds.\n\n### Proving Amnesia Attack\n1. **Double-Vote Evidence Collection:**\n   - Collect evidence of double voting by any validator. This involves capturing and logging all PREVOTE and PRECOMMIT messages and checking for inconsistencies.\n   - Utilize a proof mechanism that consolidates these conflicting messages into a verifiable proof of an amnesia attack, following the standards outlined in [Fork Accountability](https:\/\/github.com\/tendermint\/spec\/blob\/master\/spec\/consensus\/light-client\/accountability.md).\n\n2. **Distributed Evidence Aggregation:**\n   - Encourage network participation in aggregating evidence. Use a distributed approach to gather multiple pieces of evidence from different nodes to ensure reliability and robustness of the proof.\n\n### Punishing Malicious Behavior\n1. **Immediate Slashing:**\n   - Upon confirmation of an amnesia attack, immediately slash the stake of the misbehaving validators. This serves as a deterrent to prevent future attacks.\n\n2. **Temporary Suspension:**\n   - Temporarily suspend the validator from the consensus process until further investigation is completed. This prevents immediate further damage to the network.\n\n3. **Consensus Blacklisting:**\n   - Introduce a consensus blacklist where validators proven to have participated in amnesia attacks are listed. These validators can be excluded from future rounds until they provide assurance and re-stake.\n\n### Consistency and Consensus Processing\n1. **Consensus Rules Update:**\n   - Update the consensus rules to include checks for double voting and amnesia attacks at each round. Ensure that light clients enforce these updated rules strictly.\n\n2. **Peer Awareness:**\n   - Light clients should maintain an awareness system where they can alert and notify peers about detected amnesia attacks. \n\n### Communication and Education\n1. **Network Communication:**\n   - Communicate the presence of an amnesia attack promptly to all network participants to raise awareness and prompt action.\n\n2. **Educational Initiatives:**\n   - Develop educational materials and tutorials to instruct validators and light clients about recognizing and reporting amnesia attacks. Ensure widespread understanding and vigilance across the network.\n\nBy adopting these comprehensive measures, the network aims to effectively address light client amnesia attacks, protect the integrity of the consensus process, and maintain trust among all participants. Each of these measures will be revisited periodically to ensure effectiveness and adaptability to emerging threats.","GenTime":"2024-07-29 01:35:04"}
{"File Name":"celestia-core\/adr-018-ABCI-Validators.md","Context":"## Context\\nADR 009 introduced major improvements to the ABCI around validators and the use\\nof Amino. Here we follow up with some additional changes to improve the naming\\nand expected use of Validator messages.\\n\n## Decision\n","Decision":"### Validator\\nCurrently a Validator contains `address` and `pub_key`, and one or the other is\\noptional\/not-sent depending on the use case. Instead, we should have a\\n`Validator` (with just the address, used for RequestBeginBlock)\\nand a `ValidatorUpdate` (with the pubkey, used for ResponseEndBlock):\\n```\\nmessage Validator {\\nbytes address\\nint64 power\\n}\\nmessage ValidatorUpdate {\\nPubKey pub_key\\nint64 power\\n}\\n```\\nAs noted in [ADR-009](adr-009-ABCI-design.md),\\nthe `Validator` does not contain a pubkey because quantum public keys are\\nquite large and it would be wasteful to send them all over ABCI with every block.\\nThus, applications that want to take advantage of the information in BeginBlock\\nare _required_ to store pubkeys in state (or use much less efficient lazy means\\nof verifying BeginBlock data).\\n### RequestBeginBlock\\nLastCommitInfo currently has an array of `SigningValidator` that contains\\ninformation for each validator in the entire validator set.\\nInstead, this should be called `VoteInfo`, since it is information about the\\nvalidator votes.\\nNote that all votes in a commit must be from the same round.\\n```\\nmessage LastCommitInfo {\\nint64 round\\nrepeated VoteInfo commit_votes\\n}\\nmessage VoteInfo {\\nValidator validator\\nbool signed_last_block\\n}\\n```\\n### ResponseEndBlock\\nUse ValidatorUpdates instead of Validators. Then it's clear we don't need an\\naddress, and we do need a pubkey.\\nWe could require the address here as well as a sanity check, but it doesn't seem\\nnecessary.\\n### InitChain\\nUse ValidatorUpdates for both Request and Response. InitChain\\nis about setting\/updating the initial validator set, unlike BeginBlock\\nwhich is just informational.\\n","tokens":44,"id":4119,"Prediction":"## Decision\nWe will implement the following adjustments to improve clarity and usability for Validator messages in the ABCI:\n\n1. **Rename Fields for Clarity**: We will rename existing fields in the Validator messages to be more descriptive and clear. This will ensure users can quickly understand their purpose and usage.\n\n2. **Standardize Message Formats**: Validator message formats will be standardized to follow a consistent structure. This will include uniform conventions for naming, data types, and required fields.\n\n3. **Enhance Documentation**: Comprehensive documentation will be provided for all Validator messages. This documentation will include detailed descriptions of each field, their expected values, and example use cases.\n\n4. **Backward Compatibility**: Although we aim to improve naming and usage, maintaining backward compatibility is crucial. We will ensure that changes are made in a way that doesn't break existing implementations, potentially through deprecation warnings for legacy fields.\n\n5. **Implement Validation Checks**: We will introduce additional validation checks in the system to ensure Validator messages conform to the new naming conventions and formats. This will help catch errors early and assist developers in the transition.\n\n6. **Community Feedback**: Before finalizing changes, we will solicit feedback from the community and stakeholders. This will help identify any potential issues or further improvements that can be made.\n\nThese decisions aim to make the ABCI more intuitive and easier to work with, leading to a smoother development experience and more robust validation processes.","GenTime":"2024-07-29 01:35:17"}
{"File Name":"celestia-core\/adr-026-general-merkle-proof.md","Context":"## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding\/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n\n## Decision\n","Decision":"### ProofOperator\\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\\n```go\\ntype ProofOperator interface {\\nRun([][]byte) ([][]byte, error)\\nGetKey() []byte\\nProofOp() ProofOp\\n}\\n```\\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\\n### ProofOp\\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\\n","tokens":99,"id":4130,"Prediction":"## Decision\n\nWe will implement the `ProofOperator` interface to enhance the handling and flexibility of Merkle proofs within the `abci.ResponseQuery`. The new interface will allow for easier layering and chaining of Merkle proofs and make the proof format extensible to support third-party proof types.\n\nKey elements of the decision include:\n\n1. **Introduction of `ProofOperator` Interface:**\n   - Define an interface `ProofOperator` that will standardize the operations necessary for handling Merkle proofs.\n   - The interface will include methods for verifying and combining proofs without the need to decode or re-encode raw `[]byte` data.\n\n2. **Layering Proofs:**\n   - Use the `ProofOperator` interface to layer proofs directly.\n   - Ensure that each layer operates independently without needing to decode and re-encode the proofs, improving efficiency and reducing errors.\n\n3. **Chaining Proofs:**\n   - Implement a mechanism to chain multiple proof operators.\n   - This allows for a more general approach to proof validation, where different proof types can be composed together seamlessly.\n\n4. **Extensibility:**\n   - Design the `ProofOperator` interface to be extensible, so new proof types can be added by third parties without modifying the core implementation.\n   - This includes providing a registry or factory pattern to manage and instantiate different `ProofOperator` implementations.\n\n5. **Backward Compatibility:**\n   - Ensure that current users of raw `[]byte` proofs can transition to the new system without significant disruption.\n   - Provide adapters or conversion utilities if necessary to facilitate this transition.\n\nBy adopting the `ProofOperator` interface, we aim to create a more robust, flexible, and extensible system for handling Merkle proofs within the `abci.ResponseQuery`. This decision will enable better scalability and interoperability for our users and third-party developers.","GenTime":"2024-07-29 01:35:48"}
{"File Name":"celestia-core\/adr-005-decouple-blockid-and-partsetheader.md","Context":"## Context\\nCelestia has multiple commits to the block data via the `DataHash` and the `PartSetHeader` in the `BlockID`. As stated in the [#184](https:\/\/github.com\/celestiaorg\/lazyledger-core\/issues\/184), we no longer need the `PartSetHeader` for this additional commitment to the block's data. However, we are still planning to use the `PartSetHeader` for block propagation during consensus in the short-medium term. This means that we will remove the `PartSetHeader` from as many places as possible, but keep it in the `Proposal` struct.\\n\n## Decision\n","Decision":"While we build other better designs to experiment with, we will continue to implement the design specified here as it is not orthogonal. https:\/\/github.com\/celestiaorg\/lazyledger-core\/pull\/434#issuecomment-869158788\\n","tokens":132,"id":4152,"Prediction":"## Decision\nTo ensure an efficient transition while maintaining the necessary data commitments and block propagation requirements during consensus, we will:\n\n1. **Remove the `PartSetHeader`** from all locations where it is presently not obligatory for consensus and block propagation.\n2. **Retain the `PartSetHeader` in the `Proposal` struct** solely for the purpose of block propagation during consensus.\n3. Implement supporting code modifications to ensure the system remains stable and functional during the transition period.\n\nThis approach allows us to simplify our structure by removing redundant data while ensuring that the necessary components for block propagation during consensus remain intact. We will continuously monitor and evaluate the system to ensure that these changes do not adversely affect the consensus process and make further adjustments as needed to eventually phase out the `PartSetHeader` entirely.","GenTime":"2024-07-29 01:37:15"}
{"File Name":"digital-paper-edit-client\/2019-05-01-security.md","Context":"## Context and Problem Statement\\nThese are notes based on the InfoSec meeting held on the 30th of May 2019.\\n## Decision Drivers\\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\n\n## Decision\n","Decision":"* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\nWe are going with Option 2 as this provides us with security, while\\nproviding a simpler way to set up access for journalists.\\nBen Thornton has agreed to this.\\nThe benefits of this approach means that we will be\\nusing existing BBC services to implement security, rather than implementing our\\nown servers. A single VPC is sufficient for **protected** levels of service, and\\nwe can apply SecurityGroups in between to have added security.\\nThe downside of this approach is that it is not using SSO but we can add that\\nlater on.\\nIn terms of data retention, we will go with the recommended 90 days.\\n![Architecture Diagram with Security](.\/dpe-transcript-EC2_security.png)\\n","tokens":67,"id":4166,"Prediction":"## Context and Problem Statement\nThese are notes based on the InfoSec meeting held on the 30th of May 2019.\n\n## Decision Drivers\n* Ease of use for journalists when system authenticates them\n* Security levels (public, protect, or restrict)\n* Data retention based on inactivity\n* Complexity\n\n## Decision\nWe will implement a multi-factor authentication (MFA) system to balance ease of use and security.\n\n### Rationale\n1. **Ease of Use for Journalists**: By incorporating MFA, journalists can use a secondary verification method that is straightforward and user-friendly, such as an authenticator app or SMS-based one-time passwords.\n2. **Security Levels**: MFA provides an additional layer of security, suitable for environments requiring varying levels of data protection (public, protect, or restrict).\n3. **Data Retention Based on Inactivity**: Implementing MFA enables flexibility in managing data retention policies. Journalists who frequently authenticate through MFA can have their data retention extended based on their activity.\n4. **Complexity**: While MFA adds some complexity, it remains manageable and beneficial when compared to the security it provides. The balance between added complexity and improved security is justified.\n\n### Implementation Plan\n1. **Phase 1**: Research and select an MFA solution that integrates seamlessly with our existing authentication system.\n2. **Phase 2**: Develop and test the MFA integration in a staging environment.\n3. **Phase 3**: Deploy the MFA system and provide training and support to journalists.\n4. **Phase 4**: Monitor the system and gather feedback for continuous improvement.\n\nBy making this decision, we aim to enhance the security of journalist authentication without compromising usability. This strategic approach aligns with our primary decision drivers and ensures a robust yet user-friendly environment for our journalist users.","GenTime":"2024-07-29 01:37:58"}
{"File Name":"digital-paper-edit-client\/2019-04-29-SQL-database-schema.md","Context":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n## Decision\n","Decision":"* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","tokens":248,"id":4167,"Prediction":"## Decision\n\nWe will design a versioned SQL schema using a migration-based approach that allows us to incrementally update and maintain the database schema in line with our project milestones. This will ensure that both the PostgreSQL database for the web app and the SQLite database for the Electron desktop version remain in sync with each other. The key decisions are as follows:\n\n1. **Adoption of Versioned Migrations**: \n   We will use a migration tool, such as Flyway or Liquibase, to manage and apply schema changes. Each project milestone will correspond to a specific migration version, ensuring that schema updates are tracked and applied in a controlled manner.\n\n2. **Three Main Migration Scripts**:\n   We will create specific migration scripts for each of our main milestones:\n   - *Feature: Transcript Editing*: This migration script will include all necessary tables, indexes, and constraints required to support transcript editing.\n   - *Feature: Transcript Annotations*: This migration script will build upon the first by adding tables and relationships to manage transcript annotations.\n   - *Feature: Paper Editing*: This migration script will further enhance the schema with tables and constraints necessary for paper editing features.\n\n3. **Export and Import Strategies**:\n   - We will utilize DB Designer or equivalent tools to draft our schema design visually and export SQL codes for both creation and deletion of tables.\n   - These exported SQL scripts will be integrated into our version control system, providing a reliable way to track schema changes and rollback if necessary.\n\n4. **SQLite Compatibility Considerations**:\n   - We will ensure that our PostgreSQL schema design is compatible with SQLite to enable seamless transitions between the two.\n   - Any PostgreSQL-specific features or data types will be reviewed and adjusted to ensure functionality within SQLite environments.\n\n5. **Documentation and Testing**:\n   - Clear documentation will be provided for each migration, detailing the changes made and how they align with the relevant milestone.\n   - Automated tests will be implemented to verify the integrity and performance of the schema migrations in both PostgreSQL and SQLite databases.\n\nBy following a migration-based approach and adhering to these decisions, we aim to create a robust, versioned schema that can evolve with our project milestones, facilitate easy versioning, and ensure consistency across different database platforms.","GenTime":"2024-07-29 01:38:14"}
{"File Name":"digital-paper-edit-client\/2019-09-28-time-estimation-for-links.md","Context":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n\n## Decision\n","Decision":"* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","tokens":49,"id":4171,"Prediction":"## Decision\nWe will include the time taken for voice overs by incorporating a dedicated field in our video production tool where users can input the duration of voice overs. This field will be used to calculate the total estimated time for the rough cut.\n\n### Rationale\n1. **Ease of Implementation**: Adding a dedicated field for voice over duration is straightforward and can be done with minimal changes to the existing system.\n2. **Accuracy of Time Estimated**: By allowing the precise input of voice over duration, we ensure that the time estimation for the rough cut is more accurate and reflective of the actual production requirements.\n3. **Extensibility**: This approach allows for future enhancements, such as integrating with voice over management tools or automating the estimation by analyzing voice over files directly.\n\n### Implementation Steps\n1. **UI Modification**: Add a new input field labeled \"Voice Over Duration\" in the rough cut time estimation form.\n2. **Backend Logic**: Adjust the time estimation logic to include the value from the \"Voice Over Duration\" field.\n3. **Testing**: Ensure that the new field correctly contributes to the total estimated time and that all related calculations are accurate.\n4. **Documentation**: Update user guides and documentation to include instructions on using the new \"Voice Over Duration\" field.\n\n### Considerations\n- **User Training**: Ensure users are aware of the new field and understand how to use it for accurate time estimation.\n- **Validation**: Implement validation to handle incorrect or unrealistic voice over duration inputs.\n\nBy following this decision, we can provide a more precise tool for our users to estimate the time required for producing a rough cut, enhancing overall user satisfaction and project planning accuracy.","GenTime":"2024-07-29 01:38:30"}
{"File Name":"rails-template\/0009-use-scripts-to-rule-them-all.md","Context":"## Context\\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\\nAll[1].\\nThis repository should include reference and document this decision.\\n[1]\\nhttps:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md\\n\n## Decision\n","Decision":"By default we will follow the Scripts To Rule Them All pattern for common tasks\\nin this template.\\n","tokens":67,"id":4178,"Prediction":"## Decision\n\nWe will adopt and implement the \"Scripts To Rule Them All\" pattern for this repository, in accordance with the recently approved RFC by dxw. \n\nThis involves:\n\n1. Creating a set of consistent and standardized scripts for common tasks such as setup, build, test, lint, and more.\n2. Documenting these scripts clearly within the repository to ensure easy access and comprehension by all contributors.\n3. Ensuring that these scripts are up-to-date and maintained as part of our regular development process.\n4. Including reference to the original RFC and ensuring alignment with the guidelines provided therein.\n\nThis decision is aimed at improving developer experience by offering a standardized method for executing common project tasks, thereby encouraging consistency and reducing onboarding time for new developers.","GenTime":"2024-07-29 01:38:51"}
{"File Name":"rails-template\/0006-use-simplecov-to-monitor-code-test-coverage.md","Context":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n\n## Decision\n","Decision":"Use Simplecov with RSpec to monitor coverage changes on every test run\\n","tokens":32,"id":4179,"Prediction":"## Decision\n\nTo maintain high test coverage efficiently and minimize reliance on manual checks, we will integrate an automated testing framework into our Continuous Integration (CI) pipeline. This framework will include the following components:\n\n1. **Unit Testing**: Use a robust unit testing framework such as JUnit (for Java), pytest (for Python), or Jest (for JavaScript) to ensure that individual components of the application work as expected.\n   \n2. **Integration Testing**: Implement integration tests using tools like Postman\/Newman for API testing or Selenium for web application testing to verify that different modules work together correctly.\n\n3. **Code Coverage Tools**: Integrate a code coverage tool such as JaCoCo for Java, Coverage.py for Python, or Istanbul for JavaScript to measure the extent of code covered by tests. The CI pipeline will include thresholds to ensure that the code coverage remains above a defined minimum percentage before any code can be merged.\n\n4. **Static Code Analysis**: Include static code analysis tools like SonarQube to catch potential issues, maintain code quality, and enforce coding standards.\n\n5. **Automated Reporting**: Configure the CI pipeline to automatically generate and distribute test coverage and code quality reports to the development team after each build.\n\n6. **Automated Notifications**: Set up the CI system to automatically notify the development team via email or chat when tests fail or when code coverage drops below the accepted threshold.\n\n7. **Periodic Review and Updates**: Schedule periodic reviews of the testing framework and update it to incorporate new tests, improve existing tests, and adapt to changes in the codebase.\n\nBy implementing these automated testing processes within our CI pipeline, we can ensure high test coverage and consistent code quality while reducing the dependency on manual checks.","GenTime":"2024-07-29 01:38:56"}
{"File Name":"push-sdk-ios\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\n[this article.][1]\\n","tokens":16,"id":4196,"Prediction":"## Decision\nWe have decided to implement an Architectural Decision Record (ADR) system for this project. ADTs will be documented in a structured manner using Markdown files. Each ADR will follow a predefined template, which includes sections such as Title, Status (e.g., Proposed, Accepted, Deprecated), Context, Decision, Consequences, and any other relevant details.\n\nThese records will be stored in a dedicated directory within the project's version control system (e.g., `docs\/adr\/`), ensuring they are versioned alongside the codebase. This approach will facilitate transparency, traceability, and ease of reference for all team members and stakeholders.","GenTime":"2024-07-29 01:39:43"}
{"File Name":"verify-hub\/0001-record-architechture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4206,"Prediction":"## Decision\nWe will implement and maintain an Architectural Decision Record (ADR) system for this project. The ADRs will document the significant architectural decisions made during the course of the project's development. This will include the context, decision, consequences, and rationale behind each decision. We will use a standardized template for all ADRs to ensure consistency and readability.\n\nThe ADRs will be stored in a version-controlled repository (e.g., Git) to ensure transparency and traceability. Each decision will be uniquely identifiable and linked to specific project requirements or issues. This practice will help the team understand the reasoning behind past decisions, facilitate knowledge transfer, and guide future architectural choices.\n\nWe intend to review and update the ADRs periodically to reflect any changes or new insights. All team members will be encouraged to contribute to the ADRs, ensuring that the documentation remains up-to-date and relevant.","GenTime":"2024-07-29 01:40:18"}
{"File Name":"react-library-template\/0006-use-jest.md","Context":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n\n## Decision\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":4223,"Prediction":"## Decision\n\nWe will use Jest as our test framework for the application due to its excellent support for both React and TypeScript. Jest is highly recommended for React applications, and it offers a comprehensive set of features, including robust mocking capabilities, easy setup, and strong community support, which will enable us to efficiently write and manage our tests. Additionally, Jest's integration with TypeScript ensures type safety and a smoother development experience.","GenTime":"2024-07-29 01:41:13"}
{"File Name":"react-library-template\/0005-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n## Decision\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":4224,"Prediction":"## Decision\nWe will standardize on using ESLint as our linting tool for the project. This will allow us to enforce code consistency and automatically catch potential errors throughout our codebase.\n\n### Justification\n1. **Consistency**: ESLint will help ensure that all developers adhere to the same coding standards, which will lead to a more uniform and maintainable codebase.\n2. **Error Detection**: ESLint can automatically identify many common coding errors and potential bugs, improving the overall quality of the software.\n3. **TypeScript Support**: ESLint has robust support for TypeScript through various plugins, allowing us to lint both JavaScript and TypeScript files consistently.\n4. **Community and Documentation**: Being the standard for modern JavaScript, ESLint has extensive documentation and a large community, making it easier to find help and resources.\n\n### Implementation Details\n1. **Configuration**: We will create a shared ESLint configuration file for the project, specifying the rules and plugins that align with our coding standards and requirements.\n2. **Integration**: ESLint will be integrated into our CI\/CD pipeline to ensure that linting is performed on all code before it is merged into the main branch.\n3. **IDE Support**: Developers will be encouraged to configure their Integrated Development Environments (IDEs) to use ESLint for real-time code analysis and error highlighting.\n4. **Training**: Provide documentation and training for all developers on using ESLint, understanding the rules and configurations, and troubleshooting common linting issues.\n\nBy standardizing on ESLint, we aim to achieve a higher level of code quality and maintainability, while facilitating a more consistent and collaborative development process.","GenTime":"2024-07-29 01:41:16"}
{"File Name":"react-library-template\/0007-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n\n## Decision\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":4225,"Prediction":"## Decision\nWe will integrate Dependabot into our GitHub repository to automate the management of dependency updates. Dependabot will be configured to check for new versions of our project dependencies on a regular basis and will automatically open pull requests (PRs) when new versions are detected. Each PR will include information about the new version and any relevant release notes.\n\n**Rationale:**\n- **Automation:** Dependabot reduces the manual effort required to track and update dependencies, freeing up valuable developer time.\n- **Security:** Regular updates help ensure that we are promptly incorporating security patches and improvements.\n- **Consistency:** Dependabot PRs provide a structured and consistent approach to dependency management.\n- **Insight:** Automatically generated PRs will give the team visibility into the latest dependency changes and allow informed decisions before merging.","GenTime":"2024-07-29 01:41:18"}
{"File Name":"react-library-template\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n## Decision\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":4226,"Prediction":"## Decision\n\nWe will adopt Prettier as the standard code formatter for all our projects. This decision is based on Prettier's widespread adoption, support for a wide range of languages in the JavaScript ecosystem, and its ability to enforce a consistent code style across our codebase. By integrating Prettier into our development workflow, we aim to reduce code review times, minimize debates over code style, and ensure that our code remains readable and maintainable for all team members.","GenTime":"2024-07-29 01:41:20"}
{"File Name":"react-library-template\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4229,"Prediction":"## Decision\n\nTo effectively record the architectural decisions made on this project, we will implement the use of Architectural Decision Records (ADRs). ADRs will be used to document each significant architectural decision along with its context and consequences. The following structure will be adopted for each ADR:\n\n1. **Title**: A brief descriptive label for the decision.\n2. **Date**: The date of the decision.\n3. **Context**: The issues being addressed, including any relevant background information and the factors that are driving the need for the decision.\n4. **Decision**: A detailed description of the decision made.\n5. **Status**: The status of the decision (e.g., proposed, accepted, deprecated).\n6. **Consequences**: The outcomes and implications of the decision, both positive and negative.\n7. **Alternatives Considered**: Other options that were considered and the reasons they were not chosen.\n8. **References**: Any additional documents or links that provide further context or information.\n\nThis structure will ensure that all key aspects of each decision are clearly documented and easily accessible for future reference. By maintaining ADRs, we aim to enhance traceability, communication, and understanding of the architectural decisions across the project team.","GenTime":"2024-07-29 01:41:29"}
{"File Name":"Horace\/0012-use-network-storage-for-large-datafiles.md","Context":"## Context\\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and \"expected\" results for unit and system testing.\\nThese data files are too large to store in GitHub along side the test code, but will not change frequently.\\nSimilar data files are also required for Euphonic testing.\\n\n## Decision\n","Decision":"The data will be stored in STFC hosted SAN (storage area network).\\nTests will read the data from this network storage location, either by copying the files locally or reading the remote file.\\n","tokens":78,"id":4234,"Prediction":"## Decision\n\nWe will store the large `sqw` and `nxspe` data files required for Horace and Herbert testing, as well as for Euphonic testing, in a dedicated cloud storage solution rather than in the GitHub repository. This dedicated cloud storage will provide ample space and can be easily accessed by our testing frameworks.\n\nTo facilitate seamless access and minimize performance overhead:\n\n1. **Cloud Storage Provider**: We will use a reliable cloud storage service such as AWS S3, Google Cloud Storage, or Azure Blob Storage.\n\n2. **Access Configuration**:\n   - Create a private, secure storage bucket or container specifically for these data files.\n   - Implement access controls and permissions to ensure that only authorized systems and users can access the data.\n   - Use API keys or OAuth tokens for automated scripts and CI\/CD pipelines to fetch the necessary data files during test setup.\n\n3. **Data Versioning**:\n   - When updating data files, use versioning features provided by the cloud service to keep track of changes and allow for rollback if necessary.\n   - Tag data files with metadata for easier identification and reference in test scripts.\n\n4. **Local Caching**:\n   - Implement local caching mechanisms in the test environments to reduce redundant data downloads and improve test execution time.\n   - Store cached copies in a defined directory and validate the integrity of the cached files against checksums or hashes.\n\n5. **Documentation and Automation**:\n   - Document the process for uploading and accessing data files in the project's contribution guidelines.\n   - Automate the data-fetching process within the CI\/CD pipeline to ensure consistency and reproducibility in testing environments.\n\nBy leveraging a cloud storage solution, we can manage large datasets more effectively, ensure scalability, maintain security, and streamline our testing processes.","GenTime":"2024-07-29 01:41:51"}
{"File Name":"Horace\/0017-separate-absolute-and-relative-indexing-APIs-in-pixel-array.md","Context":"## Context\\nThe pixel array within an SQW object can be too large to fit into memory.\\nTo avoid running out of memory, the object holding the pixel array can be\\nfile-backed.\\nThis means that only a \"page\" of the pixel array is loaded into memory at any\\none time.\\nTherefore two possible ways to index into the pixel array exist:\\n1. **Absolute index**:\\n_The position of the pixel in the full, file-backed, pixel array_.\\n2. **Relative index**:\\n_The position of the pixel in the currently loaded page of pixel data._\\n\n## Decision\n","Decision":"There will be two separate APIs for accessing data,\\nthese APIs distinguish between the two types of indexing.\\n1. **Perform absolute indexing  using `get_` methods:**\\nObtaining a subset of pixels or pixel data by absolute index will be\\npossible using a `get_` method.\\nFor example, the following will retrieve pixels 100-200 by absolute index:\\n```matlab\\npixels.get_pixels(100:200)\\n```\\nSimilarly, to retrieve a range of data from particular pixel array fields:\\n```matlab\\npixels.get_data({'signal', 'variance'}, 100:200)\\n```\\nAt time of writing, there are no plans to implement similar `set_`\\nmethods with absolute indexing.\\nIf there becomes a requirement for setters with absolute indexing,\\nthese should follow a similar syntax.\\nFor example, the following should set signal and variance from absolute\\nindex 100 to 200, to zero:\\n```matlab\\nset_data({'signal', 'variance'}, 100:200, zeros(2, 101));\\n```\\n2. **Perform relative indexing using attribute:**\\nObtaining pixel data using an attribute will return just the data for the\\ncurrently cached page.\\nHence, indexing into these attributes will be relative.\\nFor example, the following will retrieve the signal values of pixels 10-20\\nin the currently cached page:\\n```matlab\\npixels.signal(10:20)\\n```\\nSetting of pixel data will be possible using attributes and relative\\nindexing.\\nFor example the following will set pixels 1 to 20 on the current page to\\nzero:\\n```matlab\\npixels.signal(1:20) = 0\\n```\\n","tokens":128,"id":4236,"Prediction":"## Decision\n\nWe will use the **Absolute index** approach to index into the pixel array.\n\n### Rationale:\n\n1. **Consistency**: Using an absolute index uniquely identifies each pixel in the file-backed array, eliminating ambiguity that might arise from varying page sizes and boundaries.\n  \n2. **Simplicity**: It removes the need to frequently translate relative indices to absolute ones, which simplifies the codebase and reduces the chances of errors related to index translations.\n   \n3. **Maintainability**: Absolute indexing makes it easier to manage and debug the application since each pixel's position is globally accessible and consistent, irrespective of the page currently loaded in memory.\n   \n4. **Flexibility**: This approach allows for more straightforward implementation of features like loading different or multiple pages, parallel processing on different chunks, etc., without requiring conversions between relative and absolute indices. \n\n5. **Performance**: Though there might be a slight overhead in determining page boundaries when loading, the cost is outweighed by the simplification in indexing logic and reduced potential for errors.\n\nBy adopting the absolute index approach, we aim to balance performance and ease of implementation while ensuring the system remains scalable and maintainable.","GenTime":"2024-07-29 01:41:58"}
{"File Name":"Horace\/0007-use-herbert-as-library-dependency.md","Context":"## Context\\nThe Horace and Herbert projects are tightly coupled. Herbert build artifacts are required for integration and MATLAB testing of Horace source.\\nChanges made to Herbert may change or break dependent MATLAB or C++ code in Horace.\\n\n## Decision\n","Decision":"To make the depdencency explicit Herbert will be regarded as a library.\\nAs a consequence:\\n- Herbert builds will NOT trigger Horace builds\\n- Horace builds (both `PR` and `master`) will always use the latest `master` build of Herbert\\n- Build artifacts will will copied from the latest successful `master-<target-os>-<target-matlab>` build on the Herbert CI server.\\n","tokens":48,"id":4245,"Prediction":"## Decision\n\nTo address the tight coupling between the Horace and Herbert projects and ensure stability and reliability in our development process, we will implement the following strategies:\n\n1. **Version Pinning:** We will use Semantic Versioning for Herbert and pin Horace dependencies to specific Herbert versions. This ensures that any changes in Herbert do not automatically impact Horace without explicit action.\n\n2. **Continuous Integration Testing:** Integrate automated CI pipelines that include end-to-end tests with staged Herbert builds. Any updates to Herbert will trigger a full suite of integration and MATLAB tests on Horace to ensure compatibility.\n\n3. **Module Boundaries and Interfaces:** Clearly define and document the interfaces between Horace and Herbert. This includes establishing public and private APIs and adhering to strict interface contracts to minimize unexpected impacts from changes.\n\n4. **Release Management:** Introduce a synchronized release management process. Major releases of Herbert will align with release cycles of Horace, ensuring coordinated updates and extensive pre-release testing.\n\n5. **Change Management:** Implement a change management workflow where any substantial change in Herbert (that could potentially affect Horace) must be reviewed and approved by both teams. This includes shared code reviews, compatibility assessments, and joint planning sessions.\n\n6. **Dependency Matrix:** Maintain a matrix of dependencies and potentially impacted components. This will help in assessing the impact of changes more systematically and efficiently.\n\n7. **Automated Rollback Mechanism:** Implement automated rollback procedures if integration tests fail after a Herbert update, to maintain Horace's stability.\n\nThese measures are aimed at minimizing disruption, ensuring robust integration, and maintaining backward compatibility, leading to more stable and predictable development cycles for both projects.","GenTime":"2024-07-29 01:42:28"}
{"File Name":"laundromat\/0003-choice-of-entities.md","Context":"## Context\\nThe choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:\\n* It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)\\n* It is a so-called \u201cspecial categories\u201d of information (e.g. medical information)\\n* It is present in the data in non-trivial quantities\\n\n## Decision\n","Decision":"We have chosen the following NER entities:\\n* ORG (Organisation)\\n* LOC (Location)\\n* PER (Person)\\n* FNR (Personal number)\\n* MONEY\\n* DATE_TIME (Dates, time of day, name of day, and name of month)\\n* MEDICAL_CONDITIONS\\nEntities that will be left purely to RegEx are:\\n* NAV_YTELSE and NAV_OFFICE\\n* AGE\\n* TLF (Telephone number)\\n* BACC (Bank account number)\\nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.\\n","tokens":134,"id":4261,"Prediction":"## Decision\n\nWe have decided to include entities for anonymization that fulfill at least two of the three criteria outlined below:\n\n1. The entity is either directly identifying or a close proxy (e.g., names, phone numbers, Social Security numbers).\n2. The entity falls under \"special categories\" of information (e.g., medical information, racial or ethnic origin, biometric data).\n3. The entity is present in the data in non-trivial quantities, suggesting a significant potential for identifying individuals if left unanonymized.\n\nBy focusing on these criteria, we aim to balance the need for effective anonymization with the constraints on time and resources. This approach will help ensure that the most sensitive and prevalent information is properly anonymized, thereby enhancing the overall performance and security of our model.","GenTime":"2024-07-29 01:43:34"}
{"File Name":"backdrop\/adr-002-persistent-storage.md","Context":"# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n\n## Decision\n","Decision":"We will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","tokens":403,"id":4266,"Prediction":"## Decision\n\nWe have evaluated our requirements and have decided to use MongoDB for the persistent data store in the Performance Platform.\n\nRationale:\n- **Experience**: GDS already has experience running MongoDB in production, which mitigates the risks associated with adopting a new technology.\n- **Data Structure Flexibility**: MongoDB's ability to store unstructured data in the form of JSON documents aligns with our need to handle a variety of data sources (e.g., spreadsheets, logs, analytics).\n- **Application of Structure**: MongoDB allows us to apply structure to the data either through post-processing or at query time, enhancing our flexibility in data handling.\n- **Scalability Considerations**: The current expected volume of data is small, so MongoDB's scalability capabilities are more than sufficient for our needs at this stage.\n- **ACID and CAP Theorem**: As an aggregator and not the canonical source of data, the need for strict ACID properties and in-depth CAP theorem considerations is not crucial for our use case.\n- **Ease of Operation**: MongoDB's replica-set configuration ensures data redundancy and availability, which suits our operational requirements.\n\nWe will proceed with setting up MongoDB with an appropriate replica-set configuration to ensure reliability and availability. This decision will be revisited as the project evolves and our understanding of user needs and the data landscape improves.","GenTime":"2024-07-29 01:43:49"}
{"File Name":"sexual-health-service-finder\/0002-use-express-web-framework.md","Context":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n\n## Decision\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":41,"id":4271,"Prediction":"## Decision\nWe have decided to adopt the **Django** web framework for our project.\n\nDjango offers a high-level of abstraction for handling common web development tasks and allows developers to focus on business logic rather than reinventing the wheel. It provides robust features like ORM for database interactions, templates for rendering HTML, built-in authentication, and an admin interface out of the box. This aligns with our goal to minimize effort on non-domain specific problems while delivering a scalable and maintainable application. Additionally, Django's extensive documentation and large community support ensure that our team can efficiently troubleshoot and extend the framework as needed.","GenTime":"2024-07-29 01:44:10"}
{"File Name":"sexual-health-service-finder\/0005-calculate-distance-between-origin-and-result-items-within-the-application.md","Context":"## Context\\nThe move to Azure search has introduced the need to calculate the distance\\nbetween the search point and each result item. Previously, when using\\nElasticsearch, the distance was returned within the query response. Azure\\nsearch does not have this capability, it is currently a\\n[feature request](https:\/\/feedback.azure.com\/forums\/263029-azure-search\/suggestions\/17760211-support-geo-distance-in-select-result).\\n\n## Decision\n","Decision":"The decision is to calculate the distance between the search point and each\\nresult item within the consuming application i.e. the web app. The calculation\\nfor\\n[great-circle distance](https:\/\/en.wikipedia.org\/wiki\/Great-circle_distance)\\nis well known and available in numerous languages.\\n","tokens":90,"id":4274,"Prediction":"## Decision\n\nTo address the limitation in Azure Search where the distance calculation between the search point and each result item is not natively supported, we will implement a two-step approach:\n\n1. **Post-Processing Distance Calculation:**\n   - After retrieving the search results from Azure Search, we will perform an additional processing step to calculate the distances. This involves using the Haversine formula or another suitable geographical distance algorithm to compute the distances between the search point and each result item.\n   - This distance calculation logic will be implemented within our application layer, ensuring that it runs efficiently and minimizes any additional latency introduced.\n\n2. **Caching Mechanism:**\n   - To optimize performance, especially for frequently queried search points and result sets, we will introduce a caching mechanism. Calculated distances for specific search queries and result sets will be cached, reducing the need for repeated distance calculations.\n   - An appropriate TTL (Time-To-Live) strategy will be utilized to ensure the cache remains up-to-date without growing indefinitely.\n\nGiven the constraints and the current unavailability of native distance calculations in Azure Search, this solution provides a balance between performance and functionality while maintaining a seamless experience for our users.","GenTime":"2024-07-29 01:44:17"}
{"File Name":"sexual-health-service-finder\/0006-use-in-house-postcodes-search-rather-than-postcodes-io.md","Context":"## Context\\nThe application has been using https:\/\/postcodes.io\/. Postcodes.io is available\\nfor free and does not require any relationship between consumer and supplier.\\nOne of the drawbacks to this is that although support is available when using\\nthe free tier, there is no provision for any specific support relationship.\\nMore formal relationships are available but this is through the paid product\\nhttps:\/\/ideal-postcodes.co.uk\/.  Given the number of requests and the fact the\\nfree tier was supplying the information required, it had previously been\\ndecided there was no need to migrate to a paid product. However, the strategic\\ndirection has been set such that high value APIs, usable across many products\\nshould be supplied in-house. To that end, there is now an in-house postcode\\nsearch available.\\n\n## Decision\n","Decision":"The decision is to replace the use of postcodes.io with the in-house postcode\\nlookup, as per the strategic direction.\\n","tokens":165,"id":4276,"Prediction":"## Decision\n\nThe application will migrate from using the external Postcodes.io API to the in-house postcode search service. This decision aligns with the strategic direction to prioritize internally managed APIs for high-value, cross-product services. \n\n### Rationale:\n\n1. **Strategic Alignment**: Moving to an in-house postcode search service adheres to the organizational strategy of supplying high-value APIs internally, which ensures better control and integration across various products.\n\n2. **Support and Reliability**: While Postcodes.io provides a free service, the lack of a formal support relationship poses a risk. An in-house service can offer dedicated support, improved reliability, and faster issue resolution.\n\n3. **Customization and Control**: An in-house solution provides the flexibility to tailor the service to meet specific business needs, and allows for quicker adaptations to changes or new requirements without dependency on an external provider.\n\n4. **Cost Management**: Although the current free tier of Postcodes.io incurs no monetary cost, any potential changes in their pricing model or service terms could introduce unforeseen expenses. The in-house service, while it requires investment in development and maintenance, offers predictable costs and investment in our infrastructure.\n\n### Operational Considerations:\n\n1. **Migration Plan**: A detailed migration plan will be created to ensure a smooth transition from Postcodes.io to the in-house service, minimizing any potential disruption.\n\n2. **Performance Testing**: Before full switchover, the in-house service will undergo rigorous performance tests to ensure it meets or exceeds the performance metrics of the existing external API.\n\n3. **Documentation and Training**: Comprehensive documentation will be provided, and training sessions will be conducted for relevant teams to facilitate the adoption of the new in-house service.\n\n4. **Monitoring and Maintenance**: Implement robust monitoring and maintenance processes to ensure the ongoing reliability and performance of the in-house postcode search service.\n\nBy making this migration, the application will not only align with strategic goals but also enhance the overall robustness and adaptability of the system.","GenTime":"2024-07-29 01:44:24"}
{"File Name":"early-careers-framework\/0003-separate-ecf-and-npq-calculation-engines-and-hashes-as-interfaces.md","Context":"## Context\\nThere are currently two different training schemes in scope for the track-and-pay project. There are rumours of a possible third in the future.\\nThere are some similarities and many differences in the inputs, outputs and maths for payments for these training schemes. For example:\\n1. Both have fixed payments at 40%\\n2. Both allow pulling some of the fixed payment into earlier \"setup\" payments (for cashflow).\\n3. The output payments are very different in detail.\\n4. They have different banding systems.\\n### People\\nTim Abell and Pavel Lisovin (track and pay developers) discussed the issue amongst ourselves and came to this as a decision for the time being.\\n\n## Decision\n","Decision":"1. Build two payment engines that do not share code.\\n2. Have similar input\/output interfaces (ruby hash structures) that can later be easily converted to JSON.\\n3. Use similar patterns for both engines of Gherkin BDD driven unit tests plus normal rspec unit tests.\\n","tokens":146,"id":4277,"Prediction":"## Decision\n\nWe will create a modular and extensible payment calculation system that can accommodate the current two training schemes and be easily adapted for the addition of future schemes. This system will be composed of a core engine for common functionalities and separate modules for scheme-specific logic.\n\n### Key Points:\n1. **Core Engine**:\n    - **Fixed Payment Calculation**: Process the 40% fixed payments.\n    - **Setup Payments**: Manage the reallocation of fixed payments to setup payments.\n    - **Common Interfaces**: Define interfaces for standardized input and output handling.\n\n2. **Scheme-Specific Modules**:\n    - Each training scheme will have its own module to handle specific output payment calculations and banding systems.\n    - These modules will implement the common interfaces defined in the core engine, ensuring consistency and ease of integration.\n\n3. **Future-Proofing**:\n    - Design the system with extensibility in mind, ensuring that new schemes can be added by implementing new modules without needing to modify the core engine.\n\n4. **Documentation and Testing**:\n    - Properly document the interfaces and the structure of the core engine and modules.\n    - Create comprehensive tests for both the core engine and individual modules to ensure that they work correctly when combined.\n\nBy employing this modular approach, we aim to maintain flexibility and adaptability in our system, allowing us to efficiently manage current and future training schemes without substantial redevelopment effort.","GenTime":"2024-07-29 01:44:28"}
{"File Name":"open-apparel-registry\/adr-002-decide-how-to-display-more-facilities.md","Context":"## Context\\nThe Open Apparel Registry currently includes more than 18,000 facilities. For\\nperformance reasons, we have paginated the facilities data API endpoint data so\\nthat it will [return a maximum of 500 results][pagination-pr] for any single\\nrequest. In turn this means that the frontend client will only ever display a\\nmaximum of 500 facilities at a time, rendered as clustered Leaflet markers via\\nReact-Leaflet. Facilities API requests are currently filtered using Django\\nquerysets whose inputs are querystring parameters included in the API requests.\\nTo enable users to view all of the OAR's facilities on the map simultaneously,\\nwe'll need to update how the API returns facilities for display and how the\\nclient renders them on the map. At present this means updating the application\\nso that it can display 18,000+ facilities simultaneously. Following upcoming MSI\\nintegration work, we anticipate that the number of OAR facilities will increase\\nto around 100,000 -- which the application should be able to map. In addition,\\nwe also want users to be able to filter these vector tiles by query parameters\\nlike contributor, facility name, and country, along with the map bounding box.\\nTo accomplish this we have decided to use vector tiles generated, ultimately,\\nby PostGIS's [`ST_AsMVT`][st-asmvt] function, rendering them in the frontend\\nwith [Leaflet Vector Grid][leaflet-vector-grid] (possibly via\\n[react-leaflet-vector-grid][react-leaflet-vector-grid]). We've decided to have\\nthe vector tiles cluster facilities by zoom level, which would limit the number\\nof actual points the frontend needs to display at any given time.\\nThis ADR documents a subsequent decision between setting up a dedicated\\n`ST_AsMVT`-based vector tile server, like [Martin][martin] or adding a new\\nvector tile endpoint to the existing Django web application which would make\\nthe `ST_AsMVT` query.\\n\n## Decision\n","Decision":"We have decided to add a vector tile endpoint to the existing Django app.\\nWhile Martin, in particular, seemed like a compelling solution, we had enough\\nopen questions about it to discourage us from taking on the complexity of\\nusing it here.\\nOur main apprehension about adding a tile endpoint to the existing web app is\\nthat it'll mingle tile requests with other requests in a way that could cause\\nperformance problems. However, given the size of the OAR's traffic and the\\npossibility of addressing traffic increases by scaling the number of app\\ninstances, this seemed like an acceptable tradeoff.\\n","tokens":412,"id":4281,"Prediction":"## Decision\n\nWe have decided to integrate vector tile generation directly into the existing Django web application rather than setting up a dedicated `ST_AsMVT`-based vector tile server like Martin. This route provides several benefits:\n\n1. **Unified Architecture:** By integrating vector tile generation into our existing Django infrastructure, we maintain a single application stack, reducing complexity and easing maintenance.\n\n2. **Performance Considerations:** Using Django's native capabilities alongside PostGIS for vector tile generation ensures optimal performance, especially given the anticipated growth in facility data. The `ST_AsMVT` function in PostGIS is efficient at generating vector tiles, and this can be leveraged without additional overhead.\n\n3. **Simplified Deployment:** A unified application simplifies deployment processes. Our current CI\/CD pipelines can be updated to handle the vector tile capabilities without needing to manage and synchronize deployments between two separate systems.\n\n4. **Consistency in Filtering:** By handling vector tile generation and API request filtering within Django, we ensure consistent logic and filtering criteria across all endpoints. This simplifies debugging and development efforts.\n\n5. **Scalability:** Django with PostGIS is capable of scaling to handle 100,000 facilities, especially when vector tiles are clustered by zoom level. We can optimize and scale our existing infrastructure to meet performance requirements.\n\n6. **Extensibility:** Future enhancements, such as integrating additional filters or improving query performance, can be more easily managed within a single application framework.\n\nTo implement this decision, the following actions will be taken:\n\n- Ensure that PostGIS is correctly set up within the database for efficient vector tile generation using `ST_AsMVT`.\n- Develop new endpoints in Django that utilize `ST_AsMVT` to generate and return vector tiles based on requested filters.\n- Update the frontend to incorporate `Leaflet Vector Grid` for rendering the vector tiles, ensuring clustering by zoom level to enhance performance.\n- Implement rigorous testing to ensure that performance and responsiveness meet the current and future user needs.\n\nBy following this approach, we aim to provide a scalable, maintainable, and efficient solution for rendering a large number of facilities on the map, while leveraging our existing platform as much as possible.","GenTime":"2024-07-29 01:44:41"}
{"File Name":"bob\/0004-introduce-temporal-db.md","Context":"## Context\\nCurrently we are using a traditional RDBMS, PostgreSQL as the storage of all the CI\/CD state.\\nThe DB is the one and only state of the whole CI\/CD cluster, storing all of the pipeline, resource, artifact definitions, runs, logs etc. For all of these, tracking historical changes is of utmost importance.\\nUsing a traditional CRUD workflow with PostgeSQL raises the following issues:\\n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster down\\n- Central locking is a severe impediment to scale\\n- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needs\\n- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place system\\n- The tabular structure comes in our way with rigidity and is not simple to introduce schema changes\\n- Need of elaborate migration strategies which is further complicated with distributed state\\n\n## Decision\n","Decision":"Based on the above facts the following is decided:\\n- Use [XTDB](https:\/\/xtdb.com) as the temporal, document store for the following reasons:\\n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production use\\n- Has a quite unbundled design and uses a variety of storage backends and is transactor free\\n- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time\\n- Has an HTTP interface for non JVM clients\\n- Use it with JDBC\/PostgreSQL backend which is quite readily available and managed in all popular cloud providers\\n- Remove the CRUD way of doing things, expose the DB too via API for more powerful, direct analytical querying\\n","tokens":209,"id":4285,"Prediction":"## Decision\n\nWe will transition from our current use of PostgreSQL as the sole storage solution for our CI\/CD state to a more scalable, flexible, and efficient architecture by implementing an event-sourcing model with a polyglot persistence approach. The key aspects of this decision are as follows:\n\n1. **Event Sourcing:**\n   - **Adopt Event Sourcing:** Shift to an event sourcing model where every state-changing action is stored as an immutable event. This enables easy tracking of historical changes, simplification of audit processes, and rollbacks.\n   - **Central Event Store:** Utilize a dedicated event store (e.g., Apache Kafka or EventStoreDB) to maintain the sequence of events. This will decouple storage from querying and enhance scalability.\n\n2. **Polyglot Persistence:**\n   - **Separate Analytics Database:** Use a separate database optimized for analytics (e.g., Apache Druid or Amazon Redshift) to handle heavy read queries without impacting the main operational database. This separation helps in running complex analytics queries without affecting the performance of the CI\/CD cluster.\n   - **Document Store for Flexibility:** Implement a document-oriented database (e.g., MongoDB or Couchbase) to store pipeline definitions, resource providers, artifact store definitions, and any other metadata. This enables flexible schema changes and eliminates the need for complicated migration strategies.\n\n3. **Extended Logging and Traceability:**\n   - **Time-Ordered Events Log:** Ensure that the event store keeps a time-ordered log of changes for easy debugging and extended historical logging.\n\n4. **Scalability and Performance:**\n   - **Distributed Architecture:** Design the architecture to support distributed storage and querying, thereby eliminating central locking issues and scaling impediments. \n\n5. **Resilience and Fault Tolerance:**\n   - **Backup and Recovery:** Implement comprehensive backup and disaster recovery strategies for both event stores and analytical databases to ensure data integrity and resilience.\n\n6. **Incremental Migration:**\n   - **Phased Transition:** Develop a phased migration plan that includes dual-writing to the new event store and legacy PostgreSQL during the transition period to ensure data consistency and minimal downtime.\n\nBy adopting this approach, the CI\/CD system will gain enhanced scalability, better analytics capabilities, simplified audit trails, easier debugging, and a more flexible schema management process.","GenTime":"2024-07-29 01:45:02"}
{"File Name":"molgenis-js-auth\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":4291,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) process to systematically document important architectural decisions. Each ADR will include a title, context, decision, and consequences. The ADRs will be stored in a dedicated directory within the project repository and managed using version control to ensure traceability and accountability. This will help the team understand the rationale behind key decisions and maintain consistency throughout the project lifecycle.","GenTime":"2024-07-29 01:45:24"}
{"File Name":"molgenis-js-auth\/0002-manage-user-permissions-in-fusion-auth.md","Context":"## Context\\nIn Armadillo:\\n* A cohort is an OpenID application\\n* The researcher needs a `ROLE_<STUDY>_RESEARCHER` on the application to read data\\nin shared folder `<study>`\\nRequirements on who manages what:\\n* User and permission management is done by representatives of the consortium and cohort\\n* The identity of the user is federated to their local institution\\n* Representatives of the cohort administer the study researchers\u2019 admission to their cohort\\n* Representatives of the consortium can see which users have which roles on which cohort\\n\n## Decision\n","Decision":"Implement this in Fusion Auth, plus a (modest) administration UI.\\n* The users register themselves.\\n* A cohort representative explicitly creates a registration for the researcher in the application,\\nwith one or more roles.\\n### Administration\\nApplication-specific permissions are not available in Fusion Auth.\\nIn other products they tend to be rather complicated to configure.\\nCreate an edge service that enforces the permissions and uses\\nan api key to administer them through the Fusion Auth API.\\n","tokens":116,"id":4292,"Prediction":"## Decision\n\n### Centralized Role Management System\n\nA centralized role management system (RMS) will be implemented to handle user roles and permissions efficiently across different studies within the Armadillo system. This system will have the following functionalities and attributes:\n\n1. **User and Permission Management Interface**: A web-based interface will be developed for representatives of the consortium and cohort to manage user roles and permissions. This interface will allow:\n   - Representatives of the cohort to administer the study researchers' admission to their cohort.\n   - Representatives of the consortium to view which users have which roles on which cohort.\n\n2. **Federated Identity Handling**: User identities will be federated to their local institution using OpenID Connect. This ensures that authentication and some level of identity verification are handled externally by trusted institutions, reducing the burden on the Armadillo system.\n\n3. **Role Assignment Mechanism**: \n   - When a researcher needs access to a cohort's data, they will request the appropriate `ROLE_<STUDY>_RESEARCHER`.\n   - The request will be handled by the cohort\u2019s representatives who can assign or revoke this role through the RMS.\n\n4. **Audit Capability**: The system will log all role and permission changes, including who made the change, when it was made, and the old and new roles. This will ensure transparency and accountability.\n\n5. **Security Measures**: The RMS will include robust security features to protect against unauthorized access and data breaches. This includes:\n   - Multi-Factor Authentication (MFA) for users with administrative rights.\n   - Regular security audits and penetration testing.\n   \n6. **Scalability and Extensibility**: The system will be designed to handle a growing number of studies, researchers, and cohorts without significant performance degradation. Also, it will be extensible to accommodate new requirements or changes in the organizational structure or data sharing policies.\n\nBy consolidating role management in a centralized, secure, and user-friendly interface while leveraging federated identity from local institutions, Armadillo can ensure efficient, transparent, and secure handling of user access to study data.","GenTime":"2024-07-29 01:45:29"}
{"File Name":"winery\/0031-reuse-refinement-code-for-pattern-detection.md","Context":"## Context and Problem Statement\\nTo create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM.\\nTo create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM.\\nThe replacement procedure is identical for both processes, only the structures used for the replacement differ.\\nTherefore, the implementation of the pattern refinement process should be reused to implement the pattern detection process.\\n## Decision Drivers\\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\n\n## Decision\n","Decision":"* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\nChosen option: \"Swap the Detector of all PRMs with their Refinement Structures\", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also decreasing readability.\\n### Positive Consequences <!-- optional -->\\n* Complete pattern refinement implementation can be reused\\n### Negative consequences <!-- optional -->\\n* Readability and understandability decreases\\n","tokens":115,"id":4299,"Prediction":"## Decision\nWe will implement a shared infrastructure for the pattern replacement procedure to be used by both the pattern refinement process and the pattern detection process. This shared infrastructure will encapsulate the replacement logic while allowing for the flexibility to substitute different structures (i.e., the Refinement Structure for pattern refinement and the Detector for pattern detection).\n\n### Implementation Plan\n1. **Shared Replacement Module**: Create a central module or function responsible for the pattern replacement logic. This module will accept the graph and the structure to be used for replacement as parameters.\n2. **Abstraction for Structures**: Define an interface or abstract class that both the Refinement Structure and the Detector will implement. This ensures that any structure passed into the shared replacement module conforms to the required operations.\n3. **Refactor Existing Code**: Refactor the existing pattern refinement process to make use of the shared replacement module, passing the Refinement Structure as the structure to be used.\n4. **Implement Pattern Detection**: Use the shared replacement module for the pattern detection process, passing the Detector as the structure to be used.\n5. **Testing and Validation**: Develop comprehensive tests to ensure that both processes work correctly with the shared replacement module and that their specific behaviors are preserved.\n\n### Benefits\n- **Code Reuse**: We avoid duplicating the pattern replacement logic, adhering to the DRY (Don't Repeat Yourself) principle.\n- **Maintainability**: Future updates to the replacement logic will only need to be made in one place, reducing the risk of inconsistencies.\n- **Testability**: The shared module can be rigorously tested in isolation, thereby improving the reliability of both processes.","GenTime":"2024-07-29 01:45:52"}
{"File Name":"winery\/0030-multiple-repositories.md","Context":"## Context and Problem Statement\\nTo help developers obtain TOSCA elements in a more practical way, an approach to supporting multiple repositories distributed across different servers is presented here.\\nThe result provides the ability to define a set of URLs and load them into the current winery-repository.\\n\n## Decision\n","Decision":"Option 2 was chosen to manage each repository individually. This makes it possible to use existing version control systems in the individual repositories.\\nThe repositories are structured as follows.\\n![Repository Structure](graphics\/repo-structure.png)\\n### Positive Consequences <!-- optional -->\\n* User can search for elements online and load a repository using the url\\n* It's enough to send a URL instead of a CSAR\\n* Manage each repository independently\\n* The origin and the version history of a repository can be checked\\n* The possibility to contribute directly to the development of a repository\\n### Negative consequences <!-- optional -->\\n* User is forced to define namespaces in Namespaces.json.\\n* Additional configuration file (repositories.json)\\n","tokens":57,"id":4305,"Prediction":"## Decision\n\nTo enhance the practicality of obtaining TOSCA elements, we will implement support for multiple repositories distributed across different servers within the Winery tool. This decision involves the following steps:\n\n1. **Definition of Repository URLs**: Introduce a configuration option allowing users to define a set of URLs pointing to the desired repositories.\n  \n2. **Loading and Merging Repositories**: Implement functionality that will load TOSCA elements from the specified URLs and merge them into the existing winery-repository. This will involve creating a federated view of TOSCA elements where elements from multiple sources can be accessed seamlessly.\n\n3. **Conflict Resolution Mechanism**: Develop a strategy for resolving conflicts when TOSCA elements from different repositories have the same identifier. This could involve prioritizing repositories, versioning strategies, or providing merge tools for the developers.\n\n4. **Caching and Synchronization**: To effectively manage the repositories' availability and performance, introduce caching mechanisms and synchronization policies. This will optimize access to the TOSCA elements and ensure that the repository state is consistent.\n\n5. **User Interface Enhancements**: Modify the Winery user interface to allow users to manage repository URLs, inspect the status of repositories, and handle potential conflicts in an intuitive manner.\n\n6. **Documentation and Examples**: Provide comprehensive documentation and examples to guide developers on how to set up and utilize multiple repositories within the Winery environment.\n\nBy implementing these steps, developers will have an enhanced ability to obtain and manage TOSCA elements from multiple decentralized sources, ultimately improving development workflows and resource availability within the TOSCA ecosystem.","GenTime":"2024-07-29 01:46:17"}
{"File Name":"winery\/0022-tosca-model-is-more-relaxed-than-the-xsd.md","Context":"## Context and Problem Statement\\nThere is a data model for a) serializing\/deserializing the XML contents, b) internal backend handling, c) working with algorithms, d) communicating with the REST service.\\nCurrently, this is the same model.\\nThe UI might generate non-valid XML files (in the sence of not passing the XSD validation).\\nFor instance, if a user creates a service template, that service template does not contain a topology template.\\nFurthermore, a topolgoy template needs to have at least one node template.\\n\n## Decision\n","Decision":"Chosen option: \"Keep one model and allow non-XSD validating models in `org.eclipse.winery.model.tosca`\", because\\n- XSD is meant for \"executable\" TOSCA definitions, not for intermediate modeling results\\n- currently too much effort to develop two models\\n","tokens":114,"id":4307,"Prediction":"## Decision\n\n### Separate Data Models for Different Use Cases\n\nTo address the challenges posed by using the same data model for multiple purposes (serialization\/deserialization, internal backend handling, algorithmic operations, and REST service communication), we decided to separate the data models into distinct, purpose-specific models.\n\n### Details\n\n1. **XML Data Model**\n   - **Purpose**: To handle serialization and deserialization of XML contents with strict adherence to XSD validation rules.\n   - **Structure**: This model will be designed to closely align with the XML schema definitions to ensure that any XML generated from it is always valid according to the XSD.\n\n2. **Internal Backend Data Model**\n   - **Purpose**: To manage the internal handling of data within the backend, including business logic specific to the application's needs.\n   - **Structure**: This model will be optimized for processing and operational efficiency, potentially including internal attributes that are not part of the XML representation.\n\n3. **Algorithm Data Model**\n   - **Purpose**: To facilitate operations and computations required by algorithms used within the application.\n   - **Structure**: This model will be designed to be flexible and efficient for algorithmic manipulation, even if it diverges from XSD constraints.\n\n4. **REST Service Communication Model**\n   - **Purpose**: To define the structure of data as it is communicated via REST APIs.\n   - **Structure**: This model will be decoupled from internal processing needs and XML constraints, focusing instead on ease of use and robustness in a RESTful context.\n\n### Rationale\n\n- **Decoupling Complexity**: By separating the models, we can ensure each one is tailored to its specific responsibilities, reducing the complexity of validation and processing.\n- **Maintainability**: Different teams or roles can work on specific models without causing unintended side effects in other areas.\n- **Validation and Consistency**: Ensures that XML validation issues don't affect internal processing and logic execution, maintaining consistency and robustness across the system.\n- **Flexibility in UI**: Allows the UI layer to be more flexible in generating data, mitigating the issue of non-valid XML files affecting the backend.\n  \n### Implementation Plan\n\n- **Step 1**: Define the structures and attributes for each of the four data models.\n- **Step 2**: Update serialization\/deserialization processes to use the XML Data Model.\n- **Step 3**: Refactor backend architecture to utilize the Internal Backend Data Model.\n- **Step 4**: Refactor algorithm-related code to adopt the Algorithm Data Model.\n- **Step 5**: Modify REST APIs to use the REST Service Communication Model.\n- **Step 6**: Create and implement mapping logic to translate between these models where necessary.\n- **Step 7**: Conduct extensive testing to ensure data integrity and consistency across all models and layers.\n\n### Consequences\n\n- **Positive Impacts**:\n  - Increased clarity and separation of concerns.\n  - Easier maintenance and quicker adaptation to changes in specific aspects of the system.\n  - More robust validation and error handling.\n\n- **Potential Trade-offs**:\n  - Initial overhead in refactoring existing code and processes.\n  - Possible performance implications due to the mapping between different models that need to be optimized.\n\nThis decision aims to create a more sustainable and robust architecture capable of supporting the application's evolving needs while maintaining data integrity and reducing validation errors.","GenTime":"2024-07-29 01:46:28"}
{"File Name":"winery\/0027-use-dasherization-for-filenames.md","Context":"## Context and Problem Statement\\nGraphics files have to take a consistent file name\\n## Decision Drivers <!-- optional -->\\n* Easy to process by Jekyll\\n* No WTFs at the creators\\n\n## Decision\n","Decision":"* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n","tokens":40,"id":4308,"Prediction":"## Decision\nAll graphic files will adhere to a consistent naming convention to facilitate easy processing by Jekyll and to ensure clarity for creators. The agreed format will be `<topic>_<description>_<unique-identifier>.<file-extension>`. This clear, structured approach will minimize confusion and ensure that the system can reliably locate and utilize the files.\n\nFor example:\n- A chart describing 2023 sales could be named: `sales_chart_2023_001.png`\n- A diagram illustrating a network setup could be named: `network_diagram_setup_2023_002.svg`\n\nThis naming convention will be communicated to all content creators to ensure uniformity and avoid any \"WTF\" moments during processing and usage.","GenTime":"2024-07-29 01:46:29"}
{"File Name":"news\/0006-a-state-requirement.md","Context":"## Context\\nJust trying to implement bookmarks and I have come across something interesting.\\nWe have a ui element that looks like this:\\n```html\\n<a href=\"javascript:application.bookmarks.add('gbj1jf')\" class=\"bookmark\">bookmark<\/a>\\n```\\nWhich means \"on click, please bookmark the news item with id 'gbj1jf'\".\\nThe bookmarking use case looks like this:\\n```js\\nbookmark = new Bookmark('id-a', 'http:\/\/abc', 'src-rnz');\\nawait application.bookmarks.add(bookmark);\\n```\\nIt is expecting more information for bookmarks so that it can be used later without any further lookups.\\nWe don't wish to have to look it up, especially since it may no longer exist in our sources -- and we don't want to store every single\\narticle just for this purpose.\\nWe don't really want to bake all the information into the link, either, that seems wrong for some reason.\\nAnd we don't really want to add arguments to `application.bookmarks.add`, that also feels wrong.\\n\n## Decision\n","Decision":"Introduce some idea of state in to `Application` so that it can handle notifications like this.\\nIt seems like simplifying UI elements is a good idea.\\nAnother option is giving this responsibility to the view element. But then how do we let it know?\\n","tokens":230,"id":4323,"Prediction":"## Decision\n\n**Utilize a Data Attribute with Event Listeners**\n\nTo address the requirements without over-burding the link with excessive information or modifying the `application.bookmarks.add` function, the decision is to utilize a data attribute within the HTML element and employ JavaScript to handle the event.\n\n1. **HTML Structure**:\n    Modify the UI element to incorporate a data attribute that holds the additional information required for the bookmark. This approach keeps the HTML lean while carrying the necessary data.\n\n    ```html\n    <a href=\"#\" class=\"bookmark\" data-bookmark-id=\"gbj1jf\" data-bookmark-url=\"http:\/\/abc\" data-bookmark-src=\"src-rnz\">bookmark<\/a>\n    ```\n\n2. **JavaScript Event Listener**:\n    Update the JavaScript to use an event listener that retrieves data from the relevant attributes upon the bookmark click event. This method ensures that the necessary data is passed to `application.bookmarks.add` without changing its existing argument structure.\n\n    ```js\n    document.addEventListener('DOMContentLoaded', () => {\n        document.querySelectorAll('.bookmark').forEach(bookmarkElement => {\n            bookmarkElement.addEventListener('click', async (event) => {\n                event.preventDefault();\n\n                const id = event.target.dataset.bookmarkId;\n                const url = event.target.dataset.bookmarkUrl;\n                const src = event.target.dataset.bookmarkSrc;\n\n                const bookmark = new Bookmark(id, url, src);\n\n                await application.bookmarks.add(bookmark);\n            });\n        });\n    });\n    ```\n3. **Benefits**:\n    - **Decoupled Data Management**: Keeping additional information in `data-*` attributes ensures the link remains clean and focused.\n    - **Maintainable Codebase**: No need to change `application.bookmarks.add` function or overload with arguments.\n    - **Up-to-date & Efficient**: Data used directly from attributes, ensuring it is always current without redundant lookups.\n\nBy implementing this solution, we maintain a clean HTML structure, avoid modifying existing functional signatures, and align with the flexibility and maintainability principles.","GenTime":"2024-07-29 01:47:38"}
{"File Name":"katas\/000-use-adrs.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4333,"Prediction":"## Decision\n\nTo systematically document and manage the architectural decisions made throughout the project, we will adopt the Architectural Decision Record (ADR) methodology. This approach will ensure that all significant architectural decisions are captured, along with their rationale, alternatives considered, and the consequences of each decision.\n\n### Key Points of the Decision:\n1. **Template Standardization:** We will use a standardized template for all ADRs to ensure consistency. The template will include sections such as Context, Decision, Status, Consequences, and any other relevant details.\n\n2. **Version Control:** All ADRs will be version-controlled using Git to maintain a historical record of changes and updates.\n\n3. **Review Process:** Each ADR will be subject to a review process to ensure that all stakeholders have an opportunity to provide input and that all important considerations are accounted for.\n\n4. **Decision Repository:** ADRs will be stored in a dedicated repository within our project's documentation space. This repository will be easily accessible to all team members and stakeholders.\n\n5. **Regular Updates:** The ADRs will be regularly updated to reflect the current state of the architecture and any new decisions that are made.\n\n6. **Reference in Code Base:** Wherever relevant, code comments and documentation will reference the corresponding ADR to provide context and rationale directly within the code base.\n\nBy implementing ADRs, we aim to enhance transparency, improve decision-making, and ensure that our architectural evolution is well-documented and understood by current and future team members.","GenTime":"2024-07-29 01:48:15"}
{"File Name":"katas\/002-no-eslint.md","Context":"## Context\\nSee [ADR1](.\/001-formatting-katas.md), which describes the formatting of a kata.\\nThis would normally lead to using a linter.\\n\n## Decision\n","Decision":"But I am not a big fan of a huge set of rules\\nand currently configuring a linter just for those couple of rules is overhead.\\nSomething like prettier which automatically applies a rule set is fine, but a linter\\nis kinda like someone who just says that a cleanup is needed, an auto-fixer (like prettier)\\ndoes clean up instead of just talking about it, my 2 cents.\\n","tokens":36,"id":4336,"Prediction":"## Decision\nWe have decided to use a linter for our codebase to ensure consistent formatting and adherence to coding standards across the team. Given the context from [ADR1](.\/001-formatting-katas.md), which emphasizes the importance of formatting consistency, a linter will automatically enforce these rules, reducing the likelihood of errors and improving code maintainability.\n\nBy integrating a linter into our continuous integration pipeline, we ensure that all code contributions are vetted for formatting and standardization before being merged. This decision will streamline the development process, maintain high code quality, and facilitate smoother collaboration among team members.\n\nAdditional considerations for the linter include:\n- Selection of a widely-adopted linter tool relevant to our technology stack.\n- Customization of linter rules to align with the specific formatting guidelines outlined in ADR1.\n- Documentation and training for team members on how to use the linter effectively.\n\nThis setup promotes a uniform code style, enhances readability, and aligns with our overall goal of maintaining a high-quality codebase.","GenTime":"2024-07-29 01:48:24"}
{"File Name":"katas\/003-katafication.md","Context":"## Context\\nAll katas in here are supposed to be used to fix failing tests.\\nThe problem with storing failing tests (the katas) in the repo is that there is no\\nway but fixing them each time to find out if they actually work or the engine\\nthe kata is run on supports a certain feature.\\n\n## Decision\n","Decision":"All tests have to pass and the special comment `\/\/\/\/` marks how to katafy\\nthis test.\\nFor example:\\n```\\n\/\/\/\/ const expectedType = 'not a function!';\\nconst expectedType = 'object';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nThe above test passes. Katafy means uncomment the first line and remove the following line, so\\nthat the code looks like this afterwards:\\n```\\nconst expectedType = 'not a function!';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nNow this test fails, and it can be used as a kata.\\n","tokens":66,"id":4338,"Prediction":"## Decision\nWe will implement a test validation framework that automatically verifies the functionality and compatibility of katas before they are stored in the repository.\n\n### Details:\n1. **Automated Test Validation Pipeline**: Create an automated pipeline within the CI\/CD system to run all katas in a controlled test environment. The pipeline will execute tests to ensure they are failing as expected and confirm that the engine supports the features required by the kata.\n   \n2. **Test Metadata**: Each kata will include metadata specifying the required engine features and any other dependencies. The pipeline will use this metadata to select the appropriate test environment configurations.\n\n3. **Version Compatibility Checks**: Implement version checks within the pipeline to ensure that katas are only run on compatible engine versions. Incompatible tests will be flagged, and maintainers will be notified for review.\n\n4. **Reporting and Notifications**: Generate detailed reports on the status of each kata, indicating whether it passed or failed the validation checks. Notify relevant stakeholders (e.g., kata authors, repository maintainers) about validation results and any issues detected.\n\n5. **Manual Override Mechanism**: Allow for a manual override mechanism for exceptional cases where an automated validation might fail erroneously. This will involve human reviewers who can verify and approve the kata manually.\n\nBy implementing these measures, we will ensure that all katas stored in the repository are validated for compatibility and functionality without requiring manual intervention each time. This will streamline the process of managing katas and improve the overall reliability of the repository.","GenTime":"2024-07-29 01:48:33"}
{"File Name":"oasis-core\/0008-standard-account-key-generation.md","Context":"## Context\\nCurrently, each application interacting with the [Oasis Network] defines its own\\nmethod of generating an account's private\/public key pair.\\n[Account]'s public key is in turn used to derive the account's address of the\\nform `oasis1 ... 40 characters ...` which is used to for a variety of operations\\n(i.e. token transfers, delegations\/undelegations, ...) on the network.\\nThe blockchain ecosystem has developed many standards for generating keys which\\nimprove key storage and interoperability between different applications.\\nAdopting these standards will allow the Oasis ecosystem to:\\n- Make key derivation the same across different applications (i.e. wallets).\\n- Allow users to hold keys in hardware wallets.\\n- Allow users to hold keys in cold storage more reliably (i.e. using the\\nfamiliar 24 word mnemonics).\\n- Define how users can generate multiple keys from a single seed (i.e.\\nthe 24 or 12 word mnemonic).\\n\n## Decision\n","Decision":"### Mnemonic Codes for Master Key Derivation\\nWe use Bitcoin's [BIP-0039]: _Mnemonic code for generating deterministic keys_\\nto derivate a binary seed from a mnemonic code.\\nThe binary seed is in turn used to derive the _master key_, the root key from\\nwhich a hierarchy of deterministic keys is derived, as described in\\n[Hierarchical Key Derivation Scheme][hd-scheme].\\nWe strongly recommend using 24 word mnemonics which correspond to 256 bits of\\nentropy.\\n### Hierarchical Key Derivation Scheme\\nWe use Sathoshi Labs' [SLIP-0010]: _Universal private key derivation from master\\nprivate key_, which is a superset of\\nBitcoin's [BIP-0032]: _Hierarchical Deterministic Wallets_ derivation algorithm,\\nextended to work on other curves.\\nAccount keys use the [edwards25519 curve] from the Ed25519 signature scheme\\nspecified in [RFC 8032].\\n### Key Derivation Paths\\nWe adapt [BIP-0044]: _Multi-Account Hierarchy for Deterministic Wallets_ for\\ngenerating deterministic keys where `coin_type` equals 474, as assigned to the\\nOasis Network by [SLIP-0044].\\nThe following [BIP-0032] path should be used to generate keys:\\n```\\nm\/44'\/474'\/x'\\n```\\nwhere `x` represents the key number.\\nNote that all path levels are _hardened_, e.g. `44'` is `44 | 0x8000000` or\\n`44 + 2^31`.\\nThe key corresponding to key number 0 (i.e. `m\/44'\/474'\/0'`) is called the\\n_primary key_.\\nThe account corresponding to the _primary key_ is called the _primary account_.\\nApplications (i.e. wallets) should use this account as a user's default Oasis\\naccount.\\n","tokens":208,"id":4346,"Prediction":"## Decision\n\nAdopt the BIP-32\/39\/44 standards for key derivation and address generation within the Oasis Network ecosystem.\n\n### Rationale\n\n1. **Interoperability**: Standardizing on well-established hierarchical deterministic (HD) wallet standards (BIP-32, BIP-39, and BIP-44) will ensure that keys generated are compatible across different applications and wallets, making it easier for users to switch between different solutions.\n\n2. **Hardware Wallet Support**: By using industry-standard key derivation methods, we enable users to store and manage their private keys on widely-used hardware wallets like Ledger and Trezor, enhancing overall security.\n\n3. **Cold Storage Reliability**: Utilizing the familiar 24-word mnemonic seed phrases (BIP-39) will provide users with a reliable and easy-to-understand way to back up and restore their keys for cold storage purposes.\n\n4. **Multi-Key Generation**: BIP-44 allows users to generate multiple key pairs from a single mnemonic seed, enabling more flexible and organized key management (e.g., having separate keys for different purposes or applications).\n\n### Implementation\n\n- **BIP-32**: Establish a hierarchical deterministic wallet structure to derive keys.\n- **BIP-39**: Use mnemonic phrases for seed generation and recovery.\n- **BIP-44**: Follow the multi-account hierarchy, enabling multiple address generation from a single mnemonic seed, specifying a distinct purpose for the Oasis Network.\n\n\n### Impact\n\n- **Developers**: Will need to update applications interacting with the Oasis Network to support the new key generation and address derivation methods.\n- **Users**: Will benefit from improved security and usability, with more reliable methods for key storage and cross-application compatibility.\n- **Ecosystem**: Should experience greater sense of unity and cooperation between different applications, enhancing the overall user experience on the Oasis Network. \n\nThe adoption of these standards is expected to streamline the key management process, leverage existing secure hardware solutions, and ultimately improve the security and usability of the Oasis Network.","GenTime":"2024-07-29 01:49:04"}
{"File Name":"oasis-core\/0001-tm-multi-root-apphash.md","Context":"## Context\\nCurrently the Tendermint ABCI application hash is equal to the consensus state\\nroot for a specific height. In order to allow additional uses, like proving to\\nlight clients that specific events have been emitted in a block, we should make\\nthe application hash be derivable from potentially different kinds of roots.\\n\n## Decision\n","Decision":"The proposed design is to derive the Tendermint ABCI application hash by hashing\\nall the different roots as follows:\\n```\\nAppHash := H(Context || Root_0 || ... || Root_n)\\n```\\nWhere:\\n- `H` is the SHA-512\/256 hash function.\\n- `Context` is the string `oasis-core\/tendermint: roots`.\\n- `Root_i` is the fixed-size SHA-512\/256 root hash of the specified root.\\nCurrently, the only root would be the existing consensus state root at index 0.\\nTo implement this change the following modifications would be required:\\n- Update the ABCI multiplexer's `Commit` method to calculate and return the\\napplication hash using the scheme specified above.\\n- Update the consensus API `SignedHeader` response to include the\\n`UntrustedStateRoot` (the untrusted prefix denotes that the user must verify\\nthat the state root corresponds to `AppHash` provided in the signed header in\\n`Meta`).\\nWhen new roots will be added in the future, both `Block` and `SignedHeader`\\nwill need to include them all.\\n","tokens":65,"id":4354,"Prediction":"## Decision\nTo enhance the functionality of the Tendermint ABCI application hash and support additional use cases such as proving specific events to light clients, we will modify the application hash derivation mechanism. The new approach will allow the application hash to be derived from a combination of multiple roots rather than being restricted to the consensus state root alone.\n\n### Implementation Steps\n1. **Define Multiple Roots:** Introduce new root types in addition to the consensus state root, such as:\n   - **Event Root:** A root that aggregates events emitted in a block.\n   - **Validator Set Root:** A root that tracks the validator set changes.\n   - **Transaction Root:** A root that indexes the transactions included in a block.\n\n2. **Composite Hash Structure:** Develop a composite hashing scheme where the application hash is derived from a combination of these roots. For instance, the new application hash could be a Merkle root of the individual roots.\n\n3. **ABCI Modifications:** Adapt the ABCI interface to handle and propagate these new root types along with the application hash. Ensure that the application provides the necessary data to compute these roots.\n\n4. **Proof Mechanism:** Implement the capability to generate and verify proofs against these roots. This will allow light clients to securely verify specific events, transactions, or validator changes.\n\n5. **Backward Compatibility:** Ensure that the new system remains backward-compatible with existing clients and applications that depend solely on the consensus state root.\n\n6. **Documentation and Examples:** Update the documentation to describe the new application hash derivation process and provide examples of how light clients can verify specific elements using the new proofs.\n\n### Benefits\n- **Enhanced Security:** By proving specific events or transactions to light clients, we strengthen the security guarantees of lightweight verification.\n- **Flexibility:** The ability to derive the application hash from different roots enables various new use cases and improves the system's adaptability.\n- **Future-proof:** Laying the groundwork for additional root types facilitates future enhancements and optimizes the ABCI's extendability.\n\n### Risks and Mitigation\n- **Complexity:** Increased complexity in the hashing and proof mechanism. Mitigate by thorough testing and incremental deployment.\n- **Performance Impact:** Potential performance impact due to additional hashing and root management. Optimize through efficient data structures and caching strategies.\n\nThis decision opens the door to improved functionalities and robustness in the Tendermint ecosystem while maintaining backward compatibility and security.","GenTime":"2024-07-29 01:49:34"}
{"File Name":"hello\/008-deployment.md","Context":"## Context\\nWe need a way to deploy our application to AWS.\\n\n## Decision\n","Decision":"We will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n","tokens":15,"id":4358,"Prediction":"## Decision\n\nTo effectively deploy our application to AWS, we will utilize AWS Elastic Beanstalk for its simplicity and ability to manage infrastructure and application deployment. Elastic Beanstalk allows us to quickly deploy, manage, and scale our application without worrying about the underlying infrastructure.\n\n### Key Points:\n1. **Ease of Deployment**: Elastic Beanstalk automates the deployment process, including the provisioning of the necessary infrastructure, load balancing, auto-scaling, and monitoring.\n2. **Scalability**: Built-in auto-scaling capabilities to handle traffic fluctuations and ensure high availability.\n3. **Managed Environment**: AWS manages patches and updates to the underlying infrastructure, reducing the operational burden on our team.\n4. **Integration**: Seamless integration with other AWS services (e.g., RDS, S3, CloudWatch) to enhance our application's functionality.\n5. **Customization**: Configurable environments where we can adjust settings and fine-tune resources to meet our specific needs.\n\nBy leveraging AWS Elastic Beanstalk, we can focus more on development and innovation while relying on AWS to manage and operate the underlying environment.","GenTime":"2024-07-29 01:49:47"}
{"File Name":"hello\/005-test-framework.md","Context":"## Context\\nWe want our code to be well tested. What tools or frameworks can we\\nleverage?\\n\n## Decision\n","Decision":"We will use the [pytest] framework to test our [Python] code. In\\naddition, we will use the [mock] library to prevent our tests\\nfrom interacting with external services.\\n","tokens":23,"id":4359,"Prediction":"## Decision\n\nAfter evaluating various testing tools and frameworks suitable for ensuring our code is well tested, we have decided to leverage the following solutions:\n\n1. **Unit Testing:** We will use **JUnit** for Java-based projects, **pytest** for Python projects, and **Jest** for JavaScript\/TypeScript projects. These frameworks are widely adopted, provide comprehensive testing capabilities, and integrate seamlessly with build tools and CI pipelines.\n\n2. **Integration Testing:** We will adopt **Spring Test** for Java projects using the Spring framework, **requests** and **responses** libraries for Python projects, and **supertest** for Node.js projects. These tools will help us test the interaction between different parts of our application more effectively.\n\n3. **End-to-End (E2E) Testing:** **Selenium** and **Cypress** will be our primary frameworks for web applications. Selenium provides flexibility and broad browser support, while Cypress offers a more developer-friendly experience and faster execution times.\n\n4. **Mocking and Stubbing:** For simulating external services and isolating tests, we will use **Mockito** for Java, **unittest.mock** for Python, and **Sinon.js** for JavaScript. These libraries are robust and widely supported, making them suitable for our needs.\n\n5. **Static Analysis and Linting:** We will integrate **SonarQube** for continuous static code quality analysis and use linters such as **ESLint** for JavaScript, **pylint** for Python, and **Checkstyle** for Java to maintain coding standards and detect potential issues early.\n\n6. **Continuous Integration (CI):** **Jenkins** and **GitHub Actions** will be employed to automate our testing pipeline. This ensures that our tests are run frequently, providing fast feedback and maintaining a high level of code quality.\n\nBy leveraging these tools and frameworks, we aim to create a comprehensive testing strategy that covers all aspects of our codebase, ensuring robustness and reliability.","GenTime":"2024-07-29 01:49:52"}
{"File Name":"hello\/001-record-decisions.md","Context":"## Context\\nWe want to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael\\nNygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4361,"Prediction":"We will implement the use of Architectural Decision Records (ADRs) to document and manage architectural decisions throughout the lifecycle of this project. Each ADR will include a Context section that outlines the factors and considerations leading to the decision, a Decision section to clearly state the choice made, and a Consequences section to discuss the implications of the decision.\n\nThe ADRs will be stored in a version-controlled repository to ensure they are easily accessible and traceable. This approach will help maintain historical context and rationales for decisions, facilitate better communication among team members, and support future maintenance and enhancement efforts. \n\nWe will adopt a consistent format for all ADRs to ensure clarity and completeness, and appoint a custodian to oversee the creation, review, and updating of ADRs to maintain their quality and relevance.","GenTime":"2024-07-29 01:49:57"}
{"File Name":"firefox-tv\/adr-0004-fftv-release-versioning.md","Context":"## Context\\nWe build several different versions of Firefox for purposes such as release, beta testing, and staged rollout. It's not obvious how the version numbers are related to these types of builds, and features have sometimes disappeared in rollback versions, so we document our strategy here.\\nFirefox TV releases are listed [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/releases](here). (These include LATs, which are not included in the changelog, and the changelog may include additional information, like reasons for re-releasing a version.)\\nAs of the time of writing, the current release version is `3.9`.\\n\n## Decision\n","Decision":"Firefox TV versioning is based off of [https:\/\/semver.org\/](semantic versioning) of MAJOR.MINOR.PATCH, but reflects features rather than API compatibility.\\nAdditionally, we also use alphanumeric suffixes to clearly differentiate between early test builds, releases, and re-releases.\\nEach release has a *tag* prefixed by `v`, such as `v3.8` and are listed in the [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/tags](Tags) page of the repo.\\n### Semantic Versioning\\n* MAJOR version changes signal significant changes to UI or functionality\\n* MINOR version changes are released every Sprint, unless they are skipped for release blockers\\n* PATCH version changes are for critical bug-fixes that cannot wait for the next Sprint.\\n* (LETTER-SUFFIX) reflects builds for our additional purposes that are detailed in following sections.\\n### Release\\nAs of 3.8, public releases have no suffix, and are released using the staged rollout capability of the Amazon Developer portal.\\n### Live App Testing (-LAT1)\\nAs part of our early testing, we create Live App Test (LAT) builds to send out candidate builds to our early testing groups before a release.\\nThese have a `-LAT1` suffix, where the number is incremented per test build sent out per version. For example, the second test build for 3.5 would be `3.5-LAT2`.\\nThis is first used in `3.3.0-LAT1`. These are used for testing, not general release.\\n#### Deprecated LAT versioning\\nPreviously, the versioning was much more confusing. We wanted to preserve monotonic order versioning, so a LAT would have an additional number appended at the end of the *previous* version; for example, the second LAT testing the 3.2 release would be versioned `3.1.3.2`, because the last released version before `3.2` was `3.1.3`.\\nThis deprecated LAT versioning was used between `2.1.0.1` and `3.1.3.2`.\\n### GeckoView (-GV)\\nCurrently, there are two distinct web engines that Firefox for Fire TV can be build with: the system WebView or GeckoView. Since a build currently can only use one of these, when we make a build that uses the GeckoView engine, we need a separate suffix to differentiate it.\\nThese GeckoView builds are suffixed with `-GV`.\\nThis is first used in `3.4-GV`, but is used for testing and not released to the general population.\\n### Re-Release (-A)\\nThere are two cases for re-release:\\n1) Rollback to a previous version due to critical bugs (e.g. rollback of 3.4 should be 3.3-A, although this is untested, and the platform may not allow decremented versioning, in which case, we would release the rollback as 3.4-A)\\n1) (deprecated) Release of a tested \"staged rollout\" build to the rest of the devices. (This is no longer used because staged rollout capability has been added to the app store.) This was monotonic because the \"general population\" devices had not been upgraded past this version.\\nThis is a build that has already been released before, either to a portion of the population, or because in a subsequent release we needed to do a version bump in order to push out a release, but did not change the code.\\nThese re-release builds are suffixed with a letter starting with `-A`, which is incremented with each re-release.\\nThis is first used in `3.4-A`.\\n#### Deprecated re-release versioning\\nBefore we started using letters to signify re-releases, we simply bumped the version number, so there are several versions that are simply re-releases of previous versions, but with different version numbers. These are listed below:\\n**3.1.3** is the same version as:\\n* 3.2.5\\n* 3.3\\n* 3.4-A\\n* 3.4-B\\n(and also listed in the changelog)\\n### Deprecated Split-Device Staged Rollout (-RO)\\nThese split-device staged rollout releases were suffixed with a `-RO`, e.g. `3.7-RO`.\\nBefore the staged rollout capability was added to the Amazon Developer portal, we handled staged rollout by releasing the newest version only to a single device, in this case the Firefox TV 4K Pendant (AFTN).\\nThis versioning scheme was used between `3.5-RO` and `3.7-RO`.\\n### Status: Accepted\\n","tokens":131,"id":4364,"Prediction":"## Decision\nWe will adopt a clear and consistent version numbering strategy to address the ambiguities related to different types of builds (release, beta testing, staged rollout) and prevent the disappearance of features in rollback versions.\n\n### Versioning Strategy\n1. **Major Version (X):** Indicates major updates and significant changes in the application, potentially including backward-incompatible changes.\n    - Examples: `4.0`, `5.0`\n2. **Minor Version (Y):** Indicates smaller updates that introduce new features and improvements without breaking existing functionality.\n    - Examples: `3.10`, `4.1`\n3. **Patch Version (Z):** Indicates bug fixes, security patches, and minor improvements that do not introduce new features or break existing functionality.\n    - Examples: `3.9.1`, `3.9.2`\n\n### Build Types and Suffixes\n- **Release Build:** No suffix, e.g., `3.9`.\n- **Beta Build:** Suffix `-betaN`, where `N` is the beta iteration, e.g., `3.9-beta1`.\n- **Staged Rollout:** Suffix `-staged`, e.g., `3.9-staged`.\n- **Hotfix\/Rollback:** Suffix `-hotfixN` for hotfixes, where `N` represents incremental hotfixes, e.g., `3.9-hotfix1`.\n\n### Version Documentation and Changelog\n- Maintain a detailed changelog for each version, clearly documenting new features, improvements, bug fixes, and reasons for re-releases.\n- Ensure each changelog entry includes:\n    - Version number\n    - Build type\n    - Description of changes\n    - Any known issues or rollback notes\n\n### Release Schedule\n- **Release Builds:** Monthly or bi-monthly, depending on the feature and stability.\n- **Beta Builds:** Weekly during the beta cycle, with feedback collection and bug triage.\n- **Hotfixes\/Rollbacks:** As needed, prioritized based on severity.\n\nThis strategy will help ensure a clear understanding of version relationships, consistent feature availability, and transparency in our release process.","GenTime":"2024-07-29 01:50:11"}
{"File Name":"teaching-vacancies\/0011_use_devcontainers.md","Context":"## Context and Problem Statement\\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\\nbenefitting both developers and non-developers on the team.\\n## Decision Drivers\\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\n\n## Decision\n","Decision":"- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\\nallowing other development workflows for developers who prefer different ways of working).\\n### Positive Consequences\\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\\ncausing developer tooling issues)\\n- Dependencies reduced to just Git, Docker, and VS Code\\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\\n- Moving entirety of development experience into a container fixes past Docker development workflow\\nissues experienced on the team (where tasks and services where executed from the host instead of\\ninteracting with a shell and an editor from inside the container itself)\\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\\nneed to support one single consistent environment\\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\\n- Reduces likelihood of \"works on my machine\" development environment issues\\n- \"Leave no trace\" on the host machine and complete isolation from other projects\\n- Removes possibility of \"dependency hell\" when working on multiple projects\\n- Removes need to clutter local environment with applications and dependencies that need to be\\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\\n- Removes need for language version managers (`rbenv`, `nvm`)\\n- Provides _executable documentation_ of project setup and dependencies\\n- Removes need for manually updated setup documentation that can become stale\\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\\nof setup steps and dependencies\\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\\n- Automatic passthrough of SSH and GPG keys\\n- Language extensions run within the container itself, and can be specified in the devcontainer\\nconfiguration file for instant setup of useful extensions for new users\\n- Automatic bootstrapping of personal dotfiles\\n- Ability to easily move to cloud-based workflows in the future\\n- Ability to easily propagate new tools and improved configuration to all developers on the team\\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\\n### Negative Consequences\\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\\ntechnology (overhead of containers running in an intermediate VM)\\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\\n- Can be worked around entirely by moving workspace root into the container, and we will continue\\nto investigate before we fully agree on devcontainers as our default workflow\\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\\n- Some duplication of Docker configuration between production and development configuration (but\\nthat is to be expected given that use cases are very different)\\n","tokens":224,"id":4370,"Prediction":"## Decision\nThe team will adopt the use of devcontainers for the Teaching Vacancies app to enhance the local development workflow. This decision involves the creation and maintenance of a `.devcontainer` configuration to provide a consistent, reproducible environment for all developers and non-developers involved in the project.\n\nThis configuration will include:\n1. Necessary development tools and dependencies (e.g., Ruby, Rails, PostGIS, etc.).\n2. Customizable settings and extensions to cater to the team's specific needs.\n3. Documentation to guide team members on utilizing the devcontainer setup effectively.\n\nThis approach aims to streamline onboarding and re-boarding processes, reduce setup complexity, and prepare for potential future adoption of cloud-based development environments like GitHub Codespaces.","GenTime":"2024-07-29 01:50:29"}
{"File Name":"teaching-vacancies\/0010_migrate_from_algolia_to_postgres.md","Context":"## Context and Problem Statement\\nGiven the user needs we want to meet with our search functionality, and the cost and complexity involved in our integration with Algolia, would we be better off with a simpler solution of leveraging our existing database for search?\\n## Decision Drivers\\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\n\n## Decision\n","Decision":"* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\nDecided to migrate away from Algolia and move to using our database for search functionality.\\n### Positive Consequences\\n* Simplified infrastructure and one fewer third-party service integration\\n* Significant complexity savings in search code\\n* Ability to integration test search results and keep search configuration in code (versioned and auditable)\\n* End of reliance on Algolia's goodwill in keeping us grandfathered on their legacy plan\\n* Minor cost savings in the short term (and we no longer need to avoid potential increases in queries in the long term)\\n* Improved performance and reliability due to not having to interact with a third-party service\\n* Ability to completely control and debug all aspects of how search works\\n* Richer geographical querying through use of PostGIS\\n### Negative Consequences\\n* Some implementation effort (estimated at 2x developers for ~3-4 sprints)\\n* Synonym logic will need custom implementation (but at the same time, this allows us to make it significantly \"smarter\" than on Algolia)\\n* Typo tolerance\/fuzzy search will need custom implementation (but this is an edge-case for us because the majority of queries are straightforward)\\n","tokens":162,"id":4378,"Prediction":"## Decision\nWe have decided to leverage our existing PostgreSQL database for search functionality instead of integrating with Algolia. \n\n## Rationale\n1. **Cost Considerations**: By using PostgreSQL, we avoid the financial costs associated with Algolia, including potential future increases if we lose our current grandfathered plan.\n2. **Technical Complexity**: Maintaining Algolia integration adds unnecessary complexity and brittleness to our codebase. A PostgreSQL-based solution will reduce our technical debt.\n3. **Customizability and Flexibility**: PostgreSQL offers better customizability and control over our search functionality. The configuration-as-code approach aligns with our development practices.\n4. **Matching Requirements**: Our search needs are relatively simple and do not justify the use of an enterprise-level search engine like Algolia.\n5. **Testing**: Integration testing is more straightforward when our search is implemented within our database, improving our overall test coverage and reliability.\n6. **Advanced Features**: PostgreSQL\u2019s support for full-text search and the capabilities offered by PostGIS for geographical queries meet our current needs and provide room for future enhancements.\n\n## Consequences\n1. **Development Effort**: There will be an initial development effort to implement and optimize PostgreSQL full-text search and PostGIS for our needs.\n2. **Maintenance**: The onus of maintaining the search capabilities will shift entirely to our internal team, but this is mitigated by reduced complexity and improved control.\n3. **Scalability**: While PostgreSQL is highly scalable, we will need to monitor performance and possibly optimize queries or hardware as our data grows. \n\nOverall, leveraging our existing PostgreSQL database aligns better with our cost, complexity, and customization needs while still meeting the functional requirements of our search functionality.","GenTime":"2024-07-29 01:51:00"}
{"File Name":"occurrent\/0002-mongodb-cloudevent-serialization.md","Context":"## Context\\nCurrently, Occurrent is doing \"unnecessary\" work when converting from a `CloudEvent` to `Document` and vice versa\\nSee [issue 196](https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196) in the cloud event java sdk project.\\n\n## Decision\n","Decision":"None yet\\n","tokens":59,"id":4382,"Prediction":"## Decision\n\nTo optimize the conversion process and eliminate unnecessary work when converting between `CloudEvent` and `Document`, we will refactor the existing implementation. This will involve:\n\n1. Analyzing the specific inefficiencies highlighted in issue 196 of the Cloud Events Java SDK project.\n2. Implementing a direct and streamlined conversion mechanism that reduces intermediate steps and redundant operations.\n3. Incorporating validation and error handling to ensure the new conversion process maintains data integrity and robustness.\n\nBy doing so, we aim to enhance performance, reduce resource consumption, and improve the overall efficiency of the system. This decision will be revisited periodically to ensure it continues to meet our performance and efficiency goals.","GenTime":"2024-07-29 01:51:11"}
{"File Name":"occurrent\/0001-mongodb-database-structure.md","Context":"## Context\\nWe need to record events in MongoDB in a specific structure\/schema.\\n\n## Decision\n","Decision":"The [CloudEvents](https:\/\/cloudevents.io\/) are persisted like this in the \"events collection\" in the database (1):\\n```json\\n{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"dataschema\" : \"http:\/\/someschema.com\/schema.json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n},\\n\"streamid\" : \"streamid\"\\n}\\n```\\nNote that \"streamid\" is added as an extension by the MongoDB event stores in order to read all events for a particular stream.\\nIf stream consistency is enabled then another collection, the \"stream consistency\" collection is also written to the database (2):\\n```json\\n{\\n\"_id\" : \"streamid\",\\n\"version\" : 1\\n}\\n```\\nWhen appending cloud events to the stream the consistency of the stream is maintained by comparing the version supplied by the user\\nwith the version present in (2). If they don't match then the cloud events are not written. Also if there are two threads writing to the same\\nstream at once then one of them will run into an error which means it has to retry (optimistic locking). For this to work, transactions are required!\\nAnother previous approach was instead to store the events like this:\\n```json\\n{\\n\"_id\": \"streamid\",\\n\"version\" : 1,\\n\"events\": [{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n}\\n}]\\n}\\n```\\nI.e. the events were stored inside a single document. While there are several benefits of using this approach, such as:\\n1. No transactions required, just do;\\n```java\\neventCollection.updateOne(and(eq(\"_id\", streamId), eq(\"version\", expectedStreamVersion)),\\ncombine(pushEach(\"events\", serializedEvents), set(\"version\", expectedStreamVersion + 1)),\\nnew UpdateOptions().upsert(true));\\n```\\n1. Reads could be done in a streaming fashion even though the events were stored as a subarray using aggregations\\n1. Subscriptions could take a `List<CloudEvent>`, i.e. all events written in the same transaction to the event store.\\nWhen not using the approach subscriptions gets notified once for each event and the consumer needs to reassemble\\nthe \"transaction\" somehow. This is a major drawback when not using this approach.\\nThere are however two major drawbacks that lead to not using this approach:\\n1. There's 16Mb document size limit in MongoDB so this approach wouldn't work for large streams\\n1. It's much hard to implement queries\/filters for subscriptions. The aggregation support is\\n[limited](https:\/\/stackoverflow.com\/questions\/62846085\/remove-element-from-subarray-using-an-aggregation-stage-applied-to-a-change-stre)\\nwhen working with subscriptions preventing simple filters (it would have been much simpler if `unwind`\\nwas supported since then we could flatten out the `events` subarray before applying the queries, i.e. something like\\n`(unwind(\"$events\"), replaceRoot(\"$events\"), match(filter.apply(\"type\", item))`).\\nAnother problem with subscriptions is the format, when a document is _created_ the content is specified\\nin the `fullDocument` property but it's a different property when the document is updated. Thus a filter\/query\\nwould not need to consider both these cases which is very difficult. With the new approach a query\/filter is much\\neasier since we only need to care about inserts.\\n","tokens":17,"id":4385,"Prediction":"## Decision\nWe will implement a well-defined schema for recording events in MongoDB using Mongoose (for Node.js) or a similar ODM (Object Data Modeling) library. This will enforce the structure we need and make interacting with the database more efficient and less error-prone.\n\n### Key Points:\n1. **Schema Design**: \n   - Define a schema that includes essential fields such as event name, timestamp, user ID, event type, and any additional metadata.\n   - Use appropriate data types for each field (e.g., Date for timestamps, String for event names).\n\n2. **Validation**:\n   - Implement schema validation to ensure that all required fields are present and in the correct format before saving to the database.\n\n3. **Indexes**:\n   - Create indexes on commonly queried fields (e.g., timestamp, user ID) to improve query performance.\n\n4. **ODM Library**:\n   - Choose Mongoose as the ODM for its robust schema capabilities, ease of use, and strong community support.\n\n5. **Error Handling**:\n   - Implement error handling to manage database connection issues or schema violations effectively.\n\n6. **Version Control**:\n   - Maintain version control of the schema to manage changes over time, ensuring backward compatibility where necessary.\n\n### Example Schema Definition in Mongoose:\n```javascript\nconst mongoose = require('mongoose');\nconst Schema = mongoose.Schema;\n\nconst eventSchema = new Schema({\n  eventName: { type: String, required: true },\n  timestamp: { type: Date, default: Date.now, required: true },\n  userId: { type: String, required: true },\n  eventType: { type: String, required: true },\n  metadata: { type: Map, of: String }\n});\n\nconst Event = mongoose.model('Event', eventSchema);\n```\n\nBy adhering to this decision, we ensure that our event recording is consistent, efficient, and scalable, meeting our project's needs and laying down a strong foundation for future development.","GenTime":"2024-07-29 01:51:24"}
{"File Name":"openfido-workflow-service\/0002-pipelines.md","Context":"## Context\\nA couple of client projects need infrastructure to process GridLabD jobs.\\nThe goal would be that other projects could either include this project as its\\nown service in their infrastructure, or incorporate it directly into their\\nFlask-based project.\\nThese kinds of jobs are long running, and produce artifacts that each project\\nmay store in different ways (but primarily S3).\\n\n## Decision\n","Decision":"Create a Flask Rest service, coupled with Celery for job processing.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nUse the structure of the [presence-account-service](https:\/\/github.com\/PresencePG\/presence-account-service) project as a reference.\\n","tokens":78,"id":4388,"Prediction":"## Decision\nAdopt a Modular and Extensible Microservices Architecture using Kubernetes and Flask\n\n### Overview\nGiven the context where multiple client projects require infrastructure to process GridLabD jobs, and the need for these projects to either integrate the solution as a standalone service or as part of their existing Flask-based applications, the following architectural decision is made:\n\n### Detailed Decisions\n\n1. **Microservices Architecture**:\n    - Develop the GridLabD processing functionality as a standalone microservice.\n    - Ensure the microservice can be easily deployed and scaled independently of other services.\n\n2. **Containerization with Docker**:\n    - Package the GridLabD processing service into Docker containers.\n    - Utilize Docker for consistent environments across different development, testing, and production stages.\n\n3. **Orchestration with Kubernetes**:\n    - Use Kubernetes to manage deployment, scaling, and operation of the GridLabD service containers.\n    - Leverage Kubernetes features like Helm charts for easier deployment and management.\n\n4. **REST API with Flask**:\n    - Develop the service using Flask to expose a RESTful API for job submission, status tracking, and results retrieval.\n    - Ensure the API is well-documented and follows REST conventions for ease of integration.\n\n5. **Asynchronous Job Processing**:\n    - Implement asynchronous job handling using task queues like Celery or similar tools.\n    - Design the system to handle long-running GridLabD jobs efficiently.\n\n6. **Artifact Storage**:\n    - Design the service to support flexible storage backends for job artifacts, with an initial implementation using Amazon S3.\n    - Provide an abstraction layer or configuration options to support different storage backends as required by other projects.\n\n7. **Security and Authentication**:\n    - Implement robust authentication mechanisms (e.g., OAuth, JWT) to secure the API.\n    - Ensure that sensitive data, such as job parameters and results, are transmitted securely using HTTPS.\n\n### Rationale\n- **Scalability**: Microservices architecture combined with Kubernetes allows the GridLabD job processing to scale independently based on demand.\n- **Flexibility**: Containerization with Docker ensures the service can run in any environment consistently. Flask provides a lightweight yet powerful framework for building RESTful APIs.\n- **Modularity**: A standalone service that can be incorporated into other projects either as a separate deployment or integrated into an existing Flask-based project offers maximum flexibility for clients.\n- **Extensibility**: The ability to support different storage solutions and integrate with various client requirements ensures that the architecture is adaptable to future needs.\n- **Security**: Adhering to modern security standards ensures that client data and job artifacts are secure during processing and storage.\n\nThe decision to use a microservices architecture, containerized deployment with Docker, and orchestration with Kubernetes, along with a REST API built on Flask, ensures that the solution is scalable, flexible, and secure, meeting the diverse needs of different client projects efficiently.","GenTime":"2024-07-29 01:51:41"}
{"File Name":"connaisseur\/ADR-1_bootstrap-sentinel.md","Context":"## Context\\nConnaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods.\\nIn [#3](https:\/\/github.com\/sse-secure-systems\/connaisseur\/issues\/3) it was noted that prior to version 1.1.5 of Connaisseur when looking at the `Ready` status of Connaisseur Pods, they could report `Ready` while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed _after_ the Connaisseur Pods, which was solved by checking the `Ready` state of said Pods. If one were to add a dependency to this `Ready` state, such that it only shows `Ready` when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.\\n\n## Decision\n","Decision":"We chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.\\n### Positive consequences\\nIf the Connaisseur Pods report `Ready` during the `connaisseur-bootstrap-sentinel`'s runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the Helm deployment will fail after its timeout period (default: 5min), since there won't be a running `connaisseur-bootstrap-sentinel` Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the `Ready` state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working.\\n### Negative consequences\\nOn the other hand, if an adversary can deploy a Pod named `connaisseur-bootstrap-sentinel` to Connaisseur's Namespace, the Connaisseur Pods will always show `Ready` regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the `connaisseur-bootstrap-sentinel` Pod being left behind, however since it has a very limited use case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.\\n","tokens":275,"id":4395,"Prediction":"## Decision\n\nTo avoid the aforementioned deployment deadlock between Connaisseur Pods and the MutatingWebhookConfiguration, an alternative approach will be adopted that ensures both components are deployed and become functional sequentially without causing circular dependencies.\n\n### Approach:\n1. **Staggered Deployment Strategy:**\n    - **Step 1:** Deploy Connaisseur Pods without the MutatingWebhookConfiguration initially. This allows the Pods to start up and become operational without blocking on webhook requests.\n    - **Step 2:** Once the Connaisseur Pods are confirmed to be operational (exclude the `Ready` check based on MutatingWebhookConfiguration presence), proceed to deploy the MutatingWebhookConfiguration.\n    - **Step 3:** Implement a post-deployment check ensuring the MutatingWebhookConfiguration is successfully applied and linked to functional Connaisseur Pods. \n\n2. **Readiness Probe Customization:**\n    - Modify the readiness probe logic of Connaisseur Pods to ensure they only report `Ready` based on their actual operational functionality, disregarding the initial presence of the MutatingWebhookConfiguration.\n    - This logic would be tailored to avoid checking for the MutatingWebhookConfiguration during initial deployment but include it in routine health checks post-deployment.\n\n3. **Deployment Automation Script:**\n    - Create a deployment script or Helm chart enhancement that automates the deployment in the precise sequence:\n        - Deploy Connaisseur Pods\n        - Deploy MutatingWebhookConfiguration\n        - Perform a readiness check on both components together\n\n4. **Monitoring and Alerts:**\n    - Implement monitoring around both the Connaisseur Pods and the MutatingWebhookConfiguration setup.\n    - Alert administrators if the MutatingWebhookConfiguration is not being linked to the Connaisseur Pods effectively after a timeout period.\n\nBy adopting this strategy, we ensure that the Connaisseur system components are brought up in a reliable order that prevents deadlocks and maintains system integrity and functionality.","GenTime":"2024-07-29 01:52:00"}
{"File Name":"connaisseur\/ADR-2_release-management.md","Context":"## Context\\nDuring its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed\/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart.\\nA single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility.\\n\n## Decision\n","Decision":"For choice 1, we decided to go for two branches. On the one hand, `master` being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a `develop` branch that hosts the current state of development and will be merged to `master` whenever we want to create a new release.\\nThis way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release.\\nIn the process of automating most of the release process, we will run an integration test with locally built images for pull requests to `master`. Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the `master` branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the `master` branch referencing the new release version.\\nAfter the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working.\\nWe decided for this option as it does not expose credentials to GitHub Actions, which we wanted to avoid especially in light of the [recent GitHub Actions injection attacks](https:\/\/bugs.chromium.org\/p\/project-zero\/issues\/detail?id=2070) and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys.\\n### Positive consequences\\n- We can develop without having to ship changes immediatly.\\n- Release process does not expose credentials to GitHub Actions.\\n- Code gets Git tags.\\n### Negative consequences\\n- Process from code to release for a single change is more cumbersome than right now.\\n- Release still requires human intervention.\\n","tokens":131,"id":4397,"Prediction":"## Decision\nTo address the issue of scalability and streamline the release process, we will adopt a Continuous Integration\/Continuous Deployment (CI\/CD) pipeline for Connaisseur. This pipeline will automate the building, signing, and pushing of Docker images as well as the updating of the corresponding Helm charts. Specifically, the CI\/CD pipeline will:\n\n1. Automatically trigger on every merge to the main branch.\n2. Build and test the Connaisseur Docker image to ensure it meets quality and security standards.\n3. Sign the Docker image using a trusted key.\n4. Push the signed Docker image to a designated container registry.\n5. Update the Helm chart with the new image version and deploy it, ensuring it stays synchronized with the most recent release.\n\nThis approach will ensure that the process is scalable, reduces bottlenecks, and aligns with agile development workflows. It will also ensure that all team members can focus on development rather than on manual release tasks, thereby increasing overall productivity and reliability.","GenTime":"2024-07-29 01:52:05"}
{"File Name":"gp-redirect\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4401,"Prediction":"## Decision\n\nWe will use the Architectural Decision Record (ADR) pattern to document the architectural decisions for this project. Each ADR will include the following sections:\n\n- **Title**: A descriptive title of the decision.\n- **Date**: The date the decision was made.\n- **Context**: An explanation of the issue or situation that prompted the decision.\n- **Decision**: The actual decision that was made, including any alternatives that were considered and why they were not chosen.\n- **Consequences**: The consequences of the decision, both positive and negative, including any potential risks or trade-offs.\n\nWe will store these ADRs in a dedicated folder within our source control repository to ensure they are versioned alongside our codebase. This will allow team members to easily reference the decisions and understand the rationale behind them.\n\nTo ensure consistency and clarity, we will follow a standardized template for all ADRs. This will make it easier for team members to read and write ADRs, and will help maintain the quality of the documentation over time.\n\nAlso, we will review and update ADRs as necessary to reflect any changes in context or the outcome of the decisions, ensuring that they remain accurate and useful throughout the life of the project.","GenTime":"2024-07-29 01:52:21"}
{"File Name":"rtk-companion\/0003-mvp-functionality-in-add-card-window.md","Context":"## Context and Problem Statement\\nAfter experimenting with the previous MVP goal defined by [ADR-0002](0002-define-goal-for-an-mvp.md), I've decided it's not a good approach.\\nMy main issue with is that in a way, it is re-creating Anki's \"Add Card\" screen, albeit a custom one which adds RTK cards.\\nHere's a screenshot of the UI created and filled with placeholder text.\\n![](..\/assets\/rtk-browser-screenshot.png)\\nWhile I still think it'd be cool to have an RTK browser in Anki and working with QT is ....interesting, I thought\\na simpler approach should be sought.\\nSo the problem stands, what should be done as an MVP and what should it contain?\\n\n## Decision\n","Decision":"After some experimenting and research, I've decided an MVP should augment the `Add Card`.\\nHere's the vision of how a user interacts with the MVP.\\n1. User opens the `Add Card` window which has a \"RTK\" button, and a \"Stories\" button.\\n1. User types in their desired Heisig Kanji number is the \"Number\" field.\\n1. User clicks the RTK button and the character, and stroke order get auto filled.\\n1. User clicks \"Stories\" button, which opens an embedded browser window to user stories.\\n### Positive Consequences <!-- optional -->\\n- The approach is simpler.\\n- We keep Building UI elements with `QT` to a minimum under this approach.\\n### Negative Consequences <!-- optional -->\\n- We leave out Additional keyword meanings and the radicals for the Kanji\\n","tokens":161,"id":4402,"Prediction":"## Decision\n\nWe will pivot from the previous MVP goal of creating an \"Add Card\" screen resembling Anki's to implementing a more streamlined and simpler approach that focuses on user value and ease of implementation. The new MVP should focus on the following:\n\n1. **Direct Integration with Anki:**\n   - Rather than creating new screens or modules, integrate directly with Anki\u2019s existing functionalities to simplify the development process and ensure seamless user experience.\n\n2. **Automated RTK Card Creation:**\n   - Implement a background script or plugin that automatically generates RTK cards based on user criteria without user intervention. This will significantly reduce the complexity of the user interface that needs to be developed.\n\n3. **Minimal User Interface:**\n   - Develop a minimal user interface that allows users to enable\/disable the automation and set basic parameters for the RTK card creation process. This interface should be implemented as a simple dialog or popup within Anki.\n\n4. **Enhanced User Experience:**\n   - Ensure that the integration provides real-time feedback and notifications to the user, enhancing the overall user experience without requiring extensive development of new UI components.\n\n5. **Documentation and User Guide:**\n   - Provide clear documentation and a user guide to help users understand how to use the new functionality. This should include examples and step-by-step instructions.\n\nThis approach should streamline the development process, focusing on delivering immediate value while reducing complexity and resource requirements. The primary objective will remain to enhance Anki\u2019s functionality with RTK card creation, but in a more efficient and user-friendly manner.","GenTime":"2024-07-29 01:52:25"}
{"File Name":"Conduit\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4408,"Prediction":"## Decision\nWe will adopt the practice of Architectural Decision Records (ADRs) to document important architectural decisions throughout the project lifecycle. Each decision will be captured in a structured format, including context, decision, and consequences. This will ensure transparency, traceability, and a clear rationale behind architectural choices, facilitating better communication among team members and stakeholders.","GenTime":"2024-07-29 01:52:43"}
{"File Name":"buildit-all\/0006-use-okta-as-identity-provider.md","Context":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n\n## Decision\n","Decision":"* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","tokens":279,"id":4414,"Prediction":"## Decision\n\nWe have decided to integrate Okta for authentication and authorization in our application. This decision was made based on several key factors:\n\n1. **Developer Account Limitations**: Okta offers a forever free developer account that can support up to 7000 active users per month, which is more than sufficient for our projected usage needs.\n   \n2. **Provider Flexibility**: Okta allows authentication against a range of providers, including social media accounts (such as Facebook, GitHub, Microsoft Azure), SAML, and AD\/LDAP. This offers us future-proofing flexibility if we decide to diversify our authentication methods.\n\n3. **Microsoft Social OpenConnect ID**: We will use Okta's support for the Microsoft social OpenConnect ID provider. This will enable us to authenticate Microsoft users and map them to Okta users, thus creating a proxy or facade to Microsoft while maintaining a uniform API.\n\n4. **Integration with Spring Boot and Spring Security**: Okta has out-of-the-box support for Spring Boot 2.x and Spring Security 5.x. This will facilitate smooth integration with our existing application, reducing development time and effort.\n\n5. **React Client SDK**: Using Okta's React client SDK will replace our custom client code for Microsoft Azure AD, ensuring better maintainability and alignment with current standards.\n\n6. **Role and Group Management**: Okta provides admin-level dashboards and screens for managing users, roles, and groups out of the box. This will significantly enhance our ability to manage admin-level users without additional development effort.\n\n7. **Future Scalability**: The ability to later add additional social logins or support Active Directory directly if needed aligns with our long-term goals for scalability and extensibility.\n\nThe spike implementation, available at [the provided GitHub link](https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x), has demonstrated the feasibility of this integration. Therefore, we are confident that Okta is the right choice for our authentication and authorization needs.","GenTime":"2024-07-29 01:53:05"}
{"File Name":"buildit-all\/0008-database.md","Context":"## Context\\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\\n\n## Decision\n","Decision":"* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\\n* Use MariaDB JDBC driver - has native Aurora support for failover\\n","tokens":29,"id":4415,"Prediction":"## Decision\n\nWe will utilize a relational database (RDBMS) for persisting the locations, bookables, and bookings data. The chosen RDBMS will be PostgreSQL due to its robustness, active community support, and advanced features that fit well with our anticipated requirements.\n\n### Justification:\n1. **Data Integrity:** PostgreSQL ensures data integrity through ACID compliance, vital for booking systems where consistency and reliability are crucial.\n2. **Scalability:** PostgreSQL's support for extensive data sets and its scalability options help in accommodating future growth in data volume and user base.\n3. **Complex Queries:** The need for complex queries and joins to fetch relational data efficiently can be well served by PostgreSQL's powerful querying capabilities.\n4. **Community and Support:** PostgreSQL has a large, active community and extensive documentation, which ensures continuous improvement and reliable support.\n5. **Extensions:** PostgreSQL\u2019s rich extension ecosystem allows adding capabilities (like PostGIS for geographic data) that can be useful for managing location-based data.\n6. **Cloud Compatibility:** PostgreSQL is well-supported by major cloud providers, facilitating seamless deployment and management in cloud environments.\n\n### Implementation Notes:\n- The database schema will be designed to normalize data appropriately while ensuring performance efficiency.\n- Necessary indices will be added to optimize read and write operations for frequent queries associated with bookings.\n- Regular backups and replication strategies will be put in place to ensure data availability and disaster recovery.\n\n### Future Considerations:\n- As the system grows, we might explore sharding or partitioning strategies to maintain performance.\n- Evaluate the impact on maintenance and operational overhead and consider managed database services if required.","GenTime":"2024-07-29 01:53:09"}
{"File Name":"buildit-all\/0005-use-id-token-from-microsoft-as-bearer-token.md","Context":"## Context\\nIn the interest of time and getting something to work, we are going to break up the steps further\\n\n## Decision\n","Decision":"* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\\n* Proper validation of the id_token will still occur\\n","tokens":24,"id":4416,"Prediction":"## Decision\nWe have decided to break up the steps further into smaller, manageable tasks to expedite the development process and ensure progress. Each step will be clearly defined with its own objectives, allowing for incremental progress and easier debugging. This modular approach will enable the team to work more efficiently and provide opportunities for early testing and validation of individual components. Here's the detailed plan:\n\n1. **Step Definition**: Clearly outline each step's goals, inputs, outputs, and dependencies.\n2. **Prioritization**: Prioritize steps based on their impact and dependencies.\n3. **Assignment**: Assign each step to a responsible team member or group.\n4. **Timeline**: Establish deadlines for each step to keep the project on track.\n5. **Review**: Implement regular checkpoints to review progress and make necessary adjustments.\n\nThis approach will help manage the project's complexity and allow for more flexible adjustments as we move forward.","GenTime":"2024-07-29 01:53:12"}
{"File Name":"buildit-all\/0007-use-pac4j-to-validate-tokens.md","Context":"## Context\\nhttp:\/\/www.pac4j.org\\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it's not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\\n\n## Decision\n","Decision":"* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n","tokens":144,"id":4422,"Prediction":"## Decision\n\nWe have decided to integrate pac4j library into our authentication system to handle Azure Active Directory (Azure AD) integration.\n\n### Rationale:\n1. **Ease of Integration**: Pac4j supports Azure AD out of the box and can be easily configured using the discovery URL. This minimizes the need for custom development.\n2. **Multi-Tenant Support**: Pac4j correctly handles the {tenantId} in the Issuer URL, making it suitable for our multi-tenant environment, which aligns with Microsoft's slightly non-standard implementation.\n3. **OpenID Connect Compliance**: Pac4j provides robust support for OpenID Connect, ensuring our authentication process adheres to widely recognized standards and best practices.\n4. **Reduced Maintenance**: Using a well-supported library like pac4j will likely reduce the long-term maintenance burden compared to maintaining our custom filter and token validator.\n5. **Security**: Relying on a community-vetted library enhances the security of our application, as it benefits from ongoing updates and patches from the pac4j maintainers.\n\nBy adopting pac4j, we should achieve a more streamlined, secure, and maintainable authentication process for our application.","GenTime":"2024-07-29 01:53:36"}
{"File Name":"buildit-all\/0002-version-api-via-uri-path.md","Context":"## Context\\nVersioning APIs can be controversial.  Ideally, APIs don't need to change, or be additive so as not to break backwards compatibility.  Occasionally, some changes require bigger changes.\\nWe will do our best to be open w\/ what we accept and strict w\/ what we return\\nThere are lots of ways an API can change:\\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\\n* Breaking changes at a resource level (\/v1\/booking) needs to change\\n* Breaking changes in behavior\\nThis decision currently applies to the 1st bullet.\\n\n## Decision\n","Decision":"All API endpoints will start with the version of the API (e.g. \/v1\/resource).  We don't anticipate this to change.\\n","tokens":127,"id":4424,"Prediction":"## Decision\nAPI versioning will be implemented using a version identifier in the URL path (e.g., `\/v2\/resource`). This approach will allow for the co-existence of multiple API versions, ensuring that existing clients using the current version (e.g., `\/v1\/resource`) are not disrupted by breaking changes.\n\nThe following steps will be put in place to manage versioning:\n\n1. **Versioning in URL Path**: All endpoints will include a version identifier in the URL path. For example, the current version will be identified as `\/v1\/resource` and the new version as `\/v2\/resource`.\n\n2. **Deprecation Policy**: Any breaking changes will necessitate a new version. A clear deprecation policy will be established, providing clients with ample notice and time to transition before the older version is deprecated.\n\n3. **Changelog and Documentation**: Detailed documentation will be provided for each version, highlighting the differences and breaking changes. A changelog will be maintained to track changes across versions.\n\n4. **Communication Practices**: Clients will be informed about breaking changes and new versions through multiple channels, including direct communication, release notes, and public announcements.\n\nBy adopting a URL path versioning strategy, we aim to balance the need for progress and innovation with the requirement of maintaining backward compatibility and minimizing disruptions for our API clients.","GenTime":"2024-07-29 01:53:42"}
{"File Name":"buildit-all\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4428,"Prediction":"## Decision\nWe will adopt the Architectural Decision Record (ADR) pattern to document all significant architectural decisions made during this project. Each decision will be stored in a separate document and will follow a consistent format, including the context for the decision, the decision itself, the reasoning behind it, and any consequences or implications.\n\n1. **Template Standardization**: We will use a standardized template for all ADRs to ensure consistency and ease of understanding. The template will include sections such as Context, Decision, Justification, Alternatives Considered, and Consequences.\n\n2. **Centralized Repository**: All ADRs will be stored in a centralized repository, which could be a dedicated ADR directory within our project\u2019s version control system (e.g., Git). This will make it easy to track the evolution of our architecture and ensure that all team members can access the decision records.\n\n3. **Versioning**: Each ADR will be versioned separately to capture the history of changes and updates made to individual decisions. This allows us to understand the rationale for changes over time.\n\n4. **Decision Roles**: We will assign specific roles for who can create, review, and approve ADRs. Typically, senior architects and project leads will have the authority to approve ADRs, ensuring that the decisions align with the overall architectural vision.\n\n5. **Review Process**: ADRs will undergo a peer review process before being finalized. This ensures that the decisions are well-vetted and consider input from multiple stakeholders.\n\n6. **Visibility and Communication**: We will communicate new or updated ADRs to the entire development team during regular meetings and through project communication channels. This ensures that everyone is aware of the architectural directions and can provide feedback.\n\nBy adopting the ADR process, we aim to ensure that our architectural decisions are well-documented, transparent, and can be revisited or revised as the project evolves.","GenTime":"2024-07-29 01:53:57"}
{"File Name":"verify-service-provider\/0023-we-will-report-the-version-in-a-saml-extension.md","Context":"## Context\\nThe verify-service-provider will be deployed to a number of relying parties.\\nWhich relying party is using which version could potentially become difficult\\nto keep track of.\\nTo make it as easy as possible for us to manage this we'd like the\\nverify-service-provider to report its version in some way.\\nBecause the verify-service-provider is not intended to be accessible to the\\ninternet we can't simply expose an endpoint that reports the version number.\\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\\nheader.\\nThere's also a concern about the security implications of reporting a version\\nnumber in cleartext.\\nWe considered a couple of options:\\n- Requesting metadata from Verify with a custom user-agent string\\n- Sending the version in an unencrypted saml extension\\n- Sending the version in an encrypted saml extension\\n\n## Decision\n","Decision":"We decided to send the version number in the SAML AuthnRequests as an encrypted\\nSAML extension. The XML will look roughly like this:\\n```\\n<saml:AuthnRequest>\\n<saml:Issuer>...<\/saml:Issuer>\\n<saml:Signature>...<\/saml:Signature>\\n<saml:Extensions>\\n<saml:EncryptedAttribute>...<\/saml:EncryptedAttribute>\\n<\/saml:Extensions>\\n<\/saml:AuthnRequest>\\n```\\nOnce decrypted, the Attribute in the Extensions will look like:\\n```\\n<saml:Attribute Name=\"Versions\">\\n<saml:AttributeValue xsi:type=\"metrics:VersionsType\">\\n<metrics:ApplicationVersion>3.4.1<\/metrics:ApplicationVersion>\\n<\/saml:AttributeValue>\\n<\/saml:Attribute>\\n```\\n","tokens":182,"id":4436,"Prediction":"## Decision\nWe have decided to send the version information in an encrypted SAML extension.\n\n### Rationale\n1. **Security**: Transmitting the version number in an encrypted SAML extension ensures that sensitive information is not exposed in cleartext, addressing the security concerns mentioned.\n2. **Compatibility**: This approach leverages existing SAML infrastructure and standard mechanisms for encryption, which can be easier to implement and more robust than creating a custom solution.\n3. **Maintainability**: By embedding the version information directly within the SAML messages, there's no need to introduce additional HTTP headers or endpoints, simplifying the management of version tracking.\n4. **Reliability**: Using encrypted SAML extensions ensures that version information can be securely communicated between the verify-service-provider and the relying parties without opening vulnerabilities.\n5. **Scalability**: This method scales well as the number of relying parties increases, as the version information is intrinsically tied to each SAML message.\n\nOverall, sending the version in an encrypted SAML extension is the best option to address the security and management concerns while fitting within the existing communication framework of the verify-service-provider.","GenTime":"2024-07-29 01:54:19"}
{"File Name":"verify-service-provider\/0013-we-will-write-acceptance-tests-against-compliance-tool.md","Context":"## Context\\nVerify provide a \"compliance tool\" which allows relying parties to check that their\\nimplementations are compatible with the SAML profile of Verify.\\nCurrently relying parties must test their service against the compliance tool as part\\nof the onboarding process.\\nBy writing tests against the compliance tool we can:\\n* make sure our own implementation is compliant\\n* demonstrate to relying parties how to write tests against the compliance tool\\n* learn more about the user experience of using the compliance tool\\n\n## Decision\n","Decision":"We'll write acceptance tests against the compliance tool for the verify service provider\\nand the stub relying parties.\\nTests for the service provider will give us direct feedback on whether it's compliant.\\nTests for the stub relying parties will give us confidence they work end-to-end and should\\nprovide a template for real relying parties to write their own tests.\\n","tokens":101,"id":4442,"Prediction":"## Decision\n\nWe have decided to create an automated test suite for the compliance tool. These tests will serve multiple purposes:\n\n1. **Internal Validation**:\n   - To ensure that our own implementation of the Verify service consistently adheres to the SAML profile requirements.\n   - To quickly identify and rectify any deviations or non-compliant behavior introduced during development.\n\n2. **Relying Party Guidance**:\n   - To provide clear examples and documentation on writing tests against the compliance tool. This will help reduce the onboarding time for new relying parties.\n   - To create a reference framework that demonstrates the proper usage of the compliance tool and highlights common errors or issues that might be encountered.\n\n3. **User Experience Improvement**:\n   - To gather feedback regarding the usability and functionality of the compliance tool through well-defined test scenarios.\n   - To identify potential improvements and enhancements to the tool based on the interactions and outcomes of these tests.\n\n### Implementation Steps:\n1. **Test Suite Development**:\n   - Develop a set of automated tests covering all key compliance requirements of the Verify SAML profile.\n   - Ensure the test suite is comprehensive and includes edge cases, common pitfalls, and best practices for using the compliance tool.\n\n2. **Documentation and Examples**:\n   - Create detailed documentation explaining each test in the suite, including its purpose, expected outcomes, and how to interpret the results.\n   - Provide code samples and templates to assist relying parties in writing their own tests.\n\n3. **Continuous Integration**:\n   - Integrate the test suite into the continuous integration pipeline to ensure regular validation of the Verify service against the compliance tool.\n   - Provide timely feedback to the development team on any compliance issues detected.\n\n4. **User Feedback Loop**:\n   - Establish a feedback mechanism for relying parties to report difficulties, issues, or suggestions related to the compliance tool and the provided test suite.\n   - Iterate on the tool and the test suite based on the feedback to improve the overall user experience.\n\nBy implementing this automated test suite and providing comprehensive documentation, we aim to ensure the robustness of our Verify service, facilitate smoother onboarding for relying parties, and enhance the overall reliability and usability of the compliance tool.","GenTime":"2024-07-29 01:54:43"}
{"File Name":"verify-service-provider\/0006-we-will-build-a-js-client.md","Context":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n\n## Decision\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":4443,"Prediction":"## Decision\n\nTo provide a seamless integration experience for users utilizing Node.js and Passport, we will create a dedicated module that supports authentication using Passport strategies while also being compatible with other languages and frameworks. This will involve the following steps:\n\n1. **Node.js and Passport Support:**\n   - Develop a library that wraps common Passport functionalities, making it easy for Node.js developers to integrate authentication with minimal configuration.\n   - Ensure comprehensive documentation with examples on how to integrate this library into their existing Node.js applications.\n   - Include pre-configured strategies for popular authentication methods (e.g., OAuth, JWT).\n\n2. **Cross-Language Compatibility:**\n   - Design a RESTful API that standardizes authentication flows, enabling use by applications written in any language or framework.\n   - Implement the RESTful API server-side using established best practices to ensure security and performance.\n   - Provide client libraries and SDKs in multiple languages (e.g., Python, Java, Ruby, PHP) to simplify integrating with the RESTful API.\n\n3. **Documentation and Examples:**\n   - Develop extensive documentation covering both the Node.js module and the RESTful API.\n   - Include step-by-step guides, code snippets, and best practices for integrating with various languages and frameworks.\n   - Offer example projects in different languages showing real-world usage of the API and Node.js module.\n\n4. **Community and Support:**\n   - Create a support channel (e.g., forum, chat) where users can seek help and share integration tips.\n   - Encourage contributions from the community to continually improve and expand the library and API.\n\nBy following this approach, we ensure that Node.js and Passport users have a frictionless integration experience while also supporting a diverse range of languages and frameworks through a standardized API.","GenTime":"2024-07-29 01:54:48"}
{"File Name":"verify-service-provider\/0025-we-will-only-release-one-configuration-file.md","Context":"## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n\n## Decision\n","Decision":"We will remove the verify-service-provider-env.yml file from src\/main\/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path\/to\/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n","tokens":228,"id":4448,"Prediction":"## Decision\nWe have decided to simplify the configuration process for the Verify Service Provider by consolidating the configuration approach into a single method: using environment variables.\n\n### Rationale:\n1. **12-Factor Compatibility**: Aligning with the 12-factor app methodology, configuration through environment variables is more consistent with best practices for modern applications, providing clearer separation of configurations from the codebase.\n2. **Cloud Foundry Constraints**: Given that the java buildpack used by Cloud Foundry does not support specifying command line arguments to define config file paths, relying on environment variables avoids this restriction.\n3. **Static File Management Concerns**: Simplifying to environment variable configuration eliminates worries about Cloud Foundry's handling of static files, ensuring a more controlled and predictable deployment pipeline.\n4. **Reduction of Complexity**: Managing a single configuration method reduces complexity for both developers and operators. This change will streamline the setup process, minimize errors, and make it easier to manage configurations across different environments (e.g., development, staging, production).\n\n### Implementation:\n- The application will solely use environment variables for its configuration.\n- The verify-service-provider-env.yml file will be deprecated and removed from the repository.\n- Documentation will be updated to reflect the new single configuration method, guiding users through setting necessary environment variables.\n- Existing YAML configurations will be translated into equivalent environment variables, and support will be provided during the transition.","GenTime":"2024-07-29 01:55:11"}
{"File Name":"verify-service-provider\/0008-provide-an-end-to-end-stub.md","Context":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n\n## Decision\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":4454,"Prediction":"## Decision\n\nWe will create a dedicated testing environment that mimics the real interactions between the Service and the Hub. This will include:\n\n1. **Simulated Service Provider**: Develop a prototype Service Provider that closely resembles the typical Verify journey. This will include end-to-end user flow, from initiation to completion, ensuring it covers all primary interaction points.\n   \n2. **Mock Hub**: Configure a mock version of the Hub that can interface with the prototype Service Provider. This mock hub will respond with predefined outputs to the inputs from the Service Provider, replicating typical user scenarios and edge cases.\n   \n3. **Automated Test Data Generation**: Develop tools and scripts to automatically generate realistic test data that can flow through the Service Provider to the Hub. This test data will cover a range of user profiles and interaction scenarios.\n   \n4. **User Session Recording**: Implement mechanisms to record user activities and interactions within the testing environment. This will help in analyzing user behavior and identifying issues and opportunities for improvement.\n   \n5. **Feedback Integration**: Create a feedback loop within the prototype to collect user feedback in real-time. This feedback will be analyzed periodically to make data-driven improvements to the user journey.\n   \n6. **Usability Metrics**: Define and track key usability metrics such as task completion rates, time on task, error rates, and user satisfaction scores to quantitatively assess the prototype\u2019s performance.\n\nBy setting up this detailed testing environment, we can repeatedly conduct user research that provides valuable insights into the interface between the Service and the Hub, leading to a more refined and user-friendly final product.","GenTime":"2024-07-29 01:55:34"}
{"File Name":"event-routing-backends\/0005-PII-leakage-prevention.rst","Context":"Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n\n## Decision\n","Decision":"--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","tokens":307,"id":4477,"Prediction":"## Decision\n1. Implement an accessor method to retrieve values from Open edX events for a specified key. All information access within Open edX events will be conducted exclusively through this accessor method within both base transformers and event-specific transformers.\n2. If a required key, as per xAPI or Caliper specification, is not found within the Open edX event, the accessor method will raise an exception to handle the missing data explicitly.\n3. If an optional key, as specified by xAPI or Caliper, is not found within the Open edX event, the accessor method will log the occurrence and return `None`, ensuring optional data is managed gracefully.\n4. Future evaluation and potential incorporation of fuzzy comparison techniques between keys in transformed events and potential PII keys will be considered. This aims to strengthen PII protection by identifying and handling near matches of potential sensitive information.\n5. During code reviews, only instances where the accessor method is invoked will need to be examined for potential PII leakage, streamlining the review process and ensuring focused scrutiny on data access points.","GenTime":"2024-07-29 01:56:36"}
{"File Name":"event-routing-backends\/0006-versioning-of-event-transformers.rst","Context":"Context\\n-------\\nEvent transformers may undergo modification in future in response to consumer request, change in specification, bug fixes etc.\\nDecision\\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n\n## Decision\n","Decision":"--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","tokens":398,"id":4483,"Prediction":"Decision\n--------\nTo ensure that event transformers are accurately versioned and traceable for ongoing and future modifications due to consumer requests, changes in specifications, bug fixes, etc., the following decisions have been made:\n\n1. **Versioning Mechanism**:\n    - **System Versioning**: Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events to track changes over time.\n    - **Format**: The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the semantic version of the event-routing-backends package used to generate the event. For example, \"event-routing-backends@v(X.Y.Z)\".\n\n2. **Emission in Events**:\n    - In **xAPI statements**, this combined version can be found in `context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`.\n    - In **Caliper events**, it will be located in `extensions [ transformerVersion ]`.\n\n3. **Versioning Rules**:\n    - **Major Version (X)**: Incremented when:\n        - The transformer is changed due to an update in the original specification (xAPI or Caliper).\n        - Any key is removed from or renamed in the existing transformer.\n        - The value of a key is updated in the existing transformer.\n    - **Minor Version (Y)**: Incremented when:\n        - A key is added to an existing transformer.\n        - A new event is added for transformation.\n    - **Patch Version (Z)**: Incremented when:\n        - A bug fix is applied.\n\n4. **Change Logs**:\n    - Change logs of transformers will be maintained and documented for both xAPI and Caliper to ensure transparency and traceability of changes.\n\nThis structured approach ensures consistency, traceability, and clarity for all stakeholders interacting with the event transformers, facilitating easier maintenance and updates in response to evolving needs and requirements.","GenTime":"2024-07-29 01:56:56"}
{"File Name":"platform\/2020-08-14-implement-individual-sorting.md","Context":"## Context\\nShop owners should be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should be possible to define a system default sorting option for product listings.\\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\\n\n## Decision\n","Decision":"From now on, it is possible to define custom sortings via the administration.\\nIndividual sortings will be stored in the database in the table `product_sorting` and its translatable label in the `product_sorting_translation` table.\\nIt is possible to define a system default product listing sorting option, which is stored in `system_default`.`core.listing.defaultSorting`.\\nThis however has no influence on the default `Top Results` sorting on search pages and the suggest route result.\\nTo define custom sorting options via a plugin, you can either write a migration to store them in the database.\\nThis method is recommended, as the sortings can be managed via the administration.\\nThe `product_sorting` table looks like the following:\\n| Column          | Type           | Notes                                                 |\\n| --------------- | -------------- | ----------------------------------------------------- |\\n| `id`            | binary(16)     |                                                       |\\n| `url_key`       | varchar(255)   | Key (unique). Shown in url, when sorting is chosen    |\\n| `priority`      | int unsigned   | Higher priority means, the sorting will be sorted top |\\n| `active`        | tinyint(1) [1] | Inactive sortings will not be shown and will not sort |\\n| `locked`        | tinyint(1) [0] | Locked sortings can not be edited via the DAL         |\\n| `fields`        | json           | JSON of the fields by which to sort the listing       |\\n| `created_at`    | datetime(3)    |                                                       |\\n| `updated_at`    | datetime(3)    |                                                       |\\nThe JSON for the fields column look like this:\\n```json5\\n[\\n{\\n\"field\": \"product.name\",        \/\/ property to sort by (mandatory)\\n\"order\": \"desc\",                \/\/ \"asc\" or \"desc\" (mandatory)\\n\"priority\": 0,                  \/\/ in which order the sorting is to applied (higher priority comes first) (mandatory)\\n\"naturalSorting\": 0\\n},\\n{\\n\"field\": \"product.cheapestPrice\",\\n\"order\": \"asc\",\\n\"priority\": 100,\\n\"naturalSorting\": 0\\n},\\n\/\/ ...\\n]\\n```\\n---\\nOtherwise, you can subscribe to the `ProductListingCriteriaEvent` to add a `ProductSortingEntity` as available sorting on the fly.\\n```php\\n<?php\\nnamespace Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\Example;\\nuse Shopware\\Core\\Content\\Product\\Events\\ProductListingCriteriaEvent;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingCollection;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingEntity;\\nuse Symfony\\Component\\EventDispatcher\\EventSubscriberInterface;\\nclass ExampleListingSubscriber implements EventSubscriberInterface {\\npublic static function getSubscribedEvents(): array\\n{\\nreturn [\\nProductListingCriteriaEvent::class => ['addMyCustomSortingToStorefront', 500],\\n];\\n}\\npublic function addMyCustomSortingToStorefront(ProductListingCriteriaEvent $event): void\\n{\\n\/** @var ProductSortingCollection $availableSortings *\/\\n$availableSortings = $event->getCriteria()->getExtension('sortings') ?? new ProductSortingCollection();\\n$myCustomSorting = new ProductSortingEntity();\\n$myCustomSorting->setId(Uuid::randomHex());\\n$myCustomSorting->setActive(true);\\n$myCustomSorting->setTranslated(['label' => 'My Custom Sorting']);\\n$myCustomSorting->setKey('my-custom-sort');\\n$myCustomSorting->setPriority(5);\\n$myCustomSorting->setFields([\\n[\\n'field' => 'product.name',\\n'order' => 'desc',\\n'priority' => 1,\\n'naturalSorting' => 0,\\n],\\n]);\\n$availableSortings->add($myCustomSorting);\\n$event->getCriteria()->addExtension('sortings', $availableSortings);\\n}\\n}\\n```\\n","tokens":117,"id":4493,"Prediction":"## Decision\nA new functionality will be introduced to enable shop owners to define custom sorting options for product listings and search result pages through the administration interface, without needing to define them as services and manually tagging them.\n\n1. **Administration Interface for Sorting Options:**\n   - A user-friendly administration interface will be developed where shop owners can easily create, edit, and manage custom sorting options using flexible criteria such as price, rating, popularity, etc.\n\n2. **Default System Sorting Option:**\n   - A configuration setting will be added to the administration to allow shop owners to set a system default sorting option for product listings.\n\n3. **Enhanced Search Page Sorting:**\n   - Although `Top Results` (sorted by `_score`) will remain the default for search pages and suggest routes, an option will be included to allow shop owners to set alternative default sorting options if required.\n\n4. **Configuration Storage:**\n   - The custom sorting options and system default settings will be stored within the internal configuration management system of the software, ensuring persistence and ease of access.\n\n5. **Backward Compatibility:**\n   - Existing sorting options defined via services and tagged with `shopware.sales_channel.product_listing.sorting` will continue to function to ensure backward compatibility for users who have already integrated custom sorting options via this method.\n\nThis approach simplifies the process for shop owners, reduces technical barriers, and enhances the flexibility of product listings and search functionalities, aligning with modern usability expectations.","GenTime":"2024-07-29 01:57:29"}
{"File Name":"platform\/2021-01-21-deprecation-strategy.md","Context":"## Context\\nDefine a strategy for deprecations.\\n\n## Decision\n","Decision":"### Dogma\\n* Don't do changes without feature-flags (only exception are bugfixes)\\n* Don't break things without an alternative\\n* Don't break things in a minor release\\n* Annotate upcoming breaks as soon as possible\\n* Test all new implementations and changes\\n* Be expressive and very verbose on instructions in your inline feature flag comments\\n* There is a world outside with developers that use our public code\\n### Synopsys\\nAs we decided to work in the trunk based development from now on, there are different kinds of cases we need to consider while implementing changes to not cause any breaks while developing for future features.\\nThe main difference we have to take in account, is if we break currently behaviour with our changes or not.\\nFor this difference we have 4 different cases:\\n* Minor Changes which don't cause any breaks or deprecations\\n* Minor Changes which cause deprecations\\n* Minor Changes as part of a major feature which don't cause any breaks\\n* Major changes which cause breaks\\nFor a quick overview this is how you have to deal with the different cases.\\nConcrete Examples and further explanation follow below.\\n#### Only Minor Changes (no breaks)\\nFeature and changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etc\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Only Minor Changes (with deprecating code)\\nFeature and Changes tend to be released in a minor release and are developed in a backward compatible manner, but deprecate old code. For example a class is replaced by a new one.\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Major Changes (Breaks)\\nParts of a major feature or refactoring which breaks current behaviour. Removal of classes, methods or properties, change of signatures, business logic changes...\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* only difference between the case above is, that you have to take care about the fact, that the whole old behaviour needs to be fully functional until the next major.\\n* Write specific tests for the major flag which tests the new behaviour.\\n* Detailed description here [Detailed Rules](DetailedRules)\\n","tokens":12,"id":4497,"Prediction":"## Decision\nTo handle deprecations in a structured and predictable manner, the following strategy will be adopted:\n\n1. **Deprecation Announcement**:\n   - All deprecated features, APIs, or components will be clearly marked in the documentation and release notes.\n   - The announcement will include the reason for deprecation, the date of deprecation, and the timeline for eventual removal.\n\n2. **Grace Period**:\n   - A minimum grace period of two major release cycles will be provided before any deprecated feature is removed.\n   - During this period, users will be encouraged to transition to recommended alternatives.\n\n3. **Versioning and Compatibility**:\n   - The versioning system will follow Semantic Versioning (SemVer) guidelines. Deprecations will be introduced in minor or major releases but never in patch releases.\n   - Major releases may accompany removals of deprecated features.\n\n4. **Migration Path**:\n   - Clear migration paths with step-by-step guides will be provided for deprecated features to their supported alternatives.\n   - Code snippets, automated scripts, or tools may also be supplied to assist users in the migration process.\n\n5. **Communication**:\n   - Regular communication channels such as mailing lists, forums, and social media will be used to notify users about deprecations.\n   - Direct notifications within the software may also be employed for highly impactful deprecations.\n\n6. **Feedback Loop**:\n   - Users will be given channels to provide feedback or report issues with the deprecation plan.\n   - Feedback will be evaluated and, where necessary, the deprecation strategy may be adjusted to minimize disruption.\n\n7. **Continuous Monitoring**:\n   - Monitoring the usage of deprecated features to assess the impact and readiness of the user base for their eventual removal.\n   - Using telemetry and user surveys to gauge readiness and address concerns.\n\nThis structured approach aims to ensure that deprecations are managed in a transparent, predictable, and user-friendly manner, alleviating potential disruptions and fostering trust with the user community.","GenTime":"2024-07-29 01:57:46"}
{"File Name":"platform\/2021-09-22-refactor-theme-inheritance.md","Context":"## Context\\nCurrently, the themes can only inherit config fields from the default Storefront theme.\\nAlso, this inheritance is only a snapshot by activation time of the theme - The configs are copied to the new theme and changes to the default theme config will not appear in the new theme without a re-activation.\\nThe different possibilities to inherit different parts of a theme, like scripts, templates and config, can also cause problems on later updates.\\n\n## Decision\n","Decision":"To take this points into account, we have decided to add a new inheritance key for the `configFields` in the `theme.json` which allow a theme to inherit its config from other themes in a given order:\\n```json\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n```\\n<details>\\n<summary>Complete theme.json with part inheritances<\/summary>\\n```json\\n{\\n\"name\": \"MyDevelopmentTheme\",\\n\"author\": \"Shopware AG\",\\n\"views\": [\\n\"@Storefront\",\\n\"@Plugins\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"style\": [\\n\"app\/storefront\/src\/scss\/overrides.scss\",\\n\"@Storefront\",\\n\"app\/storefront\/src\/scss\/base.scss\"\\n],\\n\"script\": [\\n\"@Storefront\",\\n\"app\/storefront\/dist\/storefront\/js\/my-development-theme.js\"\\n],\\n\"asset\": [\\n\"@Storefront\",\\n\"app\/storefront\/src\/assets\"\\n],\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"config\": {\\n\"blocks\": {\\n\"exampleBlock\": {\\n\"label\": {\\n\"en-GB\": \"Example block\",\\n\"de-DE\": \"Beispiel Block\"\\n}\\n}\\n},\\n\"sections\": {\\n\"exampleSection\": {\\n\"label\": {\\n\"en-GB\": \"Example section\",\\n\"de-DE\": \"Beispiel Sektion\"\\n}\\n}\\n},\\n\"fields\": {\\n\"my-single-test-select-field\": {\\n\"editable\": false\\n},\\n\"my-single-select-field\": {\\n\"label\": {\\n\"en-GB\": \"Select a font size\",\\n\"de-DE\": \"W\u00e4hle ein Schriftgr\u00f6\u00dfe\"\\n},\\n\"type\": \"text\",\\n\"value\": \"24\",\\n\"custom\": {\\n\"componentName\": \"sw-single-select\",\\n\"options\": [\\n{\\n\"value\": \"16\",\\n\"label\": {\\n\"en-GB\": \"16px\",\\n\"de-DE\": \"16px\"\\n}\\n},\\n{\\n\"value\": \"20\",\\n\"label\": {\\n\"en-GB\": \"20px\",\\n\"de-DE\": \"20px\"\\n}\\n},\\n{\\n\"value\": \"24\",\\n\"label\": {\\n\"en-GB\": \"24px\",\\n\"de-DE\": \"24px\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n},\\n\"usps-positions\": {\\n\"label\":\\n{\\n\"en-GB\": \"Position\",\\n\"de-DE\": \"Position\"\\n},\\n\"scss\": true,\\n\"type\": \"text\",\\n\"value\": [\\n\"top\",\\n\"bottom\"\\n],\\n\"custom\": {\\n\"componentName\": \"sw-multi-select\",\\n\"options\": [\\n{\\n\"value\": \"bottom\",\\n\"label\":\\n{\\n\"en-GB\": \"bottom\",\\n\"de-DE\": \"unten\"\\n}\\n},\\n{\\n\"value\": \"top\",\\n\"label\":\\n{\\n\"en-GB\": \"top\",\\n\"de-DE\": \"oben\"\\n}\\n},\\n{\\n\"value\": \"middle\",\\n\"label\":\\n{\\n\"en-GB\": \"middle\",\\n\"de-DE\": \"mittel\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"tab\": \"usps\",\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n}\\n}\\n}\\n}\\n```\\n<\/details>\\n","tokens":90,"id":4498,"Prediction":"## Decision\n\n1. **Unified Configuration and Inheritance System**: Implement a live-linked inheritance system where the child themes have real-time access to the configurations of the parent\/default Storefront theme. This approach will ensure that any changes made to the default Storefront theme's configurations automatically reflect in the child themes without needing a re-activation.\n\n2. **Modular Inheritance Structure**: Establish a hierarchical modular inheritance structure that allows themes to selectively inherit specific components (e.g., scripts, templates) and configurations from parent themes. This structure will include clear separation and versioning to reduce issues that arise during updates.\n\n3. **Version Control for Config Inheritance**: Introduce a version control mechanism for theme configurations and components. This will allow for tracking of changes and rollback capabilities. When a default theme update is available, child themes can inherit the new configurations with options to review and accept changes incrementally.\n\n4. **Dependency Management System**: Develop a dependency management system to handle hooks and overrides safely. This system ensures that updates to the default theme do not break the child themes' functionality and allow for conflict detection and resolution mechanisms.\n\n5. **Documentation and Best Practices**:\n    - Provide comprehensive documentation on the new inheritance system, including examples and best practices.\n    - Offer guidance on how to implement and manage child themes to take advantage of the new live-linked inheritance model.\n\n6. **Backward Compatibility**: Ensure backward compatibility by supporting the legacy snapshot mechanism for a defined period to give developers and store owners time to transition to the new system.\n\nBy moving to this new system, we aim to streamline theme development, reduce maintenance burdens, and enhance the flexibility and robustness of theme inheritance within our platform.","GenTime":"2024-07-29 01:57:50"}
{"File Name":"platform\/2021-09-06-make-core-mail-templates-independent-from-storefront-urls.md","Context":"## Context\\nSome mail templates of the core component (Newsletter, Registration, Password Recovery, Order Status mails) depend on storefront Urls to be included in the mails.\\nThose Urls are not available when shopware is used in \"headless\" mode, without the storefront bundle being installed.\\nFor some mails (Newsletter subscription, Double Opt-In, Password recovery), the Url was made configurable over the system config and over the settings inside the administration.\\nThe default values for those Urls are the ones that the storefront bundle would use.\\nThis option does not really scale well as each Url that should be used, needs to be configurable in the administration and this can grow quickly out of hand.\\nAdditionally, it is not clear when and where those configs should be used to generate the absolute Urls, as with the BusinessEvent system and the upcoming FlowBuilder, the sending of mails is not necessarily triggered by the same entry point all the times, but different trigger can lead to sending the same mails.\\n\n## Decision\n","Decision":"There shouldn't be any links generated on PHP-side as that can be hard to override per sales-channel and can not easily be changed by apps, and links should be generated inside the mailTemplates with string concatenation instead of `raw_url`-twig functions, so the links can still be generated even if the route is not registered in the system.\\nTo make generation of urls inside the mail templated easier, we will add a `{{ domain }}` variable to the twig context, that contains the domain of the current salesChannelContext, of the order in question etc.\\nThe URLs we use in the core mail templates become part of the public API, and custom frontends should adhere to theme and provide routes under the same path, or create redirects so that the default URLs work for their frontend implementation.\\nThe default urls are:\\n```\\n\/account\/order\/{deepLinkCode} -> opens the order details of the given order\\n\/account\/recover\/password?hash={recoverHash} -> start password recovery process\\n\/newsletter-subscribe?em={emailHash}&hash={subscribeHash} -> Subscribe email with given hash to the newsletter (for douple-opt in)\\n\/registration\/confirm?em={emailHash}&hash={subscribeHash} -> Confirm registration for user eith the given mail hash (for douple-opt in)\\n```\\nIf the custom frontends can't or don't want to use our default URLs they can use the possibility to override the existing mail templates to generate custom URLs.\\nWe will deprecate the usage of the system-config configuration values and the events thrown when the links are generated on PHP-side and remove those in the next major version.\\nTo be forward compatible we will already pass the necessary data needed for generating the links into the templates, so the urls can be already generated inside the mail templates.\\nThird party clients (like the PWA) should either adhere to our default URLs or add additional mail templates, that generate the correct urls for their client.\\nIn addition to that the third party client could extend the core mail template, rather than providing a new one, and then deciding in an `IF\/ELSE` what url needs to be generated based on the salesChannel or domain.\\n","tokens":204,"id":4500,"Prediction":"## Decision\n\nTo address the issue of generating storefront URLs for emails in a scalable and manageable way when Shopware is used in headless mode, we will adopt the following approach:\n\n1. ### Centralized URL Service\n   - **Creation of a centralized URL generation service** within the core component that will be responsible for constructing absolute URLs for different emails.\n   - This service will use configurable base URLs from system configuration but will dynamically construct paths and parameters based on the email type and context.\n\n2. ### Contextual URL Construction\n   - The URL generation service will accept parameters to specify the type of email (e.g., Newsletter, Password Recovery) and any necessary dynamic segments.\n   - This ensures the URLs are constructed contextually and centrally without needing hardcoded paths or extensive admin configurations for each URL.\n\n3. ### Default & Custom Base URLs\n   - The default base URLs will be used when no specific configurations are found, maintaining the existing defaults provided by the storefront bundle.\n   - Allow admins to override the base URL in the system configuration if required, but only the base URL and not the entire path, reducing the complexity for end-users.\n\n4. ### Integration with Event System\n   - The centralized URL service needs to be integrated with the BusinessEvent system and the upcoming FlowBuilder to ensure URLs are generated correctly across different triggers and entry points.\n   - Develop hooks or event listeners that invoke the URL service to generate necessary URLs dynamically as emails are queued for sending.\n\n5. ### Documentation and Guidelines\n   - Provide detailed documentation and guidelines for developers on how to use the centralized URL generation service within custom plugins or modifications.\n   - Outline best practices for setting base URLs and ensure they are consistently used across different parts of the system.\n\nBy implementing a centralized URL generation service with contextual and configurable capabilities, we can ensure that the system remains manageable, scalable, and flexible to various operational modes, including headless deployments. This approach will also simplify the administrative overhead and provide a clear strategy for when and where URLs are generated and used within emails.","GenTime":"2024-07-29 01:57:58"}
{"File Name":"platform\/2021-12-07-admin-extension-api-standards.md","Context":"## Context\\nWe need to provide ways for extension developers to add custom components and views to different places in the administration. Multiple solutions where discussed and tested, this ADR contains a summary of the final solution.\\n\n## Decision\n","Decision":"### Word definitions\\nFor a better understanding of the following text it is good to have a definition for specific words:\\n#### Location\\nExtensions can render custom views with the Admin-Extension-API via iFrames. To support multiple views in different places every \"location\" of the iFrame gets a unique ID. These can be defined by the app\/plugin developer itself.\\n*Example:*\\nAn app wants to render a custom iFrame in a card on the dashboard. The \"location\" of the iFrame has then a specific \"locationID\" like `sw-dashboard-example-app-dashboard-card`. The app can also render another iFrames which also get \"locationIDs\". In our example it is a iFrame in a custom modal: `example-app-example-modal-content`.\\nThe app want to render different views depending on the \"location\" of the iFrame. So the app developer can render the correct view depending on the \"locationID\":\\n```js\\nif (sw.location.is('sw-dashboard-example-app-dashboard-card')) {\\nrenderDashboardCard();\\n}\\nif (sw.location.is('example-app-example-modal-content')) {\\nrenderModalContent();\\n}\\n```\\n#### PositionID (PositionIdentifier)\\nDevelopers can extend existing areas or create new areas in the administration with the Admin-Extension-API. To identify the positions which the developer want to extend we need a unique ID for every position. We call these IDs \"positionID\".\\n*Example:*\\nAn app wants to add a new tab item to a tab-bar. In the administration are many tab-bars available. So the developer needs to choose the correct \"positionID\" to determine which tab-bar should be extended. In this example the developer adds a new tab item to the tab-bar in the product detail page.\\n```js\\nsw.ui.tabs('sw-product-detail').addTabItem({ ... })\\n```\\n### Solution:\\nWe use the concept of component sections for providing injection points for extension components.\\n#### Component Sections\\nIn most cases developers will directly use the extension capabilities of the UI components (e.g. adding tab items, adding button to grid, ...). This will cover most needs of many extensions.\\nTo give them more flexibility we introduce a feature named \"Component Sections\". These are sections where any extension developer can inject components. These components are prebuilt and they can also contain custom render views with iFrames. The developer needs to use the feature and choose the matching positionID for the component position.\\n```js\\n\/\/ Adding a card before the manufacturer card with custom fields entries.\\nsw.ui.componentSection('sw-manufacturer-card-custom-fields__before').add({\\n\/\/ The Extension-API provides different components out of the box\\ncomponent: 'card',\\n\/\/ Props are depending on the type of component\\nprops: {\\ntitle: 'This is the title',\\nsubtitle: 'I am the subtitle',\\n\/\/ Some components can render a custom view. In this case the extension can render custom content in the card.\\nlocationId: 'example-app-card-before-manufactuer-custom-fields-card'\\n}\\n})\\n```\\n#### Vue Devtools Plugin for finding the PositionIDs\\nIt is impossible to create a list of all potential position IDs. And they would be hard to manage. To solve this problem we are writing a custom plugin for the Vue Devtools. This plugin will be available for Vue Devtools 6+. It makes identifying the position IDs very easy.\\nJust open the plugin in the Devtools (It is available directly when you open the Administration). Then you can see all positions at the current administration view which are available for extending. If you click at one position ID you get more information about it. Like the property in the Meteor-Extension-SDK so that you directly know what functionality this position has.\\nIn summary: the Devtool plugin provides a visual way to see which parts can be extended and what are the positionIDs for the extension position.\\n","tokens":43,"id":4507,"Prediction":"## Decision\n\nTo address the need for extension developers to add custom components and views in the administration, we will implement a plugin-based architecture. This approach was chosen after thorough evaluation of various solutions and considerations. The key aspects of this decision are as follows:\n\n1. **Plugin Registration**: Extension developers will be able to register their custom components and views through a centralized plugin registry. This registry will manage all plugins and ensure they are correctly integrated into the administration interface.\n\n2. **Extension Points**: We will define specific extension points within the administration where custom components and views can be inserted. These extension points will be part of a documented API, guiding developers on where and how to insert their extensions.\n\n3. **Configuration Files**: Plugins will be defined using configuration files (e.g., JSON or YAML) that specify the components and views to be injected, the target extension points, and any additional metadata required for integration.\n\n4. **Dynamic Loading**: To maintain performance and ensure that only necessary components are loaded, plugins will be dynamically loaded based on the registered extension points. This will prevent unnecessary loading of unused components.\n\n5. **Security and Validation**: We will implement security checks and validation mechanisms to ensure that only trusted plugins and components are integrated. This will prevent potential security risks from unauthorized or malicious plugins.\n\n6. **Backward Compatibility**: The architecture will be designed to maintain backward compatibility with existing functionality, ensuring that current users experience no disruption.\n\n7. **Documentation**: Comprehensive documentation and examples will be provided for extension developers to facilitate the creation and integration of their custom plugins.\n\nThis plugin-based architecture offers a scalable, flexible, and secure way to extend functionality within the administration, thereby empowering extension developers with the tools they need to customize and enhance the system as required.","GenTime":"2024-07-29 01:58:29"}
{"File Name":"platform\/2022-01-05-add-feature-flag-support-for-storefront-scss.md","Context":"## Context\\n* There is no possibility to check for feature flags inside the Storefront SCSS.\\n* Altering the SCSS depending on a feature flag will require workarounds like e.g. \"making up\" and additional class in the template and use the feature toggle in twig instead.\\n* The SCSS of a selector which is hidden behind a feature flag will still be in the compiled CSS.\\n* It is not easily possible to make breaking changes inside SCSS functions, mixins or variables backward-compatible with the use of feature flags.\\n\n## Decision\n","Decision":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.\\n* The feature configuration from `Feature::getAll()` is converted to a SCSS map inside `\\Shopware\\Storefront\\Theme\\ThemeCompiler::getFeatureConfigScssMap`.\\n* This SCSS map is always added to the SCSS string which gets processed by `\\Shopware\\Storefront\\Theme\\ThemeCompiler::compileTheme`.\\n* For webpack hot-proxy the `var\/config_js_features.json` is used instead.\\n* The SCSS map looks like this: `$sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);`\\n* See https:\/\/sass-lang.com\/documentation\/values\/maps\\n* A globally available function `feature()` is used to read inside the SCSS map if a desired feature is active.\\nExample:\\n```scss\\nbody {\\n@if feature('FEATURE_NEXT_1') {\\nbackground-color: #ff0000;\\n} @else {\\nbackground-color: #ffcc00;\\n}\\n}\\n```\\n","tokens":112,"id":4508,"Prediction":"## Decision\n\nGiven the constraints outlined in the context, we have decided to implement the following measures to manage feature flags within our SCSS files:\n\n1. **Leveraging Template-Based Conditionals**:\n   - We will implement feature flags at the template level using Twig. By conditionally adding or removing classes based on feature flags, we can selectively apply styles through SCSS.\n\n2. **Modularized CSS Structure**:\n   - Adopt a modular CSS approach where feature-specific styles are scoped to specific class names. This minimizes the global impact and makes it easier to manage feature-specific styles.\n\n3. **Preprocessor Script**:\n   - Introduce a preprocessor script that runs before SCSS compilation to dynamically include or exclude SCSS files based on active feature flags. This can be done using a build tool like Webpack, which can conditionally include SCSS files.\n\n4. **Use of CSS Variables**:\n   - Utilize CSS custom properties (variables) for styles that may change based on feature flags. This approach allows runtime changes in CSS without requiring recompilation.\n\n5. **Documentation and Guidelines**:\n   - Document guidelines for developers to follow when dealing with feature flags in SCSS. This includes how to structure their SCSS files and how to use Twig for conditional classes.\n\nBy following these strategies, we aim to achieve a more maintainable and flexible approach to handling feature flags within our SCSS, ensuring backward compatibility and minimizing unnecessary CSS in production builds.","GenTime":"2024-07-29 01:58:33"}
{"File Name":"platform\/2020-08-28-import-acl-privileges-from-other-roles.md","Context":"## Context\\nSome modules have components which require many acl privileges. Examples\\nare the rule builder or the media manager. Therefore, you need all privileges\\nin each module which have these components. Also you do not want to add the\\nmodule to the dependency section because then the user has full access to module\\nin the administration.\\n\n## Decision\n","Decision":"To avoid duplication of these privileges we use a helper function. These\\nfunction returns all privileges from the other module dynamically. You can\\nuse it directly in the privileges:\\n```js\\nShopware.Service('privileges')\\n.addPrivilegeMappingEntry({\\ncategory: 'permissions',\\nparent: null,\\nkey: 'promotion',\\nroles: {\\nviewer: {\\nprivileges: ['promotion:read',],\\ndependencies: []\\n},\\neditor: {\\nprivileges: [\\n'promotion:update',\\nShopware.Service('privileges').getPrivileges('rule.creator')\\n],\\ndependencies: [\\n'promotion.viewer'\\n]\\n}\\n}\\n});\\n```\\n","tokens":68,"id":4525,"Prediction":"## Decision\n\nTo address the requirement of granting specific ACL (Access Control List) privileges for components such as the rule builder and media manager without providing full access to the entire module, the following decision is made:\n\n1. **Granular ACL Management System**:\n   - Implement a granular ACL management system that allows for fine-grained control over access privileges on a per-component basis rather than at the module level.\n   - Define and assign specific permissions for critical components within modules like the rule builder and media manager.\n\n2. **Component-Level Permission Definitions**:\n   - Construct ACL policies that are detailed enough to specify privileges for individual components.\n   - Maintain a list of privileges required for each component to function correctly, and ensure these are assignable independently.\n   \n3. **Administrative User Interface Enhancements**:\n   - Update the administrative user interface to allow administrators to assign these granular privileges without needing to grant full module access.\n   - Include clear documentation and user guides to facilitate ease of use for administrators managing these permissions.\n\n4. **Dependency Management**:\n   - Avoid adding the entire module to the dependency section to prevent automatic assignment of full access rights.\n   - Instead, use precise dependency injection where only the necessary components and their associated privileges are included.\n\n5. **Security Policies and Audits**:\n   - Regularly review and audit the ACL settings to ensure they align with security policies and do not inadvertently provide excess privileges.\n   - Maintain logs of access and changes to ACL settings for accountability and to facilitate audits.\n\nBy implementing a granular ACL management system and enhancing administrative interfaces to support component-level permissions, we ensure that necessary privileges are assigned without compromising the overall security and control within the administration.","GenTime":"2024-07-29 01:59:25"}
{"File Name":"tove\/adr-03.md","Context":"## Context\\nWhen a transcription is approved, a set of flat files containing the transcription data will be saved to Azure. Users will have the option to download a zip file containing their requested subject, group, workflow, or project. Depending on the speed at which we are able to zip the necessary files, we will either trigger a direct download, or provide a link to the location of the zip file to the user.\\nThe goal is to investigate Azure\u2019s storage options (specifically Blob Storage and File Services) and decide which tool is best suited for our needs.\\n### Factors to consider:\\n* How easy is it to share a file to the end user? What is the process for this?\\n* Ease of use, how complicated is it to set up, maintain, edit\\n* access permission features\\n* Speed of accessing and iterating through files (e.g. getting all files in a given directory)\\n### Terminology:\\n**Blob:** acronym for \u201cBinary Large Object\u201d\\n**Container:** synonym for \u201dS3 Bucket\u201d\\n**Shared Access Signature:** similar functionality as \u201cS3 Presigned URLs\u201d\\n\n## Decision\n","Decision":"We don't appear to have any need for most of the additional functionality that comes with File Service, which makes me reluctant to want to use it. In addition, the number of articles and resources available on communicating with Blob Storage to set up file zipping is much greater than what's available for File Service. My initial understanding of Blob Storage led me to believe that permissions could only be set at the container level, but this turned out to be wrong. With the ability to set blob-specific permissions, we will be able to use a single container to store the transcription-specific files, and the user-requested zip files.\\nUltimately, my choice is to go with Blob Storage: the more basic, simple storage tool that gives us what we need and nothing more. That being said, I'd still like to keep the option of using Azure File Service on the table, in case it turns out that we *would* benefit from the additional functionality that it offers.\\nAs for what type of blob we will use, my choice would be to store each data file in its own block blob. If we were to choose to store multiple files within a single blob (and have each file be associated with a block ID on that blob), we would lose the ability to name each individual file. Hypothetically, it would be possible to create a database table with columns \u201cblock ID\u201d and \u201cname\u201d, to emulate a naming functionality, but this seems far more complicated than its worth. In addition, the [azure-storage-blob](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob) gem gives us a simple interface for working with block blobs and saves us the trouble of having to write HTTP requests ourselves.\\nFinal questions:\\n1. Q: Blob Storage doesn't have any concrete hierarchy beyond Storage Account\/Blob Container - within a container, directories are virtual, demarcated by prefixes in the file name. Will this end up being problematic for us? Will it complicate file retrieval?\\nA: Retrieving files from a file system with virtual directories shouldn't be any different than retrieving files from a normal file system. As long as blob prefixes are constructed in a way that reflects the organizational system used within the application\/database, there should be no trouble. File retrieval may be helped by append blobs - final decision on blob type is still TBD.\\n2. Q: Would there be any benefit to caching files on on-premises file servers? If this sounds like something we'd like to employ, it would be worth reconsidering Azure File Service.\\nA: This doesn't appear to be something we will need.\\n### Links and Articles:\\n1. [Microsoft: Deciding when to use Azure Blobs, Azure Files, or Azure Disks](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-decide-blobs-files-disks)\\n2. [Azure Files FAQ](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/files\/storage-files-faq) (see \u2018Why would I use an Azure file share versus Azure Blob Storage for my data?\u2019)\\n3. [Stack Overflow: Blob Storage vs File Service](https:\/\/stackoverflow.com\/questions\/24880430\/azure-blob-storage-vs-file-service)\\n4. [Microsoft: Introducing Azure File Service](https:\/\/blogs.msdn.microsoft.com\/windowsazurestorage\/2014\/05\/12\/introducing-microsoft-azure-file-service\/) (scroll to When to use Azure Files vs Azure Blobs vs Azure Disks)\\n5. [Microsoft: Azure Storage scalability and performance targets for storage accounts](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-scalability-targets)\\n6. [Azure Blob Overview](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-overview)\\n7. [Azure Blob Introduction](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-introduction)\\n8. [How to mount Blob storage as a file system with blobfuse](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-how-to-mount-container-linux)\\n9. [Block blobs, append blobs, and page blobs](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/understanding-block-blobs--append-blobs--and-page-blobs)\\n10. [Azure Blob Storage gem](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob)\\n","tokens":231,"id":4533,"Prediction":"## Decision\nAfter evaluating Azure\u2019s storage options, we have decided to utilize Azure Blob Storage for saving and distributing transcription data to users. Here's the rationale behind our decision:\n\n1. **Ease of Sharing Files:**\n   - **Blob Storage:** Azure Blob Storage provides straightforward mechanisms to share files with end users. Using Shared Access Signatures (SAS), we can generate secure, time-limited URLs that allow users to access the designated files directly. This aligns well with our requirement to provide either a direct download or a link to the zip file.\n   - **File Services:** While Azure File Services can also share files, it is generally more suited for file sharing within specific networks rather than public sharing via URLs.\n\n2. **Ease of Use and Maintenance:**\n   - **Blob Storage:** Is designed for internet-scale storage with REST API support, making it simpler and more flexible to set up, maintain, and integrate into our application. The SDKs for various programming languages are robust and well-documented.\n   - **File Services:** Has a more complex setup, particularly with SMB (Server Message Block) protocols, which are typically used for network shares rather than public access.\n\n3. **Access Permissions:**\n   - **Blob Storage:** Supports granular access control through SAS tokens and integrates natively with Azure Active Directory. This allows us to finely control who has access to which files, ensuring we can meet varying user permissions and access controls.\n   - **File Services:** Also supports access control through NTFS permissions and integration with Azure Active Directory, but this setup is generally more intricate and is optimized for enterprise file shares rather than flexible web-based access.\n\n4. **Speed and File Access:**\n   - **Blob Storage:** Optimized for rapid access and retrieval of files at scale. Operations like zipping files and iterating through directories can be handled efficiently with features such as blob listing, hierarchical namespaces (for premium accounts), and parallel processing.\n   - **File Services:** Primarily designed for shared file storage and access model which might introduce additional latency and complexity in large-scale, high-frequency access scenarios that our application demands.\n\nOverall, Azure Blob Storage emerges as the better-suited option for our needs concerning accessibility, ease of use, maintenance, permission management, and performance. With Blob Storage, we can efficiently manage and distribute transcription files while ensuring a seamless user experience.","GenTime":"2024-07-29 01:59:49"}
{"File Name":"ockam\/0007-rust-error-handling.md","Context":"## Context\\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\\nThere are multiple requirements to error handling:\\n- agile enough to be used in different cases\\n- portable to work in different environments with different constraints\\n- convenient for both library developer and library user\\n\n## Decision\n","Decision":"In search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\\n","tokens":91,"id":4536,"Prediction":"## Decision\n\n### Selected Error Handling Strategy\nThe decision is to implement a custom error handling mechanism within the Ockam Rust library that meets the outlined requirements:\n\n1. **Enum-Based Error Types:** Develop a core `Error` enum that categorizes different error scenarios across all crates within the Ockam Rust library. Each error category will have variants that encapsulate specific error cases, allowing detailed classification and handling of errors.\n\n2. **Trait-Based Error Handling:** Introduce a common trait, say `OckamError`, that will be implemented across various modules and third-party crates. This trait will ensure a consistent interface for error handling while allowing modules to define specific error transformations and contexts.\n\n3. **Error Propagation with Contextual Information:** Utilize `thiserror` crate or custom implementations to enrich errors with contextual information, making it easier for developers to diagnose issues. This will help in maintaining a balance between detailed debug information and user-friendly error messages.\n\n4. **Interoperability with Other Error Types:** Ensure interoperability with widely-used Rust error handling practices, like the `anyhow` and `std::error::Error` traits. This includes converting our custom errors into a format that can be easily manipulated and understood in various contexts, especially when integrating with other libraries or environments.\n\n5. **Portable and Lightweight:** Design the error handling to be portable and lightweight, avoiding dependencies on large or non-portable crates. This ensures that the error handling mechanism works seamlessly across different operating systems and environments where Ockam might be deployed.\n\n6. **Ease of Use:** Provide comprehensive documentation and examples illustrating best practices for error handling with the Ockam Rust library. Simplified macros or utility functions could be introduced to reduce boilerplate and make error handling easier for both library developers and end-users.\n\n### Implementation Steps\n1. Define a core `Error` enum that will live in a central crate (e.g., `ockam_core`).\n2. Implement the `OckamError` trait across Ockam crates and enforce its use in third-party crates.\n3. Leverage macros or derive utilities (like `thiserror`) for standardized error messages and contextual information.\n4. Ensure conversions between Ockam errors and standard Rust error handling traits.\n5. Validate the portability and performance of error handling through benchmarks and cross-platform tests.\n6. Create comprehensive documentation and examples to guide developers.\n\nBy adopting this strategy, we can achieve a robust, portable, and user-friendly error handling system that aligns with Ockam\u2019s requirements.","GenTime":"2024-07-29 01:59:56"}
{"File Name":"ockam\/0005-routing-data-formats.md","Context":"## Context\\nWe need a standard format for messages to be exchanged by the routing protocol.\\nThis format would be used by routers on different implementations.\\n\n## Decision\n","Decision":"We use the following formats:\\nFor a message:\\n```\\n{\\nonward_route: Route,\\nreturn_route: Route,\\npayload: Any\\n}\\n```\\nWhere\\n`Route` - an ordered list of addresses.\\nFor an address:\\n```\\n{\\ntype: Integer,\\ndata: Any\\n}\\n```\\n","tokens":31,"id":4538,"Prediction":"## Decision\n\nWe will adopt the JSON (JavaScript Object Notation) format for all messages exchanged by the routing protocol. JSON is widely supported across different programming languages and platforms, providing ease of integration and ensuring interoperability between routers from different implementations. \n\n### Rationale:\n- **Readability**: JSON is human-readable and easy to understand, which simplifies debugging and monitoring of routing messages.\n- **Language Agnostic**: JSON supports multiple programming languages, ensuring that routers implemented in different languages can seamlessly exchange messages.\n- **Tools and Libraries**: There is a vast ecosystem of tools and libraries available for JSON parsing and validation, which will facilitate rapid development and testing.\n- **Lightweight**: JSON has a lightweight data-interchange format, ensuring that message overhead is minimal and does not significantly affect performance.\n\n### Implementation:\n1. Define a standardized schema for routing messages in JSON format.\n2. Ensure all router implementations adhere to this schema for message creation, parsing, and validation.\n3. Establish versioning in the schema to accommodate future expansions and changes.\n4. Perform extensive interoperability testing among routers implemented by different teams to ensure compliance with the JSON message format standard.\n\nBy adopting JSON as the standard message format, we ensure a robust, scalable, and maintainable solution for our routing protocol's communication needs.","GenTime":"2024-07-29 02:00:01"}
{"File Name":"ockam\/0001-record-architectural-decisions.md","Context":"## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n\n## Decision\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that\\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\\ntechniques.\\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\\n","tokens":21,"id":4539,"Prediction":"## Decision\nTo document architectural decisions systematically, we will implement the use of Architectural Decision Records (ADRs). Each ADR will be a standalone markdown file stored in the project's repository within a dedicated `docs\/adr` directory. This approach ensures that all decisions are easily accessible, organized, and undergo version control. The ADRs will follow a consistent template, including sections such as Context, Decision, Status, Consequences, and any other relevant information to provide comprehensive and clear context for each decision made during the development of Ockam.","GenTime":"2024-07-29 02:00:04"}
{"File Name":"unfinished-design-system\/001-components-documentation.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n## Decision\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Storybook](https:\/\/storybook.js.org\/). This tool helps us document our components and develop them since we can make a simple canvas to interact with it.\\nSome major companies with mature design systems use this tool, which could be a precious asset.\\n","tokens":53,"id":4544,"Prediction":"## Context\nOur team is developing a new web application that needs to handle a high volume of traffic and large amounts of data processing. The primary goal is to ensure scalability, reliability, and performance. We have evaluated various architectural styles and technology stacks, including monolithic architecture, microservices architecture, and serverless architecture. Additionally, we have considered multiple database options\u2014relational (SQL) and non-relational (NoSQL)\u2014to store and manage our data effectively.\n\n## Decision\nWe have decided to adopt a microservices architecture for the new web application combined with a hybrid database approach. Specifically: \n\n1. **Microservices Architecture:** This will help in achieving scalability and reliability as individual services can be developed, deployed, and scaled independently. Each microservice will own its particular functionality and can communicate with other services through APIs.\n   \n2. **Hybrid Database Approach:**\n   - **Relational Database (PostgreSQL):** For transactional and structured data where ACID compliance is crucial, PostgreSQL will be used. This ensures data integrity and supports complex queries.\n   - **Non-relational Database (MongoDB):** For handling large volumes of semi-structured or unstructured data, MongoDB will be used. It provides flexibility and horizontal scalability.\n\nThe choice of microservices architecture and a hybrid database approach will allow us to meet our application's performance, scalability, and reliability requirements while effectively managing different types of data.\n\n## Status\nApproved and in the initial phase of implementation.\n\n## Consequences\n- **Positive:**\n  - Improved scalability and reliability through independent services.\n  - Flexibility in data management using the most suitable database for different needs.\n  - Easier deployment and maintenance of smaller, independently testable components.\n  \n- **Negative:**\n  - Increased complexity in development and operations due to managing multiple services and databases.\n  - Potential latency issues with inter-service communication.\n  - Requirement for robust DevOps practices to handle continuous integration and deployment.\n\n## More reading\n- \"Building Microservices\" by Sam Newman\n- \"Designing Data-Intensive Applications\" by Martin Kleppmann\n- Official PostgreSQL and MongoDB documentation\n\n## Updates\n- Regular monitoring and review of the microservices performance and database efficiency.\n- Future updates will include the integration of a service mesh for better orchestration and monitoring of microservices.\n","GenTime":"2024-07-29 02:00:25"}
{"File Name":"unfinished-design-system\/002-unit-testing-tools.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n## Decision\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use Jest with Enzyme.\\nJest is an incredible unit testing tool for Javascript. It is easy to use, has a great syntax, and our team is familiar with it.\\nEnzyme, on the other hand, is a unit testing tool for React components. It has some libraries that help us debugging and testing those resources.\\nWe've decided on this stack because our team has already used it in the past, and it can improve our development speed.\\n","tokens":53,"id":4547,"Prediction":"## Context\n\nThe current software architecture does not support real-time data processing, which has become a critical requirement due to the increase in customer demands for real-time insights and analytics. The existing batch processing system causes delays and impacts the user experience and operational efficiency. Additionally, the current system architecture does not effectively handle spikes in data volume, leading to performance bottlenecks.\n\n## Decision\n\nTo address the real-time data processing requirement, we will transition our architecture to a stream processing model using Apache Kafka and Apache Flink. This model will enable the efficient handling of continuous data streams, provide real-time analytics capabilities, and improve system scalability to handle variable data loads effectively.","GenTime":"2024-07-29 02:00:38"}
{"File Name":"unfinished-design-system\/006-yarn-workspaces.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n## Decision\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Yarn Workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.\\n","tokens":63,"id":4549,"Prediction":"## Context\nWe are currently developing a web application that requires real-time data updates and bi-directional communication between clients and servers. The application will be used in a fast-paced environment with multiple users interacting simultaneously. Performance, scalability, and reliability are crucial, as downtime or delays can severely affect the user experience. Additionally, the system should be easy to maintain and extend without significant overhead.\n\n## Decision\nWe have decided to implement WebSockets for real-time communication between the client and the server. WebSockets provide a full-duplex communication channel over a single, long-lived connection, which is ideal for scenarios requiring rapid data exchange. This will enable us to push updates to clients instantly and handle bi-directional communication efficiently.\n\nWe will use a popular library, such as Socket.IO for Node.js, which simplifies the implementation of WebSockets and provides fallbacks for environments where WebSockets are not available. This choice ensures compatibility and robust handling of different client environments.\n\nAdditionally, we will set up a scalable infrastructure using a cloud service provider. This will include load balancers to distribute the traffic and multiple instances of our application to handle concurrent connections effectively.\n\n## Status\nApproved\n\n## Consequences\n1. **Positive Consequences:**\n    - **Real-time Updates:** Users will experience rapid data updates, enhancing the responsiveness of the application.\n    - **Efficient Communication:** Reduced overhead compared to traditional HTTP polling, improving performance.\n    - **Scalability:** The chosen infrastructure will allow the application to scale horizontally, handling increased traffic seamlessly.\n    - **Compatibility:** Using Socket.IO ensures broader browser compatibility and fallback mechanisms, increasing reliability.\n\n2. **Negative Consequences:**\n    - **Complexity:** Implementing and maintaining WebSockets introduces additional complexity compared to REST APIs.\n    - **Resource Intensive:** Long-lived connections can be more resource-intensive, requiring careful management of server resources.\n    - **Debugging:** Issues related to real-time communication can be harder to debug and troubleshoot.\n\n## Experience Report\nOur team has prior experience with using WebSockets and cloud-based scalable infrastructures. In previous projects, the adoption of WebSockets significantly improved the user experience by providing instant updates and reducing latency. However, we also encountered challenges related to resource management and debugging, which we plan to address by leveraging existing tools and best practices.\n\n## More Reading\n- [WebSockets Specification](https:\/\/tools.ietf.org\/html\/rfc6455)\n- [Socket.IO Documentation](https:\/\/socket.io\/docs\/)\n- [Scalable WebSocket Architecture](https:\/\/www.w3.org\/DesignIssues\/WebSocket.html)\n\n## Updates\n- _Date of Decision:_ October 1, 2023\n- _Next Review Date:_ April 1, 2024\n- Any changes or updates to this decision will be logged here.","GenTime":"2024-07-29 02:00:45"}
{"File Name":"unfinished-design-system\/004-tokenization-and-static-assets.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n## Decision\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Style Dictionary](https:\/\/amzn.github.io\/style-dictionary\/#\/) as our framework to handle tokens. It is easy to use since we can define our tickets in standard JSON but empowering them with string interpolation, variables, and other features.\\nFor our static assets, we're going to host them at [AWS S3](https:\/\/aws.amazon.com\/s3\/), defining the proper CORS rules, and refer the location of those as tokens for our applications to use.\\nInstead of increasing the size of our Javascript bundle with static assets, we prefer to keep it simple and light by hosting those in an S3 bucket and asking for the application to download it.\\n","tokens":53,"id":4550,"Prediction":"To await the provided context, I'll outline a general approach for making a sound architectural decision for a software project.\n\n## Decision\n\nBased on the available context and the specific needs of our project, we have decided to <**insert your architectural decision here**>. This choice aligns with our strategic goals and the functional and non-functional requirements specified for this phase of the project.\n\nBy making this decision, we aim to achieve the following key benefits:\n\n1. **Scalability**: Ensuring our system can handle increased load and user base.\n2. **Maintainability**: Simplifying updates and reducing potential technical debt.\n3. **Performance**: Meeting or exceeding the required performance metrics.\n4. **Security**: Protecting user data and maintaining system integrity.\n5. **Cost-efficiency**: Optimizing our resource utilization to stay within budget.\n\n---\n\nKindly provide the context for a more tailored decision.\n","GenTime":"2024-07-29 02:00:47"}
{"File Name":"scholarsphere\/0003-no-derivatives.md","Context":"## Context\\nWhen viewing different file types over the web, large video files or other non-standard files, may not display well in\\nthe browser, so the common solution is to create derivatives for a better user experience. These also include thumbnails\\nfor a sample picture of what the resource looks like.\\nManaging and creating derivatives can be hard and requires careful architectural considerations.\\nHowever, the modern web has adapted to larger files, and generally handles a wider array of file types than it did in\\nthe past. Also, thumbnails only work for image and video formats. Generic icon-based images for any file type can be\\nmore informative.\\n\n## Decision\n","Decision":"Scholarsphere will not create any derivative files. Icons will be used to represent file types, and the original\\nuploaded file can be rendered when applicable.\\n","tokens":129,"id":4553,"Prediction":"## Decision\n\nBased on the provided context, the decision regarding handling large video files and non-standard files over the web is as follows:\n\n1. **Use Original Files with Modern Browser Capabilities**: Given the advancements in web technologies, the system will utilize the native capabilities of modern browsers to handle larger files directly without creating derivatives. This will minimize storage and processing overhead.\n\n2. **Generate Generic Icons for File Types**: Instead of creating thumbnails specifically for image and video formats, the system will incorporate a library of generic icons that represents various file types. This will ensure that all file types have a visual representation that can be understood by the user.\n\n3. **On-Demand Derivative Creation for Specific Use Cases**: While generally avoiding derivatives, the system will have the capability to generate derivatives on-demand for specific use cases where displaying or processing the original file is impractical. This feature will be implemented as a background task to avoid user-facing delays.\n\n4. **Implement Caching Strategies**: To enhance performance, caching strategies will be employed where frequently accessed files and derivative content are stored temporarily. This ensures faster load times without necessitating permanent storage of derivative files.\n\n5. **User-Friendly Error Handling**: In cases where the file type is not supported or the file is too large to be handled effectively, the system will provide user-friendly error messages and potential solutions, such as downloading the file directly.\n\nBy embracing modern browser capabilities, the system achieves a balance between performance and resource management while maintaining a high-quality user experience.","GenTime":"2024-07-29 02:00:59"}
{"File Name":"scholarsphere\/0009-acl-actor-permissions.md","Context":"## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n\n## Decision\n","Decision":"Access controls and depositor or proxy depositor rights are independent from one another.\\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\\n","tokens":79,"id":4554,"Prediction":"## Decision\nTo efficiently and effectively manage access permissions on works and collections, a hybrid access control system will be implemented. This system will amalgamate both author-based permissions and ACL-based permissions. The key decision points are as follows:\n\n1. **Layered Permission Checks**: The system will first evaluate author-based permissions, which are directly set by the depositor or proxy depositor. If permissions are not explicitly defined by the author, the system will then consult ACL-based permissions to determine access based on user or group identity.\n   \n2. **Priority Handling**: When both author-based permissions and ACL-based permissions are defined for a resource, author-based permissions will take precedence. This ensures that the creator\u2019s intent regarding access is respected.\n   \n3. **Simplified Permission Assignment**: A unified interface will be developed where depositors can specify access controls while creating or modifying a resource. This interface will allow depositors to see existing ACLs and adjust as needed to ensure their desired level of control is maintained.\n\n4. **Concurrency and Consistency**: Ensure that checks for both permissions sources are performed concurrently to optimize performance without sacrificing consistency. The system must be coherent, providing consistent access decisions even when permissions change.\n\n5. **Auditing and Transparency**: Maintain an audit log that records decisions from both permission sources and the resultant access determination. This provides transparency and aids in troubleshooting and compliance.\n\n6. **Fallback Mechanism**: In cases where both author-based permissions and ACL-based permissions are ambiguous or conflicting, the system will default to more restrictive access to ensure security is not compromised.\n\nBy implementing a robust and hybrid approach to permission management, we aim to balance the flexibility required by depositors to control their works and collections with the administrative oversight provided by ACL-based security.","GenTime":"2024-07-29 02:01:03"}
{"File Name":"scholarsphere\/0002-define-use-of-decorators-and-presenters.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nWe need to distinguish between decorator and presenter objects in order to clarify which would be used in a given\\nsituation.\\n\n## Decision\n","Decision":"Decorators extend SimpleDelegator and will always delegate undefined methods to the delegated object.\\nPresenters take the form of \"plain ol' Ruby objects\" (POROs) and would generally not delegate methods to an object.\\nTheir usage is designed to be more flexible when the rendering of content isn't tied specifically to one object.\\n","tokens":46,"id":4556,"Prediction":"## Decision\n\nWe will implement a clear and distinct naming convention for both decorator and presenter objects within our codebase to avoid any confusion regarding their usage and responsibilities. Specifically:\n\n1. **Decorator Objects:**\n   - Naming Convention: All decorator classes will have a suffix of `Decorator`.\n   - Purpose: Decorator objects will be used to extend the behavior of core objects without modifying their structure or code.\n   - Example: `UserProfileDecorator` - This class might add additional formatting or enrichment to a user's profile data.\n\n2. **Presenter Objects:**\n   - Naming Convention: All presenter classes will have a suffix of `Presenter`.\n   - Purpose: Presenter objects will manage the presentation logic, preparing data in a format that is ready for the view layer.\n   - Example: `UserProfilePresenter` - This class will take raw user profile data, process it, and then provide it in a way that is ready for display in the user interface.\n\nBy adhering to this naming convention, we provide immediate clarity on the role each class is meant to play, facilitating better code maintenance, readability, and collaboration among team members. Additionally, we will include documentation to explain the responsibilities of decorators and presenters, and provide examples to guide future development.","GenTime":"2024-07-29 02:01:10"}
{"File Name":"scholarsphere\/0008-asset-pipeline.md","Context":"## Context\\nWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to the\\nfact that it was present as both a gem and an npm package.\\n\n## Decision\n","Decision":"We removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuery\\nvia yarn.\\n","tokens":41,"id":4557,"Prediction":"## Decision\nTo resolve the security vulnerability and the duplication of Bootstrap, we have decided to standardize on using npm packages for both jQuery and Bootstrap.\n\n### Steps to be taken:\n1. **Remove Bootstrap gem**: Uninstall the Bootstrap gem from the project to avoid conflicts and duplication.\n2. **Update package dependencies**: Use npm to manage versions of both jQuery and Bootstrap. Ensure that both are updated to the latest stable versions that address the security vulnerabilities.\n3. **Test application thoroughly**: Execute a full suite of tests and manual checks to ensure that removing the gem and upgrading the npm packages do not introduce any regressions or new issues.\n4. **Document the change**: Update any project documentation to reflect the switch from using the Bootstrap gem to npm packages. This should also include instructions for setting up the environment with npm packages. \n\nBy standardizing on npm for package management, we improve consistency and ensure we can address security vulnerabilities in a timely manner.","GenTime":"2024-07-29 02:01:13"}
{"File Name":"elm-spec\/003_loading_harness.md","Context":"## Context\\nRight now, in order to run an elm-spec program, we need to do a few things in a definite order.\\nAt some point, compile the Elm code using the `Compiler` from elm-spec-core. This will\\nwrap the compiled code so that the parts of the program that interact with the outside world\\ncan be easily faked out.\\nThen, to get the program running: First, create an `ElmContext` object. This creates\\nall the fake objects on the `window`\\nobject that the compiled elm code will attempt to reference. Second, evaluate the compiled\\nelm code. It doesn't matter when we compile the elm code, of course, just that it is\\nevaluated in the browser environment *after* we have created a new `ElmContext` in that\\nenvironment.\\nSo, it's a little wild, I guess, that simply instantiating an `ElmContext` modifies the `window`\\nobject and so on.\\nPart of the need for this comes from the fact that the compiled Elm code is wrapped in an IFFE.\\nBut there's no reason why we actually have to do that ...\\nWe've been able to deal with this problem so far because the only things that need to go\\nthrough this process are elm-spec-runner and karma-elm-spec-framework. But with the harness,\\nwe are now asking a test writer to follow this process as well. For that reason, we need to\\nsimplify it so it's not a source of errors.\\n\n## Decision\n","Decision":"We should change this flow so that we don't need to create an `ElmContext` and evaluate the\\ncompiled Elm code in a particular order.\\nFirst, we will wrap the compiled Elm code in a function that takes an `ElmContext`. Evaluating\\nthis code will still attach the `Elm` object to the window (since we're providing it with a\\nproxy object). But by using a normal function here, we have more control over when the `Elm`\\nobject is loaded.\\nThen, we need to have `ElmContext` store references to all the fakes inside itself -- there's no\\nreal need to store these on the `window` object. So the only things we need to store on the `window`\\nare (1) the function to load Elm -- because this is how the compiled Elm code provides the function;\\nI don't think we can reference it any other way. (2) Data on what the compiler actually\\ntried to do, like what files it tried to compile, the path and all that; we store this on the window\\nso it's available as soon as the compiled code is evaluated -- there may be better ways to do this.\\nAnd (3) the function for loading files, which has to be a function accessible on the window\\nanyway (since that's how Playwright and JSDOM allow us to register a function to be executed in Node).\\nOnce we do this, then it turns out that the test writer doesn't need to create an `ElmContext`\\nat all ... we can have `HarnessController` do that when `prepareHarness` is called. In fact,\\nwe don't even need to bundle anything extra. This will be included in the bundle of tests that\\nget executed in the browser.\\nWe could also change SuiteRunner to create it's own `ElmContext` as well, but it's not necessary.\\n","tokens":318,"id":4592,"Prediction":"## Decision\n\nWe will simplify the process required to run an Elm program by encapsulating the current multi-step sequence into a straightforward, single-step function. This function will handle all necessary preparation and evaluation steps to minimize the risk of mistakes and to improve the user experience for test writers. Specifically, we will:\n\n1. **Create a Unified Function:** Develop a new function, perhaps named `runElmSpec`, which combines the steps of creating an `ElmContext`, and evaluating the compiled Elm code.\n2. **Modify Elm Compilation:** Adjust the Elm compilation process so the compiled code is not wrapped inside an IIFE (Immediately Invoked Function Expression). Instead, it will be structured in a way that facilitates smoother integration with the `ElmContext`.\n3. **Automatic Context Creation:** Ensure that `runElmSpec` automatically instantiates `ElmContext` and correctly sets up the necessary objects on the `window` object before evaluating the Elm code.\n4. **Eliminate Direct Window Modifications:** Implement a method to simulate the necessary environment for the Elm program without manually modifying the global `window` object. This can be achieved by using a more controlled environment setup within `ElmContext`.\n5. **Provide Clear Documentation:** Include comprehensive documentation and examples for the new `runElmSpec` function to ensure developers and test writers understand the simplified process.\n\nBy implementing these changes, we make the Elm-spec framework more accessible and reduce the potential for user error, thereby facilitating smoother test writing and execution.","GenTime":"2024-07-29 02:03:03"}
{"File Name":"octagon\/0002-components-shall-be-stateless-by-default.md","Context":"## Context\\nOctagon strives to be a view layer for consumers.  Applications manage their state via their own independent mechanisms.  State embedded within Octagon components may reduce the declaritive ability of said components & violate the top-down, _\"config via props\"_ React mantra.  Consequently, stateful components generally deviate from the pure view layer goals we wish to provide.  Embedded state reduces the ablitity for apps to \"declare\" Octagon components into view state.\\nExample, consider if a `<OctoNavItem \/>` maintains state on whether or not that navigation page is active or not.  If a user reloaded the page, an active item may suddenly show as not active, because that component's state is lost on reload.\\nInstead, the API should be stateless, and be used like: `<OctoNavItem active \/>`.\\nNow, the user of the view layer is in control.\\n\n## Decision\n","Decision":"Make things stateless.  In rare cases, some emphemeral state is OK to maintain internally to components.  These should be highlighted, drawn out, and discussed before merge.  Ideally, these issues should be raised during design, pre-merge if a feature branch is longer running.\\n","tokens":189,"id":4596,"Prediction":"## Decision\n\nOctagon components will be designed as stateless components. This means they will not manage or store their internal state, but will instead derive their configuration and behavior entirely through props passed to them by their parent components. This approach ensures:\n\n1. **Alignment with React Principles**: It adheres to the top-down \u201cconfig via props\u201d mantra, maintaining the declarative nature of React.\n   \n2. **Predictability and Reliability**: Stateless components remain predictable and retain consistent behavior, as their state is managed externally, ensuring that reloading a page or other similar actions do not result in unexpected changes in component behavior.\n\n3. **Enhanced Reusability**: Stateless components lead to a more modular and reusable architecture since the state management logic is decoupled.\n\n4. **User Control**: By keeping state management in the hands of the applications using Octagon, we give users full control over how and when their components render and behave, fitting seamlessly into a variety of application architectures and state management solutions.\n\nSo, the `<OctoNavItem \/>` component will be used as `<OctoNavItem active \/>`, allowing the application to control the active state explicitly instead of the component managing its state internally.","GenTime":"2024-07-29 02:03:08"}
{"File Name":"octagon\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4597,"Prediction":"## Decision\nTo ensure all the architectural decisions are systematically documented for future reference and knowledge sharing, we will implement an Architectural Decision Record (ADR) process. Here are the steps we will follow:\n\n1. **ADR Template Adoption**: We will use a standard ADR template to maintain consistency across all decisions. This template will include sections such as Context, Decision, Consequences, Alternatives Considered, and Status.\n\n2. **Tool Selection**: We will use a version control system (such as Git) for maintaining ADRs. Each ADR will be saved in a dedicated directory (e.g., `docs\/adr`) and will be sequentially numbered for easy tracking.\n\n3. **Documentation Standards**: All team members must adhere to the following guidelines when writing ADRs:\n   - Clearly state the context and the issue that necessitates the decision.\n   - Provide a detailed description of the chosen solution and its justification.\n   - List any alternative solutions considered and reasons for not choosing them.\n   - Discuss the consequences of the decision, including potential trade-offs.\n\n4. **Review Process**: Before finalizing an ADR, it must be reviewed by at least one other team member to ensure completeness and accuracy. This can be achieved through a pull request workflow in the version control system.\n\n5. **Regular Updates**: ADRs should be revisited and updated as necessary, particularly when decisions are revisited or when the project evolves in a way that impacts the architecture.\n\n6. **Visibility and Accessibility**: Ensure that ADRs are easily accessible to all team members through the project repository and are included in onboarding documentation for new team members.\n\nBy following these steps, we will create a robust and transparent record of architectural decisions that will serve as a valuable resource throughout the lifecycle of the project.","GenTime":"2024-07-29 02:03:12"}
{"File Name":"adr\/ADR-6-git-style-guide.md","Context":"## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n\n## Decision\n","Decision":"We agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client\/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\\n### Branches\\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\\n```\\nfix\/wrong_host\\n^  ^^------------^\\n|  ||\\n|  |+----> Summary in present tense.\\n|  +-----> Slash\\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\\n```\\nOther examples are:\\n```\\ndocs\/update_readme\\nrefactor\/new_welcome_message\\n```\\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+`\\n#### Exceptions:\\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\\n","tokens":28,"id":4600,"Prediction":"## Decision\nTo maintain a clean and understandable Git history and ensure consistency across the team, we will adopt the following strategies:\n\n### Git History\n1. **Single Flow Strategy:** We will use the \"main\" branch as the main branch, with feature branches for development. Feature branches should be merged into the main branch using Pull Requests (PRs) after code review.\n2. **Rebase Strategy:** Before opening a PR, developers must rebase their feature branch on the current main branch to maintain a linear history.\n3. **Squash and Merge:** PRs should be merged into the main branch with the \"Squash and Merge\" option to condense all commits from a feature branch into a single commit, making history easier to understand.\n\n### Commit Messages\n1. **Structure:** Commit messages should follow the Conventional Commits specification to enforce a consistent structure. The format should be:\n   ```\n   <type>(<scope>): <subject>\n   \n   <body>\n   \n   <footer>\n   ```\n2. **Types:** We will use the following types:\n   - **feat:** A new feature.\n   - **fix:** A bug fix.\n   - **docs:** Documentation only changes.\n   - **style:** Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc).\n   - **refactor:** A code change that neither fixes a bug nor adds a feature.\n   - **perf:** A code change that improves performance.\n   - **test:** Adding missing tests or correcting existing tests.\n   - **chore:** Changes to the build process or auxiliary tools and libraries such as documentation generation.\n3. **Scope:** Scopes should be used to provide context about the area of the codebase that is affected (e.g., `auth`, `api`, `ui`). It is optional but recommended.\n4. **Subject:** This is a brief description of the change, kept within 50 characters.\n5. **Body:** This is optional and should follow Git\u2019s guidelines for commit message bodies, detailing what and why the changes were made. Use bullet points for clarity if necessary.\n6. **Footer:** This is for any necessary metadata, such as referencing issues (Fixes #123) or breaking changes (BREAKING CHANGE: description).\n\nBy adhering to this strategy, we can ensure our Git history is both informative and easy to navigate, supporting better collaboration and code quality.\n\n---\nFeel free to adjust these guidelines to better fit your team's specific needs or practices.","GenTime":"2024-07-29 02:03:26"}
{"File Name":"adr\/ADR-8-dao-content-servers-and-local-content-servers.md","Context":"## Context and Problem Statement\\n* Today Catalyst servers are split in two groups listed and unlisted in the DAO\\nListed servers sync with eachother by a pull mechanism (between each other, listed in the DAO).\\nUnlisted servers, sync again those who are listed but. Listed servers never pull changes from unlisted servers.\\n* Big scenes are constrained by server configurations or proxy limitations of the deployed nodes (i.e. body post size). Nowadays, those limitations are workarounded by selecting another server with a different configuration (Kyllian's).\\n### Use cases\\n* I want to push my scene to my own server and I expect it to replicate to other servers.\\n* Create a \"local-staging\" environment in which land owners or creators can synchronize the whole world from a DAO node and test locally.\\n* Broadcast of changes should be optional.\\n\n## Decision\n","Decision":"* Enable custom Catalyst servers from Explorer\\n* Define action plan for broadcasting\/commiting local deployments to the DAO servers. [issue](https:\/\/github.com\/decentraland\/catalyst\/issues\/195)\\n","tokens":178,"id":4601,"Prediction":"## Decision\nWe will implement a more flexible and configurable syncing mechanism that allows users to easily broadcast changes to their own server, choose synchronization preferences for their environments, and overcome server and proxy limitations. This decision involves the following actions:\n\n1. **Decouple Sync Direction Rules**:\n    - Allow listed servers to optionally pull changes from unlisted servers based on configurable policies. This enables users to choose which unlisted servers (if any) should be synchronized.\n\n2. **Enhanced Configuration Options**:\n    - Provide detailed configuration settings within the platform to allow users to fine-tune the synchronization behavior. This includes configuring thresholds for sync operations, data handling limits, and backup servers to handle larger scenes.\n\n3. **Local-Staging Environment**:\n    - Develop and deploy a local-staging environment that land owners and creators can use. This will enable synchronization with a specified DAO node and allow for local testing of scenes before broadcasting changes more broadly.\n\n4. **Optional Change Broadcasting**:\n    - Introduce optional broadcasting of changes, enabling users to decide when and what to broadcast to other servers. This feature should be integrated with permissions and access control lists (ACLs) to ensure secure and controlled updates.\n\nThese actions will address performance bottlenecks, provide flexibility for various use cases, and improve the overall user experience in scene replication and synchronization.","GenTime":"2024-07-29 02:03:29"}
{"File Name":"adr\/ADR-36-kernel-repository-separation.md","Context":"## Context and Problem Statement\\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\\n\n## Decision\n","Decision":"We choose to create a separated repository to not block the releases and development of explorer during this migration stage.\\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\\n","tokens":135,"id":4609,"Prediction":"## Decision\n\nWe will implement a Continuous Integration\/Continuous Deployment (CI\/CD) pipeline utilizing a microservices architecture to handle the decoupled components (`renderer`, `explorer`, `website`, and `js-sdk-toolchain`). This decision is driven by the need to ensure that changes in one component do not block or break the deployment and hotfix cycles of other components. The steps to implement this include:\n\n1. **Independent Repositories and Pipelines:** Each decoupled component will reside in its own repository and will have a dedicated CI\/CD pipeline. This allows for independent development, testing, and deployment cycles, thereby reducing interdependencies and bottlenecks in the publishing process.\n\n2. **Automated Testing:** Implement comprehensive automated testing for each component to quickly detect breaking changes. This includes unit tests, integration tests, and end-to-end tests. Automated testing ensures that changes in one component do not adversely affect other components.\n\n3. **Version Management:** Adopt semantic versioning for each component to manage dependencies and facilitate clear release cycles. This helps in tracking changes and ensures compatibility between components during integration.\n\n4. **Hotfix Strategy:** Develop a hotfix strategy that allows for quick patches and updates. Each repository must include a branch and pipeline specifically for hotfixes, enabling rapid addressing of critical issues without disrupting the main development workflow.\n\n5. **Documentation and Communication:** Maintain clear and up-to-date documentation of the decoupled components, their interfaces, and deployment processes. Regular communication between the teams responsible for each component is essential to ensure synchronization and smooth operation.\n\n6. **Monitoring and Rollback:** Implement robust monitoring mechanisms to quickly identify issues post-deployment. Also, include rollback capabilities in the CI\/CD pipelines to revert to a stable state in case of critical failures.\n\nBy adopting this CI\/CD approach and microservices architecture, we can optimize development efficiency and ensure stability across the entire ecosystem, allowing for seamless releases and hotfixes without major disruptions.","GenTime":"2024-07-29 02:04:02"}
{"File Name":"adr\/ADR-7-standards-repository.md","Context":"# Context\\n* Is the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository still useful?\\n* Does it make sense to have ADR and standards?\\n* How does it play with [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals)?\\n# Options\\n* Archive both repositories, leaving only [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n* \u2705 Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards), leaving [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\n* Only use [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n# Decision\\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n\n## Decision\n","Decision":"* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","tokens":338,"id":4610,"Prediction":"## Decision\n\nAfter careful consideration of the current status and utility of the different repositories, the following decision has been made:\n\n**Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)**\n\nRationale:\n1. **Redundancy and Maintenance**: `standards` is not actively maintained, and many of the standards defined there can instead be captured as Architectural Decision Records (ADRs). This eliminates the need to maintain multiple repositories for similar purposes.\n2. **Community Contribution and Review**: The `proposals` repository serves as a public platform where the community can contribute new ideas and improvements. It has a formal review process that can be leveraged by anyone, making it a more inclusive and dynamic repository for evolving standards and ideas.\n3. **Siloed Approvals**: The `standards` repository has a strict approval process which can be a bottleneck. Integrating these standards into the ADR process can streamline decision-making and documentation.\n\nThe following repositories will remain unchanged:\n- [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) - to capture architectural decisions.\n- [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) - to crowdsource community ideas and improvements.\n\n**Decision Date: 2020-10-16**","GenTime":"2024-07-29 02:04:06"}
{"File Name":"adr\/ADR-40-ui-dependencies-upgrades.md","Context":"## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https:\/\/github.com\/Semantic-Org\/Semantic-UI-React\/pull\/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https:\/\/github.com\/decentraland\/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy\/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n\n## Decision\n","Decision":"The option to keep the an exact copy of the `Responsive` component (from the old `react-semantic-ui` lib version) was chosen in order to have a frictionless upgrade of the library.\\nThe procedure in which we'll be handling the upgrade is the following:\\n1. A non breaking change upgrade will be provided to our [UI library](https:\/\/github.com\/decentraland\/ui), keeping the `Responsive` component as a deprecated component and an alternative (describe below) will be provided to replace it.\\n2. A breaking change upgrade will be applied to our [UI library](https:\/\/github.com\/decentraland\/ui), whenever all of our dependencies are updated, removing the `Responsive` component.\\nWe\u2019ll be providing, alongside the `Responsive` component a set of components and hooks to replace it, using the `react-responsive`, library. This library was chosen in favor of the recommended `@artsy\/fresnel` mainly because of its versatility. The need of having to set a provider at the application's root level, (coupling the users of this dependency to `@artsy\/fresnel`) to have better SSR support that we don't currently need, made us decide not to go with it.\\nThe components built with the `react-responsive` and exposed to the consumers of our [UI library](https:\/\/github.com\/decentraland\/ui) will be the following:\\n- **Desktop** (for devices with `min width: 992`)\\n- **Tablet** (for devices with `min width: 768 and max width: 991`)\\n- **TabletAndBelow** (for devices with `max width: 991`, that is taking into consideration tablets and mobile devices)\\n- **Mobile** (for devices with `max width: 767`)\\n- **NotMobile** (for devices that don't comply with the requirements specified in Mobile)\\nThese components describe a conditional rendering based on the media the page in being rendered.\\nWhere we had:\\n```tsx\\n<Responsive\\nas={Menu}\\nsecondary\\nstackable\\nminWidth={Responsive.onlyTablet.minWidth}\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Responsive>\\n<Responsive\\n{...Responsive.onlyMobile}\\nclassName=\"dcl navbar-mobile-menu\"\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/Responsive>\\n```\\nWe now have:\\n```tsx\\n<NotMobile>\\n<Menu secondary stackable>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Menu>\\n<\/NotMobile>\\n<Mobile>\\n<div className=\"dcl navbar-mobile-menu\">\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/div>\\n<\/Mobile>\\n```\\nAnd, alongside these components, as explained before, we're exposing the following set of hooks:\\n- **useDesktopMediaQuery**\\n- **useTabletMediaQuery**\\n- **useTabletAndBelowMediaQuery**\\n- **useMobileMediaQuery**\\n- **useNotMobileMediaQuery**\\nWhich return true if the device is the one defined as the name of the hook.\\nThese types of hooks will provide us with newer functionality, being able to customize small portions of our code instead of forking our components into two.\\nAs an example, we can apply certain styles by simply:\\n```tsx\\nconst isMobile = useMobileMediaQuery()\\nconst classes = isMobile ? \"dcl mobile\" : \"dcl\"\\n<div className={classes}>\\n...\\n<\/div>\\n```\\n","tokens":379,"id":4613,"Prediction":"## Decision\nBased on the provided context and the analysis of the alternatives, we have decided to proceed with the following approach to update to React 17 across all UI apps and libraries:\n\n1. **Update all UI apps and libraries to React 17**: We will proceed with updating every UI app and library to React 17 to ensure consistency and minimize version conflicts.\n\n2. **Introduce a custom, interim `Responsive` component**: To mitigate the breaking change caused by the removal of the `Responsive` component in `react-semantic-ui`, we will copy the current implementation of the `Responsive` component from `react-semantic-ui` into our UI repository. This will act as an interim solution, providing time for teams to transition to a different mechanism for responsive design.\n\n3. **Adopt react-responsive as the alternative library**: We will adopt `react-responsive` for future responsive needs in our UI components. This decision is based on the following advantages:\n   - **No need for a provider**: Eliminates the global coupling of the media query solution, making it easier to integrate with different parts of the application independently.\n   - **Provides hooks and component solutions**: Offers both hooks and components for handling media queries, making it versatile and easy to integrate into both functional and class components.\n   - **Flexibility for future updates**: Provides a robust solution for future enhancements and device-specific rendering needs.\n\n4. **Gradual migration plan**: We will implement a gradual migration plan from the interim `Responsive` component to `react-responsive`:\n   - **Documentation and Guides**: Provide comprehensive documentation and guides to help teams transition smoothly to `react-responsive`.\n   - **Communication and Support**: Communicate the changes effectively across teams and offer support and assistance during the transition period.\n\n5. **Monitor and evaluate**: Monitor the integration and performance of `react-responsive` in our applications, and gather feedback from the teams to ensure it meets our requirements and enhances our responsive capabilities.\n\nBy taking this approach, we aim to ensure a seamless transition to React 17 while maintaining the flexibility and responsiveness of our UIs with minimal disruption to ongoing projects.","GenTime":"2024-07-29 02:04:21"}
{"File Name":"adr\/ADR-24-decouple-kernel-and-unity-apis.md","Context":"## Context and Problem Statement\\nAs of today, kernel and unity have huge dependencies in business logic: sagas in TS, requests are made in TS, and everything is used from Unity.\\nThat generates coupling between technologies and teams, generating blockers and planning problems.\\n\n## Decision\n","Decision":"* Go for this proposal \u2705\\n### Actionables\\n- When starting development on new features, aim for this approach from the get-go\\n- Features that are already advanced in development or finished will be refactored gradually\\n- We will start by exposing signed requests and migrating the quests feature saga\\nNew features for the ECS\/SDK will have the common product process (RFC, roadmap, backlog, refinement). Alvaro will gather together the team when necessary to discuss implementation of new changes to the SDK.\\nCLI should be handled in the same way as the kernel and SDK developments.\\n","tokens":54,"id":4614,"Prediction":"## Decision\nWe will decouple the business logic from the kernel and Unity by creating distinct service layers and abstracting away direct dependencies between these components. Specifically, we will:\n\n1. **Introduce Middleware**: Implement a middleware layer that handles requests and responses, allowing for a clear separation between the underlying kernel and business logic. This layer will serve as an intermediary that processes and directs traffic, reducing direct dependencies on the technology stack used in Unity.\n\n2. **Modularize Business Logic**: Break down the business logic into smaller, self-contained modules that communicate via well-defined interfaces. This modular approach will ensure that individual pieces can be developed, tested, and deployed independently, minimizing the impact on other components.\n\n3. **Shared Interfaces and Contracts**: Define shared interfaces and contracts that outline how different parts of the system communicate. These interfaces will be language and platform-agnostic, allowing different teams to work independently and ensure compatibility without being constrained by specific technologies (e.g., TypeScript in kernel and Unity).\n\n4. **Dependency Injection and Inversion of Control**: Utilize dependency injection and inversion of control principles to further decouple dependencies. By injecting dependencies rather than hardcoding them, we can easily swap out implementations and reduce the coupling between different parts of the system.\n\n5. **Documentation and Communication Protocols**: Document all interfaces, contracts, and communication protocols thoroughly. Clear and comprehensive documentation will help ensure that all teams have a clear understanding of how to interact with business logic without creating unnecessary dependencies.\n\n6. **Team Alignment and Communication**: Regularly sync between teams to discuss ongoing and upcoming work, potential blockers, and dependencies. Establish clear lines of communication to ensure smooth coordination and mitigate planning issues.\n\nBy implementing these changes, we aim to achieve a well-structured, decoupled architecture that minimizes dependencies between our kernel, business logic, and Unity, leading to more manageable and scalable development processes.","GenTime":"2024-07-29 02:04:27"}
{"File Name":"adr\/0002-SYS-rdbms.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4616,"Prediction":"## Context\nOur software application currently faces challenges in handling a large volume of real-time data efficiently. The existing system struggles with scalability and performance issues as the data load increases. Additionally, the application needs to maintain high availability and quick response times to meet user expectations. We have considered various architectural patterns to address these concerns, including microservices, event-driven architecture, and cloud-native deployments. Another constraint is the team's familiarity with certain technologies and the long-term maintainability of the solution.\n\n## Decision\nWe have decided to adopt a microservices architecture combined with an event-driven approach to handle the high volume of real-time data effectively. This decision will also involve migrating certain components to a cloud-native environment to take advantage of the scalability and reliability offered by cloud platforms. By breaking the system into smaller, independently deployable services, we can improve scalability and maintainability. The event-driven model will facilitate better handling of real-time data streams and improve the system's responsiveness. We will leverage cloud services such as managed Kubernetes for container orchestration and serverless functions for specific use cases to ensure high availability and quick response times. This approach aligns with the team's skill set while providing a robust foundation for future scalability and maintainability.","GenTime":"2024-07-29 02:04:29"}
{"File Name":"adr\/ADR-9-DecentralandInterface-evolution-plan.md","Context":"## Context and Problem Statement\\nHow should we proceed to maintain compatibility between the current SDK and the next-gen SDK currently being developed?\\nTaking into consideration:\\n* All the scenes currently deployed must work as they work today.\\n* `DecentralandInterface` should work indefinitely to maintain backwards compatibility.\\n* Maintainance costs should be taken into account\\n\n## Decision\n","Decision":"We decided to develop a new and clean interface for the next-gen SDK, then create an adapter for the legacy `DecentralandInterface` (option 2) because:\\n* We should not be limited by the decisions of the past SDK\\n* The new SDK must eventually prove itself by reaching a feature parity with the current interface\\n* Avoid sunk cost fallacy with the old SDK\\n","tokens":72,"id":4618,"Prediction":"## Decision\n\nTo maintain compatibility between the current SDK and the next-gen SDK, we will implement a dual compatibility layer strategy. This approach ensures that scenes deployed using the current SDK continue to function without disruption, while also allowing for a smooth transition to the next-gen SDK.\n\nHere's how we will achieve this:\n\n1. **Backward Compatibility Layer**: \n   - Implement a backward compatibility layer within the next-gen SDK. This layer will intercept API calls and translate them to the next-gen SDK equivalents where feasible.\n   - Ensure that `DecentralandInterface` remains fully functional by maintaining its API and implementing necessary shims or adapters.\n\n2. **Modular Architecture**:\n   - Design the next-gen SDK using a modular and extensible architecture. This will allow specific modules or components to be replaced or updated independently, reducing maintenance costs and effort over time.\n\n3. **Deprecation Policy**:\n   - Establish a clear and well-documented deprecation policy. This policy will include timelines and guidelines for phasing out older features, providing developers ample time to transition to the new SDK APIs.\n   \n4. **Automated Testing and Continuous Integration**:\n   - Develop a comprehensive suite of automated tests that cover both the current SDK and the next-gen SDK functionalities. These tests should be integrated into our continuous integration pipeline to ensure that any changes do not break existing functionalities.\n   \n5. **Documentation and Support**:\n   - Provide extensive documentation and migration guides to assist developers in transitioning their scenes and assets from the current SDK to the next-gen SDK.\n   - Offer support through forums, FAQs, and direct communication channels for any compatibility issues that arise during the transition phase.\n\n6. **Versioning Strategy**:\n   - Adopt semantic versioning for the next-gen SDK to clearly communicate the nature of changes (e.g., major, minor, patch) to developers.\n   - Maintain parallel versioning for different releases of the SDK to ensure that developers can explicitly choose which version to target in their projects.\n\nBy following this strategy, we aim to ensure seamless compatibility between the current SDK and the next-gen SDK while minimizing maintenance costs and disruptions for developers.","GenTime":"2024-07-29 02:04:38"}
{"File Name":"adr\/ADR-5-how-to-organize-adr-files.md","Context":"## Context and Problem Statement\\n- How do we call the folder for assets?\\n- What is the pattern of the filename where an ADR is stored?\\n- How do we write titles in the ADR documents?\\n\n## Decision\n","Decision":"### Asset folder\\nChosen option: `resources`\\n### Filename\\nChosen option: `docs\/ADR-NNNN-title-using-dashes.md` and `docs\/resources\/ADR-NNNN\/{filename}`, because\\n- Adding `arch` or `adr` folders is redundant.\\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\\n- Having no spaces in filenames eases working in the command line.\\n- Prefixing with `docs\/ADR-` enables future new kinds of documents.\\n- Calling a folder `docs\/resources\/ADR-NNNN\/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\\n### Title (markdown)\\nChosen option: `# Plain title` because:\\n- At the moment of writing the ADR the writer may not know the final ADR number.\\n- It can be embeded by rendering tools\\n- It would duplicate data from the filename\\n","tokens":46,"id":4619,"Prediction":"## Decision\n\n- The folder for assets will be named `assets`. This provides a clear and intuitive understanding of the folder's purpose for storing images, diagrams, and other static resources.\n\n- The filename pattern for storing an ADR will be `NNNN-title.md`, where `NNNN` is a zero-padded number indicating the sequence of the decision (e.g., `0001`, `0002`). The `title` should be a concise, lower-case, hyphen-separated representation of the decision (e.g., `use-rdbms-for-storage`). This pattern ensures consistency, easy sorting, and readability.\n\n- Titles in the ADR documents will be written in Title Case. This means capitalizing the principal words while keeping articles, conjunctions, and prepositions in lowercase unless they are the first or last word of the title. For example, \"Choosing a Database Solution\" or \"Using REST API for Communication\". This approach ensures clarity and professionalism in the documentation.","GenTime":"2024-07-29 02:04:41"}
{"File Name":"adr\/ADR-1-documenting-architecture-decisions.md","Context":"## Context\\nArchitecture for agile projects has to be described and defined differently. Not all decisions will be made at once, nor will all of them be done when the project begins.\\nAgile methods are not opposed to documentation, only to valueless documentation. Documents that assist the team itself can have value, but only if they are kept up to date. Large documents are never kept up to date. Small, modular documents have at least a chance at being updated.\\nNobody ever reads large documents, either. Most developers have been on at least one project where the specification document was larger (in bytes) than the total source code size. Those documents are too large to open, read, or update. Bite sized pieces are easier for for all stakeholders to consume.\\nOne of the hardest things to track during the life of a project is the motivation behind certain decisions. A new person coming on to a project may be perplexed, baffled, delighted, or infuriated by some past decision. Without understanding the rationale or consequences, this person has only two choices:\\n1. **Blindly accept the decision.**\\nThis response may be OK, if the decision is still valid. It may not be good, however, if the context has changed and the decision should really be revisited. If the project accumulates too many decisions accepted without understanding, then the development team becomes afraid to change anything and the project collapses under its own weight.\\n2. **Blindly change it.**\\nAgain, this may be OK if the decision needs to be reversed. On the other hand, changing the decision without understanding its motivation or consequences could mean damaging the project's overall value without realizing it. (E.g., the decision supported a non-functional requirement that hasn't been tested yet.)\\nIt's better to avoid either blind acceptance or blind reversal.\\n\n## Decision\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.\\nAn architecture decision record is a short text file in a format similar to an Alexandrian pattern. (Though the decisions themselves are not necessarily patterns, they share the characteristic balancing of forces.) Each record describes a set of forces and a single decision in response to those forces. Note that the decision is the central piece here, so specific forces may appear in multiple ADRs.\\nWe will keep ADRs in the project repository under `docs\/ADR-####-title.md`\\nWe should use a lightweight text formatting language like Markdown or Textile.\\nADRs will be numbered sequentially and monotonically. Numbers will not be reused.\\nIf a decision is reversed, we will keep the old one around, but mark it as superseded. (It's still relevant to know that it _was_ the decision, but is _no longer_ the decision.)\\nWe will use a format with just a few parts, so each document is easy to digest. The format has just a few parts.\\n**Title** These documents have names that are short noun phrases. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\n**Context** This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n**Decision** This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will \u2026\"\\n**Status** A decision may be \"proposed\" if the project stakeholders haven't agreed with it yet, or \"accepted\" once it is agreed. If a later ADR changes or reverses a decision, it may be marked as \"deprecated\" or \"superseded\" with a reference to its replacement.\\n**Consequences** This section describes the resulting context, after applying the decision. All consequences should be listed here, not just the \"positive\" ones. A particular decision may have positive, negative, and neutral consequences, but all of them affect the team and project in the future.\\nThe whole document should be one or two pages long. We will write each ADR as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments. (Bullets kill people, even PowerPoint bullets.)\\n# Status\\nAccepted.\\n# Consequences\\nOne ADR describes one significant decision for a specific project. It should be something that has an effect on how the rest of the project will run.\\nThe consequences of one ADR are very likely to become the context for subsequent ADRs. This is also similar to Alexander's idea of a pattern language: the large-scale responses create spaces for the smaller scale to fit into.\\nDevelopers and project stakeholders can see the ADRs, even as the team composition changes over time.\\nThe motivation behind previous decisions is visible for everyone, present and future. Nobody is left scratching their heads to understand, \"What were they thinking?\" and the time to change old decisions will be clear from changes in the project's context.\\n---\\nOriginal document: [Documenting Architecture Decisions](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) by Michael Nygard\\nMore reading:\\n1. [Importance of architecture decisions](http:\/\/www.computer.org\/portal\/web\/csdl\/doi\/10.1109\/MS.2009.52)\\n2. [Documenting software architectures](http:\/\/www.sei.cmu.edu\/library\/abstracts\/books\/0321552687.cfm)\\n","tokens":380,"id":4620,"Prediction":"## Decision\n\nAdopt a modular and iterative approach to architectural documentation by using Agile Architectural Decision Records (ADRs).\n\n1. **Small, Modular Documents:** Each architectural decision will be documented in a small, self-contained ADR. This modular approach will ensure documents are easier to read, update, and maintain, aligning with the agile principle of producing valuable and useful documentation.\n\n2. **Just-in-Time Documentation:** Architectural decisions will not be made all at once. Instead, they will be captured as they are made throughout the project lifecycle. This aligns with agile's iterative and incremental philosophy, allowing the architecture to evolve as the project progresses.\n\n3. **Rationale and Consequences:** Each ADR will include not just the decision itself, but also the context, motivation, rationale, and potential consequences of the decision. This ensures that new team members, or existing members revisiting decisions, understand why certain choices were made, leading to informed acceptance or modification.\n\n4. **Collaboration:** The creation and updating of ADRs will be a collaborative effort, involving all relevant stakeholders to ensure that the documentation is comprehensive and reflects the collective understanding of the team.\n\n5. **Version Control:** ADRs will be maintained in a version-controlled repository. This makes it easier for the team to track changes, revisit past decisions, and understand the evolution of the project architecture.\n\nBy adopting this approach, the team can ensure that documentation remains valuable and up-to-date, helping avoid the pitfalls of blind acceptance or change of architectural decisions.","GenTime":"2024-07-29 02:04:45"}
{"File Name":"adr\/ADR-28-smart-contract-wallets-and-meta-transactions.md","Context":"## Context and Problem Statement\\nCurrently we support meta-transactions on Polygon from several of our dApps (Marketplace, Builder, Account) and the Explorer.\\nMeta-transactions require the user to sign a function signature that they want us to call on their behalf (so we can subsidize the gas).\\nThe smart contracts that support meta-transactions (like the Marketplace contract) recover the account from the signature on chain, and uses that as the message sender.\\nSmart contract wallets are smart contracts that can hold assets, and have a set of signers that can command the wallet to perform actions on those assets by providing signed messages authorizing given actions.\\nWhen a user of a contract wallet signs a meta transaction, and the public key is recovered on chain by the smart contract that supports meta-transactions, the public key is the one from the signer, and not the contract address of the smart contract wallet (which is the actual owner of the assets). This causes the meta-transactions to fail on chain.\\n\n## Decision\n","Decision":"We choose option 2 since we can never be sure that the smart contract wallet has a matching deployment on Polygon, and is not the case for the most popular smart contract wallets (like Argent or Authereum)\\n","tokens":205,"id":4623,"Prediction":"## Decision\n\nWe will implement support for smart contract wallets in our meta-transaction framework. Here are the specific steps and changes we will adopt to address the issue:\n\n1. **Smart Contract Modification**: \n   - Smart contracts that support meta-transactions (like Marketplace) need to be updated. Instead of directly recovering the public key from the signature, the contract will check if the user is utilizing a smart contract wallet. If they are, the contract will then verify the signed message by invoking a verification method on the smart contract wallet.\n   \n2. **Meta-Transaction Flow Adjustment**:\n   - When a user with a smart contract wallet signs a meta-transaction, our dApps will recognize this and add an additional layer of validation. This means that our server-side logic will check the smart contract wallet\u2019s authorized signers to ensure the signature is valid.\n   - This secondary validation step will be necessary for all actions triggered by a smart contract wallet to ensure that the correct owner (the contract address of the smart contract wallet) is recognized and authorized by our system to subsidize the gas.\n\n3. **Frontend Changes**:\n   - UI\/UX adjustments across our dApps to support users using smart contract wallets. This includes notifying the user when they are about to perform actions with a smart contract wallet and ensuring the signing process includes steps that handle the smart contract wallet verification.\n\n4. **Implement Contract Wallet Verification Service**:\n   - Introduce a new service layer that interacts with smart contract wallets. This service will handle the cross-checking of user signatures and the contract wallet authorization to ensure that the on-chain recovered public key maps correctly to an authorized signer of the wallet.\n   \n5. **Adaptation of Backend Services**:\n   - Modify backend services to handle the additional validation steps for smart contract wallets. This will involve checking the contract wallet\u2019s authorized signers and ensuring the signature matches one of the authorized entities.\n\nBy implementing these changes, we will ensure that meta-transactions from users utilizing smart contract wallets do not fail on-chain due to incorrect recovery of the signer\u2019s public key. This ensures that the user experience remains seamless, and that we can continue to subsidize gas fees effectively for all users, regardless of the wallet type they use.","GenTime":"2024-07-29 02:04:57"}
{"File Name":"adr\/ADR-10-profile-deployment-debouncing.md","Context":"## Context and Problem Statement\\nHow do we debouncing profile\/scene deployments to avoid DDoS and wasted bandwidth?\\n- As of today we only deploy profiles when changing the avatar\\n- Soon, a new deployment will happen after mute\/unmute\\n\n## Decision\n","Decision":"We decided to do Client side debouncing (both options 2 and 3). We may measure deployments in the future to revisit the server conversation.\\n","tokens":51,"id":4626,"Prediction":"## Decision\nImplement a debounce mechanism to limit the frequency of profile\/scene deployments. This will help in preventing DDoS attacks and conserving bandwidth. The debounce mechanism will ensure that profile\/scene deployments are only triggered if a significant change in state persists for a defined period of time. Specifically, we will:\n\n1. **Set a Debounce Interval**: Define a minimum interval (e.g., 1 second) between deployments to ensure that rapid, successive changes do not trigger multiple deployments.\n2. **Unified Deployment Trigger**: Consolidate deployment triggers for different actions (e.g., avatar change, mute\/unmute) into a single debounced function to manage all deployment events.\n3. **Threshold for Significant Changes**: Define thresholds for what constitutes a \"significant change\" (e.g., only deploying if mute\/unmute state changes persist for more than a few milliseconds).\n\nBy implementing these measures, we aim to strike a balance between responsiveness and resource optimization.","GenTime":"2024-07-29 02:05:12"}
{"File Name":"monocle\/0009-changes-extended-with-task-data.md","Context":"## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n\n## Decision\n","Decision":"* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\nChosen option: \"Monocle API provides an interface for external task data crawlers\", because it will ease integration between Monocle and the\\nvarious task trackers available in the market.\\nA Monocle operator will need to write the crawler for its\\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\\nFrom the Monocle side, we provide a clear API for a task tracker\\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\\n\"severity\", \"change_url\", \"title\", ... The monocle API is then able to\\nfind corresponding Changes in the database that match the \"change_url\" field.\\nEach changes get a new attribute called \"task_data\" that is a list of\\nrelated task data records. Indeed multiple tasks might be related to a\\nsingle change.\\nAlso, Monocle keeps track of task data that do not match any Changes in the\\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\\n","tokens":79,"id":4636,"Prediction":"## Decision\n\nTo address the need for insights and metrics related to changes in a task tracker without making assumptions about the specific tracker being used, we will implement a flexible and generic solution based on the following decisions:\n\n1. **Abstract Task Tracker Interface:** Define an interface or abstraction layer that can be implemented for different task trackers (e.g., JIRA, Trello, GitHub Issues). This abstraction will allow the system to interact uniformly with multiple types of task trackers.\n\n2. **Standardized Task Fields:** Support a set of generic fields that are common across most task trackers, such as task type (feature, bug, etc.), status, priority, creation date, and the user assigned. This ensures a consistent and uniform approach to gathering metrics, regardless of the underlying task tracker.\n\n3. **Change Metrics Calculation Module:** Build a module specifically to calculate and report change metrics. This module will use the standardized fields to compute metrics like the ratio of changes related to feature requests vs. bug fixes.\n\n4. **Plugin Architecture for Trackers:** Implement a plugin system where individual plugins correspond to different task trackers. Each plugin will conform to the abstract task tracker interface, enabling the main application to interact with various task trackers seamlessly.\n\n5. **Configuration-Driven Approach:** Use configuration files or settings to enable users to specify which task tracker they are using and how the standardized fields map to the specific fields in their tracker. This removes assumptions about the task tracker while providing flexibility and ease of use.\n\n6. **Incremental Integration:** Start with support for the most commonly used task trackers and progressively add more plugins based on user needs and feedback. Initial implementation could focus on popular trackers like JIRA and GitHub Issues.\n\n7. **Documentation and Extensibility:** Provide clear documentation and guidelines for extending the system to support new task trackers, allowing external developers or users to add plugins for additional task trackers with minimal effort.\n\nBy taking this approach, we achieve a simple and scalable implementation that adheres to the decision drivers and solves the problem effectively.","GenTime":"2024-07-29 02:05:58"}
{"File Name":"monocle\/0011-search-query-language.md","Context":"## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n\n## Decision\n","Decision":"Chosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n","tokens":44,"id":4637,"Prediction":"## Decision\n\nAdopt a Query Language for Flexible Search Queries\n\nIn order to support the creation of custom dashboards with complex queries, we have decided to adopt a proper query language. This will allow users to construct more sophisticated and flexible search expressions beyond the capabilities of the existing filter box form.\n\nSpecifically, we will:\n\n1. **Choose a Query Language**: After evaluating multiple options, we will select a query language that is widely used, well-documented, and aligns with the current tech stack. Potential candidates include SQL, Elasticsearch Query DSL, or GraphQL.\n\n2. **Integrate with the Dashboard System**: Develop and integrate the necessary backend and frontend components to support query language execution. This includes modifying the data access layer to interpret and run the chosen query language.\n\n3. **User Interface Enhancements**: Update the dashboard interface to allow users to input and manage these complex queries. This might involve text editors with syntax highlighting, auto-completion features, and query validation.\n\n4. **Documentation and Training**: Provide comprehensive documentation and training materials to help users understand how to construct queries using the new language. Offer sample queries and tutorials to facilitate the learning curve.\n\n5. **Testing and Validation**: Implement thorough testing procedures to ensure that the system can handle the new query capabilities without performance degradation. Validate that complex queries return correct and expected results.\n\nThis approach is expected to greatly enhance the capability of our dashboard system, allowing for more flexible, powerful, and user-driven data exploration and visualization.","GenTime":"2024-07-29 02:06:02"}
{"File Name":"monocle\/0007-production-ready-in-all-in-one-setup.md","Context":"## Context and Problem Statement\\nHow do we serve the API using a production ready server?\\n## Decision Drivers\\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\n\n## Decision\n","Decision":"* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\nChosen option: \"[option 1]\", because it is well known in the\\nindustry. It is not the most performant but it has good performances\\nto serve WSGI app like Flask thanks to the uwsgi protocol used between\\nNginx\/Apache.\\n### Negative Consequences\\n* complexify the architecture.\\n","tokens":56,"id":4642,"Prediction":"## Decision\n\nWe will use **Gunicorn** as our production-ready server to serve the API.\n\n### Rationale:\n\n1. **Production Readiness**: Gunicorn is widely regarded as a production-ready server for Python web applications.\n2. **Integration with Reverse Proxy Servers**: Gunicorn integrates well with popular reverse proxy servers like Nginx and Apache. This integration allows for better load balancing, SSL termination, and other benefits.\n3. **Flask Support**: Gunicorn natively supports running Flask applications, thus ensuring high performance and ease of use.\n4. **Simplicity and Performance**: Gunicorn is simple to set up with Flask and offers good performance out of the box, making it suitable for most production scenarios where Flask is involved.\n\n### Implementation Steps:\n\n1. **Install Gunicorn**: Add Gunicorn to the project's dependencies.\n   ```bash\n   pip install gunicorn\n   ```\n\n2. **Configure Gunicorn**: Create a Gunicorn configuration file (`gunicorn_config.py`) or pass parameters directly when starting Gunicorn.\n   Example configuration:\n   ```python\n   bind = '0.0.0.0:8000'\n   workers = 4  # Number of worker processes\n   ```\n\n3. **Integrate with Flask**: Ensure your Flask application is properly configured to run (e.g., in `app.py`).\n   Example entry point:\n   ```python\n   from my_flask_app import app  # Assuming `app` is the Flask application instance\n\n   if __name__ == \"__main__\":\n       app.run()\n   ```\n\n4. **Start Gunicorn**: Use Gunicorn to start the Flask application.\n   ```bash\n   gunicorn -c gunicorn_config.py my_flask_app:app\n   ```\n\n5. **Configure Reverse Proxy**: Configure Nginx or Apache to reverse proxy requests to the Gunicorn server.\n\n   Example Nginx configuration:\n   ```nginx\n   server {\n       listen 80;\n       server_name example.com;\n\n       location \/ {\n           proxy_pass http:\/\/127.0.0.1:8000;\n           proxy_set_header Host $host;\n           proxy_set_header X-Real-IP $remote_addr;\n           proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n           proxy_set_header X-Forwarded-Proto $scheme;\n       }\n   }\n   ```\n\nBy using Gunicorn, we ensure that the API will be served in a production-ready manner, with good performance and seamless integration with reverse proxy servers.","GenTime":"2024-07-29 02:06:22"}
{"File Name":"monocle\/0010-choice-of-protobuf.md","Context":"## Context and Problem Statement\\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs\/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\\n* How do we define and maintain the interfaces?\\n\n## Decision\n","Decision":"Chosen option: \"Protobuf\", because it comes out best (see below).\\n### Positive Consequences\\n- We need to start defining interfaces outside of their implementations.\\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\\nFor example, use `POST \/get_projects BODY { \"index\": \"name\" }` instead of `GET \/projects?index=name`.\\n### Negative Consequences\\n- The team needs to learn a new language.\\n- Follow-up decisions required for the actual implementation.\\n","tokens":100,"id":4644,"Prediction":"## Decision\n\nWe will adopt the use of an Interface Description Language (IDL) to formally define the APIs between various components of our system. Specifically, we will utilize Protocol Buffers (protobuf) due to its wide adoption, strong support for multiple programming languages, and ability to generate code. \n\n### Key Points of the Decision:\n1. **Protocol Buffers Adoption**: Choose Protocol Buffers (protobuf) as the IDL to define the APIs.\n2. **Central IDL Repository**: Establish a central repository to house all `.proto` definition files to ensure consistency and ease of access.\n3. **Automated Code Generation**: Implement a build process to automatically generate the necessary client and server code from the `.proto` files, ensuring uniformity across different modules.\n4. **Backward Compatibility**: Create policies and guidelines for making non-breaking changes to the `.proto` files to maintain backward compatibility.\n5. **Version Control**: Apply semantic versioning to the `.proto` files, capturing changes and updates systematically.\n6. **Continuous Integration**: Integrate IDL validation and code generation into CI\/CD pipelines to ensure that changes to the protocol definitions are validated, and corresponding code is generated without manual intervention.\n\n### Rationale\n1. **Consistency**: Formal definitions of APIs will reduce inconsistencies and ambiguities in the communication between different components of the system.\n2. **Maintainability**: Using protobuf ensures that the APIs are well-documented and that any refactoring efforts are easier to manage.\n3. **Language and Platform Support**: Protocol Buffers provide support for multiple programming languages, allowing for seamless integration across various modules irrespective of their implemented technology stack.\n4. **Performance**: Protocol Buffers offer better performance characteristics compared to other IDLs like XML or JSON, which is beneficial for both internal and external API communications.\n5. **Automation**: Automated code generation reduces manual coding errors and increases the development productivity.\n\n### Implementation Plan\n1. **Training and Tooling**: Provide training sessions and necessary tools to the development team for using Protocol Buffers effectively.\n2. **Define Interfaces**: Collaboratively define and document the APIs using `.proto` files for each service interface.\n3. **Repository Setup**: Set up a central repository for the `.proto` files and integrate it with the CI\/CD pipeline.\n4. **Code Generation**: Implement scripts within the build process to generate client and server stubs from the `.proto` files.\n5. **Scheduled Reviews**: Conduct regular reviews of the `.proto` files and the generated code to ensure alignment with design principles and requirements.\n6. **Monitoring and Maintenance**: Monitor the adoption and use of Protocol Buffers within the development processes, and collect feedback for improvements.\n\nBy implementing Protocol Buffers for API definitions, we aim to create a more reliable, maintainable, and scalable system architecture.","GenTime":"2024-07-29 02:06:33"}
{"File Name":"monocle\/0008-unify-idents-across-code-review-systems.md","Context":"## Context and Problem Statement\\nHow do we merge contributor identities across code review systems ?\\n## Decision Drivers\\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\n\n## Decision\n","Decision":"* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\nChosen option: \"[option 2]\", Simpler implementation, better query performance (no complexification of queries) and no potential metrics errorneous results returned compared\\nto \"[option 1]\".\\nHere is a configuration example.\\n```\\nidents:\\n- ident: John Doe\\naliases:\\n- github.com\/john-doe\\n- review.opendev.org\/John Doe\/12345\\n```\\n### Negative Consequences\\n* A tool need to crawl the whole index to discover objects to update. However this\\noperation should not happen often.\\n","tokens":47,"id":4645,"Prediction":"## Decision\nWe will merge contributor identities through the Monocle configuration file. This approach ensures that the complexity of the EL query system remains unaffected. By allowing identities to be configured within a predefined configuration file, we facilitate easy maintenance and updates without adding unnecessary complexity to the existing system. This approach also provides flexibility for administrators to manage identities efficiently, ensuring consistency across different code review systems.","GenTime":"2024-07-29 02:06:34"}
{"File Name":"ibc-rs\/adr-005-relayer-v0-implementation.md","Context":"## Context\\nThis ADR documents the implementation of the `v0.1` [relayer lib crate]\\n[ibc-relayer].\\nThis library is instantiated in the [Hermes][hermes] binary of the\\n[ibc-relayer-cli crate][ibc-relayer-cli] (which is not the focus of this discussion).\\nAs a main design goal, `v0.1` is meant to lay a foundation upon which we can\\nadd more features and enhancements incrementally with later relayer versions.\\nThis is to say that `v0.1` may be deficient in terms of features or\\nrobustness, and rather aims to be simple, adaptable, and extensible.\\nFor this reason, we primarily discuss aspects of concurrency and architecture.\\n### Relayer versioning scheme\\nOn the mid-term, the relayer architecture is set out to evolve across three\\nversions.\\nThe first of these, `v0.1`, makes several simplifying assumptions\\nabout the environment of the relayer and its features. These assumptions\\nare important towards limiting the scope that `v0.1` aims to\\ncover, and allowing a focus on the architecture and concurrency model to\\nprovide for growth in the future.\\nThese assumptions are documented below in the [decision](#decision) section.\\n\n## Decision\n","Decision":"### Configuration\\nFor the most part, the relayer configuration will be\\nstatic: the configuration for chains and their respective objects (clients,\\nconnections, or channels) will be fully specified in the relayer\\nconfiguration file and will not change throughout execution.\\nLight clients are also statically defined in the config file, and cannot be\\nswitched dynamically at runtime.\\nRecent changes to the ICS protocol specifies identifier\\nselection for clients, connections, and channels to be [deterministic][ids].\\nFor this reason, we will not need to specify any identifiers in the\\nconfiguration file.\\nWe only specify which pairs of chains should communicate with one\\nanother, and the port identifier to use for that purpose.\\nThis pair of chains plus their corresponding port identifiers is called a\\n__relaying path__.\\nAny relaying path is unidirectional.\\nAn example with the relevant section of the configuration file follows.\\n```toml\\n[[connections]]\\na_chain = 'ibc-0'\\nb_chain = 'ibc-1'\\n[[connections.paths]]\\na_port = 'transfer'\\nb_port = 'transfer'\\n```\\nHere there are two chains, ith one connection between them, and a path for\\nrelaying on the port called `transfer` on both chains, from chain `ibc-0`\\nto `ibc-1`.\\n### Links\\nA [link][link] is a relayer-level protocol that implements packet relay across\\none relaying path.\\nThe relayer at `v0.1` will focus on a single link.\\nThis limitation will be lifted in subsequent versions.\\n### Chain State\\nEach chain is assumed to start with an empty IBC state.\\nThis means that the relayer will take care of creating the client,\\nconnection, and channel objects respectively on each side of a link.\\n### Proof Verification\\nThe `v0.1` relayer will _not_ do proof verification.\\n### Feature set\\nThe [complete list of features is documented elsewhere][features] in detail.\\n","tokens":275,"id":4650,"Prediction":"## Decision\n\n### Assumptions for `v0.1`\n\n1. **Simplified Environment:**\n   - The `v0.1` release assumes a controlled and simplified environment. This encompasses factors such as network conditions, node configurations, and the scale of networks being relayed. By simplifying these elements, we aim to reduce initial complexity and focus on establishing a solid architectural foundation.\n\n2. **Limited Feature Set:**\n   - `v0.1` will have a limited feature set. The primary goal is to support essential relaying functionality with a focus on ensuring these core features are stable and well-architected. Advanced features or optimizations will be reserved for future versions.\n\n3. **Basic Concurrency Model:**\n   - A basic concurrency model will be employed. This model will allow for initial implementation of concurrent operations while keeping the complexity manageable. Future versions will enhance and refine the concurrency model based on lessons learned from `v0.1`.\n\n4. **Modularity and Extensibility:**\n   - The architecture of `v0.1` will prioritize modularity and extensibility. The design will facilitate easy addition of new features and enhancements in subsequent versions without necessitating significant rewrites of the existing codebase.\n\n5. **Error Handling Mechanisms:**\n   - Error handling in `v0.1` will be simplified to capture common error scenarios without deep recovery mechanisms. The emphasis will be on logging and notification rather than complex error recovery, which will be developed in future versions.\n\n6. **Focused Performance:**\n   - Performance optimizations will not be a primary focus for `v0.1`. Initial performance benchmarks will be established to identify bottlenecks and areas for improvement, but detailed optimization efforts will be deferred to later versions.\n\n### Future Directions\n\n- **Enhanced Feature Set:**\n  Future versions (`v0.2` and beyond) will incrementally introduce more features, addressing deficiencies from `v0.1` and expanding the relayer\u2019s capabilities.\n\n- **Improved Concurrency and Robustness:**\n  The concurrency model introduced in `v0.1` will be refined to handle more complex scenarios, ensuring robustness and reliability in real-world deployments.\n\n- **Error Recovery:**\n  Advanced error recovery mechanisms will be developed in future versions to enhance the relayer's resilience and fault tolerance.\n\n- **Performance Optimization:**\n  Subsequent versions will focus on performance improvements, optimizations, and scalability to handle larger and more complex networks efficiently.\n\nIn summary, the `v0.1` relayer lib crate aims to provide a foundational architecture designed with simplicity, adaptability, and extensibility in mind. This approach allows for incremental development and enhancement in future versions, ensuring steady progression towards a robust and feature-rich relaying solution.","GenTime":"2024-07-29 02:06:51"}
{"File Name":"ibc-rs\/adr-003-handler-implementation.md","Context":"## Context\\nIn this ADR, we provide recommendations for implementing the IBC\\nhandlers within the `ibc` (modules) crate.\\n\n## Decision\n","Decision":"Concepts are introduced in the order given by a topological sort of their dependencies on each other.\\n### Events\\nIBC handlers must be able to emit events which will then be broadcasted via the node's pub\/sub mechanism,\\nand eventually picked up by the IBC relayer.\\nAn event has an arbitrary structure, depending on the handler that produces it.\\nHere is the [list of all IBC-related events][events], as seen by the relayer.\\nNote that the consumer of these events in production would not be the relayer directly\\n(instead the consumer is the node\/SDK where the IBC module executes),\\nbut nevertheless handlers will reuse these event definitions.\\n[events]: https:\/\/github.com\/informalsystems\/hermes\/blob\/bf84a73ef7b3d5e9a434c9af96165997382dcc9d\/modules\/src\/events.rs#L15-L43\\n```rust\\npub enum IBCEvent {\\nNewBlock(NewBlock),\\nCreateClient(ClientEvents::CreateClient),\\nUpdateClient(ClientEvents::UpdateClient),\\nClientMisbehavior(ClientEvents::ClientMisbehavior),\\nOpenInitConnection(ConnectionEvents::OpenInit),\\nOpenTryConnection(ConnectionEvents::OpenTry),\\n\/\/     ...\\n}\\n```\\n### Logging\\nIBC handlers must be able to log information for introspectability and ease of debugging.\\nA handler can output multiple log records, which are expressed as a pair of a status and a\\nlog line. The interface for emitting log records is described in the next section.\\n```rust\\npub enum LogStatus {\\nSuccess,\\nInfo,\\nWarning,\\nError,\\n}\\npub struct Log {\\nstatus: LogStatus,\\nbody: String,\\n}\\nimpl Log {\\nfn success(msg: impl Display) -> Self;\\nfn info(msg: impl Display) -> Self;\\nfn warning(msg: impl Display) -> Self;\\nfn error(msg: impl Display) -> Self;\\n}\\n```\\n### Handler output\\nIBC handlers must be able to return arbitrary data, together with events and log records, as described above.\\nAs a handler may fail, it is necessary to keep track of errors.\\nTo this end, we introduce a type for the return value of a handler:\\n```rust\\npub type HandlerResult<T, E> = Result<HandlerOutput<T>, E>;\\npub struct HandlerOutput<T> {\\npub result: T,\\npub log: Vec<Log>,\\npub events: Vec<Event>,\\n}\\n```\\nWe introduce a builder interface to be used within the handler implementation to incrementally build a `HandlerOutput` value.\\n```rust\\nimpl<T> HandlerOutput<T> {\\npub fn builder() -> HandlerOutputBuilder<T> {\\nHandlerOutputBuilder::new()\\n}\\n}\\npub struct HandlerOutputBuilder<T> {\\nlog: Vec<String>,\\nevents: Vec<Event>,\\nmarker: PhantomData<T>,\\n}\\nimpl<T> HandlerOutputBuilder<T> {\\npub fn log(&mut self, log: impl Into<Log>);\\npub fn emit(&mut self, event: impl Into<Event>);\\npub fn with_result(self, result: T) -> HandlerOutput<T>;\\n}\\n```\\nWe provide below an example usage of the builder API:\\n```rust\\nfn some_ibc_handler() -> HandlerResult<u64, Error> {\\nlet mut output = HandlerOutput::builder();\\n\/\/ ...\\noutput.log(Log::info(\"did something\"))\\n\/\/ ...\\noutput.log(Log::success(\"all good\"));\\noutput.emit(SomeEvent::AllGood);\\nOk(output.with_result(42));\\n}\\n```\\n### IBC Submodule\\nThe various IBC messages and their processing logic, as described in the IBC specification,\\nare split into a collection of submodules, each pertaining to a specific aspect of\\nthe IBC protocol, eg. client lifecycle management, connection lifecycle management,\\npacket relay, etc.\\nIn this section we propose a general approach to implement the handlers for a submodule.\\nAs a running example we will use a dummy submodule that deals with connections, which should not\\nbe mistaken for the actual ICS 003 Connection submodule.\\n#### Reader\\nA typical handler will need to read data from the chain state at the current height,\\nvia the private and provable stores.\\nTo avoid coupling between the handler interface and the store API, we introduce an interface\\nfor accessing this data. This interface, called a `Reader`, is shared between all handlers\\nin a submodule, as those typically access the same data.\\nHaving a high-level interface for this purpose helps avoiding coupling which makes\\nwriting unit tests for the handlers easier, as one does not need to provide a concrete\\nstore, or to mock one.\\n```rust\\npub trait ConnectionReader\\n{\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd>;\\n}\\n```\\nA production implementation of this `Reader` would hold references to both the private and provable\\nstore at the current height where the handler executes, but we omit the actual implementation as\\nthe store interfaces are yet to be defined, as is the general IBC top-level module machinery.\\nA mock implementation of the `ConnectionReader` trait could looks as follows:\\n```rust\\nstruct MockConnectionReader {\\nconnection_id: ConnectionId,\\nconnection_end: Option<ConnectionEnd>,\\nclient_reader: MockClientReader,\\n}\\nimpl ConnectionReader for MockConnectionReader {\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd> {\\nif connection_id == &self.connection_id {\\nself.connection_end.clone()\\n} else {\\nNone\\n}\\n}\\n}\\n```\\n#### Keeper\\nOnce a handler executes successfully, some data will typically need to be persisted in the chain state\\nvia the private\/provable store interfaces. In the same vein as for the reader defined in the previous section,\\na submodule should define a trait which provides operations to persist such data.\\nThe same considerations w.r.t. to coupling and unit-testing apply here as well.\\n```rust\\npub trait ConnectionKeeper {\\nfn store_connection(\\n&mut self,\\nclient_id: ConnectionId,\\nclient_type: ConnectionType,\\n) -> Result<(), Error>;\\nfn add_connection_to_client(\\n&mut self,\\nclient_id: ClientId,\\nconnection_id: ConnectionId,\\n) -> Result<(), Error>;\\n}\\n```\\n#### Submodule implementation\\nWe now come to the actual definition of a handler for a submodule.\\nWe recommend each handler to be defined within its own Rust module, named\\nafter the handler itself. For example, the \"Create Client\" handler of ICS 002 would\\nbe defined in `modules::ics02_client::handler::create_client`.\\n##### Message type\\nEach handler must define a datatype which represent the message it can process.\\n```rust\\npub struct MsgConnectionOpenInit {\\nconnection_id: ConnectionId,\\nclient_id: ClientId,\\ncounterparty: Counterparty,\\n}\\n```\\n##### Handler implementation\\nIn this section we provide guidelines for implementing an actual handler.\\nWe divide the handler in two parts: processing and persistence.\\n###### Processing\\nThe actual logic of the handler is expressed as a pure function, typically named\\n`process`, which takes as arguments a `Reader` and the corresponding message, and returns\\na `HandlerOutput<T, E>`, where `T` is a concrete datatype and `E` is an error type which defines\\nall potential errors yielded by the handlers of the current submodule.\\n```rust\\npub struct ConnectionMsgProcessingResult {\\nconnection_id: ConnectionId,\\nconnection_end: ConnectionEnd,\\n}\\n```\\nThe `process` function will typically read data via the `Reader`, perform checks and validation, construct new\\ndatatypes, emit log records and events, and eventually return some data together with objects to be persisted.\\nTo this end, this `process` function will create and manipulate a `HandlerOutput` value like described in\\nthe corresponding section.\\n```rust\\npub fn process(\\nreader: &dyn ConnectionReader,\\nmsg: MsgConnectionOpenInit,\\n) -> HandlerResult<ConnectionMsgProcessingResult, Error>\\n{\\nlet mut output = HandlerOutput::builder();\\nlet MsgConnectionOpenInit { connection_id, client_id, counterparty, } = msg;\\nif reader.connection_end(&connection_id).is_some() {\\nreturn Err(Kind::ConnectionAlreadyExists(connection_id).into());\\n}\\noutput.log(\"success: no connection state found\");\\nif reader.client_reader.client_state(&client_id).is_none() {\\nreturn Err(Kind::ClientForConnectionMissing(client_id).into());\\n}\\noutput.log(\"success: client found\");\\noutput.emit(IBCEvent::ConnectionOpenInit(connection_id.clone()));\\nOk(output.with_result(ConnectionMsgProcessingResult {\\nconnection_id,\\nclient_id,\\ncounterparty,\\n}))\\n}\\n```\\n###### Persistence\\nIf the `process` function specified above succeeds, the result value it yielded is then\\npassed to a function named `keep`, which is responsible for persisting the objects constructed\\nby the processing function. This `keep` function takes the submodule's `Keeper` and the result\\ntype defined above, and performs side-effecting calls to the keeper's methods to persist the result.\\nBelow is given an implementation of the `keep` function for the \"Create Connection\" handlers:\\n```rust\\npub fn keep(\\nkeeper: &mut dyn ConnectionKeeper,\\nresult: ConnectionMsgProcessingResult,\\n) -> Result<(), Error>\\n{\\nkeeper.store_connection(result.connection_id.clone(), result.connection_end)?;\\nkeeper.add_connection_to_client(result.client_id, result.connection_id)?;\\nOk(())\\n}\\n```\\n##### Submodule dispatcher\\n> This section is very much a work in progress, as further investigation into what\\n> a production-ready implementation of the `ctx` parameter of the top-level dispatcher\\n> is required. As such, implementers should feel free to disregard the recommendations\\n> below, and are encouraged to come up with amendments to this ADR to better capture\\n> the actual requirements.\\nEach submodule is responsible for dispatching the messages it is given to the appropriate\\nmessage processing function and, if successful, pass the resulting data to the persistence\\nfunction defined in the previous section.\\nTo this end, the submodule should define an enumeration of all messages, in order\\nfor the top-level submodule dispatcher to forward them to the appropriate processor.\\nSuch a definition for the ICS 003 Connection submodule is given below.\\n```rust\\npub enum ConnectionMsg {\\nConnectionOpenInit(MsgConnectionOpenInit),\\nConnectionOpenTry(MsgConnectionOpenTry),\\n...\\n}\\n```\\nThe actual implementation of a submodule dispatcher is quite straightforward and unlikely to vary\\nmuch in substance between submodules. We give an implementation for the ICS 003 Connection module below.\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: Msg) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ConnectionReader + ConnectionKeeper,\\n{\\nmatch msg {\\nMsg::ConnectionOpenInit(msg) => {\\nlet HandlerOutput {\\nresult,\\nlog,\\nevents,\\n} = connection_open_init::process(ctx, msg)?;\\nconnection::keep(ctx, result)?;\\nOk(HandlerOutput::builder()\\n.with_log(log)\\n.with_events(events)\\n.with_result(()))\\n}\\nMsg::ConnectionOpenTry(msg) => \/\/ omitted\\n}\\n}\\n```\\nIn essence, a top-level dispatcher is a function of a message wrapped in the enumeration introduced above,\\nand a \"context\" which implements both the `Reader` and `Keeper` interfaces.\\n### Dealing with chain-specific datatypes\\nThe ICS 002 Client submodule stands out from the other submodules as it needs\\nto deal with chain-specific datatypes, such as `Header`, `ClientState`, and\\n`ConsensusState`.\\nTo abstract over chain-specific datatypes, we introduce a trait which specifies\\nboth which types we need to abstract over, and their interface.\\nFor the ICS 002 Client submodule, this trait looks as follow:\\n```rust\\npub trait ClientDef {\\ntype Header: Header;\\ntype ClientState: ClientState;\\ntype ConsensusState: ConsensusState;\\n}\\n```\\nThe `ClientDef` trait specifies three datatypes, and their corresponding interface, which is provided\\nvia a trait defined in the same submodule.\\nA production implementation of this interface would instantiate these types with the concrete\\ntypes used by the chain, eg. Tendermint datatypes. Each concrete datatype must be provided\\nwith a `From` instance to lift it into its corresponding `Any...` enumeration.\\nFor the purpose of unit-testing, a mock implementation of the `ClientDef` trait could look as follows:\\n```rust\\nstruct MockHeader(u32);\\nimpl Header for MockHeader {\\n\/\/ omitted\\n}\\nimpl From<MockHeader> for AnyHeader {\\nfn from(mh: MockHeader) -> Self {\\nSelf::Mock(mh)\\n}\\n}\\nstruct MockClientState(u32);\\nimpl ClientState for MockClientState {\\n\/\/ omitted\\n}\\nimpl From<MockClientState> for AnyClientState {\\nfn from(mcs: MockClientState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockConsensusState(u32);\\nimpl ConsensusState for MockConsensusState {\\n\/\/ omitted\\n}\\nimpl From<MockConsensusState> for AnyConsensusState {\\nfn from(mcs: MockConsensusState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockClient;\\nimpl ClientDef for MockClient {\\ntype Header = MockHeader;\\ntype ClientState = MockClientState;\\ntype ConsensusState = MockConsensusState;\\n}\\n```\\nSince the actual type of client can only be determined at runtime, we cannot encode\\nthe type of client within the message itself.\\nBecause of some limitations of the Rust type system, namely the lack of proper support\\nfor existential types, it is currently impossible to define `Reader` and `Keeper` traits\\nwhich are agnostic to the actual type of client being used.\\nWe could alternatively model all chain-specific datatypes as boxed trait objects (`Box<dyn Trait>`),\\nbut this approach runs into a lot of limitations of trait objects, such as the inability to easily\\nrequire such trait objects to be Clonable, or Serializable, or to define an equality relation on them.\\nSome support for such functionality can be found in third-party libraries, but the overall experience\\nfor the developer is too subpar.\\nWe thus settle on a different strategy: lifting chain-specific data into an `enum` over all\\npossible chain types.\\nFor example, to model a chain-specific `Header` type, we would define an enumeration in the following\\nway:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] \/\/ TODO: Add Eq\\npub enum AnyHeader {\\nMock(mocks::MockHeader),\\nTendermint(tendermint::header::Header),\\n}\\nimpl Header for AnyHeader {\\nfn height(&self) -> Height {\\nmatch self {\\nSelf::Mock(header) => header.height(),\\nSelf::Tendermint(header) => header.height(),\\n}\\n}\\nfn client_type(&self) -> ClientType {\\nmatch self {\\nSelf::Mock(header) => header.client_type(),\\nSelf::Tendermint(header) => header.client_type(),\\n}\\n}\\n}\\n```\\nThis enumeration dispatches method calls to the underlying datatype at runtime, while\\nhiding the latter, and is thus akin to a proper existential type without running\\ninto any limitations of the Rust type system (`impl Header` bounds not being allowed\\neverywhere, `Header` not being able to be treated as a trait objects because of `Clone`,\\n`PartialEq` and `Serialize`, `Deserialize` bounds, etc.)\\nOther chain-specific datatypes, such as `ClientState` and `ConsensusState` require their own\\nenumeration over all possible implementations.\\nOn top of that, we also need to lift the specific client definitions (`ClientDef` instances),\\ninto their own enumeration, as follows:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Eq)]\\npub enum AnyClient {\\nMock(mocks::MockClient),\\nTendermint(tendermint::TendermintClient),\\n}\\nimpl ClientDef for AnyClient {\\ntype Header = AnyHeader;\\ntype ClientState = AnyClientState;\\ntype ConsensusState = AnyConsensusState;\\n}\\n```\\nMessages can now be defined generically over the `ClientDef` instance:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]\\npub struct MsgCreateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub client_type: ClientType,\\npub consensus_state: CD::ConsensusState,\\n}\\npub struct MsgUpdateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub header: CD::Header,\\n}\\n```\\nThe `Keeper` and `Reader` traits are defined for any client:\\n```rust\\npub trait ClientReader {\\nfn client_type(&self, client_id: &ClientId) -> Option<ClientType>;\\nfn client_state(&self, client_id: &ClientId) -> Option<AnyClientState>;\\nfn consensus_state(&self, client_id: &ClientId, height: Height) -> Option<AnyConsensusState>;\\n}\\npub trait ClientKeeper {\\nfn store_client_type(\\n&mut self,\\nclient_id: ClientId,\\nclient_type: ClientType,\\n) -> Result<(), Error>;\\nfn store_client_state(\\n&mut self,\\nclient_id: ClientId,\\nclient_state: AnyClientState,\\n) -> Result<(), Error>;\\nfn store_consensus_state(\\n&mut self,\\nclient_id: ClientId,\\nconsensus_state: AnyConsensusState,\\n) -> Result<(), Error>;\\n}\\n```\\nThis way, only one implementation of the `ClientReader` and `ClientKeeper` trait is required,\\nas it can delegate eg. the serialization of the underlying datatypes to the `Serialize` bound\\nof the `Any...` wrapper.\\nBoth the `process` and `keep` function are defined to take a message generic over\\nthe actual client type:\\n```rust\\npub fn process(\\nctx: &dyn ClientReader,\\nmsg: MsgCreateClient<AnyClient>,\\n) -> HandlerResult<CreateClientResult<AnyClient>, Error>;\\npub fn keep(\\nkeeper: &mut dyn ClientKeeper,\\nresult: CreateClientResult<AnyClient>,\\n) -> Result<(), Error>;\\n```\\nSame for the top-level dispatcher:\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: ClientMsg<AnyClient>) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ClientReader + ClientKeeper;\\n```\\nWith this boilerplate out of way, one can write tests using a mock client, and associated mock datatypes\\nin a fairly straightforward way, taking advantage of the `From` instance to lift concerete mock datatypes\\ninto the `Any...` enumeration:\\n```rust\\n#[test]\\nfn test_create_client_ok() {\\nlet client_id: ClientId = \"mockclient\".parse().unwrap();\\nlet reader = MockClientReader {\\nclient_id: client_id.clone(),\\nclient_type: None,\\nclient_state: None,\\nconsensus_state: None,\\n};\\nlet msg = MsgCreateClient {\\nclient_id,\\nclient_type: ClientType::Tendermint,\\nconsensus_state: MockConsensusState(42).into(), \/\/ lift into `AnyConsensusState`\\n};\\nlet output = process(&reader, msg.clone());\\nmatch output {\\nOk(HandlerOutput {\\nresult,\\nevents,\\nlog,\\n}) => {\\n\/\/ snip\\n}\\nErr(err) => {\\npanic!(\"unexpected error: {}\", err);\\n}\\n}\\n}\\n```\\n","tokens":30,"id":4656,"Prediction":"## Decision\nWe have decided to implement the Inter-Blockchain Communication (IBC) handlers within the `ibc` (modules) crate by adhering to a modular design. This will involve the following steps:\n\n1. **Separation of Concerns**: \n   Each handler will be developed as an independent module. This will allow for clear separation of responsibilities and make maintenance and updates more manageable.\n\n2. **Reusable Components**:\n   We will design the handlers to have reusable components where applicable. This will reduce redundancy and streamline the development process. Common functions and utilities will be placed in a shared utility module.\n\n3. **Standard Interfaces**:\n   We will design and implement standard interfaces for the handlers. This will ensure consistency and interoperability between various IBC handlers, and make it easier to integrate new handlers in the future.\n\n4. **Compliance with IBC Specifications**:\n   All handlers will strictly comply with the IBC specifications to ensure interoperability with other IBC-compatible systems. We will regularly review the specifications to ensure our implementations remain up-to-date.\n\n5. **Testing and Validation**:\n   Comprehensive unit and integration tests will be written for each handler. This will help in validating the functionality and ensuring robustness. We will also implement a continuous integration pipeline to automatically run tests on each commit.\n\n6. **Documentation**:\n   Detailed documentation will be written for each handler. This documentation will include usage examples, configuration options, and guidelines for extending the handlers.\n\n7. **Security Considerations**:\n   Special emphasis will be placed on the security aspects of the IBC handlers. We will perform regular security audits and threat modeling to identify and mitigate potential vulnerabilities.\n\nBy following this approach, we aim to create a robust, maintainable, and scalable implementation of IBC handlers that aligns with both industry standards and the specific needs of our system.","GenTime":"2024-07-29 02:07:23"}
{"File Name":"docs\/0012-winery-topology-modeler.md","Context":"## Context and Problem Statement\\nWe want to use a component to graphically model nodes and edges in a web application.\\n## Decision Drivers <!-- optional -->\\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\n\n## Decision\n","Decision":"* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\nChosen option: Choosen a Combination of ngx-graph and plain d3.js, because ngx-graph might miss some usefull features but has good usability which is worth to have a look at\\n### Positive Consequences <!-- optional -->\\n* easy to use graph modeler\\n* only a few dependencies\\n* full control over features and behavior\\n### Negative consequences <!-- optional -->\\n* needs to be implemented\\n","tokens":59,"id":4689,"Prediction":"## Decision\n\nWe have decided to use a graph modeling library called **Cytoscape.js** for the following reasons:\n\n1. **Usable with Angular:**\n   - **Cytoscape.js** provides full compatibility and integrates well with Angular applications. There are also Angular-specific wrappers available that simplify integration, such as **ngx-cytoscape**.\n\n2. **Number of Dependencies:**\n   - **Cytoscape.js** has a moderate number of dependencies, making it neither too lightweight nor too heavy for our application. It strikes a balance between functionality and minimal overwriting of the application's nature.\n\n3. **Features:**\n   - **Create Nodes:**\n     - **Cytoscape.js** supports the creation of a wide variety of node types.\n   - **Create Connections:**\n     - It also supports various types of connections (edges), and they can be dynamically created, styled, and manipulated.\n\n4. **Usability:**\n   - **Cytoscape.js** offers a high degree of customization and interactivity, making it highly user-friendly for end-users who need to interact with complex graphs. Additionally, its architecture is flexible and developer-friendly, aiding in easy implementation and extension.\n\nBy selecting **Cytoscape.js**, we ensure that we meet our primary decision drivers effectively while providing a rich user experience and maintainability for future expansions.","GenTime":"2024-07-29 02:09:26"}
{"File Name":"docs\/0022-function-to-component-mapping.md","Context":"## Context and Problem Statement\\nTo implement the EAI patterns we use a combination of a generic component\\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\\n## Decision Drivers <!-- optional -->\\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\n\n## Decision\n","Decision":"* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\\n","tokens":231,"id":4694,"Prediction":"## Decision\n\nAfter evaluating various techniques to enable the integration between the generic component and the FaaS solution for implementing EAI patterns, we have decided to use environment variables for passing the necessary configuration information. Specifically, we will use environment variables to provide the address of the FaaS gateway and the specific function names needed by the generic component instances. \n\n### Rationale\n\n1. **Support by Language\/Technology**: Environment variables are widely supported across different programming languages and technologies used to implement generic components, making this approach highly compatible with our existing technology stack.\n \n2. **Ease of Integration into MICO**: Environment variables are straightforward to configure and manage within containerized environments, such as those used in MICO. This makes the integration process simpler and less error-prone compared to other methods.\n\n3. **Well-Known and Proven Solution**: Utilizing environment variables for configuration is a well-established and widely adopted practice in the industry. This approach is known for its ease of use and reliability, ensuring that we can efficiently manage configuration without introducing additional complexity.\n","GenTime":"2024-07-29 02:09:43"}
{"File Name":"docs\/0018-requirements-application-composition.md","Context":"## Context and Problem Statement\\nWe want to have clear and simple requirements when it comes to the way applications can be created in the user interface.\\n## Decision Drivers\\n* MUST be compatible with Lombok\\n\n## Decision\n","Decision":"* MUST be compatible with Lombok\\nChosen option: the first option, since we want a simple solution, in order to have the system running as soon as possible.\\n### Positive Consequences\\n* Lombok can be used.\\n* Better code quality.\\n### Negative consequences\\n* Applications cannot be created using other existing applications.\\n","tokens":42,"id":4695,"Prediction":"## Decision\n\nTo ensure the requirements for creating applications in the user interface are clear and simple, we will adopt the following approach:\n\n1. **Use Lombok for Boilerplate Code Reduction**: We will utilize the Lombok library to reduce boilerplate code in our models. This will ensure that the code remains clean, concise, and easier to maintain.\n\n2. **Clear Field Annotations**: We will use Lombok annotations such as `@Getter`, `@Setter`, `@Builder`, and `@ToString` on our model classes. These annotations will automatically generate getters, setters, builder methods, and `toString()` methods, adhering to the simplicity and clarity requirements.\n\n3. **Consistent and Clear Naming Conventions**: We will establish consistent naming conventions for fields and methods across our application. This will facilitate easier understanding and usage of the models created using the user interface.\n\n4. **Detailed Documentation**: We will provide thorough documentation explaining how to use various features provided by Lombok in our context. This will include examples and best practices to guide developers in creating applications via the user interface.\n\nBy implementing these decisions, we will achieve a clear and simple approach to application creation in our user interface, while ensuring compatibility with Lombok.","GenTime":"2024-07-29 02:09:47"}
{"File Name":"openjdk-api-v3\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4699,"Prediction":"## Decision\nTo effectively document the architectural decisions made on this project, we will adopt the use of Architectural Decision Records (ADR). Each ADR will be created as a markdown file within a dedicated `decisions` directory in the project's repository. The ADRs will include the following sections: Title, Status, Context, Decision, Consequences, and Alternatives Considered. This structured documentation practice will ensure that all significant architectural choices are clearly recorded and easily accessible for future reference and decision-making.","GenTime":"2024-07-29 02:09:54"}
{"File Name":"cloud-on-k8s\/adr-template.md","Context":"## Context and Problem Statement\\n[Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n## Decision\n","Decision":"* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best].\\n### Positive Consequences <!-- optional -->\\n* [For example, improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [For example, compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":96,"id":4701,"Prediction":"## Context and Problem Statement\nWe are currently facing performance bottlenecks in our system due to the existing monolithic architecture. As our user base has grown, the system has become difficult to scale and maintain. The monolithic structure results in tight coupling, making it hard to implement changes or updates without risking overall system stability. How do we redesign our system architecture to improve scalability and maintainability?\n\n## Decision Drivers\n* Scalability: Need to support a growing number of users without degradation in performance.\n* Maintainability: Simplify the process of making changes and updating features.\n* Flexibility: Allow for independent deployment and scaling of different components.\n* Fault isolation: Minimize the impact of failure in one component on the entire system.\n\n## Decision\nWe will transition from a monolithic architecture to a microservices architecture. This approach aligns with our decision drivers by breaking down the application into smaller, loosely coupled services that can be developed, deployed, and scaled independently. Each microservice will handle a specific business function and communicate with other services through well-defined APIs.\n\nThis architectural decision will involve the following steps:\n1. Identify and define the different services based on business domains.\n2. Implement a communication strategy among services, preferring asynchronous messaging to improve fault tolerance.\n3. Establish a continuous integration and deployment (CI\/CD) pipeline to facilitate seamless updates.\n4. Use containerization (e.g., Docker) to ensure consistency and portability across different environments.\n5. Monitor and manage services using centralized logging and performance monitoring tools.","GenTime":"2024-07-29 02:10:04"}
{"File Name":"cloud-on-k8s\/0006-sidecar-health.md","Context":"## Context and Problem Statement\\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\\nAs for now an Elasticsearch pod is composed of 2 containers:\\n- a main container for Elasticsearch\\n- a sidecar container for running the keystore-updater\\nWhat is the keystore-updater in the sidecar doing?\\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\\nTo connect to ES it depends on:\\n- an environment variable for the username\\n- secrets mounted as readonly files for the password and the CA certificate\\n- the Elasticsearch readiness\\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\\ncan impact the Elasticsearch availability by design.\\nHowever Go binaries that do simple things are very fast to start and very reliable.\\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\\ncompared to have an Elasticsearch failure.\\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\\ndo not report errors during this period. Then how to detect that ES has never started?\\nThe ES readiness probe will fail if ES never becomes ready.\\n## Decision Drivers\\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\n\n## Decision\n","Decision":"* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\\n### Positive Consequences\\n* Collecting the sidecar health from the operator side gives us more options to react to failures\\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\\n* Does not depend on the Kubernetes probes or the Kubelet\\n* Minimize external dependencies\\n### Negative Consequences\\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\\n* Add complexity and responsibility to the operator\\n","tokens":455,"id":4705,"Prediction":"## Decision\n\nWe will implement a dedicated health check mechanism for the Elasticsearch sidecar to ensure that its health status is monitored independently of the Elasticsearch container. The health check will validate the proper setup and function of the sidecar's key tasks and allow for distinct error reporting. \n\n### Key Actions:\n\n1. **Independent Health Check Endpoint:**\n    - Develop a lightweight health check endpoint in the sidecar container to confirm it can successfully:\n        * Access required environment variables (e.g., username).\n        * Read the mounted secrets for password and CA certificate.\n        * Make an HTTP call to the Elasticsearch endpoint `\/_nodes\/reload_secure_settings`.\n\n2. **Startup Grace Period:**\n    - Implement a startup grace period logic in the sidecar health check to account for expected initial Elasticsearch unavailability:\n        * The sidecar health check will initially delay error reporting for a configured period.\n        * During this delay, errors due to Elasticsearch being unready will be tolerated.\n        * A timeout will be set to ensure Elasticsearch becomes ready within a reasonable time; otherwise, the sidecar health check will start reporting failures.\n\n3. **Distinct Error Reporting:**\n    - Implement clear logging and metrics to differentiate between errors in the sidecar and Elasticsearch container, including:\n        * Authentication or connection failures in the sidecar.\n        * Errors owing to the Elasticsearch service status.\n    \n4. **Impact Mitigation:**\n    - Adjust the readiness probe configuration in Kubernetes to consider both containers:\n        * Ensure the pod is only marked as ready if both the Elasticsearch and sidecar containers are healthy.\n        * Kubernetes will continue to avoid routing traffic to an unhealthy Elasticsearch pod, preserving current behavior, but with better diagnostics for sidecar-related issues.\n\n5. **Reliability and Simplification:**\n    - Use a Kubernetes init container to perform an initial, blocking health check of the sidecar before starting the Elasticsearch container, ensuring initial setup correctness.\n    - Employ robust retry logic with exponential backoff in the sidecar for connecting to Elasticsearch in order to handle transient network or service startup issues gracefully.\n\nThis decision will promote higher observability and reliability of the Elasticsearch cluster, distinguishing sidecar-specific issues, and ensuring overall system simplicity and robustness.","GenTime":"2024-07-29 02:10:28"}
{"File Name":"cloud-on-k8s\/0005-configurable-operator.md","Context":"## Context and Problem Statement\\nMost operators out there operate in one of these two modes:\\n1. Cluster-wide operator. Can manage resources in all namespaces, with cluster-wide admin privilege. A single operator running on the cluster.\\n2. Namespaced operator. Can manage resources in the namespace it's deployed in, with admin permissions in that namespace. Several operators can be running in different namespaces.\\nThe first option (cluster-wide single operator) has some major drawbacks:\\n* it does not scale well with the number of clusters\\n* it requires elevated permissions on the cluster\\nThe second option (namespace operator) also has some major drawbacks:\\n* it does not play well with cross-namespace features (a single enterprise license pool for multiple clusters in multiple namespaces, cross-cluster search and replication on clusters across namespaces)\\n* to deploy 5 clusters in 5 different namespaces, it requires 5 operators running. A single one could have been technically enough.\\n## Decision Drivers\\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\n\n## Decision\n","Decision":"* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\nChosen option: option 2 (configurable operator), because it gives us more flexibility on the deployment strategy, and allows restricting RBAC permissions to a finer-grained level.\\n### Positive Consequences\\n* Much more flexibility to cover various deployment scenarios\\n* a single cluster-wide operator\\n* one operator per namespace\\n* one operator for all production namespaces, another one for all staging namespaces\\n* and so on\\n* We don't have to require cluster-level permissions to handle enterprise licensing\\n* A single operator concept, no namespace\/global\/ecosystem vocabulary madness\\n### Negative Consequences\\n* Too many options can lead to confusion, we need proper documentation\\n* Increased yaml complexity: need to develop a tool to generate yaml specifications\\n* The controller-runtime is not ready yet for multi-namespace watches\\n","tokens":354,"id":4707,"Prediction":"## Decision\n\nWe will implement a hybrid operator model that combines the advantages of both cluster-wide and namespace-specific operators while mitigating their drawbacks.\n\n### Description of the Hybrid Model:\n1. **Cluster-Wide Operator Core**: \n   - Deploy a single cluster-wide core operator that handles cross-namespace functionalities (e.g., enterprise licensing, cross-cluster search, and replication).\n   - This core operator will have elevated permissions but will perform limited, critical tasks that require cluster-wide visibility.\n   \n2. **Namespace-Specific Operators**:\n   - Deploy namespace-specific operators for each namespace where detailed management is required. \n   - These operators will only have admin permissions within their respective namespaces.\n   - This ensures that operations are isolated per namespace, which enhances security and scalability.\n\n### Details:\n\n- **Modular Permissions**:\n  - The cluster-wide operator will be granted minimum necessary permissions to handle its cross-namespace duties.\n  - Namespace-specific operators will have admin permissions within their own namespaces only, ensuring adherence to the principle of least privilege.\n\n- **Scalability**:\n  - Since the detailed management is decentralized to namespace-specific operators, the approach can scale up effectively with large Kubernetes installations (tens of thousands of clusters).\n  - The load is distributed among the namespace-specific operators, preventing any single operator from becoming a bottleneck.\n\n- **Security**:\n  - By limiting the elevated permissions to essential functionalities in the cluster-wide operator, and restricting other permissions to namespace-specific operators, we enhance the overall security posture.\n  - Credentials and RBAC permissions are easier to manage and audit within this structure.\n\n- **Performance Isolation**:\n  - High variance in response times for API calls from overloaded or poorly performing clusters will not affect the operations of other clusters, as each namespace-specific operator works independently.\n  - Any latency or issue is contained within the boundary of the namespace the operator manages.\n\n### Implementation Steps:\n\n1. Develop and deploy a core cluster-wide operator with limited but essential elevated permissions.\n2. Develop and deploy namespace-specific operators with admin permissions limited to their respective namespaces.\n3. Implement mechanisms for communication between the cluster-wide operator and the namespace-specific operators for cross-namespace functionalities.\n4. Establish detailed RBAC policies for each type of operator to ensure permissions adhere strictly to the principle of least privilege.\n5. Conduct thorough testing in a variety of cluster sizes to ensure the hybrid model performs well under different loads.\n\nBy adopting this hybrid model, we can achieve a solution that scales effectively both down and up while balancing security, performance isolation, and ease of management.","GenTime":"2024-07-29 02:10:35"}
{"File Name":"latis3\/0005-Operations-defined-in-FDML-schema-as-elements.md","Context":"## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"\/>\\n<\/xs:complexType>\\n<\/xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n\n## Decision\n","Decision":"* consistency\\n* expressiveness\\nChosen option: Try to describe operations as elements only\\n### Positive Consequences\\n* consistency\\n### Negative Consequences\\n* future operations may not be definable\\n","tokens":115,"id":4713,"Prediction":"## Decision\nAll operations in the FDML will be defined using attributes within complex elements rather than solely as elements. This approach provides a consistent and expressive way to describe operations, ensuring that each operation follows a uniform structure and is rich in detail.\n\nFor instance, the operation \"take\" will be described as:\n```xml\n<xs:element name=\"take\">\n    <xs:complexType>\n        <xs:attribute name=\"value\" type=\"xs:integer\"\/>\n    <\/xs:complexType>\n<\/xs:element>\n```\n\n### Rationale\n1. **Consistency**: Using a single method to define operations ensures that all operations are described in a standardized manner. This uniformity simplifies understanding, maintenance, and usage of the schema.\n   \n2. **Expressiveness**: Defining operations with attributes within complex types allows for more detailed descriptions. Attributes provide additional clarity and context for each operation, making the schema more informative and easier to use.\n\nThis decision promotes better practice for future development and maintenance of the FDML schema.","GenTime":"2024-07-29 02:10:51"}
{"File Name":"latis3\/0001-Scala-programming-language.md","Context":"## Context and Problem Statement\\nThe hybrid object-oriented\/functional language Scala is ideally suited for developing LaTiS.\\n## Decision Drivers <!-- optional -->\\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\n\n## Decision\n","Decision":"* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\nChosen option: Scala is the language-of-choice for LaTiS development.  The only decision driver above that is not in alignment with this decision is the fact that Python is the preferred language for most scientific programmers.\\n### Positive Consequences <!-- optional -->\\n* immutable data types in Scala reduce the challenges of introducing parallelism\\n* for an extensible library like LaTiS a functional programming language like Scala provides better abstractions and tools to reason about transformations of large data sets\\n* a strongly typed language like Scala helps to prevent code rot and reduces the chance that defects will slip into the code base\\n* well designed Scala applications are based on sound software engineering principles even though they may take more effort to build than similar Java or Python programs\\n* that being said, Scala programs tend to be easier to maintain and refactor which actually minimizes effort over the life of a project\\n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use cases\\n* Apache Spark, written in Scala, is the leading framework for processing large amounts of data and is becoming on of the primary framework for implementing machine learning algorithms\\n* Scala supports the creation of DSLs which will allow users of LaTiS to specify processing instructions for datasets without themselves nedding to learn Scala\\n* the full gamut of Java libraies is available to Scala developers\\n* many successful Java libraries eventually get re-written in Scala\\n* Scala encourages software craft-people to follow sound engineering principles instead of just developing something that just gets the job done, this is probably the most important factor in choosing Scala, but the hardest to document\\n### Negative Consequences\\n* Scala developers are not as common as Python developers, especially in the field of scientific computing\\n* Python is clearly the dominant language used by scientific programmers\\n","tokens":97,"id":4716,"Prediction":"## Decision\nBased on the context and problem statement, the decision is to use Scala as the primary programming language for the development of LaTiS. Scala's hybrid nature, combining object-oriented and functional programming paradigms, aligns perfectly with the project's requirements. The language's strong type system enhances code correctness, while its functional aspects facilitate explicit data transformations and reasoning about code.\n\nThe following factors influenced this decision:\n1. **Functional Programming Merits:** Scala supports higher-order functions, immutability, and allows for concise and expressive code, which are hallmarks of functional programming beneficial for data transformations.\n2. **Strongly Typed Language:** The strong type system of Scala aids in catching errors at compile-time, contributing to overall code reliability and maintenance.\n3. **Developer Availability:** There is a growing pool of Scala developers, especially within the scientific and data-driven community, ensuring that skilled developers can be sourced for the project.\n4. **Community Consensus:** Scala enjoys a positive consensus among the scientific programming community, which is essential for collaboration and community-driven development.\n5. **Large Data Set Processing:** Scala is built on the JVM and is compatible with large-scale data processing frameworks like Apache Spark, which makes it highly suitable for handling large datasets.\n6. **Code Correctness and Reasoning:** Through its functional programming features, transformations to data in Scala are explicit, and the code can be reasoned about systematically.\n7. **Support for DSLs:** Scala's flexible syntax and powerful type system make it an excellent choice for developing internal Domain Specific Languages (DSLs), which can greatly enhance expressiveness and productivity in scientific computing contexts.\n\nIn summary, Scala is selected as the development language for LaTiS due to its alignment with the outlined decision drivers and the context-specific requirements of the project.","GenTime":"2024-07-29 02:11:05"}
{"File Name":"jskatas.org\/000-use-adrs.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4722,"Prediction":"## Decision\n\nWe will implement an Architectural Decision Record (ADR) process for documenting all significant architectural decisions made during the course of this project. \n\n1. **ADR Format**: \n   - Each ADR will follow a consistent template that includes the following sections:\n     - **Title**: A descriptive title for the decision.\n     - **Context**: The circumstances that led to the decision.\n     - **Decision**: A description of the decision itself.\n     - **Consequences**: The pros and cons of the decision, including potential risks and trade-offs.\n     - **Status**: The current status of the decision (e.g., proposed, accepted, superseded, deprecated).\n\n2. **Tooling**:\n   - ADRs will be stored in a version-controlled repository to ensure that all team members can access and contribute to them. Git will be used for version control.\n   - Each ADR will be a markdown file (`.md`) housed in a dedicated `adr` directory at the root of the project repository.\n\n3. **Decision-Making Process**:\n   - Architectural decisions will be made collaboratively during architecture\/design meetings that include key stakeholders (developers, architects, product owners, etc.).\n   - Decisions will be recorded by an assigned team member during or immediately after the meeting.\n\n4. **Review and Approval**:\n   - Every ADR draft will be reviewed by at least one other team member before it is finalized.\n   - Approved ADRs will be merged into the main branch of the repository, ensuring they are part of the project\u2019s permanent record.\n\n5. **Visibility and Communication**:\n   - Regular updates and reviews of ADRs will be included as an agenda item in our recurring sprint meetings to ensure that all team members are aware of the architectural decisions.\n   - Significant ADRs will be communicated through team channels (e.g., email, Slack) to ensure everyone is informed.\n\nBy implementing this ADR process, we ensure that our architectural decisions are well-documented, easily accessible, and transparent, enhancing both current and future understanding of the project's architecture.","GenTime":"2024-07-29 02:11:23"}
{"File Name":"biosamples-v4\/0004-inverse-relationships-sample-update.md","Context":"## Context\\nWhen a relation is created between two samples, the sample target of the relationship doesn't change the update date.\\nStated in another way, the relationship inversion process doesn't change the update date of the sample.\\nThis is associated with BSD-1088 - https:\/\/www.ebi.ac.uk\/panda\/jira\/browse\/BSD-1088\\n\n## Decision\n","Decision":"We decided that is good for the relationship inversion process to not change the update date of the sample as we don't\\nhave any actual usecase for this to happen and it would also create issues for NCBI as they don't care about relationships\\nand no real information is added to the sample\\n","tokens":74,"id":4726,"Prediction":"## Decision\n\nAfter careful consideration, we have decided to update the logic to ensure that the update date of a sample is modified when new relationships are created or existing relationships are inverted. \n\n### Justification:\n1. **Data Consistency**:\n   - Ensuring that the update dates reflect the latest interactions or modifications helps maintain data integrity and transparency. Users can trust that if a sample has a recent update date, it is because something relevant, such as the creation or modification of relationships, has occurred.\n   \n2. **Traceability**:\n   - Updating the date when relationships change improves traceability. Auditors and reviewers can accurately determine when changes between samples were made, enhancing the reliability of our tracking and logging systems.\n\n3. **User Expectation**:\n   - This behavior aligns with user expectations and common practices where update timestamps generally reflect the latest changes or interactions.\n\n4. **Compliance and Reporting**:\n   - For reporting purposes, having accurate update timestamps is crucial. It ensures compliance with data governance policies, making auditing processes smoother and more trustworthy.\n\n5. **BSD-1088 Association**:\n   - Addressing the requirements and issues highlighted in BSD-1088, thereby improving our overall system's robustness and the user experience.\n\n### Implementation:\n- Update the services responsible for managing relationships to ensure the related sample\u2019s update date is modified upon the creation or inversion of relationships.\n- Thoroughly test these changes to ensure no unintended side effects occur and that the update dates reliably reflect relationship changes.\n- Document the changes in the system's update logs and notify relevant stakeholders to ensure everyone is aware of the new behavior.\n\nThis decision aims to enhance the overall consistency and reliability of our system by making sure update dates accurately represent the most recent activities involving sample relationships. \n\n---\n\nPlease refer to the associated JIRA issue BSD-1088 for further details and ongoing updates related to this decision.\n","GenTime":"2024-07-29 02:11:38"}
{"File Name":"jabref\/0015-support-an-abstract-query-syntax-for-query-conversion.md","Context":"## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n\n## Decision\n","Decision":"Chosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.\\nFurthermore parsers for lucene already exist and are tested.\\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\\n* All terms in the query are whitespace separated and will be ANDed\\n* Default and certain fielded terms are supported\\n* Fielded Terms:\\n* `author`\\n* `title`\\n* `journal`\\n* `year` (for single year)\\n* `year-range` (for range e.g. `year-range:2012-2015`)\\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\\n* The `year` and `year-range` fields are mutually exclusive\\n* Example:\\n* `author:\"Igor Steinmacher\" author:\"Christoph Treude\" year:2017` will be converted to\\n* `author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" AND year:2017`\\nThe supported syntax can be expressed in EBNF as follows:\\nQuery := {Clause} \\\\nClause:= \\[Field\\] Term \\\\nField := author: | title: | journal: | year: | year-range: | default:\\\\nTerm  := Word | Phrase \\\\nWord can be derived to any series of non-whitespace characters.\\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\\nsuch a query does not make sense semantically and therefore will not be executed.\\n### Positive Consequences\\n* Already tested\\n* Well known\\n* Easy to implement\\n* Can use an existing parser\\n","tokens":84,"id":4738,"Prediction":"## Decision\n\nWe will adopt a custom, standardized query syntax for the abstract search queries that balances usability and functionality, allowing for easy translation into various library-specific query syntaxes.\n\n### Key Features of the Syntax:\n\n1. **Basic Operations**: The syntax will support basic search operations such as AND, OR, and NOT. These logical operators are essential for combining multiple search criteria.\n   \n2. **Field-specific Searches**: Users will have the ability to specify fields within their queries to narrow down their search results. For example, searches can be conducted within titles, authors, tags, etc.\n   \n3. **Wildcard and Fuzzy Searches**: To accommodate partial and approximate matches, especially for user input errors, the syntax will support wildcard (e.g., `*` and `?`) and fuzzy search operators.\n   \n4. **Phrase Searches**: The ability to search for exact phrases by enclosing terms in quotation marks (e.g., `\"exact phrase\"`) will enable more precise searching.\n   \n5. **Range Searches**: For numeric or date fields, the syntax will support range queries (e.g., `date:[2020-01-01 TO 2020-12-31]`) to filter results within specific intervals.\n\n6. **Grouping**: Parentheses will be used to group query terms and operators, allowing for complex query constructions (e.g., `(author:Smith AND year:2021) OR title:Quantum`).\n\n### Rationale:\n\n1. **Consistency**: A standardized query syntax ensures that users have a consistent experience across different libraries, reducing the learning curve and potential confusion.\n   \n2. **Flexibility**: Providing a rich set of features ensures that users can create complex searches to meet their specific needs, enhancing the overall usability of the system.\n   \n3. **Compatibility**: The chosen syntax and features are designed to be easily translatable into the query languages of various libraries, facilitating seamless integration.\n   \n4. **Simplicity**: While the syntax provides powerful features, it remains simple enough for users to learn and apply effectively without needing to become experts in query languages.\n\n### Implementation:\n\n- **Query Parser**: Implement a query parser that translates the standardized syntax into an Abstract Syntax Tree (AST).\n- **Translators**: Develop translators that convert the AST into library-specific queries for each supported library.\n- **Documentation**: Provide thorough documentation and examples to assist users in learning and utilizing the new query syntax effectively.\n\nThis decision aims to improve the user experience by providing a powerful, yet user-friendly query language that bridges the gap between different library-specific search syntaxes.","GenTime":"2024-07-29 02:12:23"}
{"File Name":"jabref\/0021-keep-study-as-a-dto.md","Context":"## Context and Problem Statement\\nThe study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\\nThis poses the question: should the study remain a pure DTO object or should it contain direct object instances?\\n\n## Decision\n","Decision":"Chosen option: \"Keep study as DTO and use transformators\", because comes out best (see below).\\n","tokens":49,"id":4741,"Prediction":"## Decision\nThe decision is to allow the study to contain direct object instances of complex query and fetcher rather than remaining a pure DTO object.\n\n### Rationale\n1. **Encapsulation and Modularity**: By allowing the study to contain direct object instances, the design encapsulates the behavior related to complex queries and fetching within the study entity itself. This promotes modularity and clear boundaries within the system.\n  \n2. **Simplified Code Base**: Directly embedding complex query and fetcher instances within the study eliminates the need for external management of these objects. This reduces boilerplate code and results in a cleaner, more maintainable codebase.\n\n3. **Enhanced Flexibility**: Having direct object instances allows the study to have more sophisticated logic and methods, enabling it to perform complex operations on its own data without relying on external services or functions.\n\n4. **Performance Considerations**: Direct access to the query and fetcher instances within the study can offer performance benefits as it reduces the overhead of passing data around and minimizes dependency on external services, which may be costly in terms of latency.\n\n5. **Alignment with Domain-Driven Design (DDD)**: In DDD, entities are encouraged to be rich objects that encapsulate behavior as well as data. Allowing the study to directly hold instances of query and fetcher aligns with this principle, making the domain model more expressive and aligned with business rules.","GenTime":"2024-07-29 02:12:37"}
{"File Name":"jabref\/0002-use-slf4j-for-logging.md","Context":"## Context and Problem Statement\\nUp to version 4.1 JabRef uses apache-commons-logging 1.2 for logging errors and messages. However, this is not compatible with java 9 and is superseded by log4j.\\n## Decision Drivers\\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\n\n## Decision\n","Decision":"* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\nChosen option: \"SLF4J with Log4j2 binding\", because comes out best \\(see below\\).\\n","tokens":104,"id":4742,"Prediction":"## Decision\n\n### Chosen Solution\n- We will migrate from `apache-commons-logging` to `SLF4J` (Simple Logging Facade for Java) with `Log4j` as the underlying logging framework. \n\n### Reasoning\n1. **Compatibility with Java 9**: Since `apache-commons-logging` 1.2 is not compatible with Java 9, migrating to a logging framework that supports Java 9 is essential for keeping up with the latest Java versions.\n2. **Existing Dependencies**: `Log4j` and `SLF4J` are already part of the project's dependencies, either directly or through third-party libraries. This reduces the overhead of integrating a new logging framework.\n3. **Flexibility**: SLF4J acts as an abstraction layer that allows easy integration with various logging frameworks, including `Log4j`. This ensures future flexibility if the underlying logging framework needs to be changed again.\n4. **Simplification**: Utilizing a single logging fa\u00e7ade (`SLF4J`) can help streamline logging across the entire application, simplifying configuration and maintenance.\n5. **Community Support and Documentation**: Both `SLF4J` and `Log4j` are widely used and well-supported within the Java ecosystem, ensuring robust community support and comprehensive documentation.\n\n### Implementation Steps\n1. **Remove dependency** on `apache-commons-logging` from the project.\n2. **Add dependency** for `SLF4J` and ensure `Log4j` is properly configured as the implementation backend.\n3. **Refactor logging calls** throughout the codebase to use `SLF4J`'s API.\n4. **Test extensively** to confirm that logging behavior and performance remain consistent and that no issues arise due to the changes.\n\n### Consequences\n- **Pros**:\n  - Compatibility with Java 9+\n  - Future-proofing against further changes in underlying logging libraries\n  - Unified logging approach across the application\n  - Leverage existing dependencies, reducing bloat\n- **Cons**:\n  - Initial effort required for migration and refactoring\n  - Potential minor learning curve for team members unfamiliar with `SLF4J`\n\n### Conclusion\nMigrating to `SLF4J` with `Log4j` as the implementation backend presents a strategic alignment with future Java versions, simplifies logging management, and leverages existing dependencies to minimize additional complexity. This decision aligns well with our need to maintain up-to-date and efficient logging practices in a modern Java environment.","GenTime":"2024-07-29 02:12:43"}
{"File Name":"jabref\/0010-use-h2-as-internal-database.md","Context":"## Context and Problem Statement\\nWe need to store data internally in a structured way to gain performance.\\n## Decision Drivers\\n* Easy to integrate\\n* Easy to use\\n* Common technology\\n\n## Decision\n","Decision":"* Easy to integrate\\n* Easy to use\\n* Common technology\\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\\n","tokens":39,"id":4747,"Prediction":"## Decision\n\nWe have decided to use a relational database management system (RDBMS) to store data internally. \n\n### Rationale\n\n1. **Easy to Integrate**: RDBMSs, such as PostgreSQL, MySQL, or SQLite, are well-known for their integration capabilities with various programming languages and frameworks, ensuring a smooth development process.\n  \n2. **Easy to Use**: These databases provide a familiar SQL interface, which is standard and widely understood by developers, making it easier to use and maintain.\n  \n3. **Common Technology**: RDBMS is a proven technology with vast community support, extensive documentation, and a wide array of tools for development, monitoring, and debugging.\n\n### Impact\n\n- **Performance**: The structured nature of relational databases allows for efficient indexing and query optimization, which will enhance performance.\n- **Maintainability**: The standardized query language (SQL) and the plethora of tools and libraries available will aid in long-term maintainability.\n- **Scalability**: While RDBMSs are generally good for structured data and can handle moderate scaling, additional strategies like sharding or replication can be employed if higher scalability is needed in the future.\n\n### Implementation\n\n- **Database Selection**: We will evaluate PostgreSQL, MySQL, and SQLite based on specific project needs, team familiarity, and performance benchmarks.\n- **Schema Design**: A normalized database schema will be designed to efficiently handle the structured data, ensuring minimal redundancy and optimal access patterns.\n- **Integration**: Appropriate ORM (Object-Relational Mapping) tools will be considered to streamline integration with our application code. This includes evaluating tools like SQLAlchemy for Python, Hibernate for Java, or Entity Framework for .NET.\n- **Performance Monitoring**: We will set up performance monitoring tools to track and optimize query performance continuously.\n\nThis decision aligns with our need for a structured, performant data storage solution while leveraging easy integration and common technology to facilitate development and maintenance.","GenTime":"2024-07-29 02:13:03"}
{"File Name":"jabref\/0014-separate-URL-creation-to-enable-proper-logging.md","Context":"## Context and Problem Statement\\nFetchers are failing.\\nThe reason why they are failing needs to be investigated.\\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\\nHow to properly log the URL used for fetching?\\n## Decision Drivers\\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\n\n## Decision\n","Decision":"* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n","tokens":132,"id":4748,"Prediction":"## Decision\n\nWe will implement a solution where the URL used to query the fetcher will be included in the exception message instead of logging it separately. This approach adheres to best practices in exception handling and keeps the code clean and easy to read.\n\n1. **Modify Fetcher Exception Handling**: Update the fetchers to append the URL to the exception message when an exception is thrown. This ensures that any failure context is immediately available for debugging purposes without requiring separate log entries.\n\n2. **Consistent Exception Strategy**: Establish a pattern across all fetchers to handle and throw exceptions in a consistent manner, ensuring that relevant details such as the URL are always included.\n\n### Implementation Steps\n1. **Refactor Fetcher Methods**:\n   - Wrap the fetching logic inside a try-catch block.\n   - Catch specific exceptions that could occur during fetching (e.g., `IOException`, `HttpException`).\n   - Append the URL to the exception message and throw a custom exception if necessary.\n\n   ```java\n   public class Fetcher {\n       public Data fetchData(String url) throws FetcherException {\n           try {\n               \/\/ Fetching logic using the URL\n           } catch (Exception e) {\n               throw new FetcherException(\"Failed to fetch data from URL: \" + url, e);\n           }\n           return data;\n       }\n   }\n   ```\n\n2. **Custom Exception Class**: Create a `FetcherException` class if it doesn't already exist, to encapsulate additional context information.\n\n   ```java\n   public class FetcherException extends Exception {\n       public FetcherException(String message, Throwable cause) {\n           super(message, cause);\n       }\n   }\n   ```\n\n3. **Test the Implementation**: Write unit tests to ensure that the fetchers correctly include the URL in exception messages. This would involve simulating failures and verifying that the exceptions contain the correct message.\n\nBy making these changes, debugging fetcher failures will be more straightforward as the URL will always be included in the exception context. This approach balances the need for helpful debugging information with maintaining readability and following best practices in exception handling.","GenTime":"2024-07-29 02:13:08"}
{"File Name":"insight\/0002-tweepy.md","Context":"## Context\\nTo programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.\\n\n## Decision\n","Decision":"We choose [Tweepy](https:\/\/github.com\/tweepy\/tweepy) as our Twitter API Pyhon library.\\n","tokens":36,"id":4760,"Prediction":"## Decision\nWe have decided to use the `Tweepy` library for programmatically accessing Twitter APIs. This decision is based on the following considerations:\n\n1. **Community Support and Maintenance:** Tweepy is a well-maintained library with a large community of users, ensuring that any potential issues can be quickly addressed through community support or updates.\n2. **Comprehensive API Coverage:** Tweepy provides support for a wide range of Twitter API functionalities, including both REST and Streaming APIs, which fits our requirements.\n3. **Ease of Use:** The library is known for its simplicity and ease of use, which will speed up development time and reduce complexity.\n4. **Documentation:** Tweepy has extensive and well-organized documentation, which is crucial for integrating and troubleshooting the library effectively.\n5. **Popularity:** The library is one of the most popular choices for Python developers working with Twitter APIs, indicating its robustness and reliability.\n\nBy selecting Tweepy, we aim to ensure a smooth and efficient integration with the Twitter APIs, supported by a strong user base and comprehensive documentation.","GenTime":"2024-07-29 02:13:51"}
{"File Name":"insight\/0001-python.md","Context":"## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n\n## Decision\n","Decision":"We choose Python as our base programming language.\\n","tokens":33,"id":4761,"Prediction":"## Decision\n\nThe base programming language selected for this project is Python. \n\n### Rationale\n\n1. **Ease of Use and Learning Curve**: Python is known for its simple and readable syntax, making it accessible for developers of varying skill levels. This will facilitate quicker onboarding of new team members.\n  \n2. **Extensive Libraries and Frameworks**: Python offers a rich ecosystem of libraries and frameworks (e.g., Django for web development, NumPy and Pandas for data analysis, TensorFlow and PyTorch for machine learning) that can significantly speed up development processes.\n  \n3. **Community and Support**: Python has a large and active community. This means abundant resources, tutorials, and third-party packages are readily available. It also ensures that issues can be resolved quickly with community support.\n\n4. **Integration Capability**: Python integrates well with other languages and technologies. This aligns with our need to use other languages as required (e.g., C\/C++ for performance-critical components).\n  \n5. **Versatility**: Python is a general-purpose language suitable for a wide range of applications, from web development to scientific computing, making it a flexible choice for diverse functionalities within the project.\n\n### Implementation\n\n- Core functionalities will be developed using Python.\n- Any specific modules or components requiring higher performance will be implemented using languages like C++ or Java, with proper justification.\n- Code reviews and documentation will ensure that multi-language integration remains seamless and maintainable.\n\n### Contingency\n\n- Regular assessments will be conducted to ensure that Python continues to meet the project requirements.\n- If scalability or performance issues arise that cannot be mitigated, transitioning critical components to more performant languages will be considered and documented accordingly.\n\nThis strategic choice of Python as the base language is expected to yield a balanced approach of rapid development and robust functionality.","GenTime":"2024-07-29 02:13:56"}
{"File Name":"island.is-glosur\/0009-naming-files-and-directories.md","Context":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n\n## Decision\n","Decision":"* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","tokens":93,"id":4766,"Prediction":"## Decision\n\nWe will standardize on a single naming strategy for all files and directories within the monorepo. Our chosen conventions will align with the most conventional and widely accepted industry standards to maximize clarity and ease of navigation for all team members.\n\n### Filename and Directory Naming Conventions\n\n1. **File Names:**\n   - Use kebab-case for all filenames (e.g., `user-profile.ts`, `user-service.js`).\n   - Ensure filenames are descriptive and reflect their purpose or content.\n\n2. **Directory Names:**\n   - Use kebab-case for all directories (e.g., `components`, `utils`, `models`).\n   - Group similar or related files together in well-named directories to enhance discoverability.\n\n### Specific Conventions\n\n1. **Component Files:**\n   - For components, use the following structure:\n     ```\n     components\/\n       my-component\/\n         my-component.component.ts\n         my-component.component.html\n         my-component.component.scss\n         my-component.component.spec.ts\n     ```\n\n2. **Service Files:**\n   - For services, use the following structure:\n     ```\n     services\/\n       my-service.service.ts\n       my-service.service.spec.ts\n     ```\n\n3. **Module Files:**\n   - For modules, use the following structure:\n     ```\n     modules\/\n       my-module.module.ts\n       my-module.module.spec.ts\n     ```\n\n4. **Testing Files:**\n   - Keep all test files alongside the files they test and use the `.spec` suffix (e.g., `my-component.component.spec.ts`).\n\n### Implementation Guidelines\n\n1. **Adopt Tools\/Extensions:** Use linting tools and configuration settings that enforce the chosen naming conventions. \n2. **Documentation:** Update project documentation to reflect the new naming conventions and provide examples.\n3. **Code Reviews:** Ensure that code reviews include checks for adherence to the naming conventions.\n4. **Refactoring:** Gradually refactor existing files and directories to conform to these naming conventions.\n\nBy following these conventions, we ensure the monorepo remains organized, easy to navigate, and maintains a professional, consistent structure conducive to collaboration and development efficiency.","GenTime":"2024-07-29 02:14:20"}
{"File Name":"web-mapviewer\/2020_05_28_test_framework.md","Context":"## Context\\nAn integration test framework (and possibly a combo unit\/integration test) has to be chosen for this project.\\nAfter reviewing what's the state of testing with JS in 2020, here's a list of contenders.\\n### Selenium\\nHas been the standard of the industry for many years. Start to struggle with single page application framework (SPA) like Angular, Vue.js or React.js.\\n- Desktop browsers, emulators \/ simulators and real devices (E2E testing all the way)\\n- Highly scalable\\n- Has many \"sister frameworks\" that enhance the feature set (like Appium for mobile testing, Nightwach or WebdriverIO)\\nWhile very efficient for E2E testing, this framework would be cumbersome to have in development on a local machine.\\nCould be used for E2E testing for cross browser compatibility tests.\\n### TestCafe\\nGood alternative to Selenium, is compatible with Saucelab (we already have some account with them).\\n- Fast to set up\\n- Cross Browser and Devices (with SauceLabs or BrowserStack)\\nThis could also be a good cross browser testing framework, but the cross browser feature will be hard to have deployed on every developers' machine.\\n### Cypress\\nMade by devs for devs.\\n- Very convenient running and debugging tools\\n- Native access to all your application\u2019s variables\\n- Solid and clear documentation\\n- Newest addition (2017) to the big frameworks (very active community)\\n\n## Decision\n","Decision":"After playing a bit with these frameworks, Cypress clearly had something for it.\\nThe documentation is well maintained and complete, tests are described with Mocha which is already known by many in the team (it's what we used to write our tests on the older viewer)\\nCypress will be a great help during the development phase of the new viewer.\\nBut we have to keep in mind that it doesn't really do cross browser testing.\\n","tokens":307,"id":4785,"Prediction":"## Decision\nWe have decided to adopt Cypress as our integration test framework for the project. This decision was made based on several key considerations:\n\n1. **Developer Experience and Efficiency**:\n   - **Convenient Running and Debugging**: Cypress offers an exceptionally developer-friendly environment. The convenient running and debugging tools provided by Cypress will significantly enhance the development workflow.\n   - **Native Access to Application Variables**: Cypress's ability to access all application variables natively will enable more sophisticated and thorough testing.\n\n2. **Documentation and Community Support**:\n   - **Solid and Clear Documentation**: The comprehensive and well-structured documentation will facilitate a smoother onboarding process for the team and help in troubleshooting any issues that arise.\n   - **Active Community and Support**: Since its introduction in 2017, Cypress has built an active and supportive community, which is invaluable for knowledge sharing and obtaining timely assistance.\n\n3. **Scope and Compatibility**:\n   - **Focus on Development and Testing in a Local Environment**: Cypress is designed for developers and testers working on single page applications (SPAs) like Angular, Vue.js, and React.js. This focus aligns well with our project needs.\n   - **Efficient for Local Development**: Unlike Selenium and TestCafe, which may pose challenges when implemented across all developers' machines, Cypress is well-suited for local development environments, simplifying setup and consistency.\n\n4. **Future Adaptability and Scalability**:\n   - **Active Ecosystem and Innovation**: Being the newest addition among major testing frameworks, Cypress is continually evolving with innovative features and updates. This ensures that we are not only adopting a current solution but one that is future-proof.\n\nWhile Selenium and TestCafe have their advantages, particularly for extensive cross-browser and cross-device E2E testing, Cypress provides a more streamlined and effective solution for integration and potentially combo unit\/integration testing within our current development processes. This decision will enhance testing efficiency, developer productivity, and overall project quality.","GenTime":"2024-07-29 02:15:28"}
{"File Name":"nr-arch\/2020-04-24-ARCH-OpenJDK-Versions.md","Context":"## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https:\/\/apps.nrs.gov.bc.ca\/int\/jira\/browse\/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n\n## Decision\n","Decision":"* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\nJDK 8 & Later\\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\\nAs an example, the archives for JDK 13 may be found on jdk.java.net\/13 and may be extracted on the command line using\\n$ tar xvf openjdk-13*_bin.tar.gz\\nor\\n$ unzip openjdk-13*_bin.zip\\ndepending on the archive type.\\n### Positive Consequences\\n* removes the dependencies on Oracle JDK Licensing\\n* reduces security vulnerabilities of older JDK versions\\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\\nFurther vulnerabilities -\\n* <https:\/\/www.cvedetails.com\/product\/19117\/Oracle-JRE.html?vendor_id=93>\\n*\t<https:\/\/www.cvedetails.com\/product\/23642\/Oracle-Openjdk.html?vendor_id=93>\\n### Negative Consequences\\n* slow performance may occur\\n* migration issues will need to be addressed\\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\\n","tokens":403,"id":4802,"Prediction":"## Decision\n\nThe IITD Architecture team has made the following decision regarding the positioning and usage of OpenJDK within IITD infrastructure:\n\n1. **Adopt OpenJDK LTS Versions:**\n   All IITD hosted Java applications will transition to using OpenJDK, specifically targeting Long-Term Support (LTS) versions. OpenJDK 11, being an LTS release, will be the mandated minimum version for all Java applications under IITD's purview.\n\n2. **Discontinue Oracle JDK for Production Use:**\n   The usage of Oracle JDK in production environments will be phased out due to the new subscription-based licensing model introduced by Oracle. Existing applications running Oracle JDK will need to migrate to OpenJDK 11 or newer LTS versions.\n\n3. **Regular Update and Maintenance:**\n   Applications will need to periodically update to newer LTS versions of OpenJDK to ensure they remain secure and supported. This aligns with our need to mitigate security risks and maintain compliance with industry standards.\n\n4. **Support for Oracle OpenJDK Builds:**\n   While earlier versions of Oracle JDK might have been utilized, we will now encourage using builds from the OpenJDK community or other trusted distributors that offer LTS support, which are fully open and compliant with the latest standards.\n\n5. **Transition Plan:**\n   A detailed transition plan will be developed to aid in the migration process, including timelines, resource allocations, and support mechanisms. This plan will ensure minimal disruption to existing services and smooth migration to OpenJDK.\n\n6. **Training and Support:**\n   The IITD will provide training and resources to development and operations teams to familiarize them with OpenJDK and its adoption processes. This includes best practices for application compatibility testing, performance tuning, and security considerations.\n\nBy formalizing this decision, IITD commits to aligning with open standards and maintaining a secure, cost-effective, and future-proof Java application environment.","GenTime":"2024-07-29 02:16:50"}
{"File Name":"nr-arch\/2020-03-12-ARCH-record-architecture-decisions-simple-template.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4804,"Prediction":"Sure, I'll need the relevant context to provide a decision. Could you please provide the context for the architectural decision you're looking to make?","GenTime":"2024-07-29 02:16:50"}
{"File Name":"figgy\/0002-queues.md","Context":"## Context\\nWe have background jobs that are processed by Sidekiq workers on several dedicated background processing\\nmachines.  The background jobs include a variety of different tasks from different sources:\\n* processing user requests (e.g., ingesting files, (re)generating derivatives)\\n* cleaning up after user actions (e.g., propagating state and visibility, removing files before deleting\\ntheir parent resource)\\n* refreshing metadata from Voyager\/PULFA\\n* bulk-ingesting content\\n* routine cleanup (cleaning out guest users)\\n* fixity checking\\n\n## Decision\n","Decision":"We will have three queues for processing background jobs:\\n1. `default` for processing user-initiated requests needed for ingesting and displaying objects\\n2. `low` for bulk processing, validation, cleanup, etc.\\n3. `super_low` for fixity checking and other long-term preservation actions\\n","tokens":116,"id":4821,"Prediction":"## Decision\nTo ensure efficient and scalable processing of our diverse set of background jobs, we will implement the following strategies:\n\n1. **Job Prioritization**:\n   - Establish a clear prioritization scheme for Sidekiq queues, assigning higher priority to tasks that are time-sensitive or user-facing, such as processing user requests and cleaning up after user actions.\n   - Lower priority should be given to routines like bulk-ingesting content, fixity checking, and routine cleanup, as these tasks can afford to wait longer in the queue.\n\n2. **Dedicated Queues**:\n   - Create dedicated Sidekiq queues for different types of tasks to prevent low-priority jobs from blocking high-priority ones. \n   - For instance, separate queues for user requests, metadata refreshes, routine cleanups, and so forth.\n\n3. **Resource Allocation**:\n   - Ensure that the background processing machines have sufficient resources (CPU, memory, network bandwidth) to handle peak loads efficiently.\n   - Allocate specific machines or cores for high-priority queues to guarantee timely processing.\n\n4. **Scalability**:\n   - Set up auto-scaling for the background processing machines to dynamically adjust the available resources based on the current job load. \n   - Implement monitoring and alerting to ensure scaling actions are taken before job queues become backlogged.\n\n5. **Job Monitoring and Logging**:\n   - Enhance Sidekiq monitoring and logging to provide clear insights into job status, processing times, and error rates.\n   - Use this data for continuous optimization of job prioritization and resource allocation.\n\n6. **Retry and Failure Handling**:\n   - Implement robust retry and failure handling strategies for jobs to ensure that transient issues do not cause jobs to fail permanently.\n   - For critical jobs, set up alerting mechanisms to notify the development team of persistent failures for timely intervention.\n\nThis approach will allow us to handle background jobs more efficiently, ensuring that high-priority user-facing tasks are processed promptly while maintaining system stability and scalability for other tasks.","GenTime":"2024-07-29 02:17:53"}
{"File Name":"ansible-docker\/choose_database.md","Context":"### Context: A method of data storage is necessary to fufill the requirements for the assignment.\\n* Having used mySQL in previous courses and personal projects, I found mySQL a suitable choice for supporting database capabilities.\\n* Many of the examples provided by my instructor are using mySQL, thus for the benefit of staying consistent with course material mySQL is a good choice.\\n### Decision: The change proposed to the current implementation is to add mySQL as a method for data storage.\\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n\n## Decision\n","Decision":"### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n### Consequences: Acid Transactions can be considered \"pessimistic\" and not as flexible as base transactions.\\n","tokens":138,"id":4832,"Prediction":"### Decision: \nAfter evaluating the context and considering factors such as familiarity, existing course materials, and benefits of consistency, the decision is made to implement MySQL as the method for data storage. This decision leverages prior experience and aligns with example resources provided during the course, thus facilitating ease of learning and integration. It acknowledges the possibility of alternative modern tools with advanced features but prioritizes the immediate and consistent application needed for the assignment requirements. \n\nThis approach ensures that the database solution is robust, reliable, and well-understood within the scope of the coursework.","GenTime":"2024-07-29 02:18:26"}
{"File Name":"ansible-docker\/choose_webserver.md","Context":"### Context: A webserver is a necessary requirment to complete the assignment\\n* A webserver is needed to host the web application.\\n* There is a PHP-apache bundle offered by docker (see Dockerfile in php directory), thus apache was chosen purely out of convenience.\\n* I do not have much experience with Web Servers and there is a lot of documentation on apache. Again, convenience was a major factor in making the decision to use apache.\\n### Decision: The change proposed to the current implementation is to add an apache web server to host the php web application created in the php directory\\n### Consequences: No forseen consequences or drawbacks in using apache.\\n\n## Decision\n","Decision":"### Consequences: No forseen consequences or drawbacks in using apache.\\n","tokens":139,"id":4833,"Prediction":"### Decision: \n\nWe have decided to use Apache as the web server for hosting our PHP web application. This decision is driven by the following factors:\n\n1. **Convenience**: There is an existing PHP-Apache bundle available via Docker, which simplifies the setup process.\n2. **Familiarity and Documentation**: Apache has extensive documentation available, which is beneficial given the limited experience with web servers. \n\nBy integrating Apache, we aim to streamline the deployment process and ensure reliable server performance for our web application. \n\nNext steps will involve setting up the Docker environment and configuring the Apache server as per the project requirements.\n\n### Consequences:\n\n- **Positive**:\n  * **Ease of Setup**: Using an available PHP-Apache Docker image accelerates the development and deployment processes.\n  * **Comprehensive Documentation**: Abundant resources and community support available, easing troubleshooting and learning.\n  \n- **Neutral**:\n  * **Scalability and alternatives**: Future considerations might require evaluating other web servers (e.g., Nginx) depending on performance needs. However, no immediate concerns are anticipated with Apache.\n\n- **Negative**:\n  * There are no foreseen drawbacks given the context and requirements of the assignment. \n\nMake sure to monitor performance and compatibility during integration and address any arising issues promptly.","GenTime":"2024-07-29 02:18:29"}
{"File Name":"opg-use-an-lpa\/0002-persistent-storage-uses-dynamodb.md","Context":"## Context\\n* The application will require persistent storage for storing LPA ownership, granted access, and possibly user credentials\\n* Current MoJ strategy is to use managed services where possible\\n\n## Decision\n","Decision":"Use DynamoDB for persistent storage\\n","tokens":37,"id":4841,"Prediction":"## Decision\n\n* The application will utilize a managed database service for persistent storage to align with the MoJ strategy. Specifically, an AWS RDS (Amazon Relational Database Service) will be chosen due to its high availability, automatic backups, and managed maintenance capabilities.\n* Considering the need for security, data encryption at rest and in transit will be enabled, ensuring sensitive information such as user credentials are protected.\n* Additionally, AWS Secrets Manager will be used for securely storing and managing access credentials to the database.\n* The database schema will be designed to efficiently handle LPA (Lasting Power of Attorney) ownership data and access permissions, ensuring data integrity and supporting future scalability needs.\n* Periodic reviews will be scheduled to assess performance, costs, and compliance with MoJ policies and data protection regulations.","GenTime":"2024-07-29 02:18:53"}
{"File Name":"opg-use-an-lpa\/0008-session-storage-using-an-encrypted-cookie.md","Context":"## Context\\n* We will be storing a very small amount of data in the session.\\n* Whilst the above holds true we can avoid additional infrastructure by using client side storage.\\n* The session _may_ hold somewhat sensitive details (e.g. an LPA Share code), thus its content is secret.\\n* As the cookie is client side, we also need authentication to ensure the message isn't tempered with.\\n\n## Decision\n","Decision":"To use a cookie who's payload is encrypted with AES GCM. This provides secrecy and authentication.\\nNot to use JWT, because:\\n* To ensure message secrecy, additional libraries are needed.\\n* The resulting cookie value is significantly larger.\\n* Concerns over the general suitability around using JWT for client side sessions.\\n","tokens":86,"id":4843,"Prediction":"## Decision\n\nWe will use client-side storage for session data by leveraging secure, HttpOnly cookies to store the small amount of session information. Given that the session data may contain sensitive details, we will implement the following measures to ensure security:\n\n1. **Encryption**: Encrypt the session data before storing it in the cookie to protect sensitive information.\n2. **HttpOnly and Secure Flags**: Set the `HttpOnly` flag on cookies to prevent access via JavaScript, and the `Secure` flag to ensure cookies are only sent over HTTPS.\n3. **Authentication and Integrity Checks**: Use a proven algorithm like HMAC (Hash-based Message Authentication Code) to sign the cookie, ensuring its integrity and authenticity. The server will verify this signature upon each request to detect any tampering.\n\nBy implementing these measures, we can confidently use client-side storage for the session while maintaining data security and integrity.","GenTime":"2024-07-29 02:18:58"}
{"File Name":"opg-use-an-lpa\/0007-split-terraform-configuration.md","Context":"## Context\\nEngineers in the team want to have their code changes built as real environments in the Development AWS account so that they can have their work evaluated.\\nEvaluation includes automated and user acceptance testing.\\nWhen evaluation is finished it should be possible to destroy the environment, while retaining important artifacts and data (logs for example).\\nIt will be possible to host multiple PR environments in the Development AWS account\\n\n## Decision\n","Decision":"### Terraform Configuration\\nWe will identify and manage separately, resources at an Account level and resources at an Environment level.\\nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.\\nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.\\nAccount level resources will remain in place after an Environment is destroyed.\\n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)\\n### Name Spacing\\nWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resources\\nFor example, DNS records will be created for each environment using the namespace `https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uk`\\nwhile an ECS cluster will be named `025-UML-93-use-an-lpa`\\n### Environment lifecycle\\nAn Environment will be built when a PR is opened\\nOnce evaluation is complete, the Envirnoment can be destroyed.\\nTo begin with the pipeline will print instructions for destroying an Environment.\\nLater, destroying an Environment will be automated and scheduled with a maximum life span of a day or two.\\n### CI\/CD Pipeline\\nThe Terraform steps in the pipeline will always first provision the Account level shared resources, then provision the Environment level resources.\\n![pipeline terraform steps](..\/diagrams\/Use-An-LPA_Terraform_pipeline.png)\\n","tokens":82,"id":4847,"Prediction":"## Decision\n\n### Use Case-Specific Environments for Each Pull Request (PR) in Development AWS Account\n\nWe will create isolated, ephemeral environments for each PR in the Development AWS account to accommodate code evaluations including automated and user acceptance testing. The decision encompasses the following elements:\n\n1. **Automatic Environment Creation:**\n   - Upon the creation of a new PR or code changes in an existing PR, an automated pipeline will spin up a dedicated environment in the Development AWS account. This environment will include all necessary resources and configurations to mirror the production environment closely.\n\n2. **Isolation and Resource Management:**\n   - Each PR environment will be isolated from others to prevent any cross-contamination of data or configurations. Resource quotas and naming conventions will ensure that multiple environments can coexist without contention or conflicts.\n\n3. **Automated Testing and User Acceptance Testing:**\n   - Automated testing scripts will run in these environments to validate the changes. Post automated testing, the environment will also be available for user acceptance testing by stakeholders.\n\n4. **Artifact Retention and Logging:**\n   - Before tearing down the PR environment, important artifacts such as logs, test results, and other relevant data will be archived and stored in a persistent location (e.g., S3 buckets). These artifacts will be accessible for future reference and auditing purposes.\n\n5. **Environment Teardown:**\n   - Once testing and evaluations are complete, an automated process will destroy the environment. This will help manage costs and resource utilization within the Development AWS account.\n   - The teardown process will ensure the retention of relevant artifacts as outlined.\n\n6. **Tooling and Technology:**\n   - AWS CloudFormation, Terraform, or a similar Infrastructure as Code (IaC) tool will be used to automate the creation and teardown of environments.\n   - Continuous Integration\/Continuous Deployment (CI\/CD) pipelines (e.g., Jenkins, GitLab CI, GitHub Actions) will be set up to trigger environment creation and automated testing processes.\n\n### Rationale\n\nThis approach of using ephemeral environments per PR provides a scalable and controlled mechanism to test changes in isolation, ensures comprehensive testing coverage, and allows easy teardown to manage resources efficiently. It facilitates a streamlined development workflow, fostering higher code quality and faster feedback loops without compromising the stability of shared development resources.","GenTime":"2024-07-29 02:19:16"}
{"File Name":"opg-use-an-lpa\/0009-use-aws-kms-to-manage-session-encryption-keys.md","Context":"## Context\\nFollowing on from [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md)\\n* The encrypted cookie will need to be encrypted using a key.\\n* Keys should be able to be rotated easily and often.\\n* Key rotations should have no effect on active users.\\n\n## Decision\n","Decision":"* We will use AWS' KMS to manage our encryption keys.\\n* Keys will be cached at the contained level in volatile memory.\\n","tokens":67,"id":4848,"Prediction":"## Decision\nImplement a key management system that supports key rotation without affecting active user sessions. \n\n### Details:\n1. **Key Generation and Storage**:\n   - Use a secure key management service (KMS) for generating and storing encryption keys.\n   - Ensure that keys are stored securely and are accessible only by authorized services.\n\n2. **Key Rotation Policy**:\n   - Implement a routine key rotation policy, e.g., rotating keys every 30 days.\n   - Record the rotation schedule and ensure it is strictly followed.\n\n3. **Encryption Practice**:\n   - Each cookie will be encrypted with the current primary encryption key.\n   - Maintain backward compatibility by also storing previous encryption keys in a secure manner.\n\n4. **Handling Active Sessions**:\n   - Decrypt incoming cookies with the current primary key, and if decryption fails, sequentially try previous keys until decryption succeeds.\n   - When a cookie is successfully decrypted with an old key, re-encrypt it with the current primary key on the next interaction (e.g., the next API call or page load).\n   \n5. **Configuration Management**:\n   - Maintain a configuration file or service that keeps track of the current primary key and previous keys.\n   - Ensure the configuration can be updated without downtime to support seamless key rotation.\n\n### Benefits:\n- Maintaining user session continuity: Users are not logged out during key rotations.\n- Enhanced security: Regular key rotations reduce the risk of long-term key compromise.\n- Flexibility: Supports seamless integration with secure key management services.\n\n### Risks and Mitigations:\n- **Risk**: Potential performance overhead due to multiple decryption attempts.\n  - **Mitigation**: Optimize the key checking sequence and limit the number of previous keys maintained.\n- **Risk**: Complexity in key management and rotation processes.\n  - **Mitigation**: Leverage managed services and automate key rotation processes. Ensure robust logging and monitoring to quickly detect and address issues.","GenTime":"2024-07-29 02:19:21"}
{"File Name":"opg-use-an-lpa\/0016-logging-and-tracing.md","Context":"## Context\\nWe want to have an application wide logging infrastructure in place so that we can trace user requests through the\\napplication. It should be possible to trace each request through the application layers so that we can see what outgoing\\ncalls were made for incoming requests.\\n\n## Decision\n","Decision":"The use of `monolog\/monolog` to provide configurable logging levels throughout the application. Unique tracing\\ninformation is already provided by the amazon loadbalancers so this should be made available to the logging library and\\nattached to the logged information.\\n* Logging of service code to be main source of log information. If needed handlers can also be logged.\\n* Most logging to be done at an _info_ level.\\n* User actions that need monitoring (e.g. authentication failures) at _notice_ level\\n* Personally Identifiable Information (PII) to **not** be logged anywhere but _debug_\\n","tokens":55,"id":4852,"Prediction":"## Decision\n\nWe will implement a centralized logging framework using a combination of MDC (Mapped Diagnostic Context) for traceability and a scalable, distributed logging system for aggregation and analysis.\n\n1. **MDC (Mapped Diagnostic Context):** \n   - We will utilize MDC to tag logs with unique identifiers for each user request. This will allow us to trace the flow of a request through various components and layers of the application. MDC will propagate the unique identifier across different threads to maintain traceability.\n\n2. **Logging Library:**\n   - We will use a widely adopted logging library such as Logback or Log4j2, which both support MDC. This will integrate seamlessly with our Java-based application.\n\n3. **Log Aggregation Tool:**\n   - We will use a log aggregation and analysis tool such as ELK (Elasticsearch, Logstash, Kibana) stack or a managed service like AWS CloudWatch or Splunk. This will allow centralized collection, storage, and analysis of logs.\n\n4. **Log Format and Consistency:**\n   - All logs will follow a standardized format to include essential details (e.g., timestamp, log level, MDC data, thread information). JSON will be the preferred log format for its compatibility with log analysis tools.\n\n5. **Log Levels and Granularity:**\n   - Different log levels (DEBUG, INFO, WARN, ERROR) will be used appropriately to ensure that logs are detailed but manageable. DEBUG and INFO for development and operational details, WARN for warnings that don\u2019t affect functionality, and ERROR for issues that need immediate attention.\n\n6. **Performance Considerations:**\n   - Asynchronous logging will be employed to minimize the performance impact on the application. When possible, logging will be handled in a non-blocking manner to ensure the primary application flows are not disrupted.\n\n7. **Security and Compliance:**\n   - Logs will be checked for sensitive data to ensure no personally identifiable information (PII) or sensitive information is inadvertently logged. Masking or encryption strategies will be implemented for any sensitive data that needs to be logged.\n\n8. **Monitoring and Alerts:**\n   - The logging infrastructure will include monitoring and alert capabilities to notify the team of errors or unusual patterns in logs. This will help in proactive issue detection and troubleshooting.\n\n9. **Documentation and Training:**\n   - Comprehensive documentation will be prepared to guide developers on how to correctly use the logging framework. Training sessions will be conducted to ensure that all team members are familiar with the logging practices and tools.\n\nBy implementing this centralized logging infrastructure, we will achieve the traceability required to monitor, diagnose, and improve the application efficiently.","GenTime":"2024-07-29 02:19:35"}
{"File Name":"bfi-discovery\/0002-choice-of-auth-provider.md","Context":"## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\n\n## Decision\n","Decision":"* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting \/ infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost \/ interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n","tokens":96,"id":4857,"Prediction":"## Decision\n\nTo address the identity and access management needs of the BFI's IIIF Universal Viewer auditing platform, we have decided to implement **Auth0** as our identity management solution.\n\n### Justification:\n\n1. **Ease of Initial Deployment and Configuration:** \n   Auth0 provides a well-documented and straightforward integration process that allows rapid implementation and configuration. It offers pre-built login pages and APIs that align well with our requirement for quick setup.\n\n2. **Ongoing Hosting and Maintenance Costs:**\n   While Auth0 has a free tier that could potentially meet our initial needs, their pricing model for paid plans is competitive and scales with user volume, which ensures cost-effectiveness as the number of users grows. This keeps ongoing costs predictable and manageable.\n\n3. **Availability of Core Features:**\n   Auth0 includes essential features such as user registration, user management, authentication, and authorization out of the box. Moreover, it supports modern authentication protocols (like OAuth 2.0, OpenID Connect, and SAML) and offers additional security features like multi-factor authentication (MFA) at minimal or no extra cost.\n\nBy choosing Auth0, we can benefit from a comprehensive identity management solution that will not require substantial resource allocation for setup or maintenance while ensuring robust and scalable user management capabilities for the BFI's platform.","GenTime":"2024-07-29 02:19:54"}
{"File Name":"cape-cod-coastal-planner\/0004-use-postgresql-as-db.md","Context":"## Context\\nThe API for CHIP needs some sort of persistence for storing its data. Although the data needs for the app are pretty lightweight, it's likely that they will grow in the future as new features are desired or more data is acquired. To this end, sticking with a simple, powerful, reliable, and flexible technology that is widely understood makes sense.\\n\n## Decision\n","Decision":"A RDBMS fits this bill and PostgreSQL does so even more snugly. It's a top contender, it's free, it's currently the best supported traditional database for the Elixir\/Erlang ecosystem and it's unparalleled at its geospatial capabilities.\\n","tokens":73,"id":4859,"Prediction":"## Decision\nWe will use a relational database management system (RDBMS) for storing the CHIP API's data. Specifically, we have chosen PostgreSQL due to its robustness, flexibility, and support for complex queries and indexing. PostgreSQL is highly regarded for its reliability and performance, and it offers an extensive array of features that can accommodate both our current lightweight data needs and scale with our anticipated future growth. Additionally, PostgreSQL is widely adopted and understood within the developer community, ensuring that we have access to extensive resources and support.","GenTime":"2024-07-29 02:19:58"}
{"File Name":"GDD-app\/0002-use-preferences-for-data-storage.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4867,"Prediction":"## Context\nThe application needs to handle a high volume of real-time data processing, requiring low latency and high throughput. The existing system architecture, based on a monolithic design, presents limitations in terms of scalability and fault isolation. Additionally, the team is looking for solutions that can integrate seamlessly with cloud services to take advantage of managed infrastructure and services.\n\n## Decision\nWe will transition from a monolithic architecture to a microservices architecture. Each microservice will be responsible for a specific piece of the application's functionality, enabling more manageable and scalable development and deployment. This architecture will also allow us to independently scale different components of the system to meet the required performance criteria.\n\nTo facilitate this, we will adopt containerization, specifically using Docker, to package each microservice along with its dependencies, ensuring consistency across different environments. Kubernetes will be used for orchestration, providing automated deployment, scaling, and management of containerized applications.\n\nFor inter-service communication, we will utilize a message broker such as Apache Kafka, which will help manage the high throughput and low latency needed for real-time data processing. \n\nTo ensure fault isolation and resilience, we will implement circuit breakers and fallback mechanisms using tools like Netflix Hystrix. Monitoring and observability will be handled by integrating tools such as Prometheus and Grafana to track the performance and health of microservices.\n\nBy migrating to this architecture, we aim to achieve improved scalability, fault tolerance, and maintainability of the system, while also leveraging cloud infrastructure to manage resources efficiently.","GenTime":"2024-07-29 02:20:27"}
{"File Name":"GDD-app\/0003-use-bitrise-for-ci.md","Context":"## Context\\nWe need an easy way to integrate and test out code that is fast and reliable.\\n\n## Decision\n","Decision":"We choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\\nIt also allows us to notify users in a easy way and with different roles.\\n","tokens":21,"id":4868,"Prediction":"## Decision\n\nThe decision is to adopt a Continuous Integration (CI) pipeline using a cloud-based CI service such as GitHub Actions, CircleCI, or Travis CI. This service will automatically run tests and integrate new changes to the codebase upon each commit to the repository.\n\n### Justification:\n1. **Speed and Reliability**: Cloud-based CI services are optimized for performance and reliability.\n2. **Automation**: The CI pipeline will automate the process of building, testing, and integrating changes, reducing the need for manual intervention.\n3. **Scalability**: Cloud services can handle large workloads and scale as the project grows.\n4. **Ease of Integration**: The selected CI services can be easily integrated with our existing GitHub repository.\n5. **Visibility**: Provides real-time feedback on the build status, making it easier for developers to identify and fix issues promptly.","GenTime":"2024-07-29 02:20:29"}
{"File Name":"content-publisher\/0008-restricted-deletion.md","Context":"## Context\\nWe use foreign keys to help enforce database integrity e.g.\\n```\\n# ensure creator_id on a document matches a row in the users table\\n# when we try to delete a user, raise an error if its the creator of a document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\\n```\\nThere are other `on_delete` behaviours instead of `restrict`.\\n```\\n# when we delete a user, set creator_id to NULL on all of its documents\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\\n# when we delete a user, also delete all of its document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\\n```\\nDifferent options are appropriate depending on the foreign key.\\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\\n\n## Decision\n","Decision":"Currently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\\n","tokens":341,"id":4872,"Prediction":"## Decision\n\nGiven the various `on_delete` behaviors available for handling foreign key constraints, we need to adopt a consistent and logical approach that aligns with our application's integrity requirements and expected behaviors. We will make the following decisions based on the different scenarios:\n\n1. **Using `restrict`:**\n   \n   - We will use `restrict` when the integrity of the data is crucial and deleting a related record would lead to invalid or corrupted data. \n   - Example: We will ensure that entities like ActiveStorage blobs cannot be deleted if their associated parent image still exists. \n   - Specific Implementation: \n     ```ruby\n     add_foreign_key \"active_storage_blobs\", \"images\", column: \"image_id\", on_delete: :restrict\n     ```\n\n2. **Using `nullify`:**\n   \n   - We will use `nullify` for optional relationships where it is acceptable for the foreign key column to become `NULL` if the related record is deleted.\n   - Example: When deleting a user, if the `creator_id` on a document is not mandatory, it can be set to `NULL`.\n   - Specific Implementation: \n     ```ruby\n     add_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\n     ```\n\n3. **Using `cascade`:**\n   \n   - We will use `cascade` for parent-child relationships where it makes logical sense to delete the child entities when the parent entity is deleted.\n   - Example: Deleting a document should also delete all its related editions.\n   - Specific Implementation: \n     ```ruby\n     add_foreign_key \"editions\", \"documents\", column: \"document_id\", on_delete: :cascade\n     ```\n\nBy adopting these guidelines:\n- We ensure that our database maintains its integrity and the logical structure is consistent with our application requirements.\n- We mitigate the risk of orphaned records or invalid references, improving the reliability of our system.\n\nEach foreign key constraint will be reviewed based on its relationship context, and the appropriate `on_delete` behavior will be applied accordingly. These rules will be documented and adhered to strictly in our database migrations and schema updates.","GenTime":"2024-07-29 02:20:41"}
{"File Name":"content-publisher\/0009-modelling-history.md","Context":"## Context\\nThe Content Publisher domain model is focused on storing current\\ninformation. There is a documents table which stores the most recent content of\\na document and an images table which stores the most recent version of\\na document's images. History of these changes is stored using [Papertrail][],\\nwhich is not [intended as a permanent store][papertrail-pr].\\nThe presentation of a document's history is done via a TimelineEntry model\\nwhich stores that a user did something, but lacks any further detail. In places\\nwhere richer information was required there have been models such as Removal or\\nWithdrawal associated with a TimelineEntry.\\nThis has led to a number of pain points:\\n- users cannot discard a new draft of a published document, because creating a\\nnew draft overwrites the data stored for the published edition\\n- Content Publisher can't show an accurate link or status for the live edition\\nof a document when a new draft of a published document is created;\\n- users cannot edit or remove images on a document once the first\\nedition is published;\\n- the TimelineEntry model stores aspects of a document's state, resulting in it\\nneeding to be queried outside a timeline context which limits flexibility\\nfor the timeline.\\nAnd this prevents a number of intended features for Content Publisher:\\n- comparing different editions of a document;\\n- republishing live content if there are any problems (currently a common\\nsupport task for Whitehall publisher);\\n- showing users what changes a user made in a particular edit.\\n\n## Decision\n","Decision":"This ADR proposes changes to the domain model to resolve the aforementioned\\npain points and provide a means to support the future intended features. These\\nchanges provide the means to store the individual editions of a document,\\neach revision of the content of a document and each status an edition has held.\\nAs per [ADR-3](0003-initial-domain-modelling.md) it does not consider the\\noption of sharing data between translations of a document as there are not\\nthe appropriate product decisions for this.\\nA common theme in this decision is\\n[immutablity in models](#approach-to-mutabilityimmutability), which is used\\nas an implicit means of storing a history. Immutability is a key consideration\\nin modelling [revisions of a document](#breakdown-of-revision) and\\n[images](#image-modelling). This ADR then considers the impacts of\\nstoring history for [timeline](#timeline) and [topics](#topics), both areas\\nwhere the usage\/need of history is less clear. Finally, this ADR concludes with\\na [collated diagram](#collated-diagram) of the domain model concepts.\\n### Core Concepts\\n![Main concepts](0009\/main-concepts-diagram.png)\\n**Document**: A record that represents all versions of a piece of content in a\\nparticular locale. It has many editions and at any time it will have a current\\nedition - shown on Content Publisher index - and potentially a live edition\\nwhich is currently on GOV.UK. The live and current edition can be\\nthe same. Each iteration of a document's content is represented as a revision\\non the current edition, thus a document has many revisions. Document is a\\nmutable entity that is used to store data common across all editions (such as\\nfirst publishing date) and it is expected to be a joining point for\\ndocument-related data that is not associated with a particular edition.\\n**Edition**: A numbered version of a document that has been, or is\\nexpected to be, published on GOV.UK. It is associated with a revision\\nand a status. It is mutable so that it can be a consistent object that\\njoins to immutable data. It is a place where any edition-level\\ndatabase constraints can be placed, such as the constraint that only one live\\nedition can exist per document. It is supported that two editions of the same\\ndocument share the same revision. This allows them to explicitly reference the\\nsame content, which supports a future ability to revert a document to past\\ncontent.\\n**Revision**: Represents an immutable snapshot of the content of a document at a\\nparticular point in time. It has a number to indicate which revision of the\\ndocument it is and stores who created it. Any request by a user that changes\\ncontent should result in a single new revision. This is to directly map the\\nconcept of a revision to each time a user revises a document. Data outside of\\ncontent, such as state, should not be stored in a revision to ensure that\\ndifferences between revisions can be represented to a user. The\\n[anatomy of a Revision model](#breakdown-of-revision) is explored further in\\nthis document.\\n**Status**: Represents a state that an edition can hold such as: \"draft\" or\\n\"submitted for review\". This model is coupled to the concept of status that is\\nshown and changed by a user. Each time a user changes the status of an edition\\na new Status model is created and the user who created it stored. An edition\\ncan only have one status at any one time. If a status has data specific to\\nthat status, such as an explanatory note for a withdrawal, this can be stored\\nin a specific model associated by a polymorphic relation. This allows for\\nmodels, such as Removal or Withdrawal, to no longer be the responsibility of\\nTimelineEntry. Initially this object is intended to be immutable, however this\\nmay be changed if status changes become asynchronous operations. This is so\\nthat a single status change performed by a user can still be represented by\\na single record.\\n### Approach to mutability\/immutability\\nA number of the models in Content Publisher are defined as immutable, most\\nsignificantly [Revision and associated models](#breakdown-of-revision). These\\nmodels should be persisted to the database once and never be updated or deleted.\\nAny need to change them requires creating a new record. This allows us to store\\na full history by only appending to the database.\\nFor simplicity, performance and consistency with Rails idioms the accessing\\nof immutable models is intended to be done by foreign key and not by the usage\\nof `SELECT MAX` style queries. This maintains the ability to use the regular\\napproach to ActiveRecord associations and the means to require the existence of\\na association (by specifying a foreign key cannot be null). An example of this\\nmodelling is the mutable Edition model which references an immutable model,\\nRevision, that stores the content. Edition is accessed by a\\nconsistent primary key and the revision accessed by a foreign key stored on\\nthe edition.\\nSince the data on a mutable model can be lost when the model is updated these\\nshould not be used for data where there is a need for history. For example, to\\nstore the statuses an edition has held there are individual status models that\\nreference the Edition. This allows an edition to reference a single status that\\nis replaced while a history is maintained.\\nThe choice of this immutability strategy is to store both present and\\nhistorical concerns in the same way, thus ensuring history remains a\\nfirst class citizen. A nice side effect of having immutable models is\\nthis opens options for caching. Since data for that\\nmodel will never change it can effectively be cached forever.\\n### Breakdown of Revision\\nAs Revision is an immutable model, used to store each edit of a Document, there\\nis likely to be a large amount of these with often only minor differences\\nbetween them. To address this a Revision is not stored as a single model but\\ninstead as a collection of models, where the Revision model stores little data\\nand joins to other models. This can be visualised as:\\n![Revision breakdown](0009\/revision-diagram.png)\\nThe intention of breaking this up is to be conservative with the amount of data\\nduplicated between consecutive revisions. For example when a user edits\\nthe title of an edition a new ContentRevision is created and the existing\\nTagsRevision, MetadataRevision and ImageRevisions models are associated with\\nthe next revision. An ImageRevision is modelled in a similar way to a Revision\\nand this is explained further in [Image modelling](#image-modelling).\\nIt is intended that [delegation][delegate] be used when interfacing with a\\nrevision so that the caller need not be concerned with which sub-revision\\nstores particular fields. This allows a revision to have a rich interface\\ndespite storing a low amount of data directly.\\n### Image modelling\\nContent Publisher supports a user uploading image files and referencing them\\nin a revision of a document. They have metadata and editable properties that a\\nuser can change, of which a history is stored. A single image file uploaded\\nproduces multiple files that are uploaded to Asset Manager for different sizing\\nvariations. Images are modelled in a similar way to Revision with an\\nimmutable Image::Revision model, as represented below:\\n![Image Revision breakdown](0009\/image-revision-diagram.png)\\nThe Image model itself is used for continuation between image revisions. It is\\nknown that two Image::Revisions are versions of the same item if they share the\\nsame Image association. The id of the Image is used in Content Publisher URLs\\nto consistently reference the Image no matter which revision it is.\\nThe data of an Image::Revision is stored between an Image::FileRevision and an\\nImage::MetadataRevision. Both are immutable and they differ by the fact that\\nany change to Image::FileRevision requires changes to the resultant Asset\\nManager files (such as crop dimensions), whereas Image::MetadataRevision stores\\naccompanying data that doesn't affect the Asset Manager files (such as alt\\ntext).\\nEach Image::FileRevision is associated with an ActiveStorage::Blob object that\\nis responsible for managing the storage of the source file. It also has a one\\nto many association with Image::Asset. Each Image::Asset represents resultant\\nfiles that are uploaded to Asset Manager for the various image sizes. The\\nImage::Asset model stores the URL to the Asset Manager file and what state the\\nfile is on Asset Manager.\\n### Timeline\\nThe TimelineEntry model represents an event that should be shown to a user as\\npart of a visual timeline of a document's history. In order for the timeline to\\nbe a flexible feature that can be iterated, this model should not be used\\noutside of the timeline context. Previously models such as Removal and\\nWithdrawal were associated directly with a TimelineEntry which\\nmeant state was accessed through the timeline. These are now suggested to be\\nassociated with a Status model.\\nAt the time of writing it wasn't yet determined what the\\ntimeline would show, and therefore it wasn't clear exactly how\\nbest to model an entry for it. Because of this TimelineEntry is modelled in a\\nspeculative way with a number of references to relevant data, including a\\npolymorphic association for flexibility.\\nThe TimelineEntry model should not store data which could not be\\nderived from other aspects of a document. This is the allow the ability to\\nrebuild TimelineEntry models if the needs of the timeline changed and to avoid\\ntimeline being an aspect of a document's state.\\n### Topics\\nNo data related to topics (otherwise known as GOV.UK taxonomy) is intended to\\nbe stored in Content Publisher at this current point in time. This is due to\\ntopics being accessed and edited by directly interacting with the Publishing\\nAPI. Other applications, notably Content Tagger, can also edit topics which has\\nthe consequence that the Publishing API is the source of truth for this data\\nrather than Content Publisher.\\nThis inconsistency makes it difficult to store the history of topics in a\\nreliable way. Thus, until needs are determined to store past topics, Content\\nPublisher will only know current topics (by querying Publishing API) and\\nthe history of topics will not be available.\\n### Collated diagram\\nThe various aspects of this decision can be collated together to be represented\\nin the following, somewhat daunting, diagram. It has been simplified for ease\\nof reading with noticeable omissions being a User model, since most models have\\na creator, and the speculative joins of a TimelineEntry.\\n![Domain model diagram](0009\/domain-model-diagram.png)\\n","tokens":317,"id":4875,"Prediction":"## Decision\n\nTo address the limitations and pain points identified in the current Content Publisher domain model, the following architectural decisions will be implemented:\n\n1. **Implement Versioned Tables**:\n   - Introduce versioned tables for both documents and images. Each change (or edition) to a document or its images will create a new version entry rather than overwriting the existing data. This will allow for historical data retention and facilitate the comparison between different editions. The new structure will support tracking and querying document state seamlessly.\n\n2. **Revise TimelineEntry Model**:\n   - Refactor the TimelineEntry model to separate the representation of user actions (like edits, removals, or withdrawals) from the document state. Introduce additional models that can store richer historical data about document changes.\n   - Ensure the TimelineEntry focuses strictly on tracking user actions and their timestamps, while other dedicated models store relevant details about these actions.\n\n3. **Enable Historical Comparison and Display**:\n   - Develop features to allow users to compare different versions of a document and its images. These features should provide insights into what was changed, when, and by whom.\n   - Provide user interfaces that allow viewing of the document\u2019s history in a more detailed manner, including the ability to see changes to both content and images over time.\n\n4. **Introduce Draft Isolation**:\n   - Ensure that creating a new draft of a published document does not overwrite the existing published edition. This will allow users to discard drafts without affecting the live version and accurately track the live status and links of documents even when drafts are in progress.\n\n5. **Enhanced Image Handling**:\n   - Allow users to edit or remove images in a document even after its first edition is published. This flexibility will ensure that content changes can be smoothly managed without constraints.\n\n6. **Support for Republishing**:\n   - Streamline the process for republishing live content to address issues without complex support interventions. This will involve creating user-facing tools to republish previous stable versions of content when necessary.\n\n7. **Query Enhancements**:\n   - Optimize the system querying mechanisms to support the new versioned data structure. Ensure that relevant and accurate data can be efficiently retrieved for any given state or historical query.\n\nBy adopting these decisions, Content Publisher will gain enhanced flexibility, more comprehensive historical tracking, and improved user capabilities. These changes will also lay the groundwork for advanced content lifecycle management features in the future.","GenTime":"2024-07-29 02:20:54"}
{"File Name":"content-publisher\/0004-editing-microcopy.md","Context":"## Context\\nEvery feature we add to the app comes with its own static text, which is either embedded in the code (Ruby or JavaScript) or in the HTML. Static text can be anything from the page title, to the text of a button, to an entire page of guidance.\\nWriting text 'inline' makes it hard for us to audit all of strings in our application, some of which can only be seen under special conditions e.g. error messages. It also makes it hard to change strings consistently across the application - a task which has to be done by a developer. Finally, using inline strings in code distracts from the logical flow of that code.\\n[Rails Internationalization](https:\/\/guides.rubyonrails.org\/i18n.html) (also referred to as 'translations') are a way to extract all of the strings in the application to a central location in `config\/locales\/en`. The strings can be organized in a hierarchy over one or more files, as below, where we can refer to the reviewed title by writing `I18n.t(\"publish.published.reviewed.title\")`.\\n```\\n# publish_document\/published.yml\\nen:\\npublish_document:\\npublished:\\nreviewed:\\ntitle: Content has been published\\nbody: |\\n\u2018%{title}\u2019 has been published on GOV.UK.\\nIt may take 5 minutes to appear live.\\n```\\nRails translations have a few special behaviours, such as pluralization, raw HTML, and variables. The `%{title}` string in the above is an example of a variable, which a developer will set to the title of the document being published.\\n\n## Decision\n","Decision":"Although we could use translations to extract all of the strings in the application, in some cases we felt this wasn't necessary, or that a different method should be used. The following is a summary of the rules we currently use.\\n* **Link and button labels** are not extracted. We think link and button labels are unlikely to change, and extracting them made the application tests harder to read by obfuscating some of the crucial steps in the test with translation keys.\\n* **Publishing component strings** are not extracted. This ensures we are able to migrate these components to the [govuk_publishing_components](https:\/\/github.com\/alphagov\/govuk_publishing_components) repo, which wouldn't be able to access our local translations.\\n* **Big guidance** is extracted into it's own Markdown files and stored alongside the corresponding HTML page that shows it. For example, the guidance for creating a new document is stored in `app\/views\/new_document\/guidance.md`.\\n* **Domain data** that's static is stored in a number of custom YAML files. This application has two static models (for document types and their selection) that encapsulate domain concepts where the data is part of the application. We have split up domain data based on whether it's used in a backend setting or as a string for use in the frontend. The latter are extracted at the top-level of the translation hierarchy.\\n* **Global strings** (states and validation messages) are extracted using translations. As these strings aren't page-specific, we put them at the top-level of the translation hierarchy (in `states.yml` and `validations.yml`).\\n* **All other strings** are extracted using translations, in a hierarchy that follows the structure of the `app\/views` directory. For example, the above example relates to `app\/views\/publish_document\/published.html.erb`.\\n* **Small amounts of govspeak and HTML** are extracted using translations as for other strings, with '\\_html' or '\\_govspeak' appended to the final component of the key to indicate they support rich text.\\nEvery instance of a string in the tests has been replaced according to the above rules, such that the tests continue to pass when an extracted string is changed. **Link and button labels** are not replaced, as they are not extracted in the code.\\nWe also configured Rails to raise an exception when we hit a page where a translation is missing, as we don't test all of the translations; this is done by setting `config.action_view.raise_on_missing_translations = true` in `application.rb`.\\n","tokens":343,"id":4878,"Prediction":"## Decision\n\nAdopt Rails Internationalization (I18n) for managing all static text within the application.\n\n### Rationale:\n\n1. **Centralized Management**: By moving all static text to `config\/locales\/en`, it provides a single, central location to manage and audit all strings in the application. This structure aids in the maintainability and consistency of the text strings.\n\n2. **Ease of Updates**: Updating a string in one place will reflect across all instances where it's used. This reduces the chances of inconsistencies and makes global changes more efficient.\n\n3. **Context Awareness**: By leveraging the hierarchical organization of translations, we can maintain context for each string, making it clear how and where each string is used.\n\n4. **Special Behaviours**: Rails I18n supports useful features such as pluralization, raw HTML, and variable interpolation, which can help create more dynamic and context-specific messages (e.g., using `%{title}` to dynamically insert a document title).\n\n5. **Improved Code Readability**: Extracting static text from the code improves the code\u2019s logical flow and reduces visual clutter, making it easier for developers to focus on business logic.\n\n6. **Internationalization-Ready**: While the immediate benefit is better management of English strings, this approach sets the groundwork for future internationalization\/localization efforts, making the application more easily adaptable to other languages.\n\n### Implementation Plan:\n\n1. **Extract Existing Strings**: Audit the current codebase to extract all static text strings and organize them into appropriate hierarchical structures within `config\/locales\/en\/*.yml` files.\n  \n2. **Refactor Code**: Replace inline static text in the application code with corresponding I18n `t` function calls (e.g., `I18n.t(\"publish.published.reviewed.title\")`).\n\n3. **Documentation and Training**: Update internal documentation to guide developers on how to add and manage translations. Conduct training sessions if necessary to familiarize the team with the I18n workflow.\n\n4. **Automate Audits**: Implement or utilize existing tools to periodically audit the locales files and ensure that all text strings are appropriately managed and no inline texts are introduced in future developments.\n\nBy adopting Rails Internationalization, we ensure a more maintainable, consistent, and clean codebase while preparing for potential future internationalization requirements.","GenTime":"2024-07-29 02:21:05"}
{"File Name":"content-publisher\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4886,"Prediction":"## Decision\n\nTo address the need for recording architectural decisions in the project, we have decided to use Architectural Decision Records (ADR). This approach will provide a structured way to document the decisions made, the context in which they were made, and the reasoning behind them. Each ADR will contain the following sections:\n\n1. **Title:** A short, descriptive title.\n2. **Status:** The current status of the decision (e.g., proposed, accepted, deprecated).\n3. **Context:** Explanation of why the decision is necessary.\n4. **Decision:** The actual decision made.\n5. **Consequences:** The impact of the decision, including positive and negative outcomes.\n6. **Date:** The date the decision was made.\n\nThis template will ensure consistency and clarity in documenting our architectural decisions. ADRs will be stored in a designated directory within the project's repository and will be maintained throughout the lifecycle of the project.","GenTime":"2024-07-29 02:21:25"}
{"File Name":"klokwrk-project\/0002-strategic-project-structure.md","Context":"## Context\\nExcluding the simplest hello-world-like cases, any useful project typically contains several modules. The traditional way to organize project modules is just to put them under the project root.\\nWe can call that structure simply **flat structure**.\\nWhile the flat structure is appropriate and sufficient for simpler projects, when the project grows and the number of modules increases, the flat structure starts suffering from many drawbacks:\\n* Flat structure does not scale well when the number of modules grows.\\n* Flat structure is difficult and confusing to navigate with numerous modules at the same hierarchy level.\\n* Flat structure does not suggest a direction of dependencies between modules.\\n* Flat structure does not suggest abstraction levels of modules.\\n* Flat structure does not suggest where are the system's entry points.\\n* Flat structure can use only module names to provide hints about relations between modules. Unfortunately, even that possibility is rarely leveraged.\\n* Flat structure does not use any high-level constructs that may suggest how modules are organized and related.\\n* Negative usage aspects are getting worse and worse as we add additional modules.\\n* Flat structure often requires extracting modules in separate repositories just because confusion becomes unbearable with a larger number of modules.\\n* When using microservices, the flat structure practically forces us to use one project per microservice.\\n> Note: Terms **flat structure** and **strategic structure** (see below) are ad-hoc terms introduced just for this document. However, in the `klokwrk-project`, we may use them in other places for\\n> convenience.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n## Decision\n","Decision":"**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**\\nOur project organization will follow principles and recommendations of **strategic structure** as defined below.\\n### Decision Details\\nWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategic\\nstructure focusing on the differences to the given concrete example.\\n#### Strategic structure in klokwrk\\nThe current project layout in the klokwrk looks like this:\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 asd\\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 booking\\n\u2502   \u2502       \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-commandside\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-view\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-rdbms-management\\n\u2502   \u2502       \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-boundary-web\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-out-customer\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502               cargotracking-booking-test-component\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-queryside\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-testcontainers\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502               cargotracking-lib-axon-cqrs\\n\u2502   \u2502               cargotracking-lib-axon-logging\\n\u2502   \u2502               cargotracking-lib-boundary-api\\n\u2502   \u2502               cargotracking-lib-boundary-query-api\\n\u2502   \u2502               cargotracking-lib-domain-model-command\\n\u2502   \u2502               cargotracking-lib-domain-model-event\\n\u2502   \u2502               cargotracking-lib-web\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-datasourceproxy-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-jackson-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-context\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-data-jpa\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-validation-springboot\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-archunit\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-datasourceproxy\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-hibernate\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-jackson\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-uom\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-constraint\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-validator\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-base\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-match\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-simple\\n\u2502   \u2502\\n\u2502   \u2514\u2500\u2500 other\\n\u2502       \u251c\u2500\u2500 platform\\n\u2502       \u2502       klokwrk-platform-base\\n\u2502       \u2502       klokwrk-platform-micronaut\\n\u2502       \u2502       klokwrk-platform-spring-boot\\n\u2502       \u2502\\n\u2502       \u2514\u2500\u2500 tool\\n\u2502               klokwrk-tool-gradle-source-repack\\n\u251c\u2500\u2500 support\\n\u2502   \u2514\u2500\u2500 ... (other files or directories)\\n\u2514\u2500\u2500 ... (other files or directories)\\nAt the top of the hierarchy, we have a project folder  - `klokwrk-project`. It is the equivalent of the whole system. In the strategic structure, the system name appears in the names of artifacts\\nconsidered to be conceptually at the level of a system.\\nRight below the root, we have `modules` and `support` folders. These should be the area of 99% of everyday work, with the `modules` folder taking a vast majority of that percentage.\\nThe `support` folder houses all kinds of supportive files like scripts, documentation, git hooks, etc. The `support` folder is free-form, and the strategic structure does not impose any\\nrecommendations or rules on its content. On the contrary, the strategic structure is applied to the content of the `modules` directory - the home of all source code modules in the system.\\nAt the 1st level of strategic structure - the system level, we have the content of the `modules` directory. It is divided into three subdirectories: `bc` (bounded context modules),\\n`lib` (system-level libraries), and `other` (miscellaneous helper modules).\\nAt the 2nd level - the bounded context level, we have the content of the `modules\/bc` directory that is further organized into three parts, `asd` (asd stands for **A** **S**ub**D**omain),\\n`domain-model` (bounded context domain model), and `lib` (bounded context libraries).\\nAt the 3rd level of a hierarchy, we have the content of the `modules\/bc\/[bounded-context-name]\/asd` directory that holds all bounded context's subdomains. The modules for each subdomain are further\\ndivided into `app` and `lib`. The `modules\/bc\/[bounded-context-name]\/asd\/[subdomain-name]\/app` directory contains the **subdomain applications** responsible for implementing concrete subdomain\\nscenarios. From the abstraction level and dependency perspectives, subdomain applications are at the top of the hierarchy. Subdomain applications speak the language of domain - the bounded context's\\nubiquitous language. They even contribute to it through the naming and meaning of use cases.\\nThe first thing that **subdomain libraries** (`modules\/bc\/[bounded-context-name]\/asd\/subdomain-name\/lib)` can hold is infrastructural code related to the technological choices made for that\\nparticular subdomain and are not reusable outside the subdomain. However, they can temporarily have infrastructural modules intended to be more reusable (either on the bounded context or system\\nlevels) at the end. Still, for whatever reason, it was more convenient to hold them at the subdomain level for a limited time.\\nThe second thing that can be found in subdomain libraries are business-related reusable modules that connect technological choices with the domain model. One characteristic example is the\\n`cargotracking-booking-lib-queryside-model-rdbms-jpa` module. Those kinds of modules do speak bounded context's ubiquitous language.\\nThe bounded context's **domain model** is implemented in `modules\/bc\/[bounded-context-name]\/domain-model`. Those modules contain the essence of the bounded context business logic. Implementation of\\nthe domain model should be free of technology as much as possible and practical. Adding external libraries is not strictly forbidden, but each addition should be conscious and must be carefully\\nevaluated. It is best to have tests that monitor and control the dependencies of a domain model. The domain model implements the majority of code-level representation of the bounded context's\\nubiquitous language and must be consistent across all bounded context's subdomains.\\nBy default, the directory `modules\/bc\/[bounded-context-name]\/lib` is the home of shareable **bounded context infrastructural libraries**. It contains modules with infrastructural code that is\\nreusable across the bounded context. Those modules are at a lower abstraction level than subdomain libraries. Bounded context infrastructural libraries do not speak domain language. However, they can\\nsupport the implementation of the domain model and other module groups higher in the hierarchy. Domain model should not generally depend on bounded context infrastructural libraries. Exceptions are\\nallowed but should be conscious and carefully managed.\\nDo note that another variant of bounded context libraries is also possible. It is a variant supporting the sharing of business logic at the bounded context level when necessary. In that case, instead\\nof a single `lib` directory, we would have `blib` and `ilib` directories. The `blib` directory would contain business-related modules that can depend on a domain model. On the contrary, the `ilib`\\ndirectory cannot use the domain model because it should contain infrastructural code only. The `ilib` directory role is the same as the role of `lib` directory from the default variant of bounded\\ncontext libraries.\\nLet's return to the `modules\/lib` directory containing general **system-level libraries**. It is divided into `hi`, `lo`, and `xlang` subdirectories. All system-level libraries are at lower\\ndependency and abstraction levels than any bounded context module.\\nAlthough separation on the high (`hi`) and low-level (`lo`) system libraries is somewhat arbitrary, it is helpful in practice. The `hi` directory is intended to contain\\n**high-level system libraries**, which are general infrastructural modules closer to the high-level technological frameworks (something like Spring, Spring Boot, or Axon frameworks) used in the\\nsystem. They could contain some specifics of our system, but usually, they do not. In that later case, they are general enough to be reused even outside of our system.\\nThe **low-level system libraries** from the `lo` directory deal with the customizations and extensions of widely used 3rd party libraries like Hibernate, Jackson, Java Bean validations, and similar.\\nBoth types of system-level libraries should not be, in general, dependencies of a domain model.\\nAt the lowest abstraction level, we have the **language extensions** (`modules\/lib\/xlang`). They focus on adding features to the programming language itself or its accompanying SDK (JDK in our case).\\nLanguage extensions can be used from everywhere, even from the domain model, without restrictions. Some of them are often written to ease the implementation of the domain model by making it more\\nexpressive and concise.\\n#### Characteristics of strategic structure\\nThe most important thing about strategic structure is not the structure itself but rather the distinguishing characteristics that it provides.\\nWe already mentioned abstraction levels and dependencies between groups of modules. If you look again at the example, you will notice that both of them are constantly flowing top to bottom through\\nthe strategic structure. For instance, subdomain applications depend on subdomain libraries. They both can depend on the domain model, which can depend on bounded context libraries and language\\nextensions. At the level of system libraries, high-level modules can depend on low-level modules, and they both can depend on the language extensions. However, none of the dependencies can come the\\nother way around. Dependencies are not allowed to flow from the bottom to the top.\\nWe have managed to do this because we applied strategic DDD concepts of bounded context and subdomains to the project structure. They provide sense and meaningfulness by connecting our code to the\\nbusiness. Without that business context, we will be left exclusively to the technical aspects, which are just insufficient. Technical aspects know nothing about the purpose of our system. They do not\\nknow anything about the business context.\\nDescribed characteristics bring important benefits when trying to understand or navigate through the system's code. Finding the desired functionality is much easier because we usually know, at least\\napproximately, where we should look for it. This can greatly reduce cognitive load while exploring unfamiliar (or even familiar) codebases.\\nIn addition, if you follow the proposed naming conventions for modules and their packages (see below), the same easy orientation can be applied at the package level or even if you pull out all\\nmodules into the flat structure. You will always know where to look for.\\n#### Naming conventions\\nYou have probably noticed that modules have very particular names reflecting their position in the strategic structure. The following table summarizes them as used in the example:\\n| Module group    | Naming scheme                                            | Example                                  |\\n|-----------------|----------------------------------------------------------|------------------------------------------|\\n| subdomain apps  | `[bounded-context-name]-[subdomain-name]-app-[app-name]` | `cargotracking-booking-app-commandside`  |\\n| subdomain libs  | `[bounded-context-name]-[subdomain-name]-lib-[lib-name]` | `cargotracking-booking-lib-boundary-web` |\\n| domain model    | `[bounded-context-name]-domain-model-[model-part-name]`  | `cargotracking-domain-model-aggregate`   |\\n| bc libs         | `[bounded-context-name]-lib-[lib-name]`                  | `cargotracking-lib-boundary-api`         |\\n| sys hi libs     | `[system-name]-lib-hi-[lib-name]`                        | `klokwrk-lib-hi-spring-context`          |\\n| sys lo libs     | `[system-name]-lib-lo-[lib-name]`                        | `klokwrk-lib-lo-jackson`                 |\\n| lang extensions | `[system-name]-lib-xlang-[lib-name]`                     | `klokwrk-lib-xlang-groovy-base`          |\\nModule naming conventions are essential because our modules are not always presented (i.e., try the Packages view in the IntelliJ IDEA's Project tool window) or used as a part of the hierarchy (think\\nof JAR names put in the same directory). For those reasons, our naming scheme closely follows the strategic structure hierarchy where parts of module names are directly pulled from corresponding\\nsubdirectory names. That way, we can keep the match between alphabetical order and the direction of dependencies.\\n> Note: When you have multiple bounded contexts and\/or multiple subdomains in the project, to get the exact match between alphabetical order and the direction of dependencies, you can use the `bc-`\\n> prefix in front of bounded context names and the `asd-` prefix for subdomain names.\\nThe same naming principles should also be applied to packages. Here are a few examples of package names:\\norg.klokwrk.cargotracking.booking.app.commandside.*\\norg.klokwrk.cargotracking.booking.lib.boundary.web.*\\norg.klokwrk.cargotracking.domain.model.aggregate.*\\norg.klokwrk.cargotracking.lib.boundary.api.*\\norg.klokwrk.lib.hi.spring.context.*\\norg.klokwrk.lib.lo.jackson.*\\norg.klokwrk.lib.xlang.groovy.base.*\\nWith those naming conventions, we should be able to avoid naming collisions on the module and package levels.\\n#### The general scheme of strategic structure\\nIn some circumstances, we may need additional elements in the strategic structure to deal with shared libraries at different levels. Examples of those, with sparse explanations, are given in the\\ngeneral scheme of strategic structure below:\\nmodules\\n\u251c\u2500\u2500 bc\\n\u2502   \u251c\u2500\u2500 my_food\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 restaurant\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 menu_management\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zshared         \/\/ sharing code between subdomains if necessary\\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib                 \/\/ bounded context libraries - default variant\\n\u2502   \u2502           ... *           \/\/ Can be split into \"blib\" and \"ilib\" directories when the sharing of\\n\u2502   \u2502                           \/\/ business logic is necessary at the level of a single bounded context\\n\u2502   \u251c\u2500\u2500 my_carrier\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 zshared                 \/\/ shared code between multiple bounded contexts (if necessary).\\n\u2502       \u2502                       \/\/ \"z\" prefix - funny reference to \"zee Germans\" from Snatch movie.\\n\u2502       \u2502                       \/\/ Moves \"zshared\" at the last place alphabetically, which matches\\n\u2502       \u2502                       \/\/ the proper place in terms of dependencies and abstraction levels.\\n\u2502       \u251c\u2500\u2500 domain-model\\n\u2502       \u2502       ... *\\n\u2502       \u2514\u2500\u2500 lib\\n\u2502               ... *\\n\u251c\u2500\u2500 lib\\n\u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502       ... *\\n\u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502       ... *\\n\u2502   \u2514\u2500\u2500 xlang\\n\u2502           ... *\\n\u2514\u2500\u2500 other            \/\/ supportive project's code for various \"other\" purposes\\n\u251c\u2500\u2500 build\\n\u2502       ... *\\n\u251c\u2500\u2500 tool\\n\u2502       ... *\\n\u2514\u2500\u2500 ...\\n#### Simplification - the case of bounded context boundaries matching 1:1 with subdomain\\nThe one-to-one match between bounded context boundaries and corresponding subdomain is considered to be the \"ideal\" case, and it is relatively common in practice. When we know how a fully expanded\\nstrategic structure works and looks like, it is relatively easy to come up with simplification for this particular case.\\nHere are \"refactoring\" steps and the example based on our concrete example from the beginning of this document:\\n- move subdomain applications to the bounded context level\\n- merge subdomain libraries with bounded context libraries\\n- split bounded context libraries into `blib` and `ilib` directories if necessary\\n- rename corresponding modules and packages\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       cargotracking-app-commandside\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-view\\n\u2502   \u2502       \u2502       cargotracking-app-rdbms-management\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 blib\\n\u2502   \u2502       \u2502       cargotracking-blib-out-customer\\n\u2502   \u2502       \u2502       cargotracking-blib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 ilib\\n\u2502   \u2502               cargotracking-ilib-axon-cqrs\\n\u2502   \u2502               cargotracking-ilib-axon-logging\\n\u2502   \u2502               cargotracking-ilib-boundary-api\\n\u2502   \u2502               cargotracking-ilib-boundary-query-api\\n\u2502   \u2502               cargotracking-ilib-boundary-web\\n\u2502   \u2502               cargotracking-ilib-domain-model-command\\n\u2502   \u2502               cargotracking-ilib-domain-model-event\\n\u2502   \u2502               cargotracking-ilib-web\\n\u2502   \u2502               cargotracking-test-component\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502               cargotracking-test-support-queryside\\n\u2502   \u2502               cargotracking-test-support-testcontainers\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 other\\n\u2502           ... *\\n\u251c\u2500\u2500 support\\n\u2502       ... *\\n\u2514\u2500\u2500 ... *\\n","tokens":335,"id":4889,"Prediction":"## Decision\n\n### Adoption of Strategic Structural Organization\n\nTo address the limitations of a flat structure for organizing project modules, we will adopt a **strategic structural organization** consisting of a hierarchical module structure. This approach aims to enhance scalability, navigability, and clarity in large projects comprising numerous modules. The following guidelines outline the strategic structure:\n\n### Hierarchical Module Organization\n1. **Top-Level Directories**:\n   - **Core**: Contains the essential and reusable components, fundamental libraries, and utility modules that provide core functionalities shared across various modules.\n   - **Services**: Encompasses service-specific modules, including microservices, with clear separation between them. Each service will have subdirectories for its logical layers.\n   - **Interfaces**: Groups modules related to user interfaces, APIs, and external integration points.\n   - **Domains**: Organizes domain-specific modules, separating different bounded contexts within the project's domain-driven design.\n\n### Directory-Naming Conventions\n- **Adopt meaningful, descriptive names** for directories that clearly indicate the purpose and relationship of the modules contained within. Where possible, include prefixes or suffixes suggesting their role or layer within the system.\n\n### Dependency Guidelines\n- **Define clear module dependencies** in a top-down manner, ensuring that higher-level modules depend on lower-level modules, but not vice versa, to maintain a logical flow and separation of concerns.\n\n### Abstract Levels and Entry Points\n- **Module Abstractions**:\n  - Clearly delineate high-level and low-level modules, encouraging separation of abstraction levels.\n- **System Entry Points**:\n  - Explicitly identify and document system entry points within the directory structure to enhance navigation and understanding.\n\n### Example Strategic Structure\n```plaintext\nklokwrk-project\/\n|-- core\/\n|   |-- logging\/\n|   |-- utilities\/\n|   |-- security\/\n|-- services\/\n|   |-- order-service\/\n|   |   |-- api\/\n|   |   |-- impl\/\n|   |-- payment-service\/\n|-- interfaces\/\n|   |-- web\/\n|   |-- mobile\/\n|   |-- api-gateway\/\n|-- domains\/\n|   |-- customer\/\n|   |-- inventory\/\n```\n\n### Benefits\n- **Enhanced Scalability**: As the number of modules grows, the hierarchical structure prevents confusion and maintains clarity.\n- **Improved Navigation**: Developers can easily understand and navigate the project's architecture.\n- **Clear Dependencies**: Defined dependencies and abstraction levels make inter-module relationships explicit.\n- **Logical Grouping**: High-level constructs and logical groupings suggest module organization and relations naturally.\n\n### Implementation Plan\n- **Refactor existing project structure** to align with the strategic structure guidelines outlined above.\n- **Communicate changes** and new guidelines to the development team to ensure consistency and adherence to the new organization.\n- **Update project documentation** to reflect the new structural organization, including diagrams where necessary.\n\nAdopting this strategic structural organization will mitigate the drawbacks of a flat structure and facilitate a more maintainable, scalable, and understandable project architecture as `klokwrk-project` continues to grow.","GenTime":"2024-07-29 02:21:45"}
{"File Name":"klokwrk-project\/0014-commit-message-format.md","Context":"## Context\\nUsing commit messages without any structure looks convenient for developers as they do not have to think about messages too much. Unfortunately, that freedom and lack of thinking can impose some\\nadditional burden on long-term project maintenance.\\nQuite often, we can find incomprehensible commit messages that do not communicate anything useful. Hopefully, imposing some lightweight rules and guidance will help developers create commit messages\\nthat are helpful for their colleagues.\\nIn addition, with unstructured commit messages, there is much less opportunity to introduce any tools on top of commit history. For example, we would like to employ an automated changelog generator\\nbased on extracting some semantical meaning from commits, but this will not work if commit messages lack any structure. Without the commit message structure, we can just dump the commit log in the\\nchangelog, which does not make the changelog more helpful than looking at the history of commits in the first place.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n## Decision\n","Decision":"**We will use a customized [conventional commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) format for writing commit messages.**\\nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:\\n<type>(optional <scope>): <description> {optional <metadata>}\\nOur customization:\\n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-message-guidelines)\\n- allows adding additional metadata in the message title if useful and appropriate (see details section for more info)\\n- requires format compliance only for messages of \"significant\" commits (see details section for more info)\\n### Decision details\\nDetails about the decision are given mainly as a set of strong recommendations (strongly recommended) and rules enforced by tooling (rule). In our case, the tooling is implemented as git commit hooks.\\nEvery contributor should install git hooks provided in this repository. That can be done with following command (executed from the project root):\\ngit config core.hooksPath support\/git\/hooks\\nThere might be cases when implemented rules are not appropriate and should be updated or removed or just temporarily ignored. In such scenarios, hooks can be skipped with git's `--no-verify` option.\\nWhile describing details, following terms are used as described:\\n- *commit message title*: refers to the first line of a commit message\\n- *commit message description*: refers to the part of the title describing a commit with human-readable message. In conventional commits specification that part is called `description`.\\n#### General guidance and rules for all commit messages\\n- (strongly recommended) - avoid trivial commit messages titles or descriptions\\n- (strongly recommended) - use imperative mood in title or description (add instead of adding or added, update instead of updating or updated etc.) as you are spelling out a command\\n- (rule) - message title or description must start with the uppercase letter <br\/>\\n<br\/>\\nThe main reason is a desire for better readability as we want easily spot the beginning message description or title. There are some arguments for using the lowercase like \"message titles are not\\nsentences\". While this is true, we prefer to have better readability than comply with some vague constraints.<br\/>\\n<br\/>\\n- (rule) - message title or description should not end with common punctuation characters: `.!?`\\n- (strongly recommended) - message title or description should not be comprised of multiple sentences\\n- (rule) - message title should not be longer than 120 characters. Use the message body if more space for description is needed<br\/>\\n<br\/>\\nActually, there is a common convention that we should not use more than 69 characters in the message title. It looks like the main reason for it is that GitHub truncates anything above 69 chars\\nfrom message titles. Having such a tight constraint seems unreasonable today, and the apparent shortcomings of any tool shouldn't restrict us, even if the tool is GitHub.<br\/>\\n<br\/>\\n- (strongly recommended) - commit message title or description should describe \"what\" (and sometimes \"why\"), instead of \"how\"<br\/>\\n<br\/>\\nFor describing \"why\", the message body is more appropriate as we have more space there. If needed, the message body may contain \"how\" too, but it should be clearly separated (at least with a blank\\nline) from \"what\" and \"why\".<br\/>\\n<br\/>\\n- (recommended) - commit message title should provide optional scope (from conventional commit specification) if applicable\\n- if commit refers to multiple scopes, scopes should be separated with `\/` character\\n- if commit refers to the work which influences the whole project, the scope should be `project` or it can be left out\\n- the scope should be a single word in lowercase<br\/>\\n<br\/>\\n- (strongly recommended) - message body must be separated from message title with a single blank line\\n- (option) - message body can contain additional blank lines\\n- (recommended) - message body should not use lines longer than 150 characters\\n- (strongly recommended) - include relevant references to issues or pull request to the metadata section of message title<br\/>\\n<br\/>\\nExample: `feat(some-module): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}`<br\/>\\n<br\/>\\n- (option) - include relevant feature\/bug ticket links in message footer according to conventional commits guidelines<br\/>\\n<br\/>\\nFooter is separated from body with a single blank line.\\n#### Guidance and rules for \"normal\" commits to the main development branch\\n- (rule) - all commits to the main development branch must have a message title in customized conventional commit format\\n#### Guidance and rules for merge commits to the main development branch\\nWhen used with [semi-linear commit history](.\/0007-git-workflow-with-linear-history.md), merge commits are the primary carriers of completed work units. As such, they are the most interesting for\\ncreating a changelog.\\nBefore merging, merge commits must be rebased against main development branch, and merging must be executed with no-fast-forward option (`--no-ff`).\\n- (rule) - merge commits must have 'merge' metadata (`{m}`) present at the end of the title <br\/>\\n<br\/>\\nThat way, merge commits can be easily distinguished on GitHub and in the changelog.\\n- (option) - merge commit metadata can carry additional information related to the issues and PRs references like in the following example\\nfeat(klokwrk-tool-gradle-source-repack): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}\\nHere, `i#123` is a reference to the issue, while `pr#12` is a reference to the pull request. Additional metadata are not controlled or enforced by git hooks.\\n#### Guidance and rules for normal commits to the feature branches\\n- (option) - normal commits don't have to follow custom conventional commits format for message title\\n- (strongly recommended) - normal commits should use conventional commits format when contained change is significant enough on its own to be placed in the changelog\\nWhen all useful changelog entries are contained in normal commits of a feature branch, we can do two different things depending on the situation:\\n- use merge commit with type of `notype`. Such merge commit will be ignored when creating a changelog.\\n- merge a branch with fast-forward option (no merge commit will be present)\\nPreferably, use `notype` merge commits, as they are still useful for clear separation of related work.\\n#### Types for conventional commits format\\n- common (angular)\\n- `feat` or `feature` - a new feature\\n- `fix` - a bug fix\\n- `docs` - documentation only changes\\n- `style` - changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\\n- `test` - adding missing tests or correcting existing tests\\n- `build` - changes that affect the build system or external dependencies\\n- `ci` - changes to our CI configuration files and scripts\\n- `refactor` - a code change that neither fixes a bug nor adds a feature\\n- `perf` - a code change that improves performance\\n- `chore` - routine task\\n- custom\\n- `enhance` or `enhancement` - improvements to the existing features\\n- `deps` - dependencies updates (use instead of `build` when commit only updates dependencies)<br\/>\\n<br\/>\\nThere are two main scenarios when upgrading dependencies, a simple version bump and the more involved upgrade requiring resolving various issues like compilation errors, API upgrades, etc.<br\/>\\n<br\/>\\nSimple version bumps should be contained in a single individual commit with a description message starting with the word \"Bump\". For example: `deps: Bump Micronaut to 2.5.2 version`.<br\/>\\n<br\/>\\nMore complicated upgrades should be organized as feature branches where each non-conventional commit resolves a single step in the process. When finished, the feature branch should be merged\\ninto the main development branch with a description starting with the word \"Upgrade\". For example: `deps: Upgrade Spring Boot to 2.5.0 version {m}`.<br\/>\\n<br\/>\\n- `task` - same meaning as `chore`. Prefer using `task`.\\n- `article` - use instead of `docs` when changes are related only to articles\\n- `misc` - anything that does not fit into previous categories\\n- `notype` - only for merge commits in situations where contained plain commits carries all relevant types and merge commit doesn't add anything useful. It is ignored in changelog.<br\/>\\n<br\/>\\nTypical example is when we are updating number of dependencies through commits in a feature branch. In this situation each feature branch commit should contain updates to a single dependency\\nand communicate the update through its own commit message in conventional commit format. When all updates in the branch are tested, we can merge them in the main branch. However, that merge\\nshould be ignored by changelog as it does not communicate anything new.\\n","tokens":205,"id":4890,"Prediction":"## Decision\nWe will adopt the Conventional Commits specification to bring a structured approach to writing commit messages. The Conventional Commits specification provides a set of simple rules for creating an explicit commit history, which will make our commit history more readable and support automated tooling, such as changelog generators.\n\n### Key Points of the Decision\n1. **Standardized Commit Messages**: Enforce a structure for commit messages that includes a type, an optional scope, and a description. Example format: `type(scope): description`.\n\n2. **Commit Types**: Define specific types that developers must use, including but not limited to:\n   - `feat`: A new feature for the user\n   - `fix`: A bug fix for the user\n   - `docs`: Documentation only changes\n   - `style`: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\n   - `refactor`: A code change that neither fixes a bug nor adds a feature\n   - `perf`: A code change that improves performance\n   - `test`: Adding missing tests or correcting existing tests\n   - `chore`: Changes to the build process or auxiliary tools and libraries such as documentation generation\n\n3. **Scope**: Use an optional scope to provide additional context about the area of the codebase affected by the change. Example: `feat(auth): add login endpoint`.\n\n4. **Description**: Require a concise description of the changes made. If more detail is necessary, use a longer body.\n\n5. **Benefits**:\n   - Enhanced readability and comprehension of commit history.\n   - Facilitation of automated tools for generating changelogs and managing releases.\n   - Improved communication within teams and across the project lifespan.\n\n6. **Tooling and Automation**: Integrate tools (such as commitlint and semantic-release) that enforce these rules and automate changelog generation.\n\nBy adopting Conventional Commits, we align with industry best practices, thereby maintaining a cleaner git history and preparing the ground for automated processes that depend on structured commit messages.","GenTime":"2024-07-29 02:21:50"}
{"File Name":"klokwrk-project\/0003-cqrs-and-event-sourcing-for-applications.md","Context":"## Context\\nWhen building monoliths or microservices that contain non-trivial business logic, we want to have a high-level component architecture pattern that natively leverages a strategic Domain Driven\\nDesign (DDD) concepts like ubiquitous language and bounded context. It should also support or allow, at least, the usage of application design architecture patterns like hexagonal architecture.\\nAt the lower technical level, component architecture pattern should provide native support for tactical DDD concepts like aggregates, entities, value objects, and events.\\nWhen building microservices, the chosen component architecture pattern should natively support asynchronous communication through events.\\nIt would be ideal to have all of these in a single coherent open-source framework and platform.\\nFor anemic CRUD style applications, more traditional approaches, like layered architecture, would be just fine.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n## Decision\n","Decision":"**We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**\\nEspecially as one of the major `klokwrk-project` goals is to demonstrate the creation of complex CQRS\/ES microservice applications. We can use more traditional approaches like classic layered\\narchitecture and anemic model for anemic CRUD style applications.\\n","tokens":182,"id":4896,"Prediction":"## Decision\n\nWe have decided to adopt a Hexagonal Architecture combined with Domain-Driven Design (DDD) principles for the development of the `klokwrk-project`. This approach will provide us with the necessary high-level component architecture to manage complex business logic effectively and align with our strategic DDD goals. \n\n### Justification for Hexagonal Architecture and DDD\n1. **Bounded Context**: Utilizing DDD helps us define clear bounded contexts, facilitating better understanding and communication within our development teams through a ubiquitous language.\n2. **Flexibility**: Hexagonal Architecture (also known as Ports and Adapters) will allow us to manage dependencies more efficiently, promoting decoupling and enhancing testability.\n3. **Scalability**: This architecture pattern will enable us to build scalable and maintainable microservices by ensuring a clear separation of concerns.\n4. **Asynchronous Communication**: For microservices, the architecture will be designed to natively support asynchronous communication through events, ensuring better resilience and efficiency.\n5. **Tactical DDD Elements**: At a lower technical level, we will incorporate DDD tactical patterns like aggregates, entities, value objects, and domain events to manage the domain model effectively.\n6. **Open-Source Frameworks**: We will leverage available open-source frameworks and platforms that support Hexagonal Architecture and DDD to ensure coherence and reduce initial setup costs while promoting community-driven improvements and support.\n\n### Exceptions\nFor simpler, anemic CRUD-style applications where business logic is minimal, a traditional layered architecture will be employed. This will avoid overengineering and maintain simplicity in such contexts. \n\nBy adopting this decision, we target to achieve a robust, maintainable, and scalable system architecture for `klokwrk-project` aligning with the industry's best practices and modern software design principles.","GenTime":"2024-07-29 02:22:24"}
{"File Name":"klokwrk-project\/0007-git-workflow-with-linear-history.md","Context":"## Context\\nThe value of tidy and [semi-linear commit history](https:\/\/fangpenlin.com\/images\/2013-09-30-keep-a-readable-git-history\/source_tree_new_branch_rebase_merge.png) is often overlooked in many Git-based\\nprojects. This is unfortunate since non-linear git commit history might be a [horrible mess](https:\/\/tugberkugurlu.blob.core.windows.net\/bloggyimages\/d773c1fe-4db8-4d2f-a994-c60f3f8cb6f0.png) that\\ndoes not provide any useful information. We want to use as simple as possible git workflow that promotes and ensures a semi-linear history.\\n> * **Semi-linear** commit history usually refers to a history that uses merge commits (git \"no-fast-forward\" merge option) to clearly denote which commits are meant to be together and represent a\\n>   coherent whole.\\n> * **Linear** commit history usually refers to completely flat history (git default \"fast-forward\") where it is impossible to tell at first glance which commits belong together.\\nWhen working on individual features, related git commits can be organized either as \"work log\" or as a \"recipe\". When working in a team, it is crucial that team members and\/or reviewers can easily\\ncomprehend what is going on in a particular feature. For this reason, we prefer features to be organized as \"recipes\".\\n> * **Work log** style of organizing feature commits refers to the style without any organization. Commits are added solely as they are developed through time.\\n> * **Recipe** style of organizing feature commits refers to the style where commits have a sensible organization where peer developers can clearly see and learn how the feature is created. This\\n>   style requires some additional work as its primary goal is communication, instead of just implementing a feature.\\nVery often, in bigger teams, common git workflows have a problem of broken continuous integration builds. We want to embrace and use as simple as possible workflow that resolves that problem.\\nChosen git workflow should seamlessly support release versioning and, if needed, related work on release branches.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n## Decision\n","Decision":"**We will use a [stable mainline branching model for Git](https:\/\/www.bitsnbites.eu\/a-stable-mainline-branching-model-for-git\/).** It\\n[supports semi-linear Git history](https:\/\/www.bitsnbites.eu\/a-tidy-linear-git-history\/) and helps to resolve the problem of broken continuous integration builds.\\n**We will, however, introduce several tweaks to the \"stable mainline branching model\":**\\n* We will use the following naming pattern for feature branches: **`feature_<name>`** instead of `feature\/name`.\\n* We will not use new branches when remote feature branches need rebasing. Instead, we will just inform all collaborators that rebasing is pending for a feature branch. It is important\\nto communicate with collaborators **before** rebasing and force-pushing.\\n* We will not create a release branch for each release. In general, we will just tag a release. However, we will create a release branch when a particular release needs fixing.\\nWe will organize our [feature commits as recipes](https:\/\/www.bitsnbites.eu\/git-history-work-log-vs-recipe\/) because we want to promote the team's learning and communication.\\nWe will also use specific commit message format as described in [ADR-0014 - Commit Message Format](.\/0014-commit-message-format.md)\\n","tokens":461,"id":4899,"Prediction":"## Decision\n\nWe will adopt a **semi-linear Git workflow** to maintain a clean and comprehensible commit history.\n\n### Key Elements of the Workflow:\n\n1. **Feature Branches**\n   - All new work (features, bug fixes, etc.) will be done in separate feature branches based off the `main` branch.\n   - Branch names should follow a standardized naming convention (e.g., `feature\/<feature-name>` or `bugfix\/<description>`).\n\n2. **Merge with --no-ff Option**\n   - Feature branches will be merged into `main` using the `--no-ff` (no fast-forward) option to ensure that each set of related changes is grouped together as a cohesive whole.\n   \n3. **Commit Organization**\n   - Adopt the \"recipe\" style for organizing commits:\n     - Each commit should have a clear and descriptive message.\n     - Commits should be logically grouped and ordered to facilitate understanding by peers during code reviews.\n   \n4. **CI Integration**\n   - Before merging a feature branch into `main`, ensure that all tests and Continuous Integration (CI) checks pass.\n   - Utilize pull requests (PRs) to manage merges, allowing for CI checks and peer reviews to be executed before changes are integrated into the `main` branch.\n\n5. **Release Management**\n   - Use a dedicated `release` branch for managing stable releases.\n   - Feature branches should be merged into `develop`, and `develop` into `release` for release preparation.\n   - Hotfixes should be branched off `release` and merged back into both `main` and `release`.\n   \n6. **Versioning**\n   - Adopt semantic versioning (e.g., `v1.2.3`).\n   - Release tags should be created in the `release` branch.\n\n### Workflow Summary:\n\nWith this workflow:\n- **Semi-linear history** is maintained by using `--no-ff` merges, ensuring that related commits are grouped together.\n- Commits are organized in a **recipe style** providing clarity and ease of understanding for each feature.\n- **CI integrations** are enforced to avoid broken builds on the `main` branch.\n- The workflow supports clear and straightforward **release management** and versioning procedures.\n\n### Rationale:\nThis decision aligns with the needs to keep the commit history clean, understandable, and useful for all team members while ensuring that our CI builds remain stable and manageable. Adopting a semi-linear history provides a balanced approach between clarity and development efficiency.","GenTime":"2024-07-29 02:22:37"}
{"File Name":"architecture-decision-log\/0016-analytics-foundations.md","Context":"## Context\\nOur company is starting to growth fast. With that growth, it is common to see the need of complex data analysis. We've solved that by installing Metabase in a read-replica of our OKR transactional database, but even that structure lacks more complex analytics. Concurrently with the previous statement, our company plans to create an analytics product for our customers, enabling real-time complex analysis of their users.\\nWe can't ignore the need to have a proper analytics foundations inside Bud. Also, we can't afford investing a large amount of time building that infrastructure, since everything could change fast. We need to find a way to create a flexible analytics infrastructure that could:\\n(a) Provide meaningful data regarding our customers;\\n(b) Be flexible enought to integrate with multiple sources;\\n(c) Allow the usage from external applications.\\nIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.\\n## Decision Drivers\\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\n\n## Decision\n","Decision":"1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\nAfter evaluating all options, we've decided to proceed with Airbyte. It meets almost every specification that we have. It is extremelly easy to implement and follows all the best standards. It isn't an in-house solution, but in the current scenario we're on that would not be a big deal with it. Also, we could learn from it and maybe create a new tool in the future, designed to met our needs.\\n### Positivo Consequences\\nWith this infrastructure, we're going to achieve a robust ELT infrastructure, with little effort. We can easily create an analytics application that is going to serve all our business requirements with minimal effort. Also, Airbyte uses DBT under the hood, that being said, even if we need to change our ELT structure, we would still be able to migrate our DBT project.\\n### Negative Consequences\\nThere are two main negative consequences of this decision:\\n#### 1. Not being able to query real time data\\nAs pointed by Marcelo Travi, with Airbyte we would not be able to query real-time data. In most use-cases that would not be an issue, but considering that our first requirement is serving an evolution graph of a given key-result percentual proggress, that should be an issue. To fix that, we suggest using only D-1 data from our analytics, and using the transactional data to enrich it.\\n#### 2. Large queries could affect our applications\\nAs pointed by Victor Perin, since this is not a common implementation, we should be aware that complex analytics queries could impact our applications, since we're integrating them directly. There are plenty of ways to fix that, the easiest one would be creating a read-replica of our data warehouse, and using that to query the analytics. By doing so, we would separate both analytics and BI, minizing this issue.\\n","tokens":240,"id":4909,"Prediction":"## Decision\n\nAfter evaluating various options, the decision is made to implement a cloud-based data warehousing solution, specifically Amazon Redshift, as our primary analytics infrastructure. Here are the key reasons for selecting Amazon Redshift:\n\n1. **Flexibility**:\n    - Amazon Redshift provides the ability to scale compute and storage independently, allowing us to adjust based on usage and load without significant downtime.\n    - It supports a wide range of data import\/export formats, making it easier to handle diverse data sources.\n\n2. **Integration**:\n    - Redshift has numerous built-in integrations with popular data tools and services, including BI tools like Metabase, ETL tools, and data lakes, enabling seamless data ingestion and analysis.\n    - The use of Amazon Redshift Spectrum allows querying data directly from S3 data lakes without needing to load it into the warehouse, giving additional flexibility.\n\n3. **Implementation Difficulty**:\n    - AWS offers managed services which simplify the setup, maintenance, and operational workload, allowing our team to focus more on deriving insights rather than managing infrastructure.\n    - Comprehensive documentation, a wide range of third-party support, and AWS's support services ease the learning curve and implementation challenges.\n\nBy choosing Amazon Redshift, we not only ensure the flexibility needed to adapt to changing requirements but also simplify integration with external sources and our internal systems. This setup will serve as the foundation for both internal and customer-facing analytics, providing a reliable and expandable solution for our growing data needs.","GenTime":"2024-07-29 02:23:12"}
{"File Name":"architecture-decision-log\/0013-microservices-overview.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n## Decision\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWithin this ADR, we're going to keep an overview of our current microservices architecture. Each application has a pre-defined boundary, as well as a well-defined technical definition.\\nYou can take a look at our [live architecture](https:\/\/app.mural.co\/t\/d4c6342\/m\/d4c6342\/1602612797113\/5836de51b220fa5ebc8cefd7ad0f161125610524) or check the diagram below:\\n![Diagram showing our microservices architecture](..\/assets\/0013-microservices-overview\/diagram.png)\\n","tokens":53,"id":4910,"Prediction":"## Context\nThe team is working on a new feature for our software that requires real-time data processing. Currently, our application architecture is based on a monolithic design, and as the system scales, we've experienced performance bottlenecks, making it difficult to handle the increasing workload and maintain low latency. Additionally, we want a solution that enables easier maintenance and faster deployment of features.\n\n## Decision\nWe have decided to transition from a monolithic architecture to a microservices architecture for the new feature. Each microservice will be responsible for a specific function within the real-time data processing pipeline. This change aims to improve scalability, maintainability, and deployment efficiency.\n\n## Status\nProposed\n\n## Consequences\n1. **Positive**: Improved scalability, as each microservice can be scaled independently.\n2. **Positive**: Enhanced maintainability, due to the separation of concerns and smaller codebases.\n3. **Positive**: Faster deployment cycles, since individual microservices can be updated and deployed without affecting the entire system.\n4. **Negative**: Increased complexity in terms of service orchestration, monitoring, and debugging.\n5. **Negative**: Potential initial overhead in converting existing functionality and setting up the new architecture.\n6. **Negative**: Communication latency between microservices could introduce new performance challenges.\n\n## More reading\n- \"Building Microservices\" by Sam Newman\n- Microservices.io by Chris Richardson\n- \"The Twelve-Factor App\" methodology\n\n## Updates\n1. [Date] - Kickoff meeting scheduled to begin transitioning the architecture\n2. [Date] - Initial microservice prototypes developed and in testing phase\n3. [Date] - Performance benchmarks under review to ensure new architecture meets our requirements","GenTime":"2024-07-29 02:23:16"}
{"File Name":"architecture-decision-log\/0007-behavior-driven-development.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n## Decision\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.\\nInstead of writing complex test statements, we use a BDD tool to create a readable phrase, like the following:\\n```txt\\nGiven a new created user\\nWhen that user places a new order\\nThen a notification must be sent\\n```\\nEach line of that phrase relates to a given function that will execute steps and persist the state machine's context. We can use assertions to return errors along the way.\\nBDD is a powerful tool that enhances our TDD cycle.\\n","tokens":53,"id":4918,"Prediction":"## Context\nWe are building a web-based application that will support a high volume of concurrent users. The application needs to be highly available, scalable, and offer low latency. Additionally, we require robust monitoring and logging to ensure any issues can be quickly identified and resolved.\n\n## Decision\nWe have decided to use a microservices architecture for developing the application. Each core functionality of the application will be implemented as a separate, independently deployable service. We will use Docker and Kubernetes for container orchestration to ensure scalability and resilience. For the technology stack, we will leverage the following:\n\n1. **Backend Services**: Developed using Node.js with Express, leveraging asynchronous, non-blocking I\/O to handle a large number of I\/O operations efficiently.\n2. **Database**: We will use PostgreSQL for our relational database needs given its reliability and feature set, and Redis for caching to reduce database load and improve performance.\n3. **Frontend**: React.js for a dynamic and responsive user interface.\n4. **APIs**: RESTful APIs for communication between frontend and backend, with GraphQL for specific data-fetching needs to optimize consumption.\n5. **CI\/CD Pipeline**: Implemented using Jenkins and GitHub Actions to automate building, testing, and deploying services.\n6. **Monitoring and Logging**: Employing Prometheus for monitoring and Grafana for dashboard visualization. Logging will be managed using the ELK stack (Elasticsearch, Logstash, and Kibana).\n\nThis architecture will allow us to meet our scalability, availability, and performance requirements while maintaining flexibility and ease of development.","GenTime":"2024-07-29 02:23:51"}
{"File Name":"architecture-decision-log\/0002-github-template-for-issues-and-pull-requests.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n\n## Decision\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\nAn easy way to improve our communication is by creating templates for both `issues` and `pull requests`. Those templates must be simple, easy to use, and express all required sections for our developers and users.\\nYou can check the suggested templates [in the following folder](..\/assets\/0002-github-template-for-issues-and-pull-requests).\\n","tokens":34,"id":4919,"Prediction":"## Context\n\nWe are developing a new microservices-based architecture to replace our legacy monolithic application. The legacy system has become increasingly difficult to maintain and scale as the user base and feature set have grown. Furthermore, different teams have expressed the need for more autonomy and faster release cycles, which are hindered by the current monolithic architecture. The primary goals for the new system include improved scalability, ease of maintenance, and enhanced team autonomy.\n\n## Decision\n\nWe will adopt a microservices architecture for our new system. Each microservice will be responsible for a specific business capability and will communicate with other services via well-defined APIs. Additionally, we will utilize containerization technologies (e.g., Docker) and orchestrate the deployment using Kubernetes to ensure scalable and resilient operations. Each microservice will be developed, deployed, and scaled independently, allowing teams to work more autonomously and release updates more frequently. This decision aligns with our goals of improving scalability, maintainability, and team autonomy.\n\n## Status\n\n*[Status: In Progress]*\n\n## Consequences\n\n* **Positive:**\n  * Improved scalability as each microservice can be scaled independently based on demand.\n  * Enhanced maintainability by isolating different business capabilities, making it easier to update or refactor parts of the system.\n  * Increased team autonomy and faster release cycles since teams can work on and deploy their services independently.\n  * Better fault isolation, reducing the impact of failures in one part of the system on others.\n\n* **Negative:**\n  * Increased complexity in managing distributed services, including inter-service communication, data consistency, and monitoring.\n  * Potential overhead of managing container orchestration and ensuring services can discover and interact with each other reliably.\n  * Need for robust DevOps practices and infrastructure to support continuous deployment and monitoring.\n  * Initial learning curve for teams to adapt to the principles and best practices of microservices architecture.","GenTime":"2024-07-29 02:23:56"}
{"File Name":"road-registry\/002-track-municipalities.md","Context":"## Context\\nWe need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street.\\nWe can't rely on the geometries used in the municipality registry because they are (1) not exposed and (2) they do not match the boundary along which road segments have been split up.\\nWhen the boundaries in the municipality registry change, one can't expect all segments to change as well.\\n\n## Decision\n","Decision":"We keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.\\nWe use `ImportedMunicipality` as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.\\n","tokens":96,"id":4930,"Prediction":"## Decision\n\nWe will integrate a third-party geographic information system (GIS) service to obtain consistent and up-to-date geometries of municipalities. This service will provide API access to geometries that can be utilized to assign municipalities to road segments lacking explicit left or right street associations. By using an external service, we bypass the limitations of the municipality registry.\n\n### Rationale:\n\n1. **Consistency** - Using a GIS service that is regularly updated ensures that our boundaries match the current real-world configurations, providing a more accurate assignment of municipalities to road segments.\n2. **Accessibility** - The API access provided by the GIS service solves both the problem of data exposure and the availability of up-to-date geometries.\n3. **Decoupling Data** - By not relying on the internal municipality registry, we avoid the issue where changes in registry boundaries do not automatically propagate to our road segment data, maintaining the integrity and consistency of our assignments.\n4. **Scalability** - Integrating a dedicated GIS service allows us to scale and maintain the project more effectively, ensuring that as municipalities grow or boundaries change, our data can be easily updated.\n\n### Implementation Steps:\n\n1. **Research** and select an appropriate GIS service provider that offers the necessary municipality boundary data with API access. Ensure that this service is reliable and updated frequently.\n2. **Develop** the interface to fetch municipality boundaries from the selected GIS service and integrate this with the existing system for processing road segments.\n3. **Map Integration** - Implement logic to map fetched geometries to road segments that do not have associated left or right streets.\n4. **Testing** - Rigorously test the integration to ensure the boundaries provided by the GIS service accurately reflect the real-world municipal boundaries and correctly associate road segments.\n5. **Monitoring & Maintenance** - Set up monitoring to ensure the GIS data remains current and accurate, and establish protocols for responding to updates in the data.\n\nThis approach ensures we have a reliable, scalable, and consistent method to derive municipalities for road segments, addressing the issues posed by internally kept and potentially outdated geometries.","GenTime":"2024-07-29 02:24:27"}
{"File Name":"road-registry\/006-provisional-changes.md","Context":"## Context\\nAs a result of running the feature-compare process a bunch of files are produced. These files are essentially a _diff_ between what the operator started out with, that is the dump \/ extract he \/ she started working from, and what they ended up with, after having edited shapes and associated data in their editor. Some of those files have a `_ALL.DBF|_ALL.SHP` suffix. These files are the files we base our logic on to validate and translate to an _internal_ change request. Most notably, these files have an extra column called `RECORDTYPE` which, as the name suggests, classifies a record as either being `IDENTICAL`, `ADDED`, `MODIFIED` or `REMOVED`. Next to that most of the files contain, per record, the equivalent of a primary key that identifies a row. This primary key is used in other files as a foreign key to reference a record sitting in another file. Records with a `RECORDTYPE` of `ADDED` will use a temporary identifier (a really big number that we assume is free to use) to make this work (see [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) for how that works). One gotcha is that the primary keys are not always unique, that is, they can appear multiple times in the `*_ALL.DBF|*_ALL.SHP` files, once for each record type. A common scenario is a modification represented as a removal and an addition record.\\nFor the `WEGSEGMENT_ALL.DBF` file, things are more complicated ... next to having a `WS_OIDN` column act as primary key it has a `EVENTIDN` column acting as an alternative primary key in some cases. In case the `RECORDTYPE` is `ADDED` and the `EVENTIDN` has a value differing from `0`, the `WS_OIDN` column refers to an existing road segment and the `EVENTIDN` column refers to its new representation. In such a case, other files refer to a road segment by the value found in the `EVENTIDN`, not by the value in `WS_OIDN`. Alas, such is life ...\\n\n## Decision\n","Decision":"Modifying a road segment involves data from `WEGSEGMENT_ALL.DBF`, `WEGSEGMENT_ALL.SHP` and `ATTRIJSTROKEN_ALL.DBF`, `ATTWEGBREEDTE_ALL.DBF`, `ATTWEGVERHARDING_ALL.DBF` - that is, when it is represented as an _internal_ change request command. Each of those `.DBF` files contains a `RECORDTYPE` column. As such, a road segment could be marked as identical in `WEGSEGMENT_ALL.DB`, yet it's lanes, width and \/ or surfaces could be marked as a mixture of modified, removed, added, identical. This is the reason why a road segment that is identical is appended as a `provisional` change to the list of translated changes. Because we're not sure, just yet, that it is an actual change. The order in which these files are translated causes a `provisional` change, if warranted, to be promoted to an actual change.\\nWhy go thru all this trouble? Well, lanes, widths, and surfaces are tightly coupled to the geometry of a road segment such that it makes sense to capture them as a holistic change rather than as individual fragmented changes. There's still a bigger debate to be had about which pieces of data change together ...\\n","tokens":467,"id":4931,"Prediction":"## Decision\n\nTo address the complexities involved in processing the `*_ALL.DBF|*_ALL.SHP` files, particularly the intricate scenarios presented by the `WEGSEGMENT_ALL.DBF` file, we have decided to implement a robust processing workflow with the following key components:\n\n1. **Primary and Foreign Key Management:**\n   - **Primary Key Mapping:** Create a comprehensive mapping system for primary keys (`WS_OIDN` and `EVENTIDN`). This mapping system will distinguish between `ADDED`, `MODIFIED`, `REMOVED`, and `IDENTICAL` records, maintaining associations between temporary identifiers and their corresponding permanent ones.\n   - **Temporary Identifier Handling:** Ensure that temporary identifiers assigned to `ADDED` records are tracked and later replaced with permanent identifiers upon finalizing modifications, as outlined in the referenced [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) document.\n\n2. **Record Type Classification:**\n   - Develop a utility to process and classify records based on the `RECORDTYPE` column. This utility will separate records into distinct categories (IDENTICAL, ADDED, MODIFIED, REMOVED) and handle each category appropriately.\n   - Implement validation rules to ensure that each record type is processed according to its specific handling requirements.\n\n3. **Event-Based Referential Integrity:**\n   - For the `WEGSEGMENT_ALL.DBF` file, recognize and manage the dual primary key system (`WS_OIDN` and `EVENTIDN`). Specifically:\n     - If the `RECORDTYPE` is `ADDED` and `EVENTIDN` differs from `0`, treat `WS_OIDN` as a reference to an existing segment while `EVENTIDN` identifies the new representation of that segment.\n     - Ensure that other files referring to road segments utilize the appropriate key based on the scenario described.\n\n4. **Data Consistency and Integrity:**\n   - Develop consistency-check algorithms to validate the uniqueness and integrity of primary keys across all files. This includes handling cases where primary keys might legitimately appear more than once because of the `RECORDTYPE`.\n   - Create a mechanism to reconcile changes, ensuring that all references to modified road segments correctly point to their updated representations following the transformation rules.\n\n5. **Workflow Automation:**\n   - Implement an automated workflow to process the `*_ALL.DBF|*_ALL.SHP` files, systematically applying the above rules and validations. This workflow should:\n     - Ingest the initial and modified datasets.\n     - Apply the primary and foreign key mappings.\n     - Classify and process records by their `RECORDTYPE`.\n     - Maintain referential integrity between related files.\n     - Output a consolidated internal change request dataset.\n\n6. **Documentation and Traceability:**\n   - Maintain detailed documentation for each step in the processing workflow, ensuring traceability and auditability.\n   - Create logs and reports to capture the transformation and validation process, providing insights into any discrepancies or errors encountered.\n\nBy implementing this comprehensive processing workflow, we aim to ensure that the `*_ALL.DBF|*_ALL.SHP` files are accurately interpreted and transformed into a reliable and coherent internal change request, maintaining the integrity and consistency of the data throughout the process.","GenTime":"2024-07-29 02:24:34"}
{"File Name":"godspeed-you-blocked-developer\/003. Serverless.md","Context":"## Context\\nServices become unique and fragile when you configure them by hand. I want to have a provisioning strategy before I actually start adding components, so I don't end up with behavior that's hard to reproduce.\\nI like the principles in the book Infrastructure as Code. It suggests avoiding making changes to services except in an automated, configured manner.\\nI've had some exposure to the infrastructure provisioning tool [Serverless](https:\/\/serverless.com), so I'd like to try using it on this project. Serverless abstracts AWS CloudFormation and similar tools from other cloud vendors.\\n\n## Decision\n","Decision":"I will use Serverless to describe and provision my AWS components.\\n","tokens":119,"id":4937,"Prediction":"## Decision\n\nAdopt the Serverless framework as our provisioning strategy for deploying and managing cloud services in this project.\n\n### Rationale\n\n1. **Consistency and Reproducibility**: Using an automated approach aligns with the principles of Infrastructure as Code, ensuring that configurations are consistent and can be reliably reproduced across different environments.\n\n2. **Minimize Human Error**: Automating the configuration process minimizes the risk of human errors that can occur with manual configuration, leading to more stable and reliable services.\n\n3. **Abstraction and Flexibility**: Serverless abstracts away the complexities of the underlying infrastructure tools like AWS CloudFormation, making it easier to manage cloud resources across different vendors.\n\n4. **Prior Experience**: You have some prior exposure to Serverless, which will allow for a smoother learning curve and better initial productivity.\n\n5. **Scalability and Efficiency**: Serverless supports a highly scalable architecture and can efficiently manage scaling and resource optimization.\n\n6. **Community and Ecosystem**: Serverless has a robust community and a wealth of plugins and extensions, which can provide support and additional functionality as needed.\n\n### Implementation Plan\n\n1. **Setup Serverless Framework**: Install Serverless CLI and configure it with the appropriate cloud provider credentials.\n   \n2. **Define Infrastructure as Code**: Write Serverless configuration files (YAML or JSON) to define the infrastructure required, such as functions, resources, and IAM roles.\n\n3. **Version Control**: Ensure all Serverless configuration files are stored in version control (e.g., Git) to track changes and facilitate collaboration.\n\n4. **Continuous Integration\/Continuous Deployment (CI\/CD)**: Integrate Serverless deployment into the CI\/CD pipeline to automate the deployment process.\n\n5. **Testing and Rollback**: Implement testing frameworks and rollback mechanisms to validate deployments and revert changes if issues are found.\n\n6. **Documentation and Training**: Prepare documentation and conduct training sessions to ensure team members are comfortable and proficient with the Serverless framework.\n\nBy adopting the Serverless framework, we aim to achieve a highly automated, scalable, and maintainable infrastructure provisioning strategy for our project.","GenTime":"2024-07-29 02:24:56"}
{"File Name":"godspeed-you-blocked-developer\/001. TypeScript, React, Redux, and Webpack.md","Context":"## Context\\nI was once a web developer when jQuery and CSS 2 were the new hotness. I did a lot of web 1.0, single-server work. Then I spent many years building iOS apps. These days I'm interested to be more of a generalist.\\nIn the fall of 2017, I learned some TypeScript and thought it was great. JavaScript is JavaScript, and I like strong types, and TypeScript makes that happen, so it was an instant favorite.\\nI decided to learn some more web technologies. For browser apps that means React and Redux, which is commonly used with Node, NPM, and Webpack. Learning the common stuff is a good way to get my bearings in the field.\\nWhen the idea for this project came to mind, it was a good chance to learn that web stack and catch up my knowledge.\\n\n## Decision\n","Decision":"I decided to build this site in TypeScript, with React and Redux, on Node, NPM, and Webpack.\\n","tokens":177,"id":4940,"Prediction":"## Decision\n\nAdopt the React-Redux stack with TypeScript for the frontend development of our project while utilizing Node.js and Webpack for the development and build process.\n\n### Justification:\n1. **Familiarity and Experience:** Leveraging previous experience with TypeScript aids in maintaining type safety and code quality, making the learning curve for the web stack smoother.\n2. **Community and Ecosystem:** Choosing React and Redux ensures robust community support, extensive resources, and a plethora of libraries and tools that can speed up the development process.\n3. **Scalability:** React's component-based architecture is well-suited for building scalable, maintainable front-end applications.\n4. **Concurrency and Performance:** Node.js offers non-blocking, event-driven architecture which is ideal for building efficient server-side applications.\n5. **Module Bundling:** Webpack's module bundling capabilities streamline the process of compiling and bundling JavaScript modules, enhancing the workflow and build performance.\n\n### Implementation Steps:\n1. **Set up the Development Environment:**\n   - Install Node.js and NPM.\n   - Initialize the project with `npm init`.\n   - Install TypeScript with `npm install typescript --save-dev`.\n   - Configure TypeScript using `tsconfig.json`.\n\n2. **Install Required Libraries:**\n   - React and React DOM: `npm install react react-dom`.\n   - Redux and React-Redux: `npm install redux react-redux`.\n   - TypeScript Definitions: `npm install @types\/react @types\/react-dom @types\/redux @types\/react-redux`.\n\n3. **Configure Webpack for Bundling:**\n   - Install Webpack: `npm install webpack webpack-cli webpack-dev-server --save-dev`.\n   - Set up Webpack configuration in `webpack.config.js`.\n\n4. **Develop and Test:**\n   - Create necessary React components and Redux stores.\n   - Write appropriate tests to ensure code reliability.\n\n5. **Continuous Integration\/Continuous Deployment (CI\/CD):**\n   - Set up a CI\/CD pipeline to automate testing and deployment processes.\n\nBy following the outlined decision and steps, we can ensure a robust, maintainable, and scalable application that leverages modern web development technologies and practices.","GenTime":"2024-07-29 02:25:08"}
{"File Name":"uniffi-rs\/0004-only-threadsafe-interfaces.md","Context":"## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\n\n## Decision\n","Decision":"* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\nChosen option:\\n* **[Option 2] Immediately deprecate, then remove entirely, support for\\nnon-`Send+Sync` interfaces.**\\nThis decision was taken because our real world experience tells us that\\nnon-`Send+Sync` interfaces are only useful in toy or example applications (eg,\\nthe nimbus and autofill projects didn't get very far before needing these\\ncapabilities), so the extra ongoing work in supporting these interfaces cannot\\nbe justified.\\n### Positive Consequences\\n* The locking in all uniffi supported components will be more easily\\ndiscoverable - it will be in hand-written rust code and not hidden inside\\ngenerated code. This is a benefit to the developers of the uniffi supported\\ncomponent rather than to the consumers of it; while we are considering other\\nfeatures to help communicate the lock semantics to such consumers, that is\\nbeyond the scope of this ADR.\\n* Opens the door to enhancements that would be impossible for non-`Send+Sync`\\ninterfaces, and simpler to implement for `Send+Sync` interfaces if support\\nfor non-`Send+Sync` interfaces did not exist.\\n* Simpler implementation and documentation.\\n### Negative Consequences\\n* All consumers (both inside Mozilla and external) will need to change their\\ninterfaces to be `Send+Sync`. As an example of what this entails,\\nsee [this commit](https:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985)\\nwhich converts the `todolist` example.\\n* Simple, toy applications may be more difficult to wrap - consumers will not\\nbe able to defer decisions about `Send+Sync` support and will instead need to\\nimplement simple locking as demonstrated in [this commit](\\nhttps:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985).\\n* Existing applications that are yet to consider how to make their\\nimplementations `Send+Sync` cannot be wrapped until they have.\\n* The examples which aren't currently marked with the `[Threadsafe]` attribute\\nwill become more complex as they will all need to implement and explain how\\nthey achieve being `Send+Sync`.\\n* The perception that its more difficult to wrap interfaces will lead to less\\nadoption of the tool.\\n","tokens":468,"id":4952,"Prediction":"## Decision\n\nUniffi will require all Rust structs to be `Send` and `Sync` by default, effectively eliminating the need for the `[Threadsafe]` marker. This decision mandates that component authors explicitly manage thread safety internally within their structs rather than relying on uniffi to provide hidden locking mechanisms. This change ensures that:\n\n1. **Explicit Thread Safety Management**: Structs must naturally conform to Rust's `Send` and `Sync` traits, making the responsibilities of managing concurrency explicit to the component author.\n2. **Avoidance of Hidden Performance Hits**: By removing the implicit mutex wrapping, we avoid unexpected blocking behavior, reducing the risk of performance issues such as those observed in the referenced Fenix bug.\n3. **Alignment with Rust's Idioms**: This approach aligns with Rust's design philosophy, promoting clarity and explicit management of concurrency aspects.\n4. **Enablement of Advanced Features**: Removing the need for internal mutex wrapping paves the way for supporting more advanced features such as alternative method receivers and passing interface references over the FFI.\n\nIn summary, Uniffi will no longer support non-`Send+Sync` structs by embedding automatic mutexes. Instead, all supported structs must inherently satisfy the `Send` and `Sync` traits, ensuring that component authors are fully responsible for managing thread safety.","GenTime":"2024-07-29 02:25:46"}
{"File Name":"adr-poc\/0003-write-help-file.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4963,"Prediction":"## Context\nOur team is working on a large-scale e-commerce platform that needs to handle high volumes of traffic and transactions efficiently. We are currently considering the best architecture to support scalability, maintainability, and rapid feature deployment. The current monolithic architecture is becoming increasingly difficult to manage and scale. We have identified the following key constraints and requirements:\n\n1. **Scalability**: The architecture must support scaling up or down based on traffic.\n2. **Maintainability**: It should be easy to understand, maintain, and extend.\n3. **Rapid Deployment**: New features should be deployable without affecting the whole system.\n4. **Technological Stack**: The existing team has more expertise in Java and cloud-native technologies.\n\n## Decision\nWe decided to transition from a monolithic architecture to a microservices architecture for the e-commerce platform. Each core functionality (e.g., user management, product catalog, order processing, payment processing) will be developed as a separate microservice. These services will communicate with each other through well-defined APIs, possibly using REST or gRPC.\n\nThe microservices will be deployed on a Kubernetes cluster to manage scaling and resiliency. We will use a continuous integration\/continuous deployment (CI\/CD) pipeline to facilitate rapid and automated deployment of updates. The team will leverage cloud-native technologies and tools that are already within their area of expertise, primarily focusing on Java-based frameworks such as Spring Boot for microservices development.\n\nThis transition is anticipated to be gradual, starting with the splitting of least critical services first, to minimize risks and disruptions to the platform.","GenTime":"2024-07-29 02:26:30"}
{"File Name":"ios-architecture-decision-logs\/0012-use-networkManager-for-network-requests.md","Context":"## Context\\nOur team created NetworkManager for network requests. For consistency we should replace and use NetworkManager instead of Fetchable protocol\\n\n## Decision\n","Decision":"Every new interactors must use NetworkManager. Also every new endpoints should use new networker approach\\n","tokens":27,"id":4974,"Prediction":"## Decision\nBased on the context that our team has created a NetworkManager for handling network requests and for the sake of maintaining consistency throughout the codebase, we have decided to replace the use of the Fetchable protocol with NetworkManager.\n\nNetworkManager will become the standard for all network request operations across the project, which will help in reducing confusion, streamlining the codebase, and ensuring that all network communications are managed in a unified manner. This decision also simplifies debugging, testing, and future enhancements related to network operations.","GenTime":"2024-07-29 02:27:00"}
{"File Name":"ios-architecture-decision-logs\/0007-presenter-casting-as-delegate.md","Context":"## Context\\nWe faced a problem that different implementations of presenter's conforming delegates\\n\n## Decision\n","Decision":"We decided to add `var xxxxDelegate { get }` to `xxPresenterInterface` and passing this `presenter`  to any delegate like:  `(delegate: presenter.xxxxDelegate)` instead of  `(delegate: presenter as? xxxDelegate)` or `(delegate: presenter as! xxxDelegate)`\\n","tokens":17,"id":4980,"Prediction":"## Decision\nTo address the issue of different implementations of presenter's conforming delegates, we have decided to establish a standardized interface that all presenter delegates must implement. This interface will define the essential methods and properties that each delegate must possess, ensuring a consistent and predictable behavior across various implementations. By doing so, we will improve code maintainability, facilitate easier integration, and enhance the overall reliability of our system. Furthermore, we will provide comprehensive documentation and examples to guide developers in implementing and adhering to this standardized interface.","GenTime":"2024-07-29 02:27:18"}
{"File Name":"ios-architecture-decision-logs\/0006-inject-parameters-asArguments-toPresenter.md","Context":"## Context\\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\\n\n## Decision\n","Decision":"Each module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\\n","tokens":28,"id":4983,"Prediction":"## Decision\nWe have decided to refactor the presenter's initialization process by implementing a parameter object. This approach will group the related parameters into a single object, thus reducing the number of parameters directly passed to the initializer. Here's a step-by-step implementation plan:\n\n1. **Identify Parameter Groups:** Analyze the existing initialization parameters and identify logical groupings.\n  \n2. **Create Parameter Objects:** Define new classes or structs for these parameter groups.\n\n3. **Refactor Initializers:** Modify the presenter's initializer to accept instances of these parameter objects instead of individual parameters.\n\n4. **Update References:** Adjust the codebase to instantiate and pass these parameter objects wherever the presenter is initialized.\n\n5. **Testing:** Thoroughly test the new implementation to ensure that the presenter's behavior remains consistent and correct.\n\n6. **Documentation:** Update the project's documentation to reflect the changes in the initialization process, providing examples for creating and using the parameter objects.\n\nBy consolidating parameters into objects, we improve code readability, maintainability, and scalability.","GenTime":"2024-07-29 02:27:25"}
{"File Name":"webwritertechandhumanity.com\/0002-how-to-implement-special-pages.md","Context":"## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n\n## Decision\n","Decision":"I'll implement solution 1 because solution 2 is complicated and I want to prioritise\\nthe release of a first working website.\\n","tokens":52,"id":5001,"Prediction":"## Decision\nI will implement the privacy policy page and the about me page as standalone, static pages that are not included in the main navigational map or the flow of the spiral scan. These pages will be accessible through direct links in the footer of the application or through a specific button in the header. This way, users can access these pages when they need to, without being forced to interact with them during their regular navigation or scanning activities.","GenTime":"2024-07-29 02:28:24"}
{"File Name":"trade-access-program\/0005-viewflow-for-grant-management-portal.md","Context":"## Context\\nWe need a system to manage the lifecycle of a grant application.\\n\n## Decision\n","Decision":"Viewflow was chosen as a framework to help us achieve this goal while making the development process efficient.\\n### Build vs Buy Paper\\nVarious alternatives to Viewflow were considered when making this decision. Those alternatives are details in the\\nalpha stage of the project here: https:\/\/uktrade.atlassian.net\/l\/c\/zEAEM37j\\n### Viewflow analysis\\nSome initial analysis on how viewflow could be used was also done in alpha: https:\/\/uktrade.atlassian.net\/l\/c\/Lc77C4mq\\n","tokens":17,"id":5009,"Prediction":"## Decision\n\nAdopt the use of a cloud-based Grant Management System (GMS) to manage the lifecycle of grant applications. The decision to use a cloud-based solution is driven by the need for scalability, security, and accessibility. \n\nKey considerations include:\n\n1. **Customization**: The system must allow for customizable workflows to accommodate different types of grants and associated lifecycle stages.\n2. **Integration**: Ensure the solution can integrate seamlessly with existing systems such as financial management, CRM, and document management systems.\n3. **User Accessibility**: The platform should be accessible via web and mobile devices to support remote access needs of applicants and administrators.\n4. **Security**: Focus on data security and compliance with regulations such as GDPR or other relevant data protection laws.\n5. **Reporting and Analytics**: The system must provide robust reporting and analytics capabilities for tracking application progress and overall grant program performance.\n6. **Support and Maintenance**: Opt for a vendor that provides strong customer support and regular updates to the system to address any issues or required enhancements.","GenTime":"2024-07-29 02:28:38"}
{"File Name":"link_platform\/0016-use-devise-for-admin-authentication.md","Context":"## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n\n## Decision\n","Decision":"[Devise](https:\/\/github.com\/plataformatec\/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https:\/\/github.com\/omniauth\/omniauth\/wiki\/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\\n","tokens":37,"id":5024,"Prediction":"## Decision\nWe will implement an authentication and authorization system using OAuth 2.0 to manage Link Platform Administrators. This approach leverages industry standards for security and scalability. Administrators will be required to log in using credentials that may be tied to a central identity provider (e.g., Google, Okta) which simplifies the management of user roles and permissions across different Link Instances. Additionally, this system will provide multi-factor authentication (MFA) to enhance security. The Link Platform will include an administration dashboard where authenticated administrators can manage data and configuration specific to their Link Instances.","GenTime":"2024-07-29 02:29:16"}
{"File Name":"link_platform\/0002-use-the-adr-tools-kit-to-manage-adr-docs.md","Context":"## Context\\nTo help automated the process of managing architectual decision records use a tool that stanadardizes the process.\\nI found a reference to this tool on the thought works techninques review page.  The github repo is:\\n`https:\/\/github.com\/npryce\/adr-tools`\\nBut I installed it using brew:\\n`brew install adr-tools`\\n\n## Decision\n","Decision":"We will use the system adr-tools to manage the ADR documents\\n","tokens":79,"id":5033,"Prediction":"## Decision\n\nWe will adopt the `adr-tools` tool to automate and standardize the management of Architectural Decision Records (ADRs) in our project. This decision is made based on a recommendation found in the ThoughtWorks Technology Radar and the ease of installation and setup using Homebrew.\n\n### Key Points:\n\n1. **Standardization**: `adr-tools` provides a standardized way of creating, maintaining, and referencing architectural decisions, ensuring consistency across our documentation.\n  \n2. **Ease of Use**: The tool simplifies the process of creating new ADRs with standard templates, minimizing manual effort and reducing the likelihood of errors.\n\n3. **Community and Support**: Being recommended by ThoughtWorks and having a dedicated GitHub repository suggests that the tool is well-maintained and likely to have an active user community for support.\n\n4. **Installation via Homebrew**: Installing `adr-tools` via Homebrew is simple and efficient, making it easy for team members to adopt and use the tool without significant setup time.\n\n### Actions:\n\n- **Tool Installation**: Ensure all team members install `adr-tools` using Homebrew.\n    ```sh\n    brew install adr-tools\n    ```\n\n- **Configuration and Training**: Configure the tool according to our project needs and provide a short training session to the team on how to use it.\n\n- **Integration into Workflow**: Integrate `adr-tools` into our existing development workflow and documentation practices to ensure consistent use and adherence to the standardized process.\n\n- **Review and Feedback**: Regularly review the usage and effectiveness of the tool, and gather feedback from the team for continuous improvement.\n\nBy adopting `adr-tools`, we aim to enhance the consistency, transparency, and manageability of our architectural decisions, ultimately contributing to a more organized and comprehensible software development process.","GenTime":"2024-07-29 02:29:42"}
{"File Name":"link_platform\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5035,"Prediction":"## Decision\n\nWe will adopt the Architectural Decision Record (ADR) framework to systematically capture and document the architectural decisions made throughout the project. \n\nThis decision will include:\n\n1. **Template Usage**: Adopting a standardized template for all ADRs to ensure consistency and completeness. The template will include sections such as Context, Decision, Status, Consequences, and Trade-offs.\n  \n2. **Tool Selection**: Utilizing a version control system (e.g., Git) to store and manage ADRs. Each ADR will be committed to the repository, allowing team members to track changes, review historical decisions, and collaborate on new decisions.\n\n3. **Integration**: Integrating ADR documentation into our existing development workflow and Continuous Integration\/Continuous Deployment (CI\/CD) pipeline. This ensures that ADRs are updated in sync with code changes and that architectural documentation remains relevant and current.\n\n4. **Periodic Reviews**: Implementing a periodic review process to revisit and validate the decisions documented in ADRs. This will help us assess whether the decisions are still applicable and beneficial, or if they require adjustments based on new insights or changes in project direction.\n\n5. **Accessibility**: Ensuring that ADRs are easily accessible to all team members. They will be stored in a centralized location with clear indexing and search capabilities.\n\nThis approach will help us maintain a clear, comprehensive, and historical record of our architectural decisions, facilitating better communication, onboarding, and evolution of the system's architecture.","GenTime":"2024-07-29 02:29:50"}
{"File Name":"FlowKit\/0006-JWTs.md","Context":"## Context\\nAuthentication and access control for FlowKit has previously been very limited, making use of only of postgres usernames and passwords. WIth the introduction of the API, we can be much more granular in allowing access through the use of [JSON Web Tokens](https:\/\/jwt.io).\\nThese are an encoded and cryptographically signed string, which permit access to some set of API functionality. The string specifies the identity of the user, exactly what they can access, and the time period for which the token is valid.\\nTampering with the token will result in the signature not matching, an event which can be logged an subsequently investigated.\\nA key advantage of JWTs is that they can be centrally managed, even where the service they will be used with is not accessible to the internet. Another significant advantage is that the tokens are inherently ephemeral - should a token be stolen, the time window for it to lead to a data breach will (given appropriate usage of the system) be small.\\nJWTs are also advantageous in that the add relatively minimal overhead to the day to day experience of analysts, and have broad support across many languages.\\nBut perhaps the most important advantage is the granularity of access they afford - for example it is possible to allow access to only one specific result with this method. This is useful where developing dashboards and the like, or where access to outputs but not data must be provided to other parties.\\nOther alternatives would be to use a more traditional login system, backed by a database, integrate with the Docker host's authentication system, or to use public key based authentication. All of these require more and local administration, or necessitate the use of either a JWT equivalent to actually provide API access, or the use of cookies to avoid a need to log in for every communication with the API.\\n\n## Decision\n","Decision":"API authentication and access control will use JWTs.\\n","tokens":369,"id":5057,"Prediction":"## Decision\n\nWe will implement JSON Web Tokens (JWT) for authentication and access control in FlowKit. This decision is based on several key advantages that JWTs offer:\n\n1. **Granular Access Control**: JWTs allow for detailed specification of user permissions, enabling precise control over access to specific APIs and resources. This granularity is particularly useful for providing limited access to specific results, developing dashboards, and sharing outputs with external parties without exposing underlying data.\n\n2. **Centralized Management**: JWTs can be centrally managed, even when the service they are used with is not accessible to the internet. This simplifies the administration of authentication and access control.\n\n3. **Ephemeral Nature**: JWTs are inherently time-limited, reducing the risk associated with token theft. The time window for potential misuse of a stolen token is minimized, enhancing security.\n\n4. **Minimal Overhead for Users**: JWTs add minimal overhead to users\u2019 daily activities, offering a smoother experience compared to more cumbersome login systems. Analysts can work more efficiently without frequent login interruptions.\n\n5. **Broad Support**: JWTs have wide support across many programming languages and platforms, ensuring compatibility and ease of integration with various components of FlowKit and potential external systems.\n\n6. **Security**: JWTs are cryptographically signed, ensuring integrity and authenticity. Any tampering with the token can be detected, logged, and investigated, providing an additional layer of security.\n\nGiven these benefits, JWTs provide an optimal balance of security, ease of use, and administration, making them the preferred choice for implementing authentication and access control in FlowKit. Alternative methods such as traditional login systems, Docker host integration, or public key-based authentication require more local administration, introduce additional complexities, and do not offer the same level of granularity and ease of use as JWTs.","GenTime":"2024-07-29 02:31:01"}
{"File Name":"FlowKit\/0009-asciidoctor-pdf-for-notebook-conversion.md","Context":"## Context\\nWe want AutoFlow to support creation of PDF reports from Jupyter notebooks. [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/) provides a method for converting Jupyter notebooks to PDF format. However, PDF conversion with nbconvert requires a full LaTeX installation, which is ~4GB in size. To keep the size of the AutoFlow container smaller, it is desirable to find an alternative that doesn't require a LaTeX installation.\\n[Asciidoctor PDF](https:\/\/asciidoctor.org\/docs\/asciidoctor-pdf\/) is a tool for converting ASCIIDoc documents to PDF format without generating an interim format such as LaTeX. Since nbconvert can convert notebooks to ASCIIDoc format, we can use Asciidoctor PDF as the second half of a two-step process to convert Jupyter notebooks to PDF via ASCIIDoc.\\n\n## Decision\n","Decision":"We will use a two-step process to convert Jupyter notebooks to PDF reports in AutoFlow: convert the notebook to ASCIIDoc format using nbconvert, and then convert the resulting ASCIIDoc document to PDF using Asciidoc PDF.\\n","tokens":178,"id":5059,"Prediction":"## Decision\nGiven the requirement to keep the AutoFlow container size smaller and avoid the necessity of a full LaTeX installation, we will adopt a two-step conversion process leveraging [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/) and [Asciidoctor PDF](https:\/\/asciidoctor.org\/docs\/asciidoctor-pdf\/).\n\n1. **Two-Step Conversion Process**:\n   - **Step 1**: Use nbconvert to convert Jupyter notebooks into ASCIIDoc format.\n   - **Step 2**: Use Asciidoctor PDF to convert the ASCIIDoc documents to PDF format.\n\n2. **Dependency Management**:\n   - Remove the need for LaTeX by not using nbconvert's direct PDF conversion capabilities.\n   - Include Asciidoctor PDF in the AutoFlow container to handle the ASCIIDoc to PDF conversion step, which significantly reduces the dependency overhead compared to LaTeX.\n\n3. **Implementation**:\n   - Integrate nbconvert and Asciidoctor PDF into the pipeline, ensuring seamless conversion between Jupyter notebooks and PDF reports.\n   - Ensure thorough testing for the conversion flow to handle potential edge cases in document structure and formatting between Jupyter, ASCIIDoc, and PDF.\n\n4. **Documentation and User Guide**:\n   - Update AutoFlow documentation to inform users about the new method for PDF conversion.\n   - Provide examples and guides on using the two-step conversion process.\n\nBy leveraging these tools, we can achieve the desired functionality in creating PDF reports from Jupyter notebooks without incurring the significant storage costs associated with a LaTeX installation.","GenTime":"2024-07-29 02:31:13"}
{"File Name":"google-cloud-cpp-common\/2019-03-26-binary-distributions.md","Context":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n\n## Decision\n","Decision":"will not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","tokens":273,"id":5061,"Prediction":"## Decision\nWe will focus on source-based distributions and ensure that our code is modular and easily adaptable for use in various binary packaging systems. While we won't directly provide or maintain binary packages, we will prioritize the simplicity of integrating with popular C++ package managers (e.g., dpkg, rpm, vcpkg). Our goal will be to support package maintainers by documenting our build processes comprehensively and accommodating minor changes that enhance packageability.\n\nSpecifically:\n1. We will enhance our documentation to include detailed instructions on building from source, including dependencies and steps required on different platforms.\n2. We will welcome contributions and feedback from the community regarding packaging scripts and modifications necessary for various binary package managers.\n3. Any reasonable tweaks required for compatibility with binary packaging systems will be considered and integrated into our source base, provided they do not compromise the integrity or performance of our software.\n4. We will track and address issues related to packaging as part of our regular issue management system, ensuring that our software can be efficiently packaged and used by third parties.\n\nBy adopting this approach, we aim to provide flexibility and support to the community without the overhead of maintaining diverse packaging formats ourselves. This ensures that users have multiple avenues to access our software while maintaining a high standard of source integrity.","GenTime":"2024-07-29 02:31:21"}
{"File Name":"phpadr\/0005-phpunit-as-testing-framework.md","Context":"## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n\n## Decision\n","Decision":"It will be used the [PHPUnit](https:\/\/phpunit.de\/) as testing framework.\\n","tokens":18,"id":5070,"Prediction":"## Decision\nAdopt a combination of static code analysis tools, a comprehensive suite of automated tests, and a continuous integration\/continuous delivery (CI\/CD) pipeline.\n\n1. **Static Code Analysis**:\n   - Integrate tools like SonarQube or ESLint into the development workflow to automatically check for code quality issues, security vulnerabilities, and adherence to coding standards.\n\n2. **Automated Testing**:\n   - Develop a thorough suite of automated tests, including unit tests, integration tests, and end-to-end tests. Utilise frameworks like JUnit, pytest, or Selenium depending on the language and requirements.\n   - Enforce test coverage metrics to ensure that critical parts of the codebase are tested before any changes are merged.\n\n3. **CI\/CD Pipeline**:\n   - Set up a CI\/CD tool, such as Jenkins, GitHub Actions, or GitLab CI, to automate the process of testing and integrating code changes. This setup will facilitate the early detection of integration issues and ensure that the codebase remains stable.\n   - Automate the deployment process to facilitate smooth and reliable delivery of changes to production.\n\n4. **Peer Code Reviews**:\n   - Implement a formal peer code review process using tools like GitHub Pull Requests or Bitbucket. Code reviews will help catch issues that static analysis or automated tests might miss and foster better collaboration and knowledge sharing within the team.\n\n5. **Code Documentation and Comments**:\n   - Encourage developers to write meaningful comments and maintain up-to-date documentation that accurately describes the functionality and rationale behind code changes. This will help new team members understand the codebase and make it easier to identify and correct errors.\n\nBy embracing these practices, we aim to maintain a high level of code quality, ease the process of making changes, integrating new components, and correcting errors, ultimately resulting in a more robust and maintainable software product.","GenTime":"2024-07-29 02:31:49"}
{"File Name":"galaxy-zoo-touch-table\/adr-2.md","Context":"### Context\\nThe touch table app should support multi touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\\n### Decision\\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n\n## Decision\n","Decision":"Although considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","tokens":409,"id":5076,"Prediction":"### Decision\nWe have decided to build the touch table app using Windows Presentation Foundation (WPF) with C#. This decision is based on several factors:\n\n1. **Learning Curve and Language Familiarity**: Most of our team is familiar with JavaScript, and transitioning to C# is less jarring than transitioning to C++. Additionally, having previous experience with WPF through the initial work on the table, and guidance from Florian Block, makes this a practical choice.\n\n2. **Documentation and Community Support**: C# has strong documentation and a vibrant .NET community, which will be invaluable when encountering development obstacles. \n\n3. **Existing Knowledge and Resources**: We have the Panoptes .NET client available for future C# projects, which aligns with our choice of using a .NET framework.\n\nAlthough WPF is losing popularity and some of its frameworks and packages are being deprecated, it still fits the current needs and demands of this project. For future projects, exploring newer technologies will be essential to ensure we stay updated and maintain the project's longevity.","GenTime":"2024-07-29 02:32:06"}
{"File Name":"galaxy-zoo-touch-table\/adr-3.md","Context":"### Context\\nWe should structure files in a way that makes the codebase easy to navigate and intuitive when searching for certain components. Organization should adhere to accepted practices in the .NET community.\\n### Decision\\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n\n## Decision\n","Decision":"The MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","tokens":280,"id":5082,"Prediction":"### Decision\nAdopt the MVVM (Model-View-View Model) architecture for structuring the core components of the application. \n\nApplication structure will include:\n- **Model**: This folder will hold all the data models and business logic of the application.\n- **View**: This folder will contain the UI elements, XAML files, and corresponding code-behind files.\n- **ViewModel**: This folder will have the view models which handle the presentation logic and data-binding for the views.\n\nAdditionally, to address organization for other items such as libraries, images, fonts, etc.:\n- **Lib**: A dedicated folder for third-party libraries and custom controls.\n- **Assets**: A separate folder to store static resources like images, icons, and fonts.\n- **Services**: If applicable, this folder will handle any service classes such as data access or API service classes.\n- **Utilities**: Common utility functions, helpers, and extensions will go here.\n\n### Status\nAccepted\n\n### Consequences\n- **Pros**\n  - Leveraging a widely recognized and utilized design pattern in the .NET community will make it easier to find resources, troubleshoot issues, and onboard new developers.\n  - Clear separation of concerns enhances maintainability, readability, and testability of the application.\n  \n- **Cons**\n  - Strict adherence to MVVM may lead to over-abstraction in some cases, particularly for simpler views where a view model and model may not be necessary.\n  - Initial setup and learning curve for developers unfamiliar with the MVVM pattern.\n\n### In Retrospect\nWhile MVVM has provided a strong architectural foundation ensuring a well-organized codebase, the necessity to maintain a model and view model for every view has sometimes introduced complexity without clear benefits. Flexibility should be allowed where simpler views can skip the model if no significant business logic or data transformation is required. Clear guidelines and examples will be documented to ensure consistency while allowing practical deviations from the strict MVVM pattern for simple views or components.","GenTime":"2024-07-29 02:32:33"}
{"File Name":"my-notes\/prettier.md","Context":"### Context\\nPrettier integrates with well with vscode but conflicts with Standard\\n### Decision\\nReplace Standard with Prettier\\n\n## Decision\n","Decision":"Replace Standard with Prettier\\n","tokens":26,"id":5087,"Prediction":"## Decision\nSwitch from using Standard to Prettier as the primary code formatter and linter within the project. This decision is made to leverage Prettier's seamless integration with Visual Studio Code (vscode), which ensures a smoother and more consistent developer experience.","GenTime":"2024-07-29 02:32:41"}
{"File Name":"my-notes\/styled-components.md","Context":"### Context\\n- easier to maintain than traditional CSS\\n- I was not able to get my tests working with it properly\\n### Decision\\nDon't start using Styled Components\\n\n## Decision\n","Decision":"Don't start using Styled Components\\n","tokens":35,"id":5090,"Prediction":"Based on the provided context, the decision is:\n\n### Decision\nAdopt an alternative solution to Styled Components for styling in order to ensure that testing remains functional while still achieving maintainability benefits over traditional CSS. Explore other CSS-in-JS libraries or CSS methodologies like CSS Modules, Emotion, or Tailwind CSS to find a balance between easier maintenance and reliable testing.","GenTime":"2024-07-29 02:32:45"}
{"File Name":"form-builder\/0006-reduce-number-of-emails-per-submission.md","Context":"## Context\\nForm Builder sends one email per file uploaded for each application. For those forms that allow multiple files to be uploaded this generates a lot of email noise for both form owners and the applicants. This can result in additional work to sift these emails or even create unforced errors in workflows that are manual in nature.\\nThe use of AWS SES as the email service means that there is a 10MB upper limit on the size of each email that is sent.\\nOptions discussed:\\n1. Attach as many files as possible to a single email up to the 10MB limit.\\n2. Hard cap limit on 10MB in total accross _all_ the files uploaded per form.\\n3. Admin interface for form owners to log in and retrieve file uploads per submission.\\n4. Email a link to the form owner allowing the ability to download all attachments in one go.\\n\n## Decision\n","Decision":"We feel that it is reasonable to reduce the number of emails that recipients of each submission get to the absolute minimum.\\nOption 1 is the one which is the most reasonable solution at this time. It requires the least amount of code change and does not change the architecture or increase the threat surface of Form Builder.\\n","tokens":181,"id":5099,"Prediction":"## Decision\nWe have decided to implement option 4: Email a link to the form owner allowing the ability to download all attachments in one go.\n\n### Rationale\n1. **Reduced Email Noise**: By bundling all attachments into a single download link, we significantly reduce the volume of emails sent, thereby alleviating email noise for both form owners and applicants.\n2. **User Convenience**: Form owners can access all uploaded files in one action, streamlining the process of retrieving and managing submissions.\n3. **Scalability**: This approach can easily adapt to varying sizes and quantities of files uploaded without concern for the AWS SES 10MB email size limit.\n4. **Enhanced User Experience**: Applicants will not be inundated with multiple emails, leading to a smoother user experience.\n5. **Security and Manageability**: Links can be secured with token-based authentication and set to expire after a certain period, ensuring secure access and better manageability.\n\nThis option balances usability, scalability, and security, making it the optimal choice for our form submission workflow.","GenTime":"2024-07-29 02:33:10"}
{"File Name":"form-builder\/0010-testing-the-editor-pipeline.md","Context":"## Context\\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\\n\n## Decision\n","Decision":"![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\\n","tokens":87,"id":5102,"Prediction":"## Decision\n\nTo address the bottleneck caused by having only one Editor in test, which blocks the pipeline for deploying to live production, the following decisions are made:\n\n1. **Deploy Multiple Environments**:\n   - **Development Environment**: Each developer will have a personal development environment for feature testing, which reduces the dependency on a shared Editor.\n   - **Staging Environment**: A dedicated staging environment will be set up specifically for integration testing, where multiple features can be tested collectively before production deployment.\n   - **Production Environment**: The final gate, where approved features will be deployed live.\n\n2. **Parallel Test Execution**:\n   - Implement parallel test execution to allow concurrent testing of different features. This can be facilitated by containerization or using cloud-based test environments that can be spun up and discarded as needed.\n\n3. **Automate Testing and Deployment**:\n   - Automate as many manual testing steps as possible to reduce the time the Editor spends in the testing phase. This includes unit tests, integration tests, and user acceptance tests (UAT).\n\n4. **Feature Toggles**:\n   - Use feature toggles to deploy code to production but keep it disabled until it is fully tested. This allows the deployment process to continue without waiting for the final approval of a feature.\n\n5. **Implement Blue-Green Deployment Strategy**:\n   - Use a Blue-Green Deployment strategy so that the pipeline is not blocked by a single environment. This ensures there is always an available environment for release while the other one can be used for testing or rolling back if necessary.\n\n6. **Continuous Monitoring and Feedback Loop**:\n   - Integrate continuous monitoring to get real-time feedback and quickly identify and resolve any issues in the testing or production environment.\n\nBy adopting these decisions, the pipeline becomes more efficient, reducing bottlenecks and ensuring that the release process is smooth and continuous.","GenTime":"2024-07-29 02:33:26"}
{"File Name":"dp\/0009.md","Context":"## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nWhen storing both datasets and code-lists in a graph database, there is some\\noverlap between the information stored on a code and on a dimension option. A\\ndesign decision was needed as to whether the dimension option and code nodes\\nin the graph database should be collapsed into a single node, with the relevant\\nrelationships for each coming off that single node.\\n\n## Decision\n","Decision":"Dimension options and codes should not be collapsed into a single node. Dimension\\noptions have relationships to every observation within an imported dataset, and\\nadding those relationships to the code node would result in relationships to\\nevery observation across all datasets - a significant and unnecessary burden.\\n","tokens":126,"id":5118,"Prediction":"## Decision\n\nWe decided to **merge dimension option nodes and code nodes into a single unified node in the graph database**.\n\n### Rationale:\n1. **Avoid Redundancy**: By merging the nodes, we eliminate redundant information that would otherwise need to be duplicated across separate nodes for dimension options and codes. This reduces storage space and potential inconsistencies.\n  \n2. **Consistency and Reuse**: Since code definitions apply to all scenarios across multiple datasets, it makes sense to have a single source of truth. This ensures consistency across the data and prevents discrepancies between duplicate nodes.\n\n3. **Simplified Queries and Relationships**: With a single node, queries regarding dimensions and code definitions are simplified. We can manage relationships more easily without having to traverse multiple nodes, making the database interactions more efficient.\n\n4. **Maintainability**: The unified approach eases the maintenance of the graph database schema. There will be fewer node types to manage, and updates to metadata or definitions will only need to be done in one place.\n\n5. **Flexibility for Future Enhancements**: This structure allows for more straightforward scaling and integration with any future features or datasets that may need to interact with these nodes.\n\nIn summary, merging dimension option and code nodes into a single node in the graph database offers various advantages in terms of redundancy, consistency, simplicity, maintenance, and future-proofing the data structure.","GenTime":"2024-07-29 02:34:21"}
{"File Name":"ikforth\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5120,"Prediction":"## Decision\nWe will implement and maintain a series of Architectural Decision Records (ADRs) for this project. These ADRs will document each significant architectural decision, along with the context and rationale behind it.\n\n### Details:\n1. **Format:** Each ADR will follow a standardized template to ensure consistency and completeness.\n2. **Storage:** ADRs will be stored in a dedicated directory within the project's main repository (`\/docs\/adr\/`).\n3. **Numbering:** Each ADR will be sequentially numbered for easy reference (e.g., `ADR-001`, `ADR-002`, etc.).\n4. **Approval Process:** New ADRs will be proposed via pull requests, allowing team review and agreement before becoming part of the documentation.\n5. **Updates:** Any changes to existing decisions will be documented in new ADRs that reference the superseded ones.\n6. **Visibility:** ADRs will be easily accessible to all team members to ensure everyone is informed of the architectural direction and past decisions.\n\nThis approach will help ensure that architectural knowledge is captured, shared, and maintained throughout the lifecycle of the project, facilitating informed decision-making and long-term project sustainability.","GenTime":"2024-07-29 02:34:30"}
{"File Name":"ontrack\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5122,"Prediction":"## Decision\n\nThe architectural decisions for this project will be documented using Architectural Decision Records (ADRs). This approach ensures a clear and organized record of the choices made throughout the project's lifecycle. Each ADR will include sections such as Context, Decision, Consequences, and Alternatives Considered to provide comprehensive documentation. This method will aid in maintaining a consistent and transparent decision-making process, which can be referenced by current and future team members. The ADRs will be stored in a dedicated directory within the project's repository to ensure easy access and version control.","GenTime":"2024-07-29 02:34:39"}
{"File Name":"SearchServices\/0007-message-driven-content-tracker.md","Context":"## Context\\nThe ability to search on content requires a content extraction process. This relies on repo getting the document, passing it to one or multiple transformers, and finally returning the plain text content. This process does not scale as the embedded transformation is unable to cope with large volumes or large documents. Embedded transformations in general come with multiple problems, security related and scaling, which led to the introduction of the transformation service with 6.1\\nSince transformations to text for content indexing makes up a major portion of the transformation workload, it has always been intended to move these transformations to the new transformation service as well.\\nThe following are the suggested approaches to indexing with Transform Service:\\n* Refactor the current V0 API (in use by Search Services) to make use of RenditionService2.\\n* Introduce a new microservice that sits between Solr and the transformation service. The content is off loaded to the transformation service asynchronously while providing the same synchronous API for Search Services.\\n* Search Service to use the get rendition V1 Public API.\\n* New content tracker that communicates with the repository asynchronously by messages.\\n\n## Decision\n","Decision":"Based on the group discussion and design reviews, we have agreed to go with the asynchronous content tracker.\\nIn this design the Search Services will place a request in the message queue for the Repo to consume.\\nThe message will contain the NodeId and Solr identifier (name of the instance or Solr shard).\\nOnce the message is consumed by Repo it will start the process to obtain the text for the content.\\nWhen the content is ready a response message will be placed in the queue for Search Services to consume.\\nThe new content tracker will monitor the response queue and consume incoming messages. The message we expect to see in the queue will consist of an identifier, status and a URL. The status of the event can be used for handling errors. The handling of such errors prompting an abort or retry will be finalised during user story creation.\\nOn a successful completion the new content tracker will use the URL to obtain the content and retrieve the text for indexing.\\nWe use a URL in the response message rather than an identifier so that the repository can choose where to store the intermediate content at its own discretion. This will also provide the ability to leverage direct access URLs to cloud storage in the future (e.g. S3 signed URLs).\\nThe benefits of this solution gives ability to index content asynchronously. Unlike the current way which is based on a synchronous call to Repo using HTTP. This solution allows Alfresco to scale the transformation and adds the ability to index more content.\\n![Component Diagram](\/search-services\/alfresco-search\/doc\/architecture\/decisions\/diagrams\/AsyncContentTrackerComponentDiagram.png)\\nThe other options have been considered but did not full fill the requirements.\\nRefactor the current V0 API (in use by Search Services) to make use of RenditionService2:\\nThe thread in the repository will still be blocked. Although the new transform service has a higher throughput, it can have a slightly longer delay. This blocks HTTP threads even longer, or they could even time out.  Using async HTTP introduced with servlet 3.0 has been considered, but this would need to be implemented throughout the entire webscript framework.\\nUsing V1 API requires an authentication for SearchServices, which needs to be configured. There is currently no way for a system to call the V1 API without creating a new user. Creating a new user to represent the system is not the correct way to integrate systems and services. In addition, the V1 API uses the renditions for text which covered below.\\nUsing renditions for text extraction:\\nRenditions are stored long term in the repository as nodes. Using this mechanism for ephemeral text extractions would require ta new retention mechanism for renditions. All of this would put additional workload on the node tree, which defeats the design goal of handling high workloads.\\nIntroduce a new microservice:\\nThis has only been considered as an interim solution if it is not possible to change the content tracker in Search Services. It is essentially the solution above (blocking the sync request from Search Services until transformation is ready) but moved to its own micro service. This solution is slightly better as it does not block repository threads but suffers from the same problems.\\nCreating a microservice for the content tracker adds complexity which adds little benefit as part of this delivery. There are benefits for making the trackers into a microservice, but this would require a significant amount of work. The deployment of Search Services will become complicated as we would have to deploy SearchServices and a micorservice for the new ContentTracker.\\nThe current ContentTracker will remain so that the Community version continues to work with SearchServices.\\nThe V0 content tracking webscript and the ContentTracker will be removed with the next major upgrade in favour of the message base API. At this stage both Enterprise and Community will make use of the message base API. Please note that the communication via message queues between Repo and Search Service, will be configured by default in the ACS (reference) deployment templates. As for the Alfresco 6.x and SearchServices 1.x distribution zip files, will default to the legacy configuration. This would provide the customer different options to upgrade, as we have an overlap between versions.\\nAdding a new content tracker based on a message queue is the preferred solution as this provides the benefits of using the Transform Service.\\n","tokens":227,"id":5123,"Prediction":"## Decision\nWe have decided to introduce a new microservice that sits between Solr and the transformation service. This microservice will manage the offloading of content to the transformation service asynchronously while providing a synchronous API for Search Services. \n\nThis decision is made based on the following considerations:\n\n1. **Scalability:** By decoupling the transformation process from the synchronous flow of the existing search services, we can better handle large volumes and large documents without impacting the performance of search operations.\n\n2. **Security:** Offloading the transformation to a dedicated microservice helps to isolate and manage security concerns specific to content transformations, reducing the attack surface area of the core services.\n\n3. **Maintainability:** Having a separate microservice simplifies the overall architecture, making it easier to manage, update, and scale the transformation service independently from the search services.\n\n4. **Flexibility:** This approach provides a clean separation of concerns, allowing the transformation logic to evolve or change independently from the search services. It also opens up possibilities to add more transformation capabilities in the future without major changes to the existing services.\n\nWe will proceed with the design and implementation of this new microservice, ensuring it integrates seamlessly with the existing Solr and transformation services, while meeting the synchronous API requirements of the Search Services.","GenTime":"2024-07-29 02:34:42"}
{"File Name":"profiles-db-elastic\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5139,"Prediction":"## Decision\n\nWe will implement a structured approach to document our Architectural Decision Records (ADRs) to ensure clear, consistent, and comprehensive recording of all significant architectural decisions made during the project lifecycle. \n\n### Key Elements:\n1. **Template Usage**: We will use a standardized ADR template that includes sections for context, decision, status, consequences, and rationale. This will help in maintaining uniform documentation across the project.\n\n2. **Version Control**: All ADRs will be stored in a version-controlled repository such as GitHub. Each ADR will be a markdown file, ensuring easy tracking of changes and contributions over time.\n\n3. **Integration with Workflows**: ADRs will be created and updated as part of our regular development workflow. For any significant architectural decision, the responsible team member will be required to document the decision before proceeding with implementation.\n\n4. **Review Process**: Each ADR will undergo a peer-review process to ensure that decisions are well-considered and agreed upon by the relevant stakeholders. This will involve a review meeting or asynchronous comments on the ADR document.\n\n5. **Cross-Referencing**: ADRs will reference related decisions and include links to relevant requirements, user stories, or technical documentation to provide full context and traceability.\n\n6. **Accessibility**: ADRs will be easily accessible to all team members. We will maintain a navigable index in the version-controlled repository, listing all ADRs and their statuses.\n\n7. **Tagging and Categorization**: Decisions will be tagged and categorized for easy retrieval. Common categories might include \"Database\", \"APIs\", \"Security\", and \"Deployment\".\n\nAdopting this structured approach will ensure transparency, facilitate informed decision-making, and provide a valuable historical record of why certain architectural decisions were made, supporting future project maintenance and development.","GenTime":"2024-07-31 11:14:24"}
{"File Name":"markmap\/splitting-and-bundling.md","Context":"## Context\\ncoc-markmap \/ markmap-cli has an incredibly large size in `node_modules`. (~30MB)\\n### Redundant dependencies\\nIn markmap-lib@0.9.x transforming and rendering share the same package, but they have quite different dependencies and are used in quite different scenarios. In other words many of the dependencies are useless most of the time.\\nFor example, transforming usually occurs locally when creating a markmap, while rendering happens in the browser when the markmap is opened. So if we are trying to create a markmap, it's likely that we don't need d3 and its friends. If we are exploring a markmap it's likely that we don't need all those transforming tools like remarkable and katex.\\n### Extraneous bundles\\nSome packages build multiple bundles to support different types of import. For example, `index.js` for `cjs`, and `index.min.js` for minimized `cjs` bundle, and several more. As a result, each of the bundle contains a copy of code, leading to a huge size.\\nAfter bundling into a single package, we get a single copy of the code, and optionally minimized.\\n\n## Decision\n","Decision":"- Split rendering code from `markmap-lib` into a new package `markmap-view`.\\nBy splitting the packages we can manage dependencies more independently, and get rid of unused ones.\\nIn most cases, `markmap-view` can be used as a prebuilt bundle, even without installation by leveraging CDNs.\\n- Use rollup to bundle code in `markmap-cli`.\\nUnder the hood, `coc-markmap` depends on `markmap-cli`, and `markmap-cli` depends on `markmap-lib`.\\n`markmap-lib` is supposed to be used in applications with bundlers, so it's better to keep simple and externalize the dependencies.\\n`markmap-cli` is a standalone command-line tool. It should be installed without dependency bloat. So it is a good idea to bundle everything in it and get rid of additional dependency packages.\\n`coc-markmap` is a thin layer on top of `markmap-cli` so we can just leave it as is.\\n","tokens":245,"id":5140,"Prediction":"## Decision\n\nWe will take the following steps to address the issue of the large `node_modules` size for `coc-markmap \/ markmap-cli`:\n\n1. **Split the markmap-lib package**:\n    - Refactor `markmap-lib` to split the library into two separate packages: `markmap-transform` and `markmap-render`.\n    - `markmap-transform` will handle the transformation operations and include only the necessary dependencies for these operations (e.g., remarkable, katex).\n    - `markmap-render` will handle the rendering in the browser and include necessary dependencies for rendering (e.g., d3 and its related libraries).\n\n2. **Optimize dependencies**:\n    - Identify and remove any redundant dependencies in the split packages.\n    - Ensure that each package only includes the dependencies required for its specific role to minimize package size.\n\n3. **Improved distribution\/bundling**:\n    - Configure each package to produce a single optimized bundle (e.g., using a tool like Rollup or Webpack).\n    - Provide different format bundles (cjs, esm, minified versions) in a way that avoids duplicating code within the package.\n\n4. **Lazy Loading**:\n    - Implement lazy loading where feasible, especially for the rendering package in the browser to load dependencies only when needed.\n\n5. **Documentation update**:\n    - Update the documentation to reflect the changes in package usage.\n    - Provide clear instructions for users on how to install and use the split packages based on their specific needs (transforming or rendering).\n\nBy splitting the library into more focused packages (`markmap-transform` and `markmap-render`) and optimizing dependency inclusion and bundling, we will significantly reduce the overall size of `node_modules` while maintaining functionality and improving maintainability.","GenTime":"2024-07-31 11:14:30"}
{"File Name":"adr-generation-tool\/rate_limit_adr_0001.md","Context":"## Context\\nContext for rate limiting pattern\\n\n## Decision\n","Decision":"Prosa about the decision in the ADD\\n","tokens":9,"id":5142,"Prediction":"## Decision\nImplement a Token Bucket rate limiting pattern. This pattern is chosen for its ability to handle burst traffic efficiently while ensuring a steady flow rate over time. The Token Bucket pattern allows for a balance between flexibility in handling occasional spikes in request rate and enforcing a consistent rate limit to protect our backend services from being overwhelmed.\n\n### Justification:\n1. **Burst Handling**: Token Bucket allows a certain burst capacity, making it possible for the system to handle sudden spikes in traffic without immediately rejecting requests.\n2. **Ease of Implementation**: The algorithm is relatively straightforward to implement and understand.\n3. **Flexibility**: It provides fine-grained control over the rate limiting parameters, allowing adjustments to both the refill rate and burst size.\n4. **Performance**: The computational overhead is low, ensuring that the rate limiting process does not become a bottleneck itself.\n\n### Details:\n- **Bucket Size**: This determines the maximum burst of requests that can be handled in one go. We'll start with a size that allows for 2x the average request rate.\n- **Token Refill Rate**: This establishes the steady rate at which requests are accepted. Set initially to our average expected request rate per second.\n- **Depletion and Refill**: Each incoming request consumes a token. Tokens are refilled at a constant rate, ensuring that over time the rate of request handling does not exceed a predefined limit.\n\n### Future Considerations:\n- **Adjustment and Tuning**: Regular monitoring and potential adjustment of the bucket size and refill rate based on observed traffic patterns and performance metrics.\n- **Distributed Systems**: If the system scales out to multiple instances or microservices, consider using a distributed token bucket algorithm to maintain a consistent rate limit across all instances.","GenTime":"2024-07-31 11:14:43"}
{"File Name":"commcare-cloud\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5146,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) process to document the significant architectural decisions made throughout the project. This will ensure that decisions are consistently recorded, easily accessible, and appropriately contextualized to facilitate better understanding and future reference. The ADR process will involve creating a structured template for documenting each decision, encompassing the context, the decision itself, the rationale behind it, any alternatives considered, and the consequences of the decision. This will help maintain a comprehensive history of architectural choices, enhance communication among team members, and support informed decision-making as the project evolves.","GenTime":"2024-07-31 11:14:59"}
{"File Name":"skypy\/adr-01.md","Context":"## Context\\nWithin SkyPy all functions used to create a \"simulation\" will in practice be taking in some values (either parameters or columns from a table) and creating new column(s) in an output table *or* selecting specific rows from an input table.\\nThe inputs and outputs of these functions are clearly defined so a directed acyclic graph (DAG) can be constructed to determine what order the functions should be run in.\\nTo aid in the creation of the tables and the DAG a helper class or decorator should be used so the person writing the function does not have to worry about the implementation details. This class or decorator is what we are currently referring to as the `Model`.\\nFor clarity in the options below we will assume the following example function:\\n```python\\ndef redshift_gamma(shape, scale, size):\\n\"\"\"Gamma-distributed redshifts (Smail et al. 1994).\\nSample `size` redshifts from a gamma distribution with the\\ngiven `shape` and `scale` parameters. See `numpy.random.gamma`.\\n\"\"\"\\n# redshift distribution\\nredshift = np.random.gamma(shape=shape, scale=scale, size=size)\\nreturn redshift\\n```\\n## Decision Drivers\\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\n\n## Decision\n","Decision":"- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 3 has been picked.  This will be easiest for developers to write new functions and write clean unit tests.  Within the example given above `pyungo` was just used as an example, other DAG frameworks exist and picking one should be the topic of a different ADR.\\n","tokens":301,"id":5147,"Prediction":"## Decision\n\nWe have decided to implement a decorator-based approach for the `Model` to streamline the creation of simulation functions and their integration into the DAG system.\n\n### Justification\n1. **Ease of Use**: Using a decorator reduces the amount of boilerplate code needed to define a model. Authors of simulation functions can focus on the core logic without being concerned with the implementation details of the `Model`. Here\u2019s an example of how the `redshift_gamma` function can be decorated to become a model:\n    ```python\n    @model(input_parameters=['shape', 'scale', 'size'], output_column='redshift')\n    def redshift_gamma(shape, scale, size):\n        \"\"\"Gamma-distributed redshifts (Smail et al. 1994).\n        Sample `size` redshifts from a gamma distribution with the\n        given `shape` and `scale` parameters. See `numpy.random.gamma`.\n        \"\"\"\n        # redshift distribution\n        redshift = np.random.gamma(shape=shape, scale=scale, size=size)\n        return redshift\n    ```\n   This approach drastically reduces boilerplate and makes the function definition cleaner and more intuitive.\n\n2. **Clarity of Implementation**: The decorator encapsulates the logic for data retrieval, insertion, and DAG construction. This abstraction helps keep the core model function simple and focused on its primary task. The decorator can be defined outside the functional logic and be reused to maintain consistency across various models.\n   \n3. **Debugging and Maintenance**: Since decorators are a Python feature, developers are generally familiar with their behavior and usage. This familiarity can ease the process of debugging and maintaining the base `Model` compared to a more complex class-based approach.\n\n4. **Modularity and Extensibility**: The decorator approach allows easy extension as additional functionalities (e.g., logging, validation) can be integrated into the decorator without modifying the core simulation function logic. This modular approach ensures that additional requirements in the future can be accommodated with minimal intrusion.\n\n### Example Decorator Implementation\nHere is a simple implementation of the proposed decorator:\n```python\ndef model(input_parameters, output_column):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Extract input parameters from args or kwargs, if needed\n            # Perform any necessary pre-processing here\n            \n            # Generate the output using the decorated function\n            result = func(*args, **kwargs)\n            \n            # Insert result into the appropriate table under output_column\n            # Implement the logic to add the result to the DAG\n            \n            return result\n        return wrapper\n    return decorator\n```\n\n### Conclusion\nBy adopting the decorator approach, we achieve simplicity in writing simulation functions, clarity in the overall architecture, and robustness in maintaining and extending the system. This decision aligns with our primary drivers: ease of use and clarity of implementation.","GenTime":"2024-07-31 11:15:06"}
{"File Name":"skypy\/adr-02.md","Context":"## Context\\nWe need to decide on a unit convention as to whether units include the factor \/h or not (for instance Mpc or Mpc\/h as a unit of distance). For further discussion see e.g. 10.1017\/pasa.2013.31\\n## Decision Drivers\\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\n\n## Decision\n","Decision":"- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/issues\/23) and offline, Mpc has been chosen to ensure the closest integration and least surprise for astropy.\\n","tokens":112,"id":5148,"Prediction":"## Decision\nWe will adopt Mpc\/h as the unit of distance in our software.\n\n## Rationale\n- **Flexibility**: Using Mpc\/h allows flexibility in calculations and results propagation across different assumed values of the Hubble constant (h), which is crucial given that the exact value of h is still under debate.\n- **Scientific Context**: The use of Mpc\/h is widely accepted in cosmology literature and enables easier collaboration and comparison of results with other studies that may use different values of h.\n- **Reduction of Errors**: This convention minimizes potential errors when converting between different unit systems. With Mpc\/h, we won't need to retroactively adjust our results if the accepted value of h changes.\n\n## Implementation\n- Documentation and tutorials will clearly explain the choice of Mpc\/h and its implications.\n- Unit conversions will be systematically handled within the software to ensure users can easily work with Mpc or other units if needed.\n- Interface definitions and user input validations will include checks for unit consistency and warnings to guide users.\n\nBy adopting Mpc\/h, we align with best practices in cosmological research while maintaining the flexibility needed for accurate and adaptable analysis.","GenTime":"2024-07-31 11:15:09"}
{"File Name":"search-ui-extensions\/ShowViewedByCustomer.md","Context":"## Context and Problem Statement\\n<!-- Quick 2-3 sentence background of the User Story -->\\nThe `ViewedByCustomer` component needs to be added on each result template, and therefore could be accidentally missed on one or more, especially if a new template is added at a later time. This would create an inconsistent view of what content the customer has viewed. - From JIRA\\n---\\n## Decision Drivers <!-- optional -->\\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n\n## Decision\n","Decision":"### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n-   [Option 1] - Leverage the `newResultsDisplayed` event, and loop over every result, performing further action.\\n-   [Option 2] - Leverage the `newResultDisplayed` event, and perform further action.\\n**Decision 2** - Properly adding the ViewedByDocument Component\\n-   [Option 1] - Add the component using `<div class=\"CoveoViewedByCustomer\">`.\\n-   [Option 2] - Add the component using the `ViewedByCustomer` constructor.\\n**Decision 3** - Ensure we don't add the template a second time\\n-   [Option 1] - Query the results `HTMLElement` using the `getElementsByClassName` method.\\n-   [Option 2] - Query the results `HTMLElement` using the `querySelectorAll` method.\\n**Decision 4** - There should be an option whether or not to add the component\\n-   [Option 1] - Have the option be false by default.\\n-   [Option 2] - Have the option be true by default.\\n---\\n#### Decision 1: [Option 2]\\nThere are two reason behind this decision selection: First the `newResultsDisplayed` option wasn't passing back the `args.item`, which would have made editing the dom element harder. Second, using the event trigger instead of a for loop made the methods functionality more simple.\\n#### Decision 2: [Option 2]\\nThe `newResultDisplayed` dom element was firing after the completion of the search-ui, therefore using the `<div>` wasn't possible.\\n#### Decision 3: [Option 1]\\nChoosing to use `getElementsByClassName`, in this context I don't think there is a difference between using `querySelectorAll` and `getElementsByClassName`\\n#### Decision 4: [Option 2]\\nFrom what I understand it makes sense for this feature to be true by default, as it's important for their not to be inconsistencies\\n---\\n","tokens":235,"id":5149,"Prediction":"## Decision\n\n### Chosen Approach\nTo resolve the inconsistency and potential omission issues, the following approach is decided:\n\n1. **Event-Driven Mechanism:**\n   - Introduce an event listener that triggers when a new result template is being rendered. This will ensure that the system can dynamically manage the addition of the `ViewedByCustomer` component.\n\n2. **Centralized Component Handling:**\n   - Create a central handler function that will be responsible for adding the `ViewedByCustomer` component to each result template. This function will be utilized each time a template is rendered, hence eliminating manual insertion and potential oversight. Called automatically by the event listener mentioned in point 1.\n\n3. **Duplicate Check:**\n   - Within the central handler function, implement a check to ensure that if a result template already includes a `ViewedByCustomer` component, a new one will not be added. This will maintain content integrity and appearance consistency.\n\n4. **Configurable Option:**\n   - Introduce a configuration option within the `UserActions` component that will allow toggling the addition of the `ViewedByCustomer` component. This will provide flexibility for enabling or disabling this feature based on user preferences or specific use cases.\n\n### Architectural Changes\n- **Event System Integration:**\n  - Integration with an existing event system or the implementation of a new lightweight event mechanism that can monitor and react to the rendering process of result templates.\n  \n- **Handler Development:**\n  - Development of a handler function that encapsulates the logic for adding the `ViewedByCustomer` component and checking for duplicates.\n\n- **Configuration Management:**\n  - Update the `UserActions` component to include an option for enabling or disabling the `ViewedByCustomer` feature. This might entail UI adjustments and state management changes within the `UserActions` component.\n\n### Benefits\n- **Consistency:** Ensures that all result templates consistently include the `ViewedByCustomer` component.\n- **Maintainability:** Centralizes the logic for managing the component, making it easier to update or change in the future.\n- **Flexibility:** Provides users with an option to enable or disable the component as needed, enhancing user experience.\n\n### Drawbacks\n- **Complexity:** Requires integration with an event system and additional logic to manage the centralized handler function.\n- **Performance:** If not optimized, the event-driven approach may introduce overhead on template rendering processes.\n\n### Alternatives Considered\n- **Manual Addition:** Continuing to manually add the `ViewedByCustomer` component to each result template. This was deemed inefficient and prone to human error.\n- **Template Inheritance:** Using template inheritance mechanisms to include the component, which was complicated by the need to handle dynamic additions of new templates.","GenTime":"2024-07-31 11:15:16"}
{"File Name":"api-docs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5155,"Prediction":"## Decision\n\nWe will use an Architectural Decision Record (ADR) to document all architectural decisions made throughout this project. Each ADR will be a single Markdown file, stored in a dedicated 'decisions' directory within our repository. The format will include:\n\n- Title: A clear and concise description of the decision.\n- Status: The current state of the decision (e.g., proposed, accepted, deprecated).\n- Context: The rationale and background that necessitated this decision.\n- Decision: The decision itself.\n- Consequences: The implications of this decision, including technical debt and future considerations.\n\nBy adhering to this structure, we ensure consistency, clarity, and traceability in our architectural decisions. This approach will facilitate better understanding and communication among team members and stakeholders. All team members are encouraged to contribute to the ADR documentation and to review existing records periodically.","GenTime":"2024-07-31 11:15:41"}
{"File Name":"sepa-customer-platform\/0004-authenticate-backstage-users-azure-ad.md","Context":"## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage\/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n\n## Decision\n","Decision":"SEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\\n### Positive Consequences\\n* Reuse of existing SEPA logins\\n* Better backstage user experience\\n### Negative Consequences\\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\\n","tokens":90,"id":5157,"Prediction":"## Decision\n\nWe have decided to implement Azure Active Directory (AAD) as the identity management system for the frontend application used by backstage\/SEPA users to ensure seamless integration with the existing infrastructure.\n\n### Rationale\n\n1. **Unified Single Sign-On (SSO)**: Leveraging Azure Active Directory will allow SEPA users to use the same credentials they currently use for Active Directory, simplifying login processes and reducing password fatigue.\n\n2. **Improved Security**: Azure Active Directory provides robust security features, including multi-factor authentication, conditional access policies, and identity protection that aligns with SEPA's security requirements.\n\n3. **Interoperability**: Since Azure Cloud Services and Microsoft Dynamics 365 have been proposed, using AAD will maximize vendor reuse and ensure seamless interoperability among the systems, reducing the complexity of integration efforts.\n\n4. **User Management Efficiency**: A centralized identity management system will streamline user provisioning, deprovisioning, and access management, leading to more efficient operations and reducing administrative overhead.\n\n5. **Scalability and Future-Proofing**: AAD is highly scalable and can grow with SEPA\u2019s needs. Additionally, it positions SEPA to leverage other Azure services in the future with minimal friction.\n\n### Implementation Plan\n\n- **Integration**: Integrate the frontend application with AAD for user authentication using OAuth2.0\/OpenID Connect protocols.\n- **Migration**: Ensure a smooth transition by allowing a phased migration of users from the existing system to AAD.\n- **Training**: Provide training for SEPA users to familiarize them with the new SSO process.\n- **Monitoring**: Implement monitoring and logging to detect and respond to potential security threats promptly.\n\nBy adopting Azure Active Directory for identity management, we ensure a streamlined, secure, and future-proof authentication mechanism for SEPA users, aligned with the overall cloud strategy.","GenTime":"2024-07-31 11:15:56"}
{"File Name":"sepa-customer-platform\/0006-relating-documents-to-cases.md","Context":"## Context and Problem Statement\\nDocument uploads will be stored in Azure Blob storage because of the requirement to store very large files & the fact that Azure Blob storage offers high availability and low costs.\\nThe Azure Blob storage system is a separate cloud service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case\/incident entities, will be linked to the the files within Azure.\\n## Decision Drivers\\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n\n## Decision\n","Decision":"* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.\\n### Positive Consequences\\n* Minimises customisation of the CRM data, reuse of existing\\n### Negative Consequences\\n* Labelling within the CRM does not match the context of the upload.\\n","tokens":142,"id":5163,"Prediction":"## Decision\n\nTo effectively manage and display large document uploads stored in Azure Blob Storage while integrating with Dynamics CRM, we will implement the following strategy:\n\n1. **Extend CRM Case\/Incident Entity**\n   - Introduce a new attribute within the CRM case\/incident entities specifically to hold the reference URL of the document stored in Azure Blob Storage.\n   \n2. **Blob Storage URL Generation**\n   - Implement a mechanism, possibly an Azure Function or Logic App, to generate signed URLs for files stored in the Azure Blob Storage on demand, ensuring secure and temporary access to the documents.\n\n3. **WebAPI Integration**\n   - Ensure that the new file reference attribute in the CRM case\/incident entity is accessible and filterable via Dynamics CRM's WebAPI, enabling the web frontend to query and display the relevant documents to users.\n\n4. **Administrative UI Changes**\n   - Customize the Dynamics CRM UI for administrators to display links to the Azure files using the generated URLs. This will involve UI modifications to include clickable links or buttons that redirect administrators to the documents in Azure Blob Storage.\n\n5. **Authentication and Authorization**\n   - Ensure that access to the Azure Blob Storage and generation of signed URLs respects the existing authentication and authorization mechanisms in place within Dynamics CRM to maintain security and control over the documents.\n\nBy following this approach, we address the need to link large file uploads stored in Azure Blob Storage with CRM case\/incident entities in a manner that is accessible, secure, and integrates seamlessly with the existing Dynamics CRM interface and architecture.","GenTime":"2024-07-31 11:16:30"}
{"File Name":"identity-site\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5170,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) process to document all significant architectural decisions made throughout the lifecycle of the project. This process will involve:\n\n1. **Template Usage:** We will utilize a standardized template for all ADRs, ensuring consistency and completeness. The template will include sections such as Context, Decision, Status, Consequences, and Alternatives Considered.\n\n2. **Version Control:** All ADR documents will be stored in the project's version control system (e.g., Git) to provide a history of changes and facilitate collaboration.\n\n3. **Regular Updates:** ADRs will be reviewed and updated regularly to reflect the latest decisions and any modifications based on new information or project changes.\n\n4. **Review Process:** Major architectural decisions will be reviewed by the project\u2019s technical committee or architectural review board to ensure alignment with overall system goals and best practices.\n\n5. **Accessibility:** ADRs will be made accessible to all team members to provide transparent communication of architectural decisions and rationale.\n\nBy adopting this ADR process, we aim to maintain a clear and organized record of all architectural decisions, which will help in future decision-making, onboarding new team members, and facilitating knowledge transfer.","GenTime":"2024-07-31 11:17:05"}
{"File Name":"pace-developers\/0012-matlab-python-wrapper.md","Context":"## Context\\nBoth Euphonic and Brille are PACE projects with primarily a Python user interface (UI).\\nPACE, however, aims to provide both a Python and a Matlab UI to users,\\nand also to foster inter-operability between projects which are written both in Matlab and Python.\\nIn particular, `pyHorace` ([prototype](https:\/\/github.com\/mducle\/hugo)) cannot use the\\n[standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for Matlab to run Python code,\\nwhere calls to Python from Matlab are prefixed with `py.` followed by the full module specification.\\nFor example, `r = py.numpy.random.rand()` uses `numpy` to generate a random number.\\nThis is because such a call causes Matlab to\\n[automatically spawn](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/pyenv.html) a dependent Python interpreter,\\nwhich can be either created within the same process as the Matlab interpreter (`InProcess`)\\nor in an external process (`OutOfProcess`).\\n`pyHorace` already runs within a Python interpreter and the compiled Matlab library *must* be loaded in-process.\\nThus, if Matlab spawns a second Python intepreter with the default `InProcess` execution mode,\\nthe two Python interpreters will conflict causing memory errors and a crash.\\nWe can force Matlab to launch the dependent Python interpreter `OutOfProcess`\\nbut this imposes a significant performance penalty\\n(extensive testing was not done but Brille+SpinW runs about 10x slower than with `InProcess`).\\n\n## Decision\n","Decision":"At a meeting on Jan 7 2021, the developers of `pyHorace`, `brillem` and `horace-euphonic-interface` agreed to accept this proposal.\\n`brillem` and `horace-euphonic-interface` will be refactored to use the `light_python_wrapper` proposed here.\\nThe meeting also agreed implementation details which will be described in [ADR #13](0013-light-python-wrapper-implementation-detail.md).\\n","tokens":333,"id":5171,"Prediction":"## Decision\nWe have decided to develop a custom interface layer to enable smooth inter-operability between Matlab and Python without relying on Matlab's standard `py.` prefix method for calling Python code. This custom interface will ensure all necessary interactions between Matlab and Python remain in-process, thereby avoiding the conflict and memory errors associated with running two Python interpreters simultaneously.\n\n### Key Elements of the Decision:\n1. **Custom Interface Layer**: A dedicated middleware will be implemented to manage the communication between Matlab and Python.\n2. **In-Process Execution**: Ensure that the compiled Matlab library runs in-process to avoid performance penalties and conflicts.\n3. **Wrapper Functions**: Develop Matlab wrapper functions to replicate Python functionality which can be executed without requiring the dependent Python interpreter to be launched.\n4. **Efficient Data Transfer**: Optimize the data transfer mechanism between Matlab and Python to maintain high performance.\n5. **Extensive Testing**: Perform comprehensive testing to validate stability, efficiency, and performance of the custom interface.\n\n### Benefits:\n- **Compatibility**: Maintains compatibility with existing PACE projects and ensures seamless operation within the shared ecosystem.\n- **Performance**: Avoids the significant performance penalty associated with the `OutOfProcess` execution mode.\n- **Stability**: Reduces the risk of crashes and memory errors by preventing the simultaneous operation of two Python interpreters.\n\n### Implementation Strategy:\n- Initiate a design phase to map out the exact requirements and architecture of the custom interface layer.\n- Develop prototypes and test cases to validate the proposed solution.\n- Gradually roll out the custom interface across relevant PACE projects to ensure a smooth transition.\n- Provide documentation and support for developers to integrate the new interface into their workflows.\n\nBy following this approach, we aim to ensure that the PACE projects can leverage both Matlab and Python interfaces effectively without compromising on performance or stability.","GenTime":"2024-07-31 11:17:10"}
{"File Name":"pace-developers\/0002-use-github-pages-for-user-documentation.md","Context":"## Context\\nA consistent platform is required for user documentation from the PACE projects: Brille, Euphonic and Horace\/Herbert.\\nTwo platforms support, are well used for this service:\\n- [Read the Docs](https:\/\/readthedocs.org\/)\\n- [GitHub pages](https:\/\/pages.github.com\/)\\nBoth platforms will display documentation built by [Sphinx](https:\/\/www.sphinx-doc.org\/) from reStructuredText source files.\\nBrille includes compiled C libraries that contribute APIs to the project. The Read the Docs build\/deploy pipeline does not support inclusion of documentation generated from C source.\\n\n## Decision\n","Decision":"We will use GitHub pages for all PACE project documentation.\\n","tokens":132,"id":5172,"Prediction":"## Decision\n\nGiven the requirements for a consistent platform for user documentation across the PACE projects (Brille, Euphonic, and Horace\/Herbert) and considering the technical constraints regarding the inclusion of C source documentation for Brille, the decision is as follows:\n\nWe will use **GitHub Pages** as the platform for user documentation for all PACE projects.\n\n### Rationale:\n\n1. **Support for All Source Types**: GitHub Pages supports hosting documentation generated by Sphinx, including content from reStructuredText files and C source documentation, which is crucial for Brille.\n   \n2. **Consistent Experience**: Utilizing a single platform (GitHub Pages) ensures a consistent documentation presentation and user experience across all projects.\n\n3. **Ease of Integration**: GitHub Pages integrates seamlessly with GitHub repositories, offering a straightforward setup for continuous documentation updates directly from the project's repository.\n\n4. **Flexibility and Control**: GitHub Pages provides more flexibility with the look and feel of the documentation, which can be advantageous for tailoring the user interface to better fit the needs of each project.\n\n5. **Community Familiarity**: GitHub is widely used in the development community, including those who may contribute to or utilize the PACE projects, making it a familiar and accessible platform for viewing and hosting documentation.","GenTime":"2024-07-31 11:17:14"}
{"File Name":"pace-developers\/0007-developer-scripts-storage-location.md","Context":"## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo\/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n\n## Decision\n","Decision":"Developer scripts will be stored in an appropriately placed `scripts`\\ndirectory in the `pace-developers` repository. Depending on whether the\\nscript is tied to a particular software, or general algorithm development\\nit could be stored in `pace-developers\/euphonic\/scripts` or\\n`pace-developers\/powder_averaging\/scripts` for example.\\n","tokens":283,"id":5176,"Prediction":"## Decision\n\nWe will store the demo\/visualisation scripts in a `scripts` directory in the `pace-developers` repository.\n\n### Rationale:\n1. **Proximity to Decision-Making Documentation**: Having the scripts in the `pace-developers` repository places them near the developer documentation that outlines the design decisions, making it easier to understand the context and rationale behind each script.\n2. **Version Specification**: In the `pace-developers` repository, we can specify the versions of project dependencies, ensuring that the scripts are reproducible and functional with the specified versions of the software.\n3. **Clarity of Purpose**: Storing these scripts separately from the main project repositories reduces the risk of them being mistaken for officially supported and maintained scripts. This separation ensures that their exploratory and unpolished nature is clear to all developers.\n4. **Cross-Project Utilization**: By placing the scripts in a centralized location, we eliminate confusion about where to store scripts that involve multiple projects. This centralization also makes it easier for developers to find and share useful scripts across different projects.\n\nThis decision optimally balances accessibility, maintenance clarity, and cross-project usability.","GenTime":"2024-07-31 11:17:40"}
{"File Name":"pace-developers\/0005-pace-projects-must-be-semantically-versioned.md","Context":"## Context\\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build.\\n[Semantic versioning](https:\/\/semver.org\/) defines a schema in which releases are given `major.minor.patch` version numbers where increments are made to the:\\n- `major` version with incompatible API changes,\\n- `minor` version when functionality is added in a backwards compatible manner, and\\n- `patch` version for backwards compatible bug fixes.\\nSphinx plugins are available that support multiple documentation versions ([sphinx-multiversion](https:\/\/pypi.org\/project\/sphinx-multiversion\/)) - this will not work correctly for Brille where documentation is extracted from the build artifacts.\\n\n## Decision\n","Decision":"Projects will be semantically versioned and documentation will be retained and be available for users to access for each major or minor version released.\\nThere is no requirement to have patch-release specific documentation.\\n","tokens":159,"id":5177,"Prediction":"## Decision\n\nWe will implement the use of semantic versioning for the PACE project documentation, adhering to the `major.minor.patch` schema. This versioning will ensure users can easily locate the correct documentation corresponding to their build version. \n\nTo support multiple documentation versions and mitigate issues with Sphinx plugins, we will:\n\n1. **Generate Documentation Artifacts Separately**: Generate and store documentation artifacts as separate entities during each build. This method allows us to maintain clear, version-specific documentation that aligns with the semantic-versioning scheme.\n  \n2. **Versioned Documentation Hosting**: Host the documentation across different URLs or folders for each `major.minor.patch` version. Each version's documentation will be accessible at a dedicated URL (e.g., `docs\/project\/v1.2.3\/`) to facilitate easy access.\n\n3. **Use Sphinx**: Continue to use Sphinx for generating documentation. We will integrate a build pipeline that supports versioned artifacts, ensuring compatibility with Sphinx while accommodating our documentation versioning needs.\n\n4. **Automate Documentation Version Management**: Develop automation scripts to handle documentation generation, artifact storage, and deployment to minimize manual errors and ensure consistency across all versions.\n\n5. **Version Selector in Documentation**: Implement a version selection feature within the documentation web interface, allowing users to switch between different versions easily.\n\nBy applying semantic versioning to our documentation and adopting these practices, we ensure that users can always find the accurate and relevant documentation corresponding to their specific build versions.","GenTime":"2024-07-31 11:17:44"}
{"File Name":"pace-developers\/0008-pace-syntax-guiding-principles.md","Context":"## Context\\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\\nPACE involves many other codes, including [Euphonic](https:\/\/github.com\/pace-neutrons\/Euphonic),\\n[Brille](https:\/\/github.com\/brille\/brille) and [SpinW](https:\/\/spinw.org).\\nThe way in which these programs interact with each other is presently unclear.\\nFurthermore the programs are written in several different languages.\\nIn this ADR we propose some guiding principles in the design of the interfaces\\nbetween the programs or components which make up PACE.\\nThis was discussed at a meeting whose\\n[minutes are here.](https:\/\/stfc365.sharepoint.com\/:w:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/pace_syntax_meeting1_minutes.docx))\\n\n## Decision\n","Decision":"The guiding principle for PACE-supported programs is **commonality**.\\nThat is, interacting with the different programs of PACE should be **seamless**\\nto the user, and the user interface presented to them should be **uniform**.\\nThe uniform interface means that function names and how users specify parameters\\nor arguments to functions in Matlab and Python flavours of PACE should align\\nas closely as possible.\\n","tokens":183,"id":5178,"Prediction":"## Decision\n\n### Establish Multi-Language Interoperability Framework\n\n1. **Adopt a Standard Interface Language**\n    - **Objective**: Ensure seamless communication between different components of the PACE project regardless of their implementation languages.\n    - **Action**: Use a common interface language such as **C** or **Fortran** for binding different modules. Tools like **SWIG (Simplified Wrapper and Interface Generator)** will be considered for generating wrapping code to allow integration between languages including C++, Python, and Matlab.\n\n2. **Define and Document Interfaces and Protocols**\n    - **Objective**: Provide clear and consistent communication protocols among all programs\/components involved in PACE.\n    - **Action**: Establish a well-documented API and use a common data interchange format such as **HDF5** or **JSON** for communication between components. This documentation will include function signatures, data types, and error handling mechanisms.\n\n3. **Utilize Language-Specific Wrappers**\n    - **Objective**: Allow each program to maximize performance and maintain readability in its native language while providing accessible interfaces to other components.\n    - **Action**: Develop and maintain language wrappers for each program\/component, enabling them to be called from other languages. For example, Python wrappers for C++ code or Matlab interfaces for Fortran modules.\n\n4. **Modular and Extensible Design**\n    - **Objective**: Facilitate the integration of additional tools and programs into the PACE ecosystem in the future.\n    - **Action**: Encourage modular design where each component follows the interface guidelines, ensuring any new component can be added with minimal changes to the existing system.\n\n5. **Frequent Review and Updates**\n    - **Objective**: Ensure the interface design remains robust and effective in accommodating the evolving needs of the PACE project.\n    - **Action**: Conduct periodic reviews of the interfaces and integration points, and update them as necessary to handle new requirements or optimize performance.\n    \nBy adopting these principles, we aim to create a cohesive, maintainable, and scalable PACE ecosystem, allowing diverse programs to collaborate efficiently and enabling the incorporation of future components with ease.","GenTime":"2024-07-31 11:17:50"}
{"File Name":"pace-developers\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5182,"Prediction":"## Decision\nWe will adopt the use of an Architectural Decision Record (ADR) template to document and track architectural decisions made throughout the project. This will be stored in a centralized version control system, making it easily accessible for all team members. The ADR will include specific sections for Context, Decision, Consequences, Rationale, and Status to ensure comprehensive documentation and facilitate better communication and understanding among the team. This approach will ensure that all decisions are made transparently and can be referenced or revisited as needed in the future.","GenTime":"2024-07-31 11:17:58"}
{"File Name":"umbrella\/0003-component-configuration-via-context.md","Context":"## Context\\nAn alternative configuration procedure to ADR-0002, possibly better\\nsuited for dynamic theming, theme changes and separating the component\\nconfiguration between behavioral and stylistic aspects. This new\\napproach utilizes the hdom context object to retrieve theme attributes,\\nwhereas the previous solution ignored the context object entirely.\\nA live demo of the code discussed here is available at:\\n[demo.thi.ng\/umbrella\/hdom-theme-adr-0003](https:\/\/demo.thi.ng\/umbrella\/hdom-theme-adr-0003)\\n\n## Decision\n","Decision":"### Split component configuration\\n#### Behavioral aspects\\nComponent pre-configuration options SHOULD purely consist of behavioral\\nsettings and NOT include any aesthetic \/ theme oriented options. To\\nbetter express this intention, it's recommended to suffix these\\ninterface names with `Behavior`, e.g. `ButtonBehavior`.\\n```ts\\ninterface ButtonBehavior {\\n\/**\\n* Element name to use for enabled buttons.\\n* Default: \"a\"\\n*\/\\ntag: string;\\n\/**\\n* Element name to use for disabled buttons.\\n* Default: \"span\"\\n*\/\\ntagDisabled: string;\\n\/**\\n* Default attribs, always injected for active button states\\n* and overridable at runtime.\\n* Default: `{ href: \"#\", role: \"button\" }`\\n*\/\\nattribs: IObjectOf<any>;\\n}\\n```\\n#### Theme stored in hdom context\\nEven though there's work underway to develop a flexble theming system\\nfor hdom components, the components themselves SHOULD be agnostic to\\nthis and only expect to somehow obtain styling attributes from the hdom\\ncontext object passed to each component function. How is shown further\\nbelow.\\nIn this example we define a `theme` key in the context object, under\\nwhich theme options for all participating components are stored.\\n```ts\\nconst ctx = {\\n...\\ntheme: {\\nprimaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\nsecondaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\n...\\n}\\n};\\n```\\n### Component definition\\n```ts\\nimport { getIn, Path } from \"@thi.ng\/paths\";\\n\/**\\n* Instance specific runtime args. All optional.\\n*\/\\ninterface ButtonArgs {\\n\/**\\n* Click event handler to be wrapped with preventDefault() call\\n*\/\\nonclick: EventListener;\\n\/**\\n* Disabled flag. Used to determine themed version.\\n*\/\\ndisabled: boolean;\\n\/**\\n* Selected flag. Used to determine themed version.\\n*\/\\nselected: boolean;\\n\/**\\n* Link target.\\n*\/\\nhref: string;\\n}\\nconst button = (themeCtxPath: Path, behavior?: Partial<ButtonBehavior>) => {\\n\/\/ init with defaults\\nbehavior = {\\ntag: \"a\",\\ntagDisabled: \"span\",\\n...behavior\\n};\\nbehavior.attribs = { href: \"#\", role: \"button\", ...behavior.attribs };\\n\/\/ return component function as closure\\nreturn (ctx: any, args: Partial<ButtonArgs>, ...body: any[]) => {\\n\/\/ lookup component theme config in context\\nconst theme = getIn(ctx, themeCtxPath);\\nif (args.disabled) {\\nreturn [behavior.tagDisabled, {\\n...behavior.attribs,\\n...theme.disabled,\\n...args,\\n}, ...body];\\n} else {\\nconst attribs = {\\n...behavior.attribs,\\n...theme[args.selected ? \"selected\" : \"default\"],\\n...args\\n};\\nif (args && args.onclick && (args.href == null || args.href === \"#\")) {\\nattribs.onclick = (e) => (e.preventDefault(), args.onclick(e));\\n}\\nreturn [behavior.tag, attribs, ...body];\\n}\\n};\\n};\\n```\\n### Component usage\\n```ts\\nconst darkTheme = {\\nid: \"dark\",\\nbody: {\\nclass: \"vh-100 bg-black moon-gray pa3 sans-serif\"\\n},\\nlink: {\\nclass: \"link dim b light-silver\"\\n},\\nbutton: {\\ndefault: {\\nclass: \"dib link mr2 ph3 pv2 blue hover-lightest-blue hover-b--current br3 ba b--blue\"\\n},\\nselected: {\\nclass: \"dib link mr2 ph3 pv2 red hover-gold hover-b--current br3 ba b--red\"\\n},\\ndisabled: {\\nclass: \"dib mr2 ph3 pv2 mid-gray br3 ba b--mid-gray\"\\n}\\n}\\n};\\nconst bt = button(\"theme.button\");\\nconst btFixed = button(\"theme.button\", { attribs: { style: { width: \"8rem\" } } });\\nconst app = (ctx) =>\\n[\"div\", ctx.theme.body,\\n[bt, { onclick: () => alert(\"toggle\") }, \"Toggle\"],\\n[bt, { href: \"https:\/\/github.com\/thi-ng\/umbrella\" }, \"External\"],\\n[btFixed, { onclick: () => alert(\"hi\"), selected: true }, \"Selected\"],\\n[btFixed, { disabled: true }, \"Disabled\"] ];\\n\/\/ start app with theme in context\\nstart(\"app\", app, { theme: darkTheme })\\n```\\n","tokens":117,"id":5185,"Prediction":"## Decision\n\nAfter evaluating the alternatives for configuring a theming system that allows dynamic changes and clearly separates behavioral and stylistic component configurations, we have decided to adopt the new approach that leverages the hdom context object. \n\nThis decision is based on the following points:\n1. **Dynamic Theming:** The new system supports dynamic theming more effectively, allowing for theme changes to be applied immediately without requiring a full reload or reinitialization of components.\n2. **Separation of Concerns:** By utilizing the hdom context object, we can cleanly separate behavioral logic from styling attributes, leading to more maintainable and readable code.\n3. **Context Utilization:** The previous approach ignored the context object, which limited the flexibility and adaptability of the theming system. The new approach fully embraces the context object, making it easier to manage and update theme-related properties.\n4. **Live Demonstration:** The development of a live demo (available at the provided link) demonstrates the feasibility and benefits of the new approach in a real-world scenario.\n\nTherefore, we are moving forward with implementing this new configuration procedure as our standard for theming to enhance the modularity, flexibility, and maintainability of our software components.","GenTime":"2024-07-31 11:18:15"}
{"File Name":"js-sdk\/0008-add-kwargs-to-3bot-start.md","Context":"## Context\\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\\n\n## Decision\n","Decision":"Add kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\\n","tokens":28,"id":5188,"Prediction":"## Decision\n\nWe will refactor the package management system to require keyword arguments (kwargs) only once during the initial addition of the package. This change will be implemented to enhance usability and reduce redundancy by ensuring that the kwargs are stored and reused internally without requiring the user to provide them repeatedly each time the `start` function is called. This approach simplifies the API and improves the developer experience. Additionally, we will add comprehensive validation and error handling to ensure that the kwargs provided during the package addition are correctly formatted and sufficient for all subsequent operations.","GenTime":"2024-07-31 11:18:24"}
{"File Name":"js-sdk\/0003-allow-only-tft-token.md","Context":"## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n\n## Decision\n","Decision":"Completely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\\n","tokens":14,"id":5198,"Prediction":"## Decision\nThe FreeTFT (Free Test Tokens) and TFTA (Test Faucet Tokens) tokens will be removed from the software development kit (SDK) to streamline and simplify the token offerings within the SDK. This decision comes as a response to reduce the complexity and redundancy in token management. The SDK will focus on supporting more essential and widely used tokens that provide greater value and utility to the users, ensuring a more efficient and effective development environment. This change will involve updating relevant documentation, performing necessary code refactorings, and ensuring backward compatibility where required.","GenTime":"2024-07-31 11:18:57"}
{"File Name":"js-sdk\/0016-update-threebot-deployer-flist.md","Context":"## Context\\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\\n\n## Decision\n","Decision":"Update the flist with the up to date one\\n","tokens":48,"id":5201,"Prediction":"## Decision\nTo optimize the deployment process of hosted 3Bot and significantly reduce the cloning and installation time, we will transition to utilizing the new flist built by our Continuous Integration (CI) system. This flist will always be up to date with the latest changes and dependencies required by 3Bot, ensuring rapid and efficient deployment. By adopting this practice, we can streamline our deployment pipeline, reduce downtime, and enhance the overall reliability and speed of our service deployments.","GenTime":"2024-07-31 11:19:06"}
{"File Name":"report-a-defect\/0011-use-auth0-for-authentication.md","Context":"## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n\n## Decision\n","Decision":"Use Auth0 as the single sign-on provider, and remove basic auth from production.\\n","tokens":234,"id":5211,"Prediction":"## Decision\n\nTo ensure the immediate needs of user identification and security are met, we have decided to implement Auth0 as a temporary single sign-on solution for the Report a Defect application. This interim solution will provide individual credentials for each user, addressing security concerns associated with Basic Authentication and ensuring that agent names are properly logged within the service.\n\nIn detail:\n\n- **Use Auth0**: Auth0 will be implemented as the authentication provider for the short term.\n- **Facilitate Individual Credentials**: Each user will receive individual credentials, as opposed to the shared credentials used in Basic Authentication.\n- **Monitor and Adapt**: We will closely monitor the implementation to ensure it meets our current security requirements and user needs.\n- **Future Review**: This solution will be revisited once Hackney finalizes its choice for a single sign-on provider by the end of 2019. Depending on the outcomes, we may transition to the officially endorsed solution, ensuring alignment with Hackney's broader authentication strategy.\n\nThis decision helps us achieve immediate operational requirements while allowing flexibility to integrate a more permanent solution as soon as it becomes available.","GenTime":"2024-07-31 11:19:42"}
{"File Name":"report-a-defect\/0008-usegovuknotifyforemailing.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThis service has a need to send emails and SMS to users. As Hackney is approved as an organisation that can use GOV.UK Notify we are able to use this service and take advantage of the reliability and low costs.\\nHackney already use Notify for other services, although it is not yet referenced in their playbook https:\/\/github.com\/LBHackney-IT\/API-Playbook\\n\n## Decision\n","Decision":"Use Notify over a third party service such as SendGrid for the sending of emails and SMS\\n","tokens":101,"id":5213,"Prediction":"## Decision\nWe will use the GOV.UK Notify service for sending emails and SMS to users. Given Hackney's pre-existing approval to use this government-approved service, it ensures reliability, security, and compliance with governmental standards. Additionally, leveraging GOV.UK Notify aligns with cost-effective practices due to its low costs compared to other third-party services.\n\nOur integration with GOV.UK Notify will enable us to benefit from existing infrastructure, simplify our implementation process, and maintain consistency with other Hackney services already utilizing the platform. Therefore, for this service, implementing GOV.UK Notify is the chosen solution for managing our email and SMS communication needs.","GenTime":"2024-07-31 11:19:47"}
{"File Name":"report-a-defect\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5220,"Prediction":"## Decision\nWe will adopt the use of Architectural Decision Records (ADRs) to systematically document all significant architectural decisions made throughout the project's lifecycle. This approach ensures that decisions are well-communicated, traceable, and can be reviewed over time. We will use a standardized template for creating ADRs, ensuring consistency and comprehensiveness. Each ADR will include sections such as Context, Decision, Consequences, and any Alternatives considered. These records will be stored in a dedicated repository within our project's version control system to facilitate easy access and collaboration.","GenTime":"2024-07-31 11:20:13"}
{"File Name":"reactive-interaction-gateway\/0003-for-config-prefer-prefix-over-nesting-and-don-t-hide-defaults-in-code.md","Context":"## Context\\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\\n### Issue 1: Nested keys\\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\\n```\\nApplication.fetch_env!(:rig, :required_key)\\n```\\nwill show a nice error, while\\n```\\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\\n```\\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\\n### Issue 2: Default values\\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\\n\n## Decision\n","Decision":"* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\\n","tokens":182,"id":5227,"Prediction":"## Decision\n\nWe will implement a unified configuration handling module that addresses both nested keys and default values effectively.\n\n### Nested Keys\nTo handle nested keys more robustly, we will implement a custom fetching function that will allow for nested key access with clear error messages when a key is missing. This function will recursively navigate the nested keys and provide a meaningful error message if any required key in the chain is missing. Here is a conceptual implementation:\n\n```elixir\ndefmodule Config do\n  @moduledoc \"\"\"\n  Provides functions to fetch configuration values with support for nested keys and default values.\n  \"\"\"\n\n  @spec fetch_env!(atom, [atom | String.t()], term) :: term\n  def fetch_env!(app, keys, default \\\\ nil) when is_list(keys) do\n    keys\n    |> Enum.reduce(Application.fetch_env!(app, hd(keys)), fn key, acc ->\n      case acc do\n        %{^key => value} -> value\n        _ -> raise \"Missing required configuration key: #{Enum.join(keys, \".\")}\"\n      end\n    end)\n    |> case do\n      nil -> default\n      value -> value\n    end\n  end\nend\n```\n\n### Default Values\nTo address the need for centralized default values, we will maintain a default configuration module. This module will house all default values in a single location, making them easily inspectable and modifiable. Configuration retrieval will first check the application's environment configuration, and if not found, it will fallback to the defaults defined in this module:\n\n```elixir\ndefmodule DefaultConfig do\n  @moduledoc \"\"\"\n  Default configuration values for the application.\n  \"\"\"\n  \n  @defaults %{\n    rig: %{\n      required_key: %{\n        required_subkey: \"default_value\"\n      }\n    }\n  }\n\n  @spec get(atom, list) :: term\n  def get(app, keys) when is_list(keys) do\n    Map.get_in(@defaults, [app | keys], nil)\n  end\nend\n```\n\nUsing these modules, configuration values can be accessed as follows:\n\n```elixir\nconfig_value = Config.fetch_env!(:rig, [:required_key, :required_subkey], DefaultConfig.get(:rig, [:required_key, :required_subkey]))\n```\n\nThis ensures that there is a consistent and robust method for accessing configuration values, including nested keys, with default values being managed centrally and transparently.","GenTime":"2024-07-31 11:20:47"}
{"File Name":"php-docker-template\/0004-default-php-settings-memory-limit.md","Context":"## Context\\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\\n\n## Decision\n","Decision":"Set php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\\n","tokens":92,"id":5235,"Prediction":"## Decision\nWe have decided to adjust the PHP settings within our Docker images to align with the resource management features provided by Kubernetes. Specifically, we will:\n\n1. **Remove PHP Memory Limits**: Disable memory limits in PHP configurations to prevent PHP from terminating due to memory exhaustion. This will ensure that the orchestrator (Kubernetes) manages memory allocation and recovery more effectively, and it will have a clearer understanding of the actual resource requirements and usage patterns.\n\n2. **Monitor Resource Utilization**: Implement Kubernetes resource limits and requests (CPU and memory) to manage resource allocation and monitor usage effectively. This will allow Kubernetes to take appropriate action, such as restarting Pods or reallocating resources as needed.\n\n3. **Log and Alert on High Memory Usage**: Develop logging and alerting mechanisms to monitor memory usage by the PHP processes. This will allow timely actions if high memory usage is detected, propelling proactive responses and avoiding unexpected downtime.\n\n4. **Stress Testing**: Conduct thorough stress testing on the applications to ensure that they behave as expected under varied resource constraints. This will help in fine-tuning resource requests and limits within the Docker orchestrator.\n\n5. **Documentation and Best Practices**: Update the documentation to include the rationale behind these changes, and provide guidelines on configuring Kubernetes resource management effectively for these Docker images.\n\nBy making these adjustments, we leverage Kubernetes' robust resource management capabilities, enhancing the stability and resilience of PHP applications while ensuring smooth operations without unnecessary termination due to out-of-memory errors originating from PHP's internal limits.","GenTime":"2024-07-31 11:21:17"}
{"File Name":"php-docker-template\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n","Decision":"We will use Architecture Decision Records, as [described by Usabilla Architecture Chapter](https:\/\/github.com\/usabilla\/architecture-chapter\/blob\/master\/docs\/adr\/0001-record-architecture-decisions.md).\\n","tokens":16,"id":5237,"Prediction":"## Decision\nWe will implement an Architectural Decision Record (ADR) documentation process for this project. This will involve creating a structured format to capture key decisions, along with their context, rationale, and potential consequences.\n\n### Key Details:\n1. **ADR Template**: We will use a standardized ADR template that includes sections such as Title, Context, Decision, Status, Consequences, and any other relevant information.\n2. **Tooling**: ADRs will be documented using a Markdown format and stored in a version-controlled repository (e.g., GitHub). This ensures that changes can be tracked and reviewed easily.\n3. **Review Process**: Each ADR will be reviewed by key stakeholders before being officially recorded. This includes architects, lead developers, and project managers.\n4. **Decision Logging**: ADRs will be numbered sequentially and logged in a dedicated directory within the project repository (e.g., `\/docs\/adr\/`).\n5. **Updates**: ADRs will be updated or superseded as the project evolves, ensuring that the documentation remains current and relevant.\n6. **Communication**: New or updated ADRs will be communicated to the team during regular project meetings and via internal communication channels.\n\n### Outcomes:\n- Improved transparency of decision-making processes.\n- Enhanced documentation for future reference and understanding of architectural choices.\n- Better alignment and agreement among team members and stakeholders on key architectural decisions.\n- Historical record of decisions and their rationale, helpful for onboarding and future project phases.","GenTime":"2024-07-31 11:21:29"}
{"File Name":"digital-paper-edit-firebase\/2020-03-12-integrating-with-new-pstt-module.md","Context":"## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n\n## Decision\n","Decision":"Ashley is building a service that will have an API Gateway and an S3 bucket. Uploading to the S3 bucket will trigger the STT event. We will be uploading to a bucket, as the previous architecture, with limitations that will define how we will communicate with Newslabs' PSTT service.\\nThere will only be one shared bucket per environment for services using this service. The name of the bucket is \"newslabs-stt-media-to-transcribe\" and \"newslabs-stt-media-to-transcribe-test\".\\n### Limitations\\n- Newslabs pstt will only handle audio files (`mp3`, `mp4`, `wav`, `flac`)\\n- the client (dpe) need to ensure they upload audio not video\\n- do not use the eTag as the reference, use object key instead: `<service_name>\/<object_key>.<ext>`\\n- assume that the client is sending a unique object key\\n### Example\\n#### Upload\\nWhen uploading a file with Object Key: `280612.mp3`, the Object Key should be prepended with the service name: `dpe\/280612.mp3`\\n#### Status\\nThe endpoint for requesting the status of a transcription uses [this lambda](https:\/\/github.com\/bbc\/newslabs-stt\/tree\/master\/newslabs-stt-check-transcription), which returns the transcription status.\\nThere is an example response in the README.\\nMake a request to an API Gateway endpoint (please ask) with something like this in the request body:\\n```json\\n{\\n\"objectKey\": \"dpe\/uuid.ext\"\\n}\\n```\\n","tokens":28,"id":5243,"Prediction":"## Decision\n\nWe will integrate the Newslabs PSTT service into our application using a REST API connection. This method of integration was chosen to ensure compatibility, scalability, and ease of maintenance. REST APIs are widely adopted standards for connecting with external services, and Newslabs PSTT provides comprehensive API documentation and support, which will facilitate a smooth integration process.\n\n### Key Points for the Decision:\n\n1. **Compatibility**: Our existing architecture supports RESTful web services, meaning the integration won't require significant changes to our current system.\n2. **Scalability**: Using REST APIs ensures the system can scale efficiently as it processes more data or requires more instances of the STT service.\n3. **Ease of Maintenance**: The REST API approach aligns with our team\u2019s existing skill sets and experience. The modular nature of RESTful service integration also aids in troubleshooting and updating individual components.\n4. **Security**: REST APIs typically offer robust security mechanisms including HTTPS, OAuth, and API keys. This ensures that data transmitted between our application and Newslabs PSTT will be secure.\n5. **Documentation and Support**: Newslabs PSTT\u2019s existing documentation and support for their REST API will minimize the learning curve and development time.\n\nBy following this strategy, we aim to ensure a robust, maintainable, and secure integration that leverages the strengths of our current architecture while reaping the benefits of Newslabs PSTT\u2019s services.","GenTime":"2024-07-31 11:22:04"}
{"File Name":"digital-paper-edit-firebase\/2019-04-29-SQL-database-schema.md","Context":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n## Decision\n","Decision":"* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","tokens":248,"id":5244,"Prediction":"## Context and Problem Statement\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\nEg have 3 versions of the schema for the 3 main milestones:\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\n\n## Decision Drivers\n* Easy to reason around\n* Maintainability with the project milestones\n* Compatibility across PostgreSQL and SQLite\n* Simplicity in versioning and incremental updates\n\n## Decision\n\n**Adopt a Schema Versioning Strategy using Migration Files**\n\n1. **Schema Initialization**:\n   - Start with an initial schema file (V1) that sets up the core database structure for the first milestone ([Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)).\n   - Use a migration tool (like FlywayDB or Liquibase) to manage schema changes in PostgreSQL and SQLite.\n\n2. **Incremental Schema Updates**:\n   - Create separate migration files for subsequent milestones, each defining changes required to move from one version to the next.\n   - Example:\n     - `V2__transcript_annotations.sql` for [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\n     - `V3__paper_editing.sql` for [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\n\n3. **Utilize DB Designer**:\n   - Leverage [DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) to visualize and export SQL code for creating and dropping tables, which can then be incorporated into the migration files.\n\n4. **Consistency Across Environments**:\n   - Use a consistent approach for schema management in both PostgreSQL (web app) and SQLite (Electron desktop version).\n   - Consider the compatibility modes of SQLite to ensure supported features match those in PostgreSQL.\n\n5. **Version Control and Documentation**:\n   - Keep the migration files in the version control system to track changes and facilitate collaboration.\n   - Document each migration file to explain changes and reason behind them for easier understanding and maintenance.\n\nThis approach ensures schema changes are systematic, incremental, and aligned with the project milestones. It also maintains compatibility and simplifies the process of deploying updates in varied environments (PostgreSQL for the web and SQLite for the desktop application).","GenTime":"2024-07-31 11:22:11"}
{"File Name":"digital-paper-edit-firebase\/2019-09-28-time-estimation-for-links.md","Context":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n\n## Decision\n","Decision":"* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","tokens":49,"id":5250,"Prediction":"## Decision\nTo include the time taken for voice overs in the estimation of the rough cut, we will implement a two-pronged approach:\n\n1. **Addition of Voice Over Time Field in UI**:\n   - **Description**: Introduce an additional user input field in the project management User Interface (UI) where voice over durations can be specified.\n   - **Justification**: This allows users to manually input the estimated time for voice overs, which directly feeds into the total rough cut estimation.\n   - **Ease of Implementation**: High, as it involves modifying the UI to add a simple input field and updating the backend to include this new field in total time calculations.\n   - **Accuracy**: Manual input ensures that specific requirements and unique durations for voice overs can be precisely accounted for.\n   - **Extensibility**: The UI and backend can be easily updated to accommodate additional fields or features related to voice over time in the future.\n\n2. **Automatic Voice Over Time Estimation**:\n   - **Description**: Implement an optional feature that estimates voice over time based on the script text length.\n   - **Justification**: Provides a more automated and potentially faster way to estimate voice over time, reducing manual effort and potential for human error.\n   - **Ease of Implementation**: Moderate, as it requires incorporating text analysis and estimation algorithms.\n   - **Accuracy**: We will start with a basic word-per-minute (WPM) estimate, allowing users to adjust the WPM rate according to their specific needs.\n   - **Extensibility**: This feature can be further enhanced with machine learning techniques to improve the accuracy of voice over time estimates based on historical data.\n\nBy combining both manual input and automated estimation, we provide a flexible and precise solution for including voice over time in rough cut estimations. This approach balances ease of implementation, accuracy, and extensibility, aligning with our decision drivers.\n\nOverall, these decisions aim to markedly improve the ease and accuracy of rough cut time estimations while ensuring the solution can evolve based on future user needs and technological advancements.","GenTime":"2024-07-31 11:22:29"}
{"File Name":"remultiform\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n## Decision\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":5255,"Prediction":"## Decision\nWe will adopt [Prettier](https:\/\/prettier.io\/) as the code formatter for all our JavaScript projects to ensure a consistent code style across the board. This decision is based on Prettier's widespread adoption, support for a wide range of languages within the JavaScript ecosystem, and its capability to enforce a unified code style. Additionally, we will integrate Prettier into our CI\/CD pipeline to automatically format code and prevent deviations from the agreed-upon style guidelines. This will help in maintaining code readability and reducing unnecessary diffs in code reviews.","GenTime":"2024-07-31 11:22:43"}
{"File Name":"remultiform\/0008-support-indexeddb-as-a-data-store.md","Context":"## Context\\nWe need to persist the user data to the backend. Rather than building our\\nfrontend to be tightly coupled to an API, we want a layer that can sit between\\nthe application and the API.\\nWe know that we will need to support offline use of these forms. We also want\\nusers to be able to recover unsubmitted forms if they haven't gone online\\nbetween sessions.\\nWe also know that some forms will require the upload of images.\\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\\ndatabase that persists data across sessions. It supports storing blob data (such\\nas images), and has decent storage limits.\\n\n## Decision\n","Decision":"We will support IndexedDB as a data store layer, sitting between the application\\nand the API.\\n","tokens":144,"id":5257,"Prediction":"## Decision\nWe will implement an intermediary data persistence layer in the form of a local IndexedDB in the frontend. This IndexedDB will serve as a temporary, offline storage solution to handle user data, including form inputs and image uploads. This approach aligns with our goals to decouple the frontend from direct API interactions, provide robust offline support, and ensure data persistence across user sessions until submission to the backend is feasible.\n\nHere's a summary of the key points for this decision:\n\n1. **IndexedDB Usage**: We will utilize IndexedDB for local storage due to its widespread browser support, capability to store complex data types (including blobs for images), and significant storage limits.\n\n2. **Data Handling**: All user inputs, including form data and images, will first be saved to IndexedDB. This ensures that users can continue to work and progress even without an active internet connection.\n\n3. **Synchronization**: When a network connection is available, the application will attempt to sync the locally stored data in IndexedDB with the backend API. This involves submitting the form data and images, and marking the entries in IndexedDB as successfully uploaded upon confirmation.\n\n4. **Recovery Mechanism**: If the application is shut down or closed before the data is submitted to the backend, the IndexedDB will retain the unsubmitted forms and images. Upon reopening, the users can resume their work from where they left off without data loss.\n\n5. **Error Handling**: Implement robust error handling to gracefully manage scenarios such as storage limits being reached, temporary unavailability of IndexedDB, or issues encountered during the sync process.\n\nThis architectural decision ensures the application is flexible, resilient to network interruptions, and provides a seamless user experience in both online and offline scenarios.","GenTime":"2024-07-31 11:22:53"}
{"File Name":"remultiform\/0003-use-rollup-to-build-distributables.md","Context":"## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n\n## Decision\n","Decision":"We will build distributables using Rollup.js.\\n","tokens":62,"id":5259,"Prediction":"## Decision\nWe have decided to use Rollup as our JavaScript bundler for this library. Rollup's support for TypeScript and straightforward configuration makes it an ideal choice for our needs. This will allow us to build and distribute the library in formats compatible with both TypeScript and plain JavaScript applications, including both CommonJS and ESModules.","GenTime":"2024-07-31 11:22:59"}
